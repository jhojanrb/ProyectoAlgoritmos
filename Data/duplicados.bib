@article{ref_dup0,
  title = {Making Noise: Using Sound-Art to Explore Technological Fluency}},
  author = {Brunvand, Erik and McCurdy, Nina}},
  year = {2017}},
  journal = {ACM Inroads}},
  tipo = {Article}},
  publisher = {Association for Computing Machinery}},
  abstract = {We describe our experience designing and delivering a general education technological fluency course that frames the discussion of computer science and engineering technology (electronics and programming) in the context of sound-art: art that uses sound as its medium. This course is aimed at undergraduate students from a wide variety of backgrounds and is designed to fit into the ``Intellectual Explorations'' area of a general undergraduate program. The goal is to introduce computer engineering and computational principles to non-CS students through an exploration of sound-art, experimental and electronic music, noise-making circuits, hardware hacking, and circuit bending.}},
  url = {https://doi.org/10.1145/3095781.3017714},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1,
  title = {Computing with CORGIS: Diverse, Real-world Datasets for Introductory Computing}},
  author = {Bart, Austin Cory and Whitcomb, Ryan and Kafura, Dennis and Shaffer, Clifford A. and Tilevich, Eli}},
  year = {2017}},
  journal = {ACM Inroads}},
  tipo = {Article}},
  publisher = {Association for Computing Machinery}},
  abstract = {To successfully bring introductory computing to non-CS majors, one needs to create a curriculum that will appeal to students from diverse disciplines. Several educational theories emphasize the need for introductory contexts that align with students' long-term goals and are perceived as useful. Data Science, using algorithms to manipulate real-world data and interpreting the results, has emerged as a field with cross-disciplinary value, and has strong potential as an appealing context for introductory computing courses. However, it is not easy to find, clean, and integrate datasets that will satisfy a broad variety of learners. The CORGIS project (https://think.cs.vt.edu/corgis) enables instructors to easily incorporate data science into their classroom. Specifically, it provides over 40 datasets in areas including history, politics, medicine, and education. Additionally, the CORGIS infrastructure supports the integration of new datasets with simple libraries for Java, Python, and Racket, thus empowering introductory students to write programs that manipulate real data. Finally, the CORGIS web-based tools allow learners to visualize and explore datasets without programming, enabling data science lessons on day one. We have incorporated CORGIS assignments into an introductory course for non-majors to study their impact on learners' motivation, with positive initial results. These results indicate that external adopters are likely to find the CORGIS tools and materials useful in their own pedagogical pursuits.}},
  url = {https://doi.org/10.1145/3095781.3017708},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup2,
  title = {The Computational Sprinting Game}},
  author = {Fan, Songchun and Zahedi, Seyed Majid and Lee, Benjamin C.}},
  year = {2016}},
  journal = {Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Computational sprinting is a class of mechanisms that boost performance but dissipate additional power. We describe a sprinting architecture in which many, independent chip multiprocessors share a power supply and sprints are constrained by the chips' thermal limits and the rack's power limits. Moreover, we present the computational sprinting game, a multi-agent perspective on managing sprints. Strategic agents decide whether to sprint based on application phases and system conditions. The game produces an equilibrium that improves task throughput for data analytics workloads by 4-6\texttimes{}},
  url = {https://doi.org/10.1145/2872362.2872383},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup3,
  title = {Computational higher-dimensional type theory}},
  author = {Angiuli, Carlo and Harper, Robert and Wilson, Todd}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Formal constructive type theory has proved to be an effective language for mechanized proof. By avoiding non-constructive principles, such as the law of the excluded middle, type theory admits sharper proofs and broader interpretations of results. From a computer science perspective, interest in type theory arises from its applications to programming languages. Standard constructive type theories used in mechanization admit computational interpretations based on meta-mathematical normalization theorems. These proofs are notoriously brittle; any change to the theory potentially invalidates its computational meaning. As a case in point, Voevodsky's univalence axiom raises questions about the computational meaning of proofs. We consider the question: Can higher-dimensional type theory be construed as a programming language? We answer this question affirmatively by providing a direct, deterministic operational interpretation for a representative higher-dimensional dependent type theory with higher inductive types and an instance of univalence. Rather than being a formal type theory defined by rules, it is instead a computational type theory in the sense of Martin-L\"{o}},
  url = {https://doi.org/10.1145/3009837.3009861},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup4,
  title = {Experience report: growing and shrinking polygons for random testing of computational geometry algorithms}},
  author = {Sergey, Ilya}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper documents our experience of adapting and using the QuickCheck-style approach for extensive randomised property-based testing of computational geometry algorithms. The need in rigorous evaluation of computational geometry procedures has naturally arisen in our quest of organising a medium-size programming contest for second year university students—an experiment we conducted as an attempt to introduce them to computational geometry. The main effort in organising the event was implementation of a solid infrastructure for testing and ranking solutions. For this, we employed functional programming techniques. The choice of the language and the paradigm made it possible for us to engineer, from scratch and in a very short period of time, a series of robust geometric primitives and algorithms, as well as implement a scalable framework for their randomised testing. We describe the main insights, enabling efficient random testing of geometric procedures, and report on our experience of using the testing framework, which helped us to detect and fix a number of issues not just in our programming artefacts, but also in the published algorithms we had implemented.}},
  url = {https://doi.org/10.1145/2951913.2951927},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup5,
  title = {Parallel thinking}},
  author = {Blelloch, Guy E.}},
  year = {2009}},
  journal = {Proceedings of the 14th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Assuming that the multicore revolution plays out the way the microprocessor industry expects, it seems that within a decade most programming will involve parallelism at some level. One needs to ask how this affects the the way we teach computer science, or even how we have people think about computation. With regards to teaching there seem to be three basic choices: (1) we only train a small number of experts in parallel computation who develop a collection of libraries, and everyone else just uses them; (2) we leave our core curriculum pretty much as is, but add some advanced courses on parallelism or perhaps tack on a few lectures at the end of existing courses; or (3) we start teaching parallelism from the start and embed it throughout the curriculum with the idea of getting students to think about parallelism as the most natural form of computation and sequential computation as a special case.This talk will examine some of the implications of the third option. It will argue that thinking about parallelism, when treated in an appropriate way, might be as easy or easier that thinking sequentially. A key prerequisite, however, is to identify what the core ideas in parallelism are and how they might be layered and integrated with existing concepts. Another more difficult issue is how to cleanly integrate these ideas among courses. After all much of the success of sequential computation follows from the concept of a random access machine and its ability to serve as a simple, albeit imperfect, interface between programming languages, algorithm analysis, and hardware design. The talk will go through an initial list of some core ideas in parallelism, and an approach to integrating these ideas between parallel algorithms, programming languages, and, to some extent, hardware. This requires, however, moving away from the concept of a machine model as a interface for thinking about computation.}},
  url = {https://doi.org/10.1145/1504176.1504177},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup6,
  title = {A type theory for incremental computational complexity with control flow changes}},
  author = {\c{C}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Incremental computation aims to speed up re-runs of a program after its inputs have been modified slightly. It works by recording a trace of the program's first run and propagating changes through the trace in incremental runs, trying to re-use as much of the original trace as possible. The recent work CostIt is a type and effect system to establish the time complexity of incremental runs of a program, as a function of input changes. However, CostIt is limited in two ways. First, it prohibits input changes that influence control flow. This makes it impossible to type programs that, for instance, branch on inputs that may change. Second, the soundness of CostIt is proved relative to an abstract cost semantics, but it is unclear how the semantics can be realized. In this paper, we address both these limitations. We present DuCostIt, a re-design of CostIt, that combines reasoning about costs of change propagation and costs of from-scratch evaluation. The latter lifts the restriction on control flow changes. To obtain the type system, we refine Flow Caml, a type system for information flow analysis, with cost effects. Additionally, we inherit from CostIt index refinements to track data structure sizes and a co-monadic type. Using a combination of binary and unary step-indexed logical relations, we prove DuCostIt's cost analysis sound relative to not only an abstract cost semantics, but also a concrete semantics, which is obtained by translation to an ML-like language.}},
  url = {https://doi.org/10.1145/2951913.2951950},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup7,
  title = {Functional geometry and the Trait\'{e}},
  author = {Mairson, Harry George}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We describe a functional programming approach to the design of outlines of eighteenth-century string instruments. The approach is based on the research described in Fran\c{c}},
  url = {https://doi.org/10.1145/2500365.2500617},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup8,
  title = {Linear types for computational effects}},
  author = {Simpson, Alex}},
  year = {2009}},
  journal = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {I shall present an extension of Moggi's computational metalanguage with primitives from linear logic, the enriched effect-calculus. Illustrative applications to side effects, continuations, nondeterminism and polymorphism will be considered. The talk is based on joint work with Jeff Egger and Rasmus Mogelberg.}},
  url = {https://doi.org/10.1145/1480881.1480919},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup9,
  title = {Computational sprinting on a hardware/software testbed}},
  author = {Raghavan, Arun and Emurian, Laurel and Shao, Lei and Papaefthymiou, Marios and Pipe, Kevin P. and Wenisch, Thomas F. and Martin, Milo M.K.}},
  year = {2013}},
  journal = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {CMOS scaling trends have led to an inflection point where thermal constraints (especially in mobile devices that employ only passive cooling) preclude sustained operation of all transistors on a chip --- a phenomenon called "dark silicon." Recent research proposed computational sprinting --- exceeding sustainable thermal limits for short intervals --- to improve responsiveness in light of the bursty computation demands of many media-rich interactive mobile applications. Computational sprinting improves responsiveness by activating reserve cores (parallel sprinting) and/or boosting frequency/voltage (frequency sprinting) to power levels that far exceed the system's sustainable cooling capabilities, relying on thermal capacitance to buffer heat.Prior work analyzed the feasibility of sprinting through modeling and simulation. In this work, we investigate sprinting using a hardware/software testbed. First, we study unabridged sprints, wherein the computation completes before temperature becomes critical, demonstrating a 6.3x responsiveness gain, and a 6\% energy efficiency improvement by racing to idle. We then analyze truncated sprints, wherein our software runtime system must intervene to prevent overheating by throttling parallelism and frequency before the computation is complete. To avoid oversubscription penalties (context switching inefficiencies after a truncated parallel sprint), we develop a sprint-aware task-based parallel runtime. We find that maximal-intensity sprinting is not always best, introduce the concept of sprint pacing, and evaluate an adaptive policy for selecting sprint intensity. We report initial results using a phase change heat sink to extend maximum sprint duration. Finally, we demonstrate that a sprint-and-rest operating regime can actually outperform thermally-limited sustained execution.}},
  url = {https://doi.org/10.1145/2451116.2451135},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup10,
  title = {Disengaged scheduling for fair, protected access to fast computational accelerators}},
  author = {Menychtas, Konstantinos and Shen, Kai and Scott, Michael L.}},
  year = {2014}},
  journal = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Today's operating systems treat GPUs and other computational accelerators as if they were simple devices, with bounded and predictable response times. With accelerators assuming an increasing share of the workload on modern machines, this strategy is already problematic, and likely to become untenable soon. If the operating system is to enforce fair sharing of the machine, it must assume responsibility for accelerator scheduling and resource management.Fair, safe scheduling is a particular challenge on fast accelerators, which allow applications to avoid kernel-crossing overhead by interacting directly with the device. We propose a disengaged scheduling strategy in which the kernel intercedes between applications and the accelerator on an infrequent basis, to monitor their use of accelerator cycles and to determine which applications should be granted access over the next time interval.Our strategy assumes a well defined, narrow interface exported by the accelerator. We build upon such an interface, systematically inferred for the latest Nvidia GPUs. We construct several example schedulers, including Disengaged Timeslice with overuse control that guarantees fairness and Disengaged Fair Queueing that is effective in limiting resource idleness, but probabilistic. Both schedulers ensure fair sharing of the GPU, even among uncooperative or adversarial applications; Disengaged Fair Queueing incurs a 4\% overhead on average (max 18\%) compared to direct device access across our evaluation scenarios.}},
  url = {https://doi.org/10.1145/2541940.2541963},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup11,
  title = {Think like a vertex, behave like a function! a functional DSL for vertex-centric big graph processing}},
  author = {Emoto, Kento and Matsuzaki, Kiminori and Hu, Zhenjiang and Morihata, Akimasa and Iwasaki, Hideya}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The vertex-centric programming model, known as “think like a vertex”, is being used more and more to support various big graph processing methods through iterative supersteps that execute in parallel a user-defined vertex program over each vertex of a graph. However, the imperative and message-passing style of existing systems makes defining a vertex program unintuitive. In this paper, we show that one can benefit more from “Thinking like a vertex” by “Behaving like a function” rather than “Acting like a procedure” with full use of side effects and explicit control of message passing, state, and termination. We propose a functional approach to vertex-centric graph processing in which the computation at every vertex is abstracted as a higher-order function and present Fregel, a new domain-specific language. Fregel has clear functional semantics, supports declarative description of vertex computation, and can be automatically translated into Pregel, an emerging imperative-style distributed graph processing framework, and thereby achieve promising performance. Experimental results for several typical examples show the promise of this functional approach.}},
  url = {https://doi.org/10.1145/2951913.2951938},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup12,
  title = {On the computational soundness of cryptographically masked flows}},
  author = {Laud, Peeter}},
  year = {2008}},
  journal = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {To speak about the security of information flow in programs employing cryptographic operations, definitions based on computational indistinguish ability of distributions over program states have to be used. These definitions, as well as the accompanying analysis tools, are complex and error-prone to argue about. Cryptographically masked flows, proposed by Askarov, Hedin and Sabelfeld, are an abstract execution model and security definition that attempt to abstract away the details of computational security. This abstract model is useful because analysis of programs can be conducted using the usual techniques for enforcing non-interference.In this paper we investigate under which conditions this abstract model is computationally sound, i.e. when does the security of a program in their model imply the computational security of this program. This paper spells out a reasonable set of conditions and then proposes a simpler abstract model that is nevertheless no more restrictive than the cryptographically masked flows together with these conditions for soundness.}},
  url = {https://doi.org/10.1145/1328438.1328479},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup13,
  title = {Mostly-automated verification of low-level programs in computational separation logic}},
  author = {Chlipala, Adam}},
  year = {2011}},
  journal = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Several recent projects have shown the feasibility of verifying low-level systems software. Verifications based on automated theorem-proving have omitted reasoning about first-class code pointers, which is critical for tasks like certifying implementations of threads and processes. Conversely, verifications that deal with first-class code pointers have featured long, complex, manual proofs. In this paper, we introduce the Bedrock framework, which supports mostly-automated proofs about programs with the full range of features needed to implement, e.g., language runtime systems.The heart of our approach is in mostly-automated discharge of verification conditions inspired by separation logic. Our take on separation logic is computational, in the sense that function specifications are usually written in terms of reference implementations in a purely functional language. Logical quantifiers are the most challenging feature for most automated verifiers; by relying on functional programs (written in the expressive language of the Coq proof assistant), we are able to avoid quantifiers almost entirely. This leads to some dramatic improvements compared to both past work in classical verification, which we compare against with implementations of data structures like binary search trees and hash tables; and past work in verified programming with code pointers, which we compare against with examples like function memoization and a cooperative threading library.}},
  url = {https://doi.org/10.1145/1993498.1993526},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup14,
  title = {Enhancing computation-to-core assignment with physical location information}},
  author = {Kislal, Orhan and Kotra, Jagadish and Tang, Xulong and Kandemir, Mahmut Taylan and Jung, Myoungsoo}},
  year = {2018}},
  journal = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Going beyond a certain number of cores in modern architectures requires an on-chip network more scalable than conventional buses. However, employing an on-chip network in a manycore system (to improve scalability) makes the latencies of the data accesses issued by a core non-uniform. This non-uniformity can play a significant role in shaping the overall application performance. This work presents a novel compiler strategy which involves exposing architecture information to the compiler to enable an optimized computation-to-core mapping. Specifically, we propose a compiler-guided scheme that takes into account the relative positions of (and distances between) cores, last-level caches (LLCs) and memory controllers (MCs) in a manycore system, and generates a mapping of computations to cores with the goal of minimizing the on-chip network traffic. The experimental data collected using a set of 21 multi-threaded applications reveal that, on an average, our approach reduces the on-chip network latency in a 6\texttimes{}},
  url = {https://doi.org/10.1145/3192366.3192386},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup15,
  title = {Experience report: Haskell in computational biology}},
  author = {Daniels, Noah M. and Gallant, Andrew and Ramsey, Norman}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Haskell gives computational biologists the flexibility and rapid prototyping of a scripting language, plus the performance of native code. In our experience, higher-order functions, lazy evaluation, and monads really worked, but profiling and debugging presented obstacles. Also, Haskell libraries vary greatly: memoization combinators and parallel-evaluation strategies helped us a lot, but other, nameless libraries mostly got in our way. Despite the obstacles and the uncertain quality of some libraries, Haskell's ecosystem made it easy for us to develop new algorithms in computational biology.}},
  url = {https://doi.org/10.1145/2364527.2364560},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup16,
  title = {Regular expression containment: coinductive axiomatization and computational interpretation}},
  author = {Henglein, Fritz and Nielsen, Lasse}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a new sound and complete axiomatization of regular expression containment. It consists of the conventional axiomatization of concatenation, alternation, empty set and (the singleton set containing) the empty string as an idempotent semiring, the fixed- point rule E* = 1 + E \texttimes{}},
  url = {https://doi.org/10.1145/1926385.1926429},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup17,
  title = {Intersection types and computational effects}},
  author = {Davies, Rowan and Pfenning, Frank}},
  year = {2000}},
  journal = {Proceedings of the Fifth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We show that standard formulations of intersection type systems are unsound in the presence of computational effects, and propose a solution similar to the value restriction for polymorphism adopted in the revised definition of Standard ML. It differs in that it is not tied to let-expressions and requires an additional weakening of the usual subtyping rules. We also present a bi-directional type-checking algorithm for the resulting language that does not require an excessive amount of type annotations and illustrate it through some examples. We further show that the type assignment system can be extended to incorporate parametric polymorphism. Taken together, we see our system and associated type-checking algorithm as a significant step towards the introduction of intersection types into realistic programming languages. The added expressive power would allow many more properties of programs to be stated by the programmer and statically verified by a compiler.}},
  url = {https://doi.org/10.1145/351240.351259},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup18,
  title = {Incremental computation with names}},
  author = {Hammer, Matthew A. and Dunfield, Jana and Headley, Kyle and Labich, Nicholas and Foster, Jeffrey S. and Hicks, Michael and Van Horn, David}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Over the past thirty years, there has been significant progress in developing general-purpose, language-based approaches to incremental computation, which aims to efficiently update the result of a computation when an input is changed. A key design challenge in such approaches is how to provide efficient incremental support for a broad range of programs. In this paper, we argue that first-class names are a critical linguistic feature for efficient incremental computation. Names identify computations to be reused across differing runs of a program, and making them first class gives programmers a high level of control over reuse. We demonstrate the benefits of names by presenting Nominal Adapton, an ML-like language for incremental computation with names. We describe how to use Nominal Adapton to efficiently incrementalize several standard programming patterns---including maps, folds, and unfolds---and show how to build efficient, incremental probabilistic trees and tries. Since Nominal Adapton's implementation is subtle, we formalize it as a core calculus and prove it is from-scratch consistent, meaning it always produces the same answer as simply re-running the computation. Finally, we demonstrate that Nominal Adapton can provide large speedups over both from-scratch computation and Adapton, a previous state-of-the-art incremental computation system.}},
  url = {https://doi.org/10.1145/2814270.2814305},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup19,
  title = {Using JavaScript and WebCL for numerical computations: a comparative study of native and web technologies}},
  author = {Khan, Faiz and Foley-Bourgon, Vincent and Kathrotia, Sujay and Lavoie, Erick and Hendren, Laurie}},
  year = {2014}},
  journal = {Proceedings of the 10th ACM Symposium on Dynamic Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {From its modest beginnings as a tool to validate forms, JavaScript is now an industrial-strength language used to power online applications such as spreadsheets, IDEs, image editors and even 3D games. Since all modern web browsers support JavaScript, it provides a medium that is both easy to distribute for developers and easy to access for users. This paper provides empirical data to answer the question: Is JavaScript fast enough for numerical computations? By measuring and comparing the runtime performance of benchmarks representative of a wide variety of scientific applications, we show that sequential JavaScript is within a factor of 2 of native code. Parallel code using WebCL shows speed improvements of up to 2.28 over JavaScript for the majority of the benchmarks.}},
  url = {https://doi.org/10.1145/2661088.2661090},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup20,
  title = {Chisel: reliability- and accuracy-aware optimization of approximate computational kernels}},
  author = {Misailovic, Sasa and Carbin, Michael and Achour, Sara and Qi, Zichao and Rinard, Martin C.}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The accuracy of an approximate computation is the distance between the result that the computation produces and the corresponding fully accurate result. The reliability of the computation is the probability that it will produce an acceptably accurate result. Emerging approximate hardware platforms provide approximate operations that, in return for reduced energy consumption and/or increased performance, exhibit reduced reliability and/or accuracy. We present Chisel, a system for reliability- and accuracy-aware optimization of approximate computational kernels that run on approximate hardware platforms. Given a combined reliability and/or accuracy specification, Chisel automatically selects approximate kernel operations to synthesize an approximate computation that minimizes energy consumption while satisfying its reliability and accuracy specification. We evaluate Chisel on five applications from the image processing, scientific computing, and financial analysis domains. The experimental results show that our implemented optimization algorithm enables Chisel to optimize our set of benchmark kernels to obtain energy savings from 8.7\% to 19.8\% compared to the fully reliable kernel implementations while preserving important reliability guarantees.}},
  url = {https://doi.org/10.1145/2660193.2660231},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup21,
  title = {Scalable and efficient implementation of 3d unstructured meshes computation: a case study on matrix assembly}},
  author = {Th\'{e}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Exposing massive parallelism on 3D unstructured meshes computation with efficient load balancing and minimal synchronizations is challenging. Current approaches relying on domain decomposition and mesh coloring struggle to scale with the increasing number of cores per nodes, especially with new many-core processors. In this paper, we propose an hybrid approach using domain decomposition to exploit distributed memory parallelism, Divide-and-Conquer, D&amp;C, to exploit shared memory parallelism and improve locality, and mesh coloring at core level to exploit vectors. It illustrates a new trade-off for many-cores between structuredness, memory locality, and vectorization. We evaluate our approach on the finite element matrix assembly of an industrial fluid dynamic code developed by Dassault Aviation. We compare our D&amp;C approach to domain decomposition and to mesh coloring. D&amp;C achieves a high parallel efficiency, a good data locality as well as an improved bandwidth usage. It competes on current nodes with the optimized pure MPI version with a minimum 10\% speed-up. D&amp;C shows an impressive 319x strong scaling on 512 cores (32 nodes) with only 2000 vertices per core. Finally, the Intel Xeon Phi version has a performance similar to 10 Intel E5-2665 Xeon Sandy Bridge cores and 95\% parallel efficiency on the 60 physical cores. Running on 4 Xeon Phi (240 cores), D&amp;C has 92\% efficiency on the physical cores and performance similar to 33 Intel E5-2665 Xeon Sandy Bridge cores.}},
  url = {https://doi.org/10.1145/2688500.2688517},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup22,
  title = {A distributed OpenCL framework using redundant computation and data replication}},
  author = {Kim, Junghyun and Jo, Gangwon and Jung, Jaehoon and Kim, Jungwon and Lee, Jaejin}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Applications written solely in OpenCL or CUDA cannot execute on a cluster as a whole. Most previous approaches that extend these programming models to clusters are based on a common idea: designating a centralized host node and coordinating the other nodes with the host for computation. However, the centralized host node is a serious performance bottleneck when the number of nodes is large. In this paper, we propose a scalable and distributed OpenCL framework called SnuCL-D for large-scale clusters. SnuCL-D's remote device virtualization provides an OpenCL application with an illusion that all compute devices in a cluster are confined in a single node. To reduce the amount of control-message and data communication between nodes, SnuCL-D replicates the OpenCL host program execution and data in each node. We also propose a new OpenCL host API function and a queueing optimization technique that significantly reduce the overhead incurred by the previous centralized approaches. To show the effectiveness of SnuCL-D, we evaluate SnuCL-D with a microbenchmark and eleven benchmark applications on a large-scale CPU cluster and a medium-scale GPU cluster.}},
  url = {https://doi.org/10.1145/2908080.2908094},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup23,
  title = {Distributed memory code generation for mixed Irregular/Regular computations}},
  author = {Ravishankar, Mahesh and Dathathri, Roshan and Elango, Venmugil and Pouchet, Louis-No\"{e}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many applications feature a mix of irregular and regular computational structures. For example, codes using adaptive mesh refinement (AMR) typically use a collection of regular blocks, where the number of blocks and the relationship between blocks is irregular. The computational structure in such applications generally involves regular (affine) loop computations within some number of innermost loops, while outer loops exhibit irregularity due to data-dependent control flow and indirect array access patterns. Prior approaches to distributed memory parallelization do not handle such computations effectively. They either target loop nests that are completely affine using polyhedral frameworks, or treat all loops as irregular. Consequently, the generated distributed memory code contains artifacts that disrupt the regular nature of previously affine innermost loops of the computation. This hampers subsequent optimizations to improve on-node performance. We propose a code generation framework that can effectively transform such applications for execution on distributed memory systems. Our approach generates distributed memory code which preserves program properties that enable subsequent polyhederal optimizations. Simultaneously, it addresses a major memory bottleneck of prior techniques that limits the scalability of the generated code. The effectiveness of the proposed framework is demonstrated on computations that are mixed regular/irregular, completely regular, and completely irregular.}},
  url = {https://doi.org/10.1145/2688500.2688515},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup24,
  title = {Hardware support for fine-grained event-driven computation in Anton 2}},
  author = {Grossman, J. P. and Kuskin, Jeffrey S. and Bank, Joseph A. and Theobald, Michael and Dror, Ron O. and Ierardi, Douglas J. and Larson, Richard H. and Schafer, U. Ben and Towles, Brian and Young, Cliff and Shaw, David E.}},
  year = {2013}},
  journal = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Exploiting parallelism to accelerate a computation typically involves dividing it into many small tasks that can be assigned to different processing elements. An efficient execution schedule for these tasks can be difficult or impossible to determine in advance, however, if there is uncertainty as to when each task's input data will be available. Ideally, each task would run in direct response to the arrival of its input data, thus allowing the computation to proceed in a fine-grained event-driven manner. Realizing this ideal is difficult in practice, and typically requires sacrificing flexibility for performance.In Anton 2, a massively parallel special-purpose supercomputer for molecular dynamics simulations, we addressed this challenge by including a hardware block, called the dispatch unit, that provides flexible and efficient support for fine-grained event-driven computation. Its novel features include a many-to-many mapping from input data to a set of synchronization counters, and the ability to prioritize tasks based on their type. To solve the additional problem of using a fixed set of synchronization counters to track input data for a potentially large number of tasks, we created a software library that allows programmers to treat Anton 2 as an idealized machine with infinitely many synchronization counters. The dispatch unit, together with this library, made it possible to simplify our molecular dynamics software by expressing it as a collection of independent tasks, and the resulting fine-grained execution schedule improved overall performance by up to 16\% relative to a coarse-grained schedule for precisely the same computation.}},
  url = {https://doi.org/10.1145/2451116.2451175},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup25,
  title = {Functional programming for dynamic and large data with self-adjusting computation}},
  author = {Chen, Yan and Acar, Umut A. and Tangwongsan, Kanat}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Combining type theory, language design, and empirical work, we present techniques for computing with large and dynamically changing datasets. Based on lambda calculus, our techniques are suitable for expressing a diverse set of algorithms on large datasets and, via self-adjusting computation, enable computations to respond automatically to changes in their data. To improve the scalability of self-adjusting computation, we present a type system for precise dependency tracking that minimizes the time and space for storing dependency metadata. The type system eliminates an important assumption of prior work that can lead to recording spurious dependencies. We present a type-directed translation algorithm that generates correct self-adjusting programs without relying on this assumption. We then show a probabilistic-chunking technique to further decrease space usage by controlling the fundamental space-time tradeoff in self-adjusting computation. We implement and evaluate these techniques, showing promising results on challenging benchmarks involving large graphs.}},
  url = {https://doi.org/10.1145/2628136.2628150},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup26,
  title = {SYNC or ASYNC: time to fuse for distributed graph-parallel computation}},
  author = {Xie, Chenning and Chen, Rong and Guan, Haibing and Zang, Binyu and Chen, Haibo}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Large-scale graph-structured computation usually exhibits iterative and convergence-oriented computing nature, where input data is computed iteratively until a convergence condition is reached. Such features have led to the development of two different computation modes for graph-structured programs, namely synchronous (Sync) and asynchronous (Async) modes. Unfortunately, there is currently no in-depth study on their execution properties and thus programmers have to manually choose a mode, either requiring a deep understanding of underlying graph engines, or suffering from suboptimal performance. This paper makes the first comprehensive characterization on the performance of the two modes on a set of typical graph-parallel applications. Our study shows that the performance of the two modes varies significantly with different graph algorithms, partitioning methods, execution stages, input graphs and cluster scales, and no single mode consistently outperforms the other. To this end, this paper proposes Hsync, a hybrid graph computation mode that adaptively switches a graph-parallel program between the two modes for optimal performance. Hsync constantly collects execution statistics on-the-fly and leverages a set of heuristics to predict future performance and determine when a mode switch could be profitable. We have built online sampling and offline profiling approaches combined with a set of heuristics to accurately predicting future performance in the two modes. A prototype called PowerSwitch has been built based on PowerGraph, a state-of-the-art distributed graph-parallel system, to support adaptive execution of graph algorithms. On a 48-node EC2-like cluster, PowerSwitch consistently outperforms the best of both modes, with a speedup ranging from 9\% to 73\% due to timely switch between two modes.}},
  url = {https://doi.org/10.1145/2688500.2688508},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup27,
  title = {Responsive parallel computation: bridging competitive and cooperative threading}},
  author = {Muller, Stefan K. and Acar, Umut A. and Harper, Robert}},
  year = {2017}},
  journal = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Competitive and cooperative threading are widely used abstractions in computing. In competitive threading, threads are scheduled preemptively with the goal of minimizing response time, usually of interactive applications. In cooperative threading, threads are scheduled non-preemptively with the goal of maximizing throughput or minimizing the completion time, usually in compute-intensive applications, e.g. scientific computing, machine learning and AI. Although both of these forms of threading rely on the same abstraction of a thread, they have, to date, remained largely separate forms of computing. Motivated by the recent increase in the mainstream use of multicore computers, we propose a threading model that aims to unify competitive and cooperative threading. To this end, we extend the classic graph-based cost model for cooperative threading to allow for competitive threading, and describe how such a cost model may be used in a programming language by presenting a language and a corresponding cost semantics. Finally, we show that the cost model and the semantics are realizable by presenting an operational semantics for the language that specifies the behavior of an implementation, as well as an implementation and a small empirical evaluation.}},
  url = {https://doi.org/10.1145/3062341.3062370},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup28,
  title = {A laboratory for teaching object oriented thinking}},
  author = {Beck, K. and Cunningham, W.}},
  year = {1989}},
  journal = {Conference Proceedings on Object-Oriented Programming Systems, Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {It is difficult to introduce both novice and experienced procedural programmers to the anthropomorphic perspective necessary for object-oriented design. We introduce CRC cards, which characterize objects by class name, responsibilities, and collaborators, as a way of giving learners a direct experience of objects. We have found this approach successful in teaching novice programmers the concepts of objects, and in introducing experienced programmers to complicated existing designs.}},
  url = {https://doi.org/10.1145/74877.74879},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup29,
  title = {Lightweight computation tree tracing for lazy functional languages}},
  author = {Faddegon, Maarten and Chitil, Olaf}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A computation tree of a program execution describes computations of functions and their dependencies. A computation tree describes how a program works and is at the heart of algorithmic debugging. To generate a computation tree, existing algorithmic debuggers either use a complex implementation or yield a less informative approximation. We present a method for lazy functional languages that requires only a simple tracing library to generate a detailed computation tree. With our algorithmic debugger a programmer can debug any Haskell program by only importing our library and annotating suspected functions.}},
  url = {https://doi.org/10.1145/2908080.2908104},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup30,
  title = {Dag-calculus: a calculus for parallel computation}},
  author = {Acar, Umut A. and Chargu\'{e}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Increasing availability of multicore systems has led to greater focus on the design and implementation of languages for writing parallel programs. Such languages support various abstractions for parallelism, such as fork-join, async-finish, futures. While they may seem similar, these abstractions lead to different semantics, language design and implementation decisions, and can significantly impact the performance of end-user applications. In this paper, we consider the question of whether it would be possible to unify various paradigms of parallel computing. To this end, we propose a calculus, called dag calculus, that can encode fork-join, async-finish, and futures, and possibly others. We describe dag calculus and its semantics, establish translations from the aforementioned paradigms into dag calculus. These translations establish that dag calculus is sufficiently powerful for encoding programs written in prevailing paradigms of parallelism. We present concurrent algorithms and data structures for realizing dag calculus on multicore hardware and prove that the proposed techniques are consistent with the semantics. Finally, we present an implementation of the calculus and evaluate it empirically by comparing its performance to highly optimized code from prior work. The results show that the calculus is expressive and that it competes well with, and sometimes outperforms, the state of the art.}},
  url = {https://doi.org/10.1145/2951913.2951946},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup31,
  title = {Canonicity for 2-dimensional type theory}},
  author = {Licata, Daniel R. and Harper, Robert}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Higher-dimensional dependent type theory enriches conventional one-dimensional dependent type theory with additional structure expressing equivalence of elements of a type. This structure may be employed in a variety of ways to capture rather coarse identifications of elements, such as a universe of sets considered modulo isomorphism. Equivalence must be respected by all families of types and terms, as witnessed computationally by a type-generic program. Higher-dimensional type theory has applications to code reuse for dependently typed programming, and to the formalization of mathematics. In this paper, we develop a novel judgemental formulation of a two-dimensional type theory, which enjoys a canonicity property: a closed term of boolean type is definitionally equal to true or false. Canonicity is a necessary condition for a computational interpretation of type theory as a programming language, and does not hold for existing axiomatic presentations of higher-dimensional type theory. The method of proof is a generalization of the NuPRL semantics, interpreting types as syntactic groupoids rather than equivalence relations.}},
  url = {https://doi.org/10.1145/2103656.2103697},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup32,
  title = {Symbolic computation of differential equivalences}},
  author = {Cardelli, Luca and Tribastone, Mirco and Tschaikowski, Max and Vandin, Andrea}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Ordinary differential equations (ODEs) are widespread in many natural sciences including chemistry, ecology, and systems biology, and in disciplines such as control theory and electrical engineering. Building on the celebrated molecules-as-processes paradigm, they have become increasingly popular in computer science, with high-level languages and formal methods such as Petri nets, process algebra, and rule-based systems that are interpreted as ODEs. We consider the problem of comparing and minimizing ODEs automatically. Influenced by traditional approaches in the theory of programming, we propose differential equivalence relations. We study them for a basic intermediate language, for which we have decidability results, that can be targeted by a class of high-level specifications. An ODE implicitly represents an uncountable state space, hence reasoning techniques cannot be borrowed from established domains such as probabilistic programs with finite-state Markov chain semantics. We provide novel symbolic procedures to check an equivalence and compute the largest one via partition refinement algorithms that use satisfiability modulo theories. We illustrate the generality of our framework by showing that differential equivalences include (i) well-known notions for the minimization of continuous-time Markov chains (lumpability), (ii)~bisimulations for chemical reaction networks recently proposed by Cardelli et al., and (iii) behavioral relations for process algebra with ODE semantics. With a prototype implementation we are able to detect equivalences in biochemical models from the literature that cannot be reduced using competing automatic techniques.}},
  url = {https://doi.org/10.1145/2837614.2837649},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup33,
  title = {Verified lifting of stencil computations}},
  author = {Kamil, Shoaib and Cheung, Alvin and Itzhaky, Shachar and Solar-Lezama, Armando}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper demonstrates a novel combination of program synthesis and verification to lift stencil computations from low-level Fortran code to a high-level summary expressed using a predicate language. The technique is sound and mostly automated, and leverages counter-example guided inductive synthesis (CEGIS) to find provably correct translations. Lifting existing code to a high-performance description language has a number of benefits, including maintainability and performance portability. For example, our experiments show that the lifted summaries can enable domain specific compilers to do a better job of parallelization as compared to an off-the-shelf compiler working on the original code, and can even support fully automatic migration to hardware accelerators such as GPUs. We have implemented verified lifting in a system called STNG and have evaluated it using microbenchmarks, mini-apps, and real-world applications. We demonstrate the benefits of verified lifting by first automatically summarizing Fortran source code into a high-level predicate language, and subsequently translating the lifted summaries into Halide, with the translated code achieving median performance speedups of 4.1X and up to 24X for non-trivial stencils as compared to the original implementation.}},
  url = {https://doi.org/10.1145/2908080.2908117},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup34,
  title = {Approximate computation with outlier detection in Topaz}},
  author = {Achour, Sara and Rinard, Martin C.}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present Topaz, a new task-based language for computations that execute on approximate computing platforms that may occasionally produce arbitrarily inaccurate results. Topaz maps tasks onto the approximate hardware and integrates the generated results into the main computation. To prevent unacceptably inaccurate task results from corrupting the main computation, Topaz deploys a novel outlier detection mechanism that recognizes and precisely reexecutes outlier tasks. Outlier detection enables Topaz to work effectively with approximate hardware platforms that have complex fault characteristics, including platforms with bit pattern dependent faults (in which the presence of faults may depend on values stored in adjacent memory cells). Our experimental results show that, for our set of benchmark applications, outlier detection enables Topaz to deliver acceptably accurate results (less than 1\% error) on our target approximate hardware platforms. Depending on the application and the hardware platform, the overall energy savings range from 5 to 13 percent. Without outlier detection, only one of the applications produces acceptably accurate results.}},
  url = {https://doi.org/10.1145/2814270.2814314},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup35,
  title = {Trustworthy numerical computation in Scala}},
  author = {Darulova, Eva and Kuncak, Viktor}},
  year = {2011}},
  journal = {Proceedings of the 2011 ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Modern computing has adopted the floating point type as a default way to describe computations with real numbers. Thanks to dedicated hardware support, such computations are efficient on modern architectures, even in double precision. However, rigorous reasoning about the resulting programs remains difficult. This is in part due to a large gap between the finite floating point representation and the infinite-precision real-number semantics that serves as the developers' mental model. Because programming languages do not provide support for estimating errors, some computations in practice are performed more and some less precisely than needed.We present a library solution for rigorous arithmetic computation. Our numerical data type library tracks a (double) floating point value, but also a guaranteed upper bound on the error between this value and the ideal value that would be computed in the real-value semantics. Our implementation involves a set of linear approximations based on an extension of affine arithmetic. The derived approximations cover most of the standard mathematical operations, including trigonometric functions, and are more comprehensive than any publicly available ones. Moreover, while interval arithmetic rapidly yields overly pessimistic estimates, our approach remains precise for several computational tasks of interest. We evaluate the library on a number of examples from numerical analysis and physical simulations. We found it to be a useful tool for gaining confidence in the correctness of the computation.}},
  url = {https://doi.org/10.1145/2048066.2048094},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup36,
  title = {Implicit self-adjusting computation for purely functional programs}},
  author = {Chen, Yan and Dunfield, Jana and Hammer, Matthew A. and Acar, Umut A.}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Computational problems that involve dynamic data, such as physics simulations and program development environments, have been an important subject of study in programming languages. Building on this work, recent advances in self-adjusting computation have developed techniques that enable programs to respond automatically and efficiently to dynamic changes in their inputs. Self-adjusting programs have been shown to be efficient for a reasonably broad range of problems but the approach still requires an explicit programming style, where the programmer must use specific monadic types and primitives to identify, create and operate on data that can change over time.We describe techniques for automatically translating purely functional programs into self-adjusting programs. In this implicit approach, the programmer need only annotate the (top-level) input types of the programs to be translated. Type inference finds all other types, and a type-directed translation rewrites the source program into an explicitly self-adjusting target program. The type system is related to information-flow type systems and enjoys decidable type inference via constraint solving. We prove that the translation outputs well-typed self-adjusting programs and preserves the source program's input-output behavior, guaranteeing that translated programs respond correctly to all changes to their data. Using a cost semantics, we also prove that the translation preserves the asymptotic complexity of the source program.}},
  url = {https://doi.org/10.1145/2034773.2034792},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup37,
  title = {Compiler Management of Communication and Parallelism for Quantum Computation}},
  author = {Heckey, Jeff and Patil, Shruti and JavadiAbhari, Ali and Holmes, Adam and Kudrow, Daniel and Brown, Kenneth R. and Franklin, Diana and Chong, Frederic T. and Martonosi, Margaret}},
  year = {2015}},
  journal = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Quantum computing (QC) offers huge promise to accelerate a range of computationally intensive benchmarks. Quantum computing is limited, however, by the challenges of decoherence: i.e., a quantum state can only be maintained for short windows of time before it decoheres. While quantum error correction codes can protect against decoherence, fast execution time is the best defense against decoherence, so efficient architectures and effective scheduling algorithms are necessary. This paper proposes the Multi-SIMD QC architecture and then proposes and evaluates effective schedulers to map benchmark descriptions onto Multi-SIMD architectures. The Multi-SIMD model consists of a small number of SIMD regions, each of which may support operations on up to thousands of qubits per cycle.Efficient Multi-SIMD operation requires efficient scheduling. This work develops schedulers to reduce communication requirements of qubits between operating regions, while also improving parallelism.We find that communication to global memory is a dominant cost in QC. We also note that many quantum benchmarks have long serial operation paths (although each operation may be data parallel). To exploit this characteristic, we introduce Longest-Path-First Scheduling (LPFS) which pins operations to SIMD regions to keep data in-place and reduce communication to memory. The use of small, local scratchpad memories also further reduces communication. Our results show a 3\% to 308\% improvement for LPFS over conventional scheduling algorithms, and an additional 3\% to 64\% improvement using scratchpad memories. Our work is the most comprehensive software-to-quantum toolflow published to date, with efficient and practical scheduling techniques that reduce communication and increase parallelism for full-scale quantum code executing up to a trillion quantum gate operations.}},
  url = {https://doi.org/10.1145/2694344.2694357},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup38,
  title = {Efficient lookup-table protocol in secure multiparty computation}},
  author = {Launchbury, John and Diatchki, Iavor S. and DuBuisson, Thomas and Adams-Moran, Andy}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Secure multiparty computation (SMC) permits a collection of parties to compute a collaborative result, without any of the parties gaining any knowledge about the inputs provided by other parties. Specifications for SMC are commonly presented as boolean circuits, where optimizations come mostly from reducing the number of multiply-operations (including and-gates) - these are the operations which incur significant cost, either in computation overhead or in communication between the parties. Instead, we take a language-oriented approach, and consequently are able to explore many other kinds of optimizations. We present an efficient and general purpose SMC table-lookup algorithm that can serve as a direct alternative to circuits. Looking up a private (i.e. shared, or encrypted) n-bit argument in a public table requires log(n) parallel-and operations. We use the advanced encryption standard algorithm (AES) as a driving motivation, and by introducing different kinds of parallelization techniques, produce the fastest current SMC implementation of AES, improving the best previously reported results by well over an order of magnitude.}},
  url = {https://doi.org/10.1145/2364527.2364556},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup39,
  title = {Efficient deadlock avoidance for streaming computation with filtering}},
  author = {Buhler, Jeremy D. and Agrawal, Kunal and Li, Peng and Chamberlain, Roger D.}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Parallel streaming computations have been studied extensively, and many languages, libraries, and systems have been designed to support this model of computation. In particular, we consider acyclic streaming computations in which individual nodes can choose to filter, or discard, some of their inputs in a data-dependent manner. In these applications, if the channels between nodes have finite buffers, the computation can deadlock. One method of deadlock avoidance is to augment the data streams between nodes with occasional dummy messages; however, for general DAG topologies, no polynomial time algorithm is known to compute the intervals at which dummy messages must be sent to avoid deadlock.In this paper, we show that deadlock avoidance for streaming computations with filtering can be performed efficiently for a large class of DAG topologies. We first present a new method where each dummy message is tagged with a destination, so as to reduce the number of dummy messages sent over the network. We then give efficient algorithms for dummy interval computation in series-parallel DAGs. We finally generalize our results to a larger graph family, which we call the CS4 DAGs, in which every undirected Cycle is Single-Source and Single-Sink (CS4). Our results show that, for a large set of application topologies that are both intuitively useful and formalizable, the streaming model with filtering can be implemented safely with reasonable overhead.}},
  url = {https://doi.org/10.1145/2145816.2145846},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup40,
  title = {Efficiently speeding up sequential computation through the n-way programming model}},
  author = {Cledat, Romain E. and Kumar, Tushar and Pande, Santosh}},
  year = {2011}},
  journal = {Proceedings of the 2011 ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {With core counts on the rise, the sequential components of applications are becoming the major bottleneck in performance scaling as predicted by Amdahl's law. We are therefore faced with the simultaneous problems of occupying an increasing number of cores and speeding up sequential sections. In this work, we reconcile these two seemingly incompatible problems with a novel programming model called N-way. The core idea behind N-way is to benefit from the algorithmic diversity available to express certain key computational steps. By simultaneously launching in parallel multiple ways to solve a given computation, a runtime can just-in-time pick the best (for example the fastest) way and therefore achieve speedup.Previous work has demonstrated the benefits of such an approach but has not addressed its inherent waste. In this work, we focus on providing a mathematically sound learning-based statistical model that can be used by a runtime to determine the optimal balance between resources used and benefits obtainable through N-way. We further describe a dynamic culling mechanism to further reduce resource waste.We present abstractions and a runtime support to cleanly encapsulate the computational-options and monitor their progress. We demonstrate a low-overhead runtime that achieves significant speedup over a range of widely used kernels. Our results demonstrate super-linear speedups in certain cases.}},
  url = {https://doi.org/10.1145/2048066.2048109},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup41,
  title = {Refactoring android Java code for on-demand computation offloading}},
  author = {Zhang, Ying and Huang, Gang and Liu, Xuanzhe and Zhang, Wei and Mei, Hong and Yang, Shunxiang}},
  year = {2012}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Computation offloading is a promising way to improve the performance as well as reducing the battery power consumption of a smartphone application by executing some parts of the application on a remote server. Supporting such capability is not easy for smartphone application developers due to (1) correctness: some code, e.g., that for GPS, gravity, and other sensors, can run only on the smartphone so that developers have to identify which parts of the application cannot be offloaded; (2) effectiveness: the reduced execution time must be greater than the network delay caused by computation offloading so that developers need to calculate which parts are worth offloading; (3) adaptability: smartphone applications often face changes of user requirements and runtime environments so that developers need to implement the adaptation on offloading. More importantly, considering the large number of today's smartphone applications, solutions applicable for legacy applications will be much more valuable. In this paper, we present a tool, named DPartner, that automatically refactors Android applications to be the ones with computation offloading capability. For a given Android application, DPartner first analyzes its bytecode for discovering the parts worth offloading, then rewrites the bytecode to implement a special program structure supporting on-demand offloading, and finally generates two artifacts to be deployed onto an Android phone and the server, respectively. We evaluated DPartner on three real-world Android applications, demonstrating the reduction of execution time by 46\%-97\% and battery power consumption by 27\%-83\%.}},
  url = {https://doi.org/10.1145/2384616.2384634},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup42,
  title = {The HdpH DSLs for scalable reliable computation}},
  author = {Maier, Patrick and Stewart, Robert and Trinder, Phil}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM SIGPLAN Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The statelessness of functional computations facilitates both parallelism and fault recovery. Faults and non-uniform communication topologies are key challenges for emergent large scale parallel architectures. We report on HdpH and HdpH-RS, a pair of Haskell DSLs designed to address these challenges for irregular task-parallel computations on large distributed-memory architectures. Both DSLs share an API combining explicit task placement with sophisticated work stealing. HdpH focuses on scalability by making placement and stealing topology aware whereas HdpH-RS delivers reliability by means of fault tolerant work stealing.We present operational semantics for both DSLs and investigate conditions for semantic equivalence of HdpH and HdpH-RS programs, that is, conditions under which topology awareness can be transparently traded for fault tolerance. We detail how the DSL implementations realise topology awareness and fault tolerance. We report an initial evaluation of scalability and fault tolerance on a 256-core cluster and on up to 32K cores of an HPC platform.}},
  url = {https://doi.org/10.1145/2633357.2633363},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup43,
  title = {SPEED: precise and efficient static estimation of program computational complexity}},
  author = {Gulwani, Sumit and Mehra, Krishna K. and Chilimbi, Trishul}},
  year = {2009}},
  journal = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper describes an inter-procedural technique for computing symbolic bounds on the number of statements a procedure executes in terms of its scalar inputs and user-defined quantitative functions of input data-structures. Such computational complexity bounds for even simple programs are usually disjunctive, non-linear, and involve numerical properties of heaps. We address the challenges of generating these bounds using two novel ideas.We introduce a proof methodology based on multiple counter instrumentation (each counter can be initialized and incremented at potentially multiple program locations) that allows a given linear invariant generation tool to compute linear bounds individually on these counter variables. The bounds on these counters are then composed together to generate total bounds that are non-linear and disjunctive. We also give an algorithm for automating this proof methodology. Our algorithm generates complexity bounds that are usually precise not only in terms of the computational complexity, but also in terms of the constant factors.Next, we introduce the notion of user-defined quantitative functions that can be associated with abstract data-structures, e.g., length of a list, height of a tree, etc. We show how to compute bounds in terms of these quantitative functions using a linear invariant generation tool that has support for handling uninterpreted functions. We show application of this methodology to commonly used data-structures (namely lists, list of lists, trees, bit-vectors) using examples from Microsoft product code. We observe that a few quantitative functions for each data-structure are usually sufficient to allow generation of symbolic complexity bounds of a variety of loops that iterate over these data-structures, and that it is straightforward to define these quantitative functions.The combination of these techniques enables generation of precise computational complexity bounds for real-world examples (drawn from Microsoft product code and C++ STL library code) for some of which it is non-trivial to even prove termination. Such automatically generated bounds are very useful for early detection of egregious performance problems in large modular codebases that are constantly being changed by multiple developers who make heavy use of code written by others without a good understanding of their implementation complexity.}},
  url = {https://doi.org/10.1145/1480881.1480898},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup44,
  title = {Concepts and experiments in computational reflection}},
  author = {Maes, Pattie}},
  year = {1987}},
  journal = {Conference Proceedings on Object-Oriented Programming Systems, Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper brings some perspective to various concepts in computational reflection. A definition of computational reflection is presented, the importance of computational reflection is discussed and the architecture of languages that support reflection is studied. Further, this paper presents a survey of some experiments in reflection which have been performed. Examples of existing procedural, logic-based and rule-based languages with an architecture for reflection are briefly presented. The main part of the paper describes an original experiment to introduce a reflective architecture in an object-oriented language. It stresses the contributions of this language to the field of object-oriented programming and illustrates the new programming style made possible. The examples show that a lot of programming problems that were previously handled on an ad hoc basis, can in a reflective architecture be solved more elegantly.}},
  url = {https://doi.org/10.1145/38765.38821},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup45,
  title = {Designing and auto-tuning parallel 3-D FFT for computation-communication overlap}},
  author = {Song, Sukhyun and Hollingsworth, Jeffrey K.}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents a method to design and auto-tune a new parallel 3-D FFT code using the non-blocking MPI all-to-all operation. We achieve high performance by optimizing computation-communication overlap. Our code performs fully asynchronous communication without any support from special hardware. We also improve cache performance through loop tiling. To cope with the complex trade-off regarding our optimization techniques, we parameterize our code and auto-tune the parameters efficiently in a large parameter space. Experimental results from two systems confirm that our code achieves a speedup of up to 1.76x over the FFTW library.}},
  url = {https://doi.org/10.1145/2555243.2555249},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup46,
  title = {Towards nominal computation}},
  author = {Bojanczyk, Mikolaj and Braud, Laurent and Klin, Bartek and Lasota, Slawomir}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Nominal sets are a different kind of set theory, with a more relaxed notion of finiteness. They offer an elegant formalism for describing lambda-terms modulo alpha-conversion, or automata on data words. This paper is an attempt at defining computation in nominal sets. We present a rudimentary programming language, called Nlambda. The key idea is that it includes a native type for finite sets in the nominal sense. To illustrate the power of our language, we write short programs that process automata on data words.}},
  url = {https://doi.org/10.1145/2103656.2103704},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup47,
  title = {Randomized accuracy-aware program transformations for efficient approximate computations}},
  author = {Zhu, Zeyuan Allen and Misailovic, Sasa and Kelner, Jonathan A. and Rinard, Martin}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Despite the fact that approximate computations have come to dominate many areas of computer science, the field of program transformations has focused almost exclusively on traditional semantics-preserving transformations that do not attempt to exploit the opportunity, available in many computations, to acceptably trade off accuracy for benefits such as increased performance and reduced resource consumption.We present a model of computation for approximate computations and an algorithm for optimizing these computations. The algorithm works with two classes of transformations: substitution transformations (which select one of a number of available implementations for a given function, with each implementation offering a different combination of accuracy and resource consumption) and sampling transformations (which randomly discard some of the inputs to a given reduction). The algorithm produces a (1+ε) randomized approximation to the optimal randomized computation (which minimizes resource consumption subject to a probabilistic accuracy specification in the form of a maximum expected error or maximum error variance).}},
  url = {https://doi.org/10.1145/2103656.2103710},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup48,
  title = {Towards verifiable resource accounting for outsourced computation}},
  author = {Chen, Chen and Maniatis, Petros and Perrig, Adrian and Vasudevan, Amit and Sekar, Vyas}},
  year = {2013}},
  journal = {Proceedings of the 9th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Outsourced computation services should ideally only charge customers for the resources used by their applications. Unfortunately, no verifiable basis for service providers and customers to reconcile resource accounting exists today. This leads to undesirable outcomes for both providers and consumers-providers cannot prove to customers that they really devoted the resources charged, and customers cannot verify that their invoice maps to their actual usage. As a result, many practical and theoretical attacks exist, aimed at charging customers for resources that their applications did not consume. Moreover, providers cannot charge consumers precisely, which causes them to bear the cost of unaccounted resources or pass these costs inefficiently to their customers.We introduce ALIBI, a first step toward a vision for verifiable resource accounting. ALIBI places a minimal, trusted reference monitor underneath the service provider's software platform. This monitor observes resource allocation to customers' guest virtual machines and reports those observations to customers, for verifiable reconciliation. In this paper, we show that ALIBI efficiently and verifiably tracks guests' memory use and CPU-cycle consumption.}},
  url = {https://doi.org/10.1145/2451512.2451546},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup49,
  title = {What exactly is inexact computation good for?}},
  author = {Palem, Krishna}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Our willingness to deliberately trade accuracy of computing systems for significant resource savings, notably energy consumption, got a boost from two directions. First, energy (or power, the more popularly used measure) consumption started emerging as a serious hurdle to our ability to continue scaling the complexity of processors, and thus enable ever richer computing applications. This "energy hurdle" spanned the gamut from large data-centers to portable embedded computing systems. Second, many believed that an engine of growth that supported scaling, captured by Gordon Moore's remarkable prophecy (Moore's law), was headed towards an irrevocable cliff edge - when this happens, our ability to produce computing systems whose hardware would support precise or exact computing would diminish greatly. In this talk which emphasizes the physical and hardware layers of abstraction where all of these troubles start (after all energy is rooted in thermodynamics), I will first review reasons that compelled and encouraged us to consider trading accuracy for energy savings deliberately resulting in inexact computing.}},
  url = {https://doi.org/10.1145/2594291.2604001},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup50,
  title = {Scalable parallel minimum spanning forest computation}},
  author = {Nobari, Sadegh and Cao, Thanh-Tung and Karras, Panagiotis and Bressan, St\'{e}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The proliferation of data in graph form calls for the development of scalable graph algorithms that exploit parallel processing environments. One such problem is the computation of a graph's minimum spanning forest (MSF). Past research has proposed several parallel algorithms for this problem, yet none of them scales to large, high-density graphs. In this paper we propose a novel, scalable, parallel MSF algorithm for undirected weighted graphs. Our algorithm leverages Prim's algorithm in a parallel fashion, concurrently expanding several subsets of the computed MSF. Our effort focuses on minimizing the communication among different processors without constraining the local growth of a processor's computed subtree. In effect, we achieve a scalability that previous approaches lacked. We implement our algorithm in CUDA, running on a GPU and study its performance using real and synthetic, sparse as well as dense, structured and unstructured graph data. Our experimental study demonstrates that our algorithm outperforms the previous state-of-the-art GPU-based MSF algorithm, while being several orders of magnitude faster than sequential CPU-based algorithms.}},
  url = {https://doi.org/10.1145/2145816.2145842},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup51,
  title = {Lazygraph: lazy data coherency for replicas in distributed graph-parallel computation}},
  author = {Wang, Lei and Zhuang, Liangji and Chen, Junhang and Cui, Huimin and Lv, Fang and Liu, Ying and Feng, Xiaobing}},
  year = {2018}},
  journal = {Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Replicas 1 of a vertex play an important role in existing distributed graph processing systems which make a single vertex to be parallel processed by multiple machines and access remote neighbors locally without any remote access. However, replicas of vertices introduce data coherency problem. Existing distributed graph systems treat replicas of a vertex v as an atomic and indivisible vertex, and use an eager data coherency approach to guarantee replicas atomicity. In eager data coherency approach, any changes to vertex data must be immediately communicated to all replicas of v, thus leading to frequent global synchronizations and communications.In this paper, we propose a lazy data coherency approach, called LazyAsync, which treats replicas of a vertex as independent vertices and maintains the data coherency by computations, rather than communications in existing eager approach. Our approach automatically selects some data coherency points from the graph algorithm, and maintains all replicas to share the same global view only at such points, which means the replicas are enabled to maintain different local views between any two adjacent data coherency points. Based on PowerGraph, we develop a distributed graph processing system LazyGraph to implement the LazyAsync approach and exploit graph-aware optimizations. On a 48-node EC2-like cluster, LazyGraph outperforms PowerGraph on four widely used graph algorithms across a variety of real-world graphs, with a speedup ranging from 1.25x to 10.69x.}},
  url = {https://doi.org/10.1145/3178487.3178508},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup52,
  title = {Traceable data types for self-adjusting computation}},
  author = {Acar, Umut A. and Blelloch, Guy and Ley-Wild, Ruy and Tangwongsan, Kanat and Turkoglu, Duru}},
  year = {2010}},
  journal = {Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Self-adjusting computation provides an evaluation model where computations can respond automatically to modifications to their data by using a mechanism for propagating modifications through the computation. Current approaches to self-adjusting computation guarantee correctness by recording dependencies in a trace at the granularity of individual memory operations. Tracing at the granularity of memory operations, however, has some limitations: it can be asymptotically inefficient (eg, compared to optimal solutions) because it cannot take advantage of problem-specific structure, it requires keeping a large computation trace (often proportional to the runtime of the program on the current input), and it introduces moderately large constant factors in practice.In this paper, we extend dependence-tracing to work at the granularity of the query and update operations of arbitrary (abstract) data types, instead of just reads and writes on memory cells. This can significantly reduce the number of dependencies that need to be kept in the trace and followed during an update. We define an interface for supporting a traceable version of a data type, which reports the earliest query that depends on (is changed by) revising operations back in time, and implement several such structures, including priority queues, queues, dictionaries, and counters. We develop a semantics for tracing, extend an existing self-adjusting language, ΔML, and its implementation to support traceable data types, and present an experimental evaluation by considering a number of benchmarks. Our experiments show dramatic improvements on space and time, sometimes by as much as two orders of magnitude.}},
  url = {https://doi.org/10.1145/1806596.1806650},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup53,
  title = {Space profiling for parallel functional programs}},
  author = {Spoonhower, Daniel and Blelloch, Guy E. and Harper, Robert and Gibbons, Phillip B.}},
  year = {2008}},
  journal = {Proceedings of the 13th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents a semantic space profiler for parallel functional programs. Building on previous work in sequential profiling, our tools help programmers to relate runtime resource use back to program source code. Unlike many profiling tools, our profiler is based on a cost semantics. This provides a means to reason about performance without requiring a detailed understanding of the compiler or runtime system. It also provides a specification for language implementers. This is critical in that it enables us to separate cleanly the performance of the application from that of the language implementation.Some aspects of the implementation can have significant effects on performance. Our cost semantics enables programmers to understand the impact of different scheduling policies while hiding many of the details of their implementations. We show applications where the choice of scheduling policy has asymptotic effects on space use. We explain these use patterns through a demonstration of our tools. We also validate our methodology by observing similar performance in our implementation of a parallel extension of Standard ML.}},
  url = {https://doi.org/10.1145/1411204.1411240},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup54,
  title = {Imperative self-adjusting computation}},
  author = {Acar, Umut A. and Ahmed, Amal and Blume, Matthias}},
  year = {2008}},
  journal = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Self-adjusting computation enables writing programs that can automatically and efficiently respond to changes to their data (e.g., inputs). The idea behind the approach is to store all data that can change over time in modifiable references and to let computations construct traces that can drive change propagation. After changes have occurred, change propagation updates the result of the computation by re-evaluating only those expressions that depend on the changed data. Previous approaches to self-adjusting computation require that modifiable references be written at most once during execution---this makes the model applicable only in a purely functional setting.In this paper, we present techniques for imperative self-adjusting computation where modifiable references can be written multiple times. We define a language SAIL (Self-Adjusting Imperative Language) and prove consistency, i.e., that change propagation and from-scratch execution are observationally equivalent. Since SAIL programs are imperative, they can create cyclic data structures. To prove equivalence in the presence of cycles in the store, we formulate and use an untyped, step-indexed logical relation, where step indices are used to ensure well-foundedness. We show that SAIL accepts an asymptotically efficient implementation by presenting algorithms and data structures for its implementation. When the number of operations (reads and writes) per modifiable is bounded by a constant, we show that change propagation becomes as efficient as in the non-imperative case. The general case incurs a slowdown that is logarithmic in the maximum number of such operations. We describe a prototype implementation of SAIL as a Standard ML library.}},
  url = {https://doi.org/10.1145/1328438.1328476},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup55,
  title = {An experimental analysis of self-adjusting computation}},
  author = {Acar, Umut A. and Blelloch, Guy E. and Blume, Matthias and Tangwongsan, Kanat}},
  year = {2006}},
  journal = {Proceedings of the 27th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Dependence graphs and memoization can be used to efficiently update the output of a program as the input changes dynamically. Recent work has studied techniques for combining these approaches to effectively dynamize a wide range of applications. Toward this end various theoretical results were given. In this paper we describe the implementation of a library based on these ideas, and present experimental results on the efficiency of this library on a variety of applications. The results of the experiments indicate that the approach is effective in practice, often requiring orders of magnitude less time than recomputing the output from scratch. We believe this is the first experimental evidence that incremental computation of any type is effective in practice for a reasonably broad set of applications.}},
  url = {https://doi.org/10.1145/1133981.1133993},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup56,
  title = {Generative type abstraction and type-level computation}},
  author = {Weirich, Stephanie and Vytiniotis, Dimitrios and Peyton Jones, Simon and Zdancewic, Steve}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Modular languages support generative type abstraction, ensuring that an abstract type is distinct from its representation, except inside the implementation where the two are synonymous. We show that this well-established feature is in tension with the non-parametric features of newer type systems, such as indexed type families and GADTs. In this paper we solve the problem by using kinds to distinguish between parametric and non-parametric contexts. The result is directly applicable to Haskell, which is rapidly developing support for type-level computation, but the same issues should arise whenever generativity and non-parametric features are combined.}},
  url = {https://doi.org/10.1145/1926385.1926411},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup57,
  title = {A cost semantics for self-adjusting computation}},
  author = {Ley-Wild, Ruy and Acar, Umut A. and Fluet, Matthew}},
  year = {2009}},
  journal = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Self-adjusting computation is an evaluation model in which programs can respond efficiently to small changes to their input data by using a change-propagation mechanism that updates computation by re-building only the parts affected by changes. Previous work has proposed language techniques for self-adjusting computation and showed the approach to be effective in a number of application areas. However, due to the complex semantics of change propagation and the indirect nature of previously proposed language techniques, it remains difficult to reason about the efficiency of self-adjusting programs and change propagation.In this paper, we propose a cost semantics for self-adjusting computation that enables reasoning about its effectiveness. As our source language, we consider a direct-style λ-calculus with first-class mutable references and develop a notion of trace distance for source programs. To facilitate asymptotic analysis, we propose techniques for composing and generalizing concrete distances via trace contexts (traces with holes). We then show how to translate the source language into a self-adjusting target language such that the translation (1) preserves the extensional semantics of the source programs and the cost of from-scratch runs, and (2) ensures that change propagation between two evaluations takes time bounded by their relative distance. We consider several examples and analyze their effectiveness by considering upper and lower bounds.}},
  url = {https://doi.org/10.1145/1480881.1480907},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup58,
  title = {A universe of binding and computation}},
  author = {Licata, Daniel R. and Harper, Robert}},
  year = {2009}},
  journal = {Proceedings of the 14th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We construct a logical framework supporting datatypes that mix binding and computation, implemented as a universe in the dependently typed programming language Agda 2. We represent binding pronominally, using well-scoped de Bruijn indices, so that types can be used to reason about the scoping of variables. We equip our universe with datatype-generic implementations of weakening, substitution, exchange, contraction, and subordination-based strengthening, so that programmers need not reimplement these operations for each individual language they define. In our mixed, pronominal setting, weakening and substitution hold only under some conditions on types, but we show that these conditions can be discharged automatically in many cases. Finally, we program a variety of standard difficult test cases from the literature, such as normalization-by-evaluation for the untyped lambda-calculus, demonstrating that we can express detailed invariants about variable usage in a program's type while still writing clean and clear code.}},
  url = {https://doi.org/10.1145/1596550.1596571},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup59,
  title = {Effective communication and computation overlap with hybrid MPI/SMPSs}},
  author = {Marjanovic, Vladimir and Labarta, Jes\'{u}},
  year = {2010}},
  journal = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Communication overhead is one of the dominant factors affecting performance in high-performance computing systems. To reduce the negative impact of communication, programmers overlap communication and computation by using asynchronous communication primitives. This increases code complexity, requiring more development effort and making less readable programs. This paper presents the hybrid use of MPI and SMPSs (SMP superscalar, a task-based shared-memory programming model) that allows the programmer to easily introduce the asynchrony necessary to overlap communication and computation. We demonstrate the hybrid use of MPI/SMPSs with the high-performance LINPACK benchmark (HPL), and compare it to the pure MPI implementation, which uses the look-ahead technique to overlap communication and computation. The hybrid MPI/SMPSs version significantly improves the performance of the pure MPI version, getting close to the asymptotic performance at medium problem sizes and still getting significant benefits at small/large problem sizes.}},
  url = {https://doi.org/10.1145/1693453.1693502},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup60,
  title = {iThreads: A Threading Library for Parallel Incremental Computation}},
  author = {Bhatotia, Pramod and Fonseca, Pedro and Acar, Umut A. and Brandenburg, Bj\"{o}},
  year = {2015}},
  journal = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Incremental computation strives for efficient successive runs of applications by re-executing only those parts of the computation that are affected by a given input change instead of recomputing everything from scratch. To realize these benefits automatically, we describe iThreads, a threading library for parallel incremental computation. iThreads supports unmodified shared-memory multithreaded programs: it can be used as a replacement for pthreads by a simple exchange of dynamically linked libraries, without even recompiling the application code. To enable such an interface, we designed algorithms and an implementation to operate at the compiled binary code level by leveraging MMU-assisted memory access tracking and process-based thread isolation. Our evaluation on a multicore platform using applications from the PARSEC and Phoenix benchmarks and two case-studies shows significant performance gains.}},
  url = {https://doi.org/10.1145/2694344.2694371},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup61,
  title = {Potluck: Cross-Application Approximate Deduplication for Computation-Intensive Mobile Applications}},
  author = {Guo, Peizhen and Hu, Wenjun}},
  year = {2018}},
  journal = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Emerging mobile applications, such as cognitive assistance and augmented reality (AR) based gaming, are increasingly computation-intensive and latency-sensitive, while running on resource-constrained devices. The standard approaches to addressing these involve either offloading to a cloud(let) or local system optimizations to speed up the computation, often trading off computation quality for low latency. Instead, we observe that these applications often operate on similar input data from the camera feed and share common processing components, both within the same (type of) applications and across different ones. Therefore, deduplicating processing across applications could deliver the best of both worlds. In this paper, we present Potluck, to achieve approximate deduplication. At the core of the system is a cache service that stores and shares processing results between applications and a set of algorithms to process the input data to maximize deduplication opportunities. This is implemented as a background service on Android. Extensive evaluation shows that Potluck can reduce the processing latency for our AR and vision workloads by a factor of 2.5 to 10.}},
  url = {https://doi.org/10.1145/3173162.3173185},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup62,
  title = {Adapton: composable, demand-driven incremental computation}},
  author = {Hammer, Matthew A. and Phang, Khoo Yit and Hicks, Michael and Foster, Jeffrey S.}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many researchers have proposed programming languages that support incremental computation (IC), which allows programs to be efficiently re-executed after a small change to the input. However, existing implementations of such languages have two important drawbacks. First, recomputation is oblivious to specific demands on the program output; that is, if a program input changes, all dependencies will be recomputed, even if an observer no longer requires certain outputs. Second, programs are made incremental as a unit, with little or no support for reusing results outside of their original context, e.g., when reordered.To address these problems, we present λiccdd, a core calculus that applies a demand-driven semantics to incremental computation, tracking changes in a hierarchical fashion in a novel demanded computation graph. λiccdd also formalizes an explicit separation between inner, incremental computations and outer observers. This combination ensures λiccdd programs only recompute computations as demanded by observers, and allows inner computations to be reused more liberally. We present Adapton, an OCaml library implementing λiccdd. We evaluated Adapton on a range of benchmarks, and found that it provides reliable speedups, and in many cases dramatically outperforms state-of-the-art IC approaches.}},
  url = {https://doi.org/10.1145/2594291.2594324},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup63,
  title = {LogGPS: a parallel computational model for synchronization analysis}},
  author = {Ino, Fumihiko and Fujimoto, Noriyuki and Hagihara, Kenichi}},
  year = {2001}},
  journal = {Proceedings of the Eighth ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a new parallel computational model, named LogGPS, which captures synchronization.The LogGPS model is an extension of the LogGP model, which abstracts communication on parallel platforms. Although the LogGP model captures long messages with one bandwidth parameter (G), it does not capture synchronization that is needed before sending a long message by high-level communication libraries. Our model has one additional parameter, S, defined as the threshold for message length, above which synchronous messages are sent.We also present some experimental results using both models. The results include (1) a verification of the LogGPS model, (2) an example of synchronization analysis using an MPI program and (3) a comparison of the models. The results indicate that the LogGPS model is more accurate than the LogGP model, and analyzing synchronization costs is important when improving parallel program performance.}},
  url = {https://doi.org/10.1145/379539.379592},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup64,
  title = {Cache topology aware computation mapping for multicores}},
  author = {Kandemir, Mahmut and Yemliha, Taylan and Muralidhara, SaiPrashanth and Srikantaiah, Shekhar and Irwin, Mary Jane and Zhnag, Yuanrui}},
  year = {2010}},
  journal = {Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The main contribution of this paper is a compiler based, cache topology aware code optimization scheme for emerging multicore systems. This scheme distributes the iterations of a loop to be executed in parallel across the cores of a target multicore machine and schedules the iterations assigned to each core. Our goal is to improve the utilization of the on-chip multi-layer cache hierarchy and to maximize overall application performance. We evaluate our cache topology aware approach using a set of twelve applications and three different commercial multicore machines. In addition, to study some of our experimental parameters in detail and to explore future multicore machines (with higher core counts and deeper on-chip cache hierarchies), we also conduct a simulation based study. The results collected from our experiments with three Intel multicore machines show that the proposed compiler-based approach is very effective in enhancing performance. In addition, our simulation results indicate that optimizing for the on-chip cache hierarchy will be even more important in future multicores with increasing numbers of cores and cache levels.}},
  url = {https://doi.org/10.1145/1806596.1806605},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup65,
  title = {The strength of non-size increasing computation}},
  author = {Hofmann, Martin}},
  year = {2002}},
  journal = {Proceedings of the 29th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We study the expressive power of non-size increasing recursive definitions over lists. This notion of computation is such that the size of all intermediate results will automatically be bounded by the size of the input so that the interpretation in a finite model is sound with respect to the standard semantics. Many well-known algorithms with this property such as the usual sorting algorithms are definable in the system in the natural way. The main result is that a characteristic function is definable if and only if it is computable in time O(2p(n)) for some polynomial p.The method used to establish the lower bound on the expressive power also shows that the complexity becomes polynomial time if we allow primitive recursion only. This settles an open question posed in [1, 7].The key tool for establishing upper bounds on the complexity of derivable functions is an interpretation in a finite relational model whose correctness with respect to the standard interpretation is shown using a semantic technique.}},
  url = {https://doi.org/10.1145/503272.503297},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup66,
  title = {Mementos: system support for long-running computation on RFID-scale devices}},
  author = {Ransford, Benjamin and Sorber, Jacob and Fu, Kevin}},
  year = {2011}},
  journal = {Proceedings of the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Transiently powered computing devices such as RFID tags, kinetic energy harvesters, and smart cards typically rely on programs that complete a task under tight time constraints before energy starvation leads to complete loss of volatile memory. Mementos is a software system that transforms general-purpose programs into interruptible computations that are protected from frequent power losses by automatic, energy-aware state checkpointing. Mementos comprises a collection of optimization passes for the LLVM compiler infrastructure and a linkable library that exercises hardware support for energy measurement while managing state checkpoints stored in nonvolatile memory. We evaluate Mementos against diverse test cases in a trace-driven simulator of transiently powered RFID-scale devices. Although Mementos's energy checks increase run time when energy is plentiful, they allow Mementos to safely suspend execution when energy dwindles, effectively spreading computation across zero or more power failures. This paper's contributions are: a study of the runtime environment for programs on RFID-scale devices; an energy-aware state checkpointing system for these devices that is implemented for the MSP430 family of microcontrollers; and a trace-driven simulator of transiently powered RFID-scale devices.}},
  url = {https://doi.org/10.1145/1950365.1950386},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup67,
  title = {Spatial computation}},
  author = {Budiu, Mihai and Venkataramani, Girish and Chelcea, Tiberiu and Goldstein, Seth Copen}},
  year = {2004}},
  journal = {Proceedings of the 11th International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper describes a computer architecture, Spatial Computation (SC), which is based on the translation of high-level language programs directly into hardware structures. SC program implementations are completely distributed, with no centralized control. SC circuits are optimized for wires at the expense of computation units.In this paper we investigate a particular implementation of SC: ASH (Application-Specific Hardware). Under the assumption that computation is cheaper than communication, ASH replicates computation units to simplify interconnect, building a system which uses very simple, completely dedicated communication channels. As a consequence, communication on the datapath never requires arbitration; the only arbitration required is for accessing memory. ASH relies on very simple hardware primitives, using no associative structures, no multiported register files, no scheduling logic, no broadcast, and no clocks. As a consequence, ASH hardware is fast and extremely power efficient.In this work we demonstrate three features of ASH: (1) that such architectures can be built by automatic compilation of C programs; (2) that distributed computation is in some respects fundamentally different from monolithic superscalar processors; and (3) that ASIC implementations of ASH use three orders of magnitude less energy compared to high-end superscalar processors, while being on average only 33\% slower in performance (3.5x worst-case).}},
  url = {https://doi.org/10.1145/1024393.1024396},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup68,
  title = {IncBricks: Toward In-Network Computation with an In-Network Cache}},
  author = {Liu, Ming and Luo, Liang and Nelson, Jacob and Ceze, Luis and Krishnamurthy, Arvind and Atreya, Kishore}},
  year = {2017}},
  journal = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The emergence of programmable network devices and the increasing data traffic of datacenters motivate the idea of in-network computation. By offloading compute operations onto intermediate networking devices (e.g., switches, network accelerators, middleboxes), one can (1) serve network requests on the fly with low latency; (2) reduce datacenter traffic and mitigate network congestion; and (3) save energy by running servers in a low-power mode. However, since (1) existing switch technology doesn't provide general computing capabilities, and (2) commodity datacenter networks are complex (e.g., hierarchical fat-tree topologies, multipath communication), enabling in-network computation inside a datacenter is challenging.In this paper, as a step towards in-network computing, we present IncBricks, an in-network caching fabric with basic computing primitives. IncBricks is a hardware-software co-designed system that supports caching in the network using a programmable network middlebox. As a key-value store accelerator, our prototype lowers request latency by over 30\% and doubles throughput for 1024 byte values in a common cluster configuration. Our results demonstrate the effectiveness of in-network computing and that efficient datacenter network request processing is possible if we carefully split the computation across the different programmable computing elements in a datacenter, including programmable switches, network accelerators, and end hosts.}},
  url = {https://doi.org/10.1145/3037697.3037731},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup69,
  title = {KickStarter: Fast and Accurate Computations on Streaming Graphs via Trimmed Approximations}},
  author = {Vora, Keval and Gupta, Rajiv and Xu, Guoqing}},
  year = {2017}},
  journal = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Continuous processing of a streaming graph maintains an approximate result of the iterative computation on a recent version of the graph. Upon a user query, the accurate result on the current graph can be quickly computed by feeding the approximate results to the iterative computation --- a form of incremental computation that corrects the (small amount of) error in the approximate result. Despite the effectiveness of this approach in processing growing graphs, it is generally not applicable when edge deletions are present --- existing approximations can lead to either incorrect results (e.g., monotonic computations terminate at an incorrect minima/maxima) or poor performance (e.g., with approximations, convergence takes longer than performing the computation from scratch).This paper presents KickStarter, a runtime technique that can trim the approximate values for a subset of vertices impacted by the deleted edges. The trimmed approximation is both safe and profitable, enabling the computation to produce correct results and converge quickly. KickStarter works for a class of monotonic graph algorithms and can be readily incorporated in any existing streaming graph system. Our experiments with four streaming algorithms on five large graphs demonstrate that trimming not only produces correct results but also accelerates these algorithms by 8.5--23.7x.}},
  url = {https://doi.org/10.1145/3037697.3037748},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup70,
  title = {Architectural support for cilk computations on many-core architectures}},
  author = {Long, Guoping and Fan, Dongrui and Zhang, Junchao}},
  year = {2009}},
  journal = {Proceedings of the 14th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {},
  url = {https://doi.org/10.1145/1504176.1504217},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup71,
  title = {Decision problems in computational models}},
  author = {Paterson, Michael S.}},
  year = {1972}},
  journal = {Proceedings of ACM Conference on Proving Assertions about Programs}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Some of the assertions about programs which we might be interested in proving are concerned with correctness, equivalence, accessibility of subroutines and guarantees of termination. We should like to develop techniques for determining such properties efficiently and intelligently wherever possible. Though theory tells us that for a realistic programming language almost any interesting property of the behaviour is effectively undecidable, this situation may not be intolerable in practice. An unsolvability result just gives us warning that we may not be able to solve all of the problems we are presented with, and that some of the ones we can solve will be very hard.In such circumstances it is very reasonable to try and determine necessary or sufficient conditions on programs for our techniques to be assured of success; however, in this paper we shall discuss a more qualitative, indirect, approach. We consider a range of more or less simplified computer models, chosen judiciously to exemplify some particular feature or features of computation. A demonstration of unsolvability in such a model reveals more accurately those sources which can contribute to unsolvability in a more complicated structure. On the other hand a decision procedure may illustrate a technique of practical use. It is our thesis that this kind of strategy of exploration can and will yield insight and practical advances in the theory of computation. Provided that the model retains some practical relevance, the dividends are the greater the nearer the decision problem lies to the frontier between solvability and unsolvability.}},
  url = {https://doi.org/10.1145/800235.807074},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup72,
  title = {Groute: An Asynchronous Multi-GPU Programming Model for Irregular Computations}},
  author = {Ben-Nun, Tal and Sutton, Michael and Pai, Sreepathi and Pingali, Keshav}},
  year = {2017}},
  journal = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Nodes with multiple GPUs are becoming the platform of choice for high-performance computing. However, most applications are written using bulk-synchronous programming models, which may not be optimal for irregular algorithms that benefit from low-latency, asynchronous communication. This paper proposes constructs for asynchronous multi-GPU programming, and describes their implementation in a thin runtime environment called Groute. Groute also implements common collective operations and distributed work-lists, enabling the development of irregular applications without substantial programming effort. We demonstrate that this approach achieves state-of-the-art performance and exhibits strong scaling for a suite of irregular applications on 8-GPU and heterogeneous systems, yielding over 7x speedup for some algorithms.}},
  url = {https://doi.org/10.1145/3018743.3018756},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup73,
  title = {Computation spreading: employing hardware migration to specialize CMP cores on-the-fly}},
  author = {Chakraborty, Koushik and Wells, Philip M. and Sohi, Gurindar S.}},
  year = {2006}},
  journal = {Proceedings of the 12th International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In canonical parallel processing, the operating system (OS) assigns a processing core to a single thread from a multithreaded server application. Since different threads from the same application often carry out similar computation, albeit at different times, we observe extensive code reuse among different processors, causing redundancy (e.g., in our server workloads, 45-65\% of all instruction blocks are accessed by all processors). Moreover, largely independent fragments of computation compete for the same private resources causing destructive interference. Together, this redundancy and interference lead to poor utilization of private microarchitecture resources such as caches and branch predictors.We present Computation Spreading (CSP), which employs hardware migration to distribute a thread's dissimilar fragments of computation across the multiple processing cores of a chip multiprocessor (CMP), while grouping similar computation fragments from different threads together. This paper focuses on a specific example of CSP for OS intensive server applications: separating application level (user) computation from the OS calls it makes.When performing CSP, each core becomes temporally specialized to execute certain computation fragments, and the same core is repeatedly used for such fragments. We examine two specific thread assignment policies for CSP, and show that these policies, across four server workloads, are able to reduce instruction misses in private L2 caches by 27-58\%, private L2 load misses by 0-19\%, and branch mispredictions by 9-25\%.}},
  url = {https://doi.org/10.1145/1168857.1168893},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup74,
  title = {Two for the price of one: a model for parallel and incremental computation}},
  author = {Burckhardt, Sebastian and Leijen, Daan and Sadowski, Caitlin and Yi, Jaeheon and Ball, Thomas}},
  year = {2011}},
  journal = {Proceedings of the 2011 ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Parallel or incremental versions of an algorithm can significantly outperform their counterparts, but are often difficult to develop. Programming models that provide appropriate abstractions to decompose data and tasks can simplify parallelization. We show in this work that the same abstractions can enable both parallel and incremental execution. We present a novel algorithm for parallel self-adjusting computation. This algorithm extends a deterministic parallel programming model (concurrent revisions) with support for recording and repeating computations. On record, we construct a dynamic dependence graph of the parallel computation. On repeat, we reexecute only parts whose dependencies have changed.We implement and evaluate our idea by studying five example programs, including a realistic multi-pass CSS layout algorithm. We describe programming techniques that proved particularly useful to improve the performance of self-adjustment in practice. Our final results show significant speedups on all examples (up to 37x on an 8-core machine). These speedups are well beyond what can be achieved by parallelization alone, while requiring a comparable effort by the programmer.}},
  url = {https://doi.org/10.1145/2048066.2048101},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup75,
  title = {MrCrypt: static analysis for secure cloud computations}},
  author = {Tetali, Sai Deep and Lesani, Mohsen and Majumdar, Rupak and Millstein, Todd}},
  year = {2013}},
  journal = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In a common use case for cloud computing, clients upload data and computation to servers that are managed by a third-party infrastructure provider. We describe MrCrypt, a system that provides data confidentiality in this setting by executing client computations on encrypted data. MrCrypt statically analyzes a program to identify the set of operations on each input data column, in order to select an appropriate homomorphic encryption scheme for that column, and then transforms the program to operate over encrypted data. The encrypted data and transformed program are uploaded to the server and executed as usual, and the result of the computation is decrypted on the client side. We have implemented MrCrypt for Java and illustrate its practicality on three standard benchmark suites for the Hadoop MapReduce framework. We have also formalized the approach and proven several soundness and security guarantees.}},
  url = {https://doi.org/10.1145/2509136.2509554},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup76,
  title = {VeriML: typed computation of logical terms inside a language with effects}},
  author = {Stampoulis, Antonis and Shao, Zhong}},
  year = {2010}},
  journal = {Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Modern proof assistants such as Coq and Isabelle provide high degrees of expressiveness and assurance because they support formal reasoning in higher-order logic and supply explicit machine-checkable proof objects. Unfortunately, large scale proof development in these proof assistants is still an extremely difficult and time-consuming task. One major weakness of these proof assistants is the lack of a single language where users can develop complex tactics and decision procedures using a rich programming model and in a typeful manner. This limits the scalability of the proof development process, as users avoid developing domain-specific tactics and decision procedures.In this paper, we present VeriML - a novel language design that couples a type-safe effectful computational language with first-class support for manipulating logical terms such as propositions and proofs. The main idea behind our design is to integrate a rich logical framework - similar to the one supported by Coq - inside a computational language inspired by ML. The language design is such that the added features are orthogonal to the rest of the computational language, and also do not require significant additions to the logic language, so soundness is guaranteed. We have built a prototype implementation of VeriML including both its type-checker and an interpreter. We demonstrate the effectiveness of our design by showing a number of type-safe tactics and decision procedures written in VeriML.}},
  url = {https://doi.org/10.1145/1863543.1863591},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup77,
  title = {AutoMan: a platform for integrating human-based and digital computation}},
  author = {Barowy, Daniel W. and Curtsinger, Charlie and Berger, Emery D. and McGregor, Andrew}},
  year = {2012}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Humans can perform many tasks with ease that remain difficult or impossible for computers. Crowdsourcing platforms like Amazon's Mechanical Turk make it possible to harness human-based computational power at an unprecedented scale. However, their utility as a general-purpose computational platform remains limited. The lack of complete automation makes it difficult to orchestrate complex or interrelated tasks. Scheduling more human workers to reduce latency costs real money, and jobs must be monitored and rescheduled when workers fail to complete their tasks. Furthermore, it is often difficult to predict the length of time and payment that should be budgeted for a given task. Finally, the results of human-based computations are not necessarily reliable, both because human skills and accuracy vary widely, and because workers have a financial incentive to minimize their effort.This paper introduces AutoMan, the first fully automatic crowdprogramming system. AutoMan integrates human-based computations into a standard programming language as ordinary function calls, which can be intermixed freely with traditional functions. This abstraction lets AutoMan programmers focus on their programming logic. An AutoMan program specifies a confidence level for the overall computation and a budget. The AutoMan runtime system then transparently manages all details necessary for scheduling, pricing, and quality control. AutoMan automatically schedules human tasks for each computation until it achieves the desired confidence level; monitors, reprices, and restarts human tasks as necessary; and maximizes parallelism across human workers while staying under budget.}},
  url = {https://doi.org/10.1145/2384616.2384663},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup78,
  title = {Macros as multi-stage computations: type-safe, generative, binding macros in MacroML}},
  author = {Ganz, Steven E. and Sabry, Amr and Taha, Walid}},
  year = {2001}},
  journal = {Proceedings of the Sixth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {With few exceptions, macros have traditionally been viewed as operations on syntax trees or even on plain strings. This view makes macros seem ad hoc, and is at odds with two desirable features of contemporary typed functional languages: static typing and static scoping. At a deeper level, there is a need for a simple, usable semantics for macros. This paper argues that these problems can be addressed by formally viewing macros as multi-stage computations. This view eliminates the need for freshness conditions and tests on variable names, and provides a compositional interpretation that can serve as a basis for designing a sound type system for languages supporting macros, or even for compilation. To illustrate our approach, we develop and present MacroML, an extension of ML that supports inlining, recursive macros, and the definition of new binding constructs. The latter is subtle, and is the most novel addition in a statically typed setting. The semantics of a core subset of MacroML is given by an interpretation into MetaML, a statically-typed multi-stage programming language. It is then easy to show that MacroML is stage- and type-safe: macro expansion does not depend on runtime evaluation, and both stages do not "go wrong.}},
  url = {https://doi.org/10.1145/507635.507646},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup79,
  title = {Coeffects: a calculus of context-dependent computation}},
  author = {Petricek, Tomas and Orchard, Dominic and Mycroft, Alan}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The notion of context in functional languages no longer refers just to variables in scope. Context can capture additional properties of variables (usage patterns in linear logics; caching requirements in dataflow languages) as well as additional resources or properties of the execution environment (rebindable resources; platform version in a cross-platform application). The recently introduced notion of coeffects captures the latter, whole-context properties, but it failed to capture fine-grained per-variable properties.We remedy this by developing a generalized coeffect system with annotations indexed by a coeffect shape. By instantiating a concrete shape, our system captures previously studied flat (whole-context) coeffects, but also structural (per-variable) coeffects, making coeffect analyses more useful. We show that the structural system enjoys desirable syntactic properties and we give a categorical semantics using extended notions of indexed comonad.The examples presented in this paper are based on analysis of established language features (liveness, linear logics, dataflow, dynamic scoping) and we argue that such context-aware properties will also be useful for future development of languages for increasingly heterogeneous and distributed platforms.}},
  url = {https://doi.org/10.1145/2628136.2628160},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup80,
  title = {Stateful manifest contracts}},
  author = {Sekiyama, Taro and Igarashi, Atsushi}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper studies hybrid contract verification for an imperative higher-order language based on a so-called manifest contract system. In manifest contract systems, contracts are part of static types and contract verification is hybrid in the sense that some contracts are statically verified, typically by subtyping, but others are dynamically by casts. It is, however, not trivial to extend existing manifest contract systems, which have been designed mostly for pure functional languages, to imperative features, mainly because of the lack of flow-sensitivity, which should be taken into account in verifying imperative programs statically. We develop an imperative higher-order manifest contract system λrefH for flow-sensitive hybrid contract verification. We introduce a computational variant of Nanevski et al's Hoare types, which are flow-sensitive types to represent pre- and postconditions of impure computation. Our Hoare types are computational in the sense that pre- and postconditions are given by Booleans in the same language as programs so that they are dynamically verifiable. λrefH also supports refinement types as in existing manifest contract systems to describe flow-insensitive, state-independent contracts of pure computation. While it is desirable that any-possibly state-manipulating-predicate can be used in contracts, abuse of stateful operations will break the system. To control stateful operations in contracts, we introduce a region-based effect system, which allows contracts in refinement types and computational Hoare types to manipulate states, as long as they are observationally pure and read-only, respectively. We show that dynamic contract checking in our calculus is consistent with static typing in the sense that the final result obtained without dynamic contract violations satisfies contracts in its static type. It in particular means that the state after stateful computations satisfies their postconditions. As in some of prior manifest contract systems, static contract verification in this work is "post facto," that is, we first define our manifest contract system so that all contracts are checked at run time, formalize conditions when dynamic checks can be removed safely, and show that programs with and without such removable checks are contextually equivalent. We also apply the idea of post facto verification to region-based local reasoning, inspired by the frame rule of Separation Logic.}},
  url = {https://doi.org/10.1145/3009837.3009875},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup81,
  title = {Incremental computation of complex object queries}},
  author = {Nakamura, Hiroaki}},
  year = {2001}},
  journal = {Proceedings of the 16th ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The need for incremental algorithms for evaluating database queries is well known, but constructing algorithms that work on object-oriented databases (OODBs) has been difficult. The reason is that OODB query languages involve complex data types including composite objects and nested collections. As a result, existing algorithms have limitations in that the kinds of database updates are restricted, the operations found in many query languages are not supported, or the algorithms are too complex to be described precisely. We present an incremental computation algorithm that can handle any kind of database updates, can accept any expressions in complex query languages such as OQL, and can be described precisely. By translating primitive values and records into collections, we can reduce all query expressions comprehension. This makes the problems with incremental computation less complicated and thus allows us to decribe of two parts: one is to maintain the consistency in each comprehension occurrence and the other is to update the value of an entire expression. The algorithm is so flexible that we can use strict updates, lazy updates, and their combinations. By comparing the performance of applications built with our mechanism and that of equivalent hand written update programs, we show that our incremental algorithm can be iplemented efficiently.}},
  url = {https://doi.org/10.1145/504282.504294},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup82,
  title = {GhostRider: A Hardware-Software System for Memory Trace Oblivious Computation}},
  author = {Liu, Chang and Harris, Austin and Maas, Martin and Hicks, Michael and Tiwari, Mohit and Shi, Elaine}},
  year = {2015}},
  journal = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents a new, co-designed compiler and architecture called GhostRider for supporting privacy preserving computation in the cloud. GhostRider ensures all programs satisfy a property called memory-trace obliviousness (MTO): Even an adversary that observes memory, bus traffic, and access times while the program executes can learn nothing about the program's sensitive inputs and outputs. One way to achieve MTO is to employ Oblivious RAM (ORAM), allocating all code and data in a single ORAM bank, and to also disable caches or fix the rate of memory traffic. This baseline approach can be inefficient, and so GhostRider's compiler uses a program analysis to do better, allocating data to non-oblivious, encrypted RAM (ERAM) and employing a scratchpad when doing so will not compromise MTO. The compiler can also allocate to multiple ORAM banks, which sometimes significantly reduces access times.We have formalized our approach and proved it enjoys MTO. Our FPGA-based hardware prototype and simulation results show that GhostRider significantly outperforms the baseline strategy.}},
  url = {https://doi.org/10.1145/2694344.2694385},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup83,
  title = {ASC: automatically scalable computation}},
  author = {Waterland, Amos and Angelino, Elaine and Adams, Ryan P. and Appavoo, Jonathan and Seltzer, Margo}},
  year = {2014}},
  journal = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present an architecture designed to transparently and automatically scale the performance of sequential programs as a function of the hardware resources available. The architecture is predicated on a model of computation that views program execution as a walk through the enormous state space composed of the memory and registers of a single-threaded processor. Each instruction execution in this model moves the system from its current point in state space to a deterministic subsequent point. We can parallelize such execution by predictively partitioning the complete path and speculatively executing each partition in parallel. Accurately partitioning the path is a challenging prediction problem. We have implemented our system using a functional simulator that emulates the x86 instruction set, including a collection of state predictors and a mechanism for speculatively executing threads that explore potential states along the execution path. While the overhead of our simulation makes it impractical to measure speedup relative to native x86 execution, experiments on three benchmarks show scalability of up to a factor of 256 on a 1024 core machine when executing unmodified sequential programs.}},
  url = {https://doi.org/10.1145/2541940.2541985},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup84,
  title = {Software caching and computation migration in Olden}},
  author = {Carlisle, Martin C. and Rogers, Anne}},
  year = {1995}},
  journal = {Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The goal of the Olden project is to build a system that provides parallelism for general purpose C programs with minimal programmer annotations. We focus on programs using dynamic structures such as trees, lists, and DAGs. We demonstrate that providing both software caching and computation migration can improve the performance of these programs, and provide a compile-time heuristic that selects between them for each pointer dereference. We have implemented a prototype system on the Thinking Machines CM-5. We describe our implementation and report on experiments with ten benchmarks.}},
  url = {https://doi.org/10.1145/209936.209941},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup85,
  title = {Mixed-initiative interaction = mixed computation}},
  author = {Ramakrishnan, Naren and Capra, Robert G. and P\'{e}},
  year = {2002}},
  journal = {Proceedings of the 2002 ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Based Program Manipulation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We show that partial evaluation can be usefully viewed as a programming model for realizing mixed-initiative functionality in interactive applications. Mixed-initiative interaction between two participants is one where the parties can take turns at any time to change and steer the flow of interaction. We concentrate on the facet of mixed-initiative referred to as 'unsolicited reporting' and demonstrate how out-of-turn interactions by users can be modeled by 'jumping ahead' to nested dialogs (via partial evaluation). Our approach permits the view of dialog management systems in terms of their support for staging and simplifying inter-actions; we characterize three different voice-based interaction technologies using this viewpoint. In particular, we show that the built-in form interpretation algorithm (FIA) in the VoiceXML dialog management architecture is actually a (well disguised) combination of an interpreter and a partial evaluator.}},
  url = {https://doi.org/10.1145/503032.503042},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup86,
  title = {Challenging the "embarrassingly sequential": parallelizing finite state machine-based computations through principled speculation}},
  author = {Zhao, Zhijia and Wu, Bo and Shen, Xipeng}},
  year = {2014}},
  journal = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Finite-State Machine (FSM) applications are important for many domains. But FSM computation is inherently sequential, making such applications notoriously difficult to parallelize. Most prior methods address the problem through speculations on simple heuristics, offering limited applicability and inconsistent speedups.This paper provides some principled understanding of FSM parallelization, and offers the first disciplined way to exploit application-specific information to inform speculations for parallelization. Through a series of rigorous analysis, it presents a probabilistic model that captures the relations between speculative executions and the properties of the target FSM and its inputs. With the formulation, it proposes two model-based speculation schemes that automatically customize themselves with the suitable configurations to maximize the parallelization benefits. This rigorous treatment yields near-linear speedup on applications that state-of-the-art techniques can barely accelerate.}},
  url = {https://doi.org/10.1145/2541940.2541989},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup87,
  title = {The duality of computation}},
  author = {Curien, Pierre-Louis and Herbelin, Hugo}},
  year = {2000}},
  journal = {Proceedings of the Fifth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present the μ -calculus, a syntax for λ-calculus + control operators exhibiting symmetries such as program/context and call-by-name/call-by-value. This calculus is derived from implicational Gentzen's sequent calculus LK, a key classical logical system in proof theory. Under the Curry-Howard correspondence between proofs and programs, we can see LK, or more precisely a formulation called LKμ , as a syntax-directed system of simple types for μ -calculus. For μ -calculus, choosing a call-by-name or call-by-value discipline for reduction amounts to choosing one of the two possible symmetric orientations of a critical pair. Our analysis leads us to revisit the question of what is a natural syntax for call-by-value functional computation. We define a translation of λμ-calculus into μ -calculus and two dual translations back to λ-calculus, and we recover known CPS translations by composing these translations.}},
  url = {https://doi.org/10.1145/351240.351262},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup88,
  title = {A sound reduction semantics for untyped CBN multi-stage computation. Or, the theory of MetaML is non-trival}},
  author = {Taha, Walid}},
  year = {1999}},
  journal = {Proceedings of the 2000 ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Based Program Manipulation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A multi-stage computation is one involving more than one stage of execution. MetaML is a language for programming multi-stage computations. Previous studies presented big-step semantics, categorical semantics, and sound type systems for MetaML. In this paper, we report on a confluent and sound reduction semantics for untyped call-by name (CBN) MetaML. The reduction semantics can be used to formally justify some optimization performed by a CBN MetaML implementation. The reduction semantics demonstrates that non-trivial equalities hold for object-code, even in the untyped setting. The paper also emphasizes that adding intensional analysis (that is, taking-apart object programs) to MetaML remains an interesting open problem.}},
  url = {https://doi.org/10.1145/328690.328697},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup89,
  title = {CEAL: a C-based language for self-adjusting computation}},
  author = {Hammer, Matthew A. and Acar, Umut A. and Chen, Yan}},
  year = {2009}},
  journal = {Proceedings of the 30th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Self-adjusting computation offers a language-centric approach to writing programs that can automatically respond to modifications to their data (e.g., inputs). Except for several domain-specific implementations, however, all previous implementations of self-adjusting computation assume mostly functional, higher-order languages such as Standard ML. Prior to this work, it was not known if self-adjusting computation can be made to work with low-level, imperative languages such as C without placing undue burden on the programmer.We describe the design and implementation of CEAL: a C-based language for self-adjusting computation. The language is fully general and extends C with a small number of primitives to enable writing self-adjusting programs in a style similar to conventional C programs. We present efficient compilation techniques for translating CEAL programs into C that can be compiled with existing C compilers using primitives supplied by a run-time library for self-adjusting computation. We implement the proposed compiler and evaluate its effectiveness. Our experiments show that CEAL is effective in practice: compiled self-adjusting programs respond to small modifications to their data by orders of magnitude faster than recomputing from scratch while slowing down a from-scratch run by a moderate constant factor. Compared to previous work, we measure significant space and time improvements.}},
  url = {https://doi.org/10.1145/1542476.1542480},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup90,
  title = {Much ado about two (pearl): a pearl on parallel prefix computation}},
  author = {Voigtl\"{a}},
  year = {2008}},
  journal = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This pearl develops a statement about parallel prefix computation in the spirit of Knuth's 0-1-Principle for oblivious sorting algorithms. It turns out that 0-1 is not quite enough here. The perfect hammer for the nails we are going to drive in is relational parametricity.}},
  url = {https://doi.org/10.1145/1328438.1328445},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup91,
  title = {Algebras and coalgebras in the light affine Lambda calculus}},
  author = {Gaboardi, Marco and P\'{e}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Algebra and coalgebra are widely used to model data types in functional programming languages and proof assistants. Their use permits to better structure the computations and also to enhance the expressivity of a language or of a proof system. Interestingly, parametric polymorphism \`{a}},
  url = {https://doi.org/10.1145/2784731.2784759},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup92,
  title = {Lazy computation with exact real numbers}},
  author = {Edalat, Abbas and Potts, Peter John and S\"{u}},
  year = {1998}},
  journal = {Proceedings of the Third ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We provide a semantical framework for exact real arithmetic using linear fractional transformations on the extended real line. We present an extension of PCF with a real type which introduces an eventually breadth-first strategy for lazy evaluation of exact real numbers. In this language, we present the constant redundant if, rif, for defining functions by cases which, in contrast to parallel if (pif), overcomes the problem of undecidability of comparison of real numbers in finite time. We use the upper space of the one-point compactification of the real line to develop a denotational semantics for the lazy evaluation of real programs. Finally two adequacy results are proved, one for programs containing rif and one for those not containing it. Our adequacy results in particular provide the proof of correctness of algorithms for computation of single-valued elementary functions.}},
  url = {https://doi.org/10.1145/289423.289441},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup93,
  title = {A theory of effects and resources: adjunction models and polarised calculi}},
  author = {Curien, Pierre-Louis and Fiore, Marcelo and Munch-Maccagnoni, Guillaume}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We consider the Curry-Howard-Lambek correspondence for effectful computation and resource management, specifically proposing polarised calculi together with presheaf-enriched adjunction models as the starting point for a comprehensive semantic theory relating logical systems, typed calculi, and categorical models in this context. Our thesis is that the combination of effects and resources should be considered orthogonally. Model theoretically, this leads to an understanding of our categorical models from two complementary perspectives: (i) as a linearisation of CBPV (Call-by-Push-Value) adjunction models, and (ii) as an extension of linear/non-linear adjunction models with an adjoint resolution of computational effects. When the linear structure is cartesian and the resource structure is trivial we recover Levy’s notion of CBPV adjunction model, while when the effect structure is trivial we have Benton’s linear/non-linear adjunction models. Further instances of our model theory include the dialogue categories with a resource modality of Melli\`{e}},
  url = {https://doi.org/10.1145/2837614.2837652},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup94,
  title = {Ur: statically-typed metaprogramming with type-level record computation}},
  author = {Chlipala, Adam}},
  year = {2010}},
  journal = {Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Dependent types provide a strong foundation for specifying and verifying rich properties of programs through type-checking. The earliest implementations combined dependency, which allows types to mention program variables; with type-level computation, which facilitates expressive specifications that compute with recursive functions over types. While many recent applications of dependent types omit the latter facility, we argue in this paper that it deserves more attention, even when implemented without dependency.In particular, the ability to use functional programs as specifications enables statically-typed metaprogramming: programs write programs, and static type-checking guarantees that the generating process never produces invalid code. Since our focus is on generic validity properties rather than full correctness verification, it is possible to engineer type inference systems that are very effective in narrow domains. As a demonstration, we present Ur, a programming language designed to facilitate metaprogramming with first-class records and names. On top of Ur, we implement Ur/Web, a special standard library that enables the development of modern Web applications. Ad-hoc code generation is already in wide use in the popular Web application frameworks, and we show how that generation may be tamed using types, without forcing metaprogram authors to write proofs or forcing metaprogram users to write any fancy types.}},
  url = {https://doi.org/10.1145/1806596.1806612},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup95,
  title = {Towards compositional and generative tensor optimizations}},
  author = {Susungi, Adilla and Rink, Norman A. and Castrill\'{o}},
  year = {2017}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many numerical algorithms are naturally expressed as operations on tensors (i.e. multi-dimensional arrays). Hence, tensor expressions occur in a wide range of application domains, e.g. quantum chemistry and physics; big data analysis and machine learning; and computational fluid dynamics. Each domain, typically, has developed its own strategies for efficiently generating optimized code, supported by tools such as domain-specific languages, compilers, and libraries. However, strategies and tools are rarely portable between domains, and generic solutions typically act as ''black boxes'' that offer little control over code generation and optimization. As a consequence, there are application domains without adequate support for easily generating optimized code, e.g. computational fluid dynamics. In this paper we propose a generic and easily extensible intermediate language for expressing tensor computations and code transformations in a modular and generative fashion. Beyond being an intermediate language, our solution also offers meta-programming capabilities for experts in code optimization. While applications from the domain of computational fluid dynamics serve to illustrate our proposed solution, we believe that our general approach can help unify research in tensor optimizations and make solutions more portable between domains.}},
  url = {https://doi.org/10.1145/3136040.3136050},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup96,
  title = {Improving cache performance in dynamic applications through data and computation reorganization at run time}},
  author = {Ding, Chen and Kennedy, Ken}},
  year = {1999}},
  journal = {Proceedings of the ACM SIGPLAN 1999 Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {With the rapid improvement of processor speed, performance of the memory hierarchy has become the principal bottleneck for most applications. A number of compiler transformations have been developed to improve data reuse in cache and registers, thus reducing the total number of direct memory accesses in a program. Until now, however, most data reuse transformations have been static---applied only at compile time. As a result, these transformations cannot be used to optimize irregular and dynamic applications, in which the data layout and data access patterns remain unknown until run time and may even change during the computation.In this paper, we explore ways to achieve better data reuse in irregular and dynamic applications by building on the inspector-executor method used by Saltz for run-time parallelization. In particular, we present and evaluate a dynamic approach for improving both computation and data locality in irregular programs. Our results demonstrate that run-time program transformations can substantially improve computation and data locality and, despite the complexity and cost involved, a compiler can automate such transformations, eliminating much of the associated run-time overhead.}},
  url = {https://doi.org/10.1145/301618.301670},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup97,
  title = {Space and time efficient execution of parallel irregular computations}},
  author = {Fu, Cong and Yang, Tao}},
  year = {1997}},
  journal = {Proceedings of the Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Solving problems of large sizes is an important goal for parallel machines with multiple CPU and memory resources. In this paper, issues of efficient execution of overhead-sensitive parallel irregular computation under memory constraints are addressed. The irregular parallelism is modeled by task dependence graphs with mixed granularities. The trade-off in achieving both time and space efficiency is investigated. The main difficulty of designing efficient run-time system support is caused by the use of fast communication primitives available on modern parallel architectures. A run-time active memory management scheme and new scheduling techniques are proposed to improve memory utilization while retaining good time efficiency, and a theoretical analysis on correctness and performance is provided. This work is implemented in the context of RAPID system [5] which provides run-time support for parallelizing irregular code on distributed memory machines and the effectiveness of the proposed techniques is verified on sparse Cholesky and LU factorization with partial pivoting. The experimental results on Cray-T3D show that solvable problem sizes can be increased substantially under limited memory capacities and the loss of execution efficiency caused by the extra memory managing overhead is reasonable.}},
  url = {https://doi.org/10.1145/263764.263773},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup98,
  title = {Contextual isomorphisms}},
  author = {Levy, Paul Blain}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {What is the right notion of "isomorphism" between types, in a simple type theory? The traditional answer is: a pair of terms that are inverse up to a specified congruence. We firstly argue that, in the presence of effects, this answer is too liberal and needs to be restricted, using F\"{u}},
  url = {https://doi.org/10.1145/3009837.3009898},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup99,
  title = {LogP: towards a realistic model of parallel computation}},
  author = {Culler, David and Karp, Richard and Patterson, David and Sahay, Abhijit and Schauser, Klaus Erik and Santos, Eunice and Subramonian, Ramesh and von Eicken, Thorsten}},
  year = {1993}},
  journal = {Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A vast body of theoretical research has focused either on overly simplistic models of parallel computation, notably the PRAM, or overly specific models that have few representatives in the real world. Both kinds of models encourage exploitation of formal loopholes, rather than rewarding development of techniques that yield performance across a range of current and future parallel machines. This paper offers a new parallel machine model, called LogP, that reflects the critical technology trends underlying parallel computers. it is intended to serve as a basis for developing fast, portable parallel algorithms and to offer guidelines to machine designers. Such a model must strike a balance between detail and simplicity in order to reveal important bottlenecks without making analysis of interesting problems intractable. The model is based on four parameters that specify abstractly the computing bandwidth, the communication bandwidth, the communication delay, and the efficiency of coupling communication and computation. Portable parallel algorithms typically adapt to the machine configuration, in terms of these parameters. The utility of the model is demonstrated through examples that are implemented on the CM-5.}},
  url = {https://doi.org/10.1145/155332.155333},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup100,
  title = {Fortran at ten gigaflops: the connection machine convolution compiler}},
  author = {Bromley, Mark and Heller, Steven and McNerney, Tim and Steele, Guy L.}},
  year = {1991}},
  journal = {Proceedings of the ACM SIGPLAN 1991 Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {},
  url = {https://doi.org/10.1145/113445.113458},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup101,
  title = {EnerJ: approximate data types for safe and general low-power computation}},
  author = {Sampson, Adrian and Dietl, Werner and Fortuna, Emily and Gnanapragasam, Danushen and Ceze, Luis and Grossman, Dan}},
  year = {2011}},
  journal = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Energy is increasingly a first-order concern in computer systems. Exploiting energy-accuracy trade-offs is an attractive choice in applications that can tolerate inaccuracies. Recent work has explored exposing this trade-off in programming models. A key challenge, though, is how to isolate parts of the program that must be precise from those that can be approximated so that a program functions correctly even as quality of service degrades.We propose using type qualifiers to declare data that may be subject to approximate computation. Using these types, the system automatically maps approximate variables to low-power storage, uses low-power operations, and even applies more energy-efficient algorithms provided by the programmer. In addition, the system can statically guarantee isolation of the precise program component from the approximate component. This allows a programmer to control explicitly how information flows from approximate data to precise data. Importantly, employing static analysis eliminates the need for dynamic checks, further improving energy savings. As a proof of concept, we develop EnerJ, an extension to Java that adds approximate data types. We also propose a hardware architecture that offers explicit approximate storage and computation. We port several applications to EnerJ and show that our extensions are expressive and effective; a small number of annotations lead to significant potential energy savings (10\%-50\%) at very little accuracy cost.}},
  url = {https://doi.org/10.1145/1993498.1993518},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup102,
  title = {Tartan: evaluating spatial computation for whole program execution}},
  author = {Mishra, Mahim and Callahan, Timothy J. and Chelcea, Tiberiu and Venkataramani, Girish and Goldstein, Seth C. and Budiu, Mihai}},
  year = {2006}},
  journal = {Proceedings of the 12th International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Spatial Computing (SC) has been shown to be an energy-efficient model for implementing program kernels. In this paper we explore the feasibility of using SC for more than small kernels. To this end, we evaluate the performance and energy efficiency of entire applications on Tartan, a general-purpose architecture which integrates a reconfigurable fabric (RF) with a superscalar core. Our compiler automatically partitions and compiles an application into an instruction stream for the core and a configuration for the RF. We use a detailed simulator to capture both timing and energy numbers for all parts of the system.Our results indicate that a hierarchical RF architecture, designed around a scalable interconnect, is instrumental in harnessing the benefits of spatial computation. The interconnect uses static configuration and routing at the lower levels and a packet-switched, dynamically-routed network at the top level. Tartan is most energyefficient when almost all of the application is mapped to the RF, indicating the need for the RF to support most general-purpose programming constructs. Our initial investigation reveals that such a system can provide, on average, an order of magnitude improvement in energy-delay compared to an aggressive superscalar core on single-threaded workloads.}},
  url = {https://doi.org/10.1145/1168857.1168878},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup103,
  title = {Towards a comprehensive theory of monadic effects}},
  author = {Filinski, Andrzej}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {It has been more than 20 years since monads were proposed as a unifying concept for computational effects, in both formal semantics and functional programs. Over that period, there has been substantial incremental progress on several fronts within the ensuing research area, including denotational, operational, and axiomatic characterizations of effects; principles and frameworks for combining effects; prescriptive vs. descriptive effect-type systems; specification vs. implementation of effects; and realizations of effect-related theoretical constructions in practical functional languages, both eager and lazy. Yet few would confidently claim that programs with computational effects are by now as well understood, and as thoroughly supported by formal reasoning techniques, as types and terms in purely functional settings.This talk outlines (one view of) the landscape of effectful functional programming, and attempts to assess our collective progress towards the goal of a broad yet coherent theory of monadic effects. We are not quite there yet, but intriguingly, many potential ingredients of such a theory have been repeatedly discovered and developed, with only minor variations, in seemingly unrelated contexts. Some stronger-than-expected ties between the research topics mentioned above also instill hope that there is indeed a natural, comprehensive theory of monadic effects, waiting to be fully explicated.}},
  url = {https://doi.org/10.1145/2034773.2034775},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup104,
  title = {Algebraic foundations for effect-dependent optimisations}},
  author = {Kammar, Ohad and Plotkin, Gordon D.}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a general theory of Gifford-style type and effect annotations, where effect annotations are sets of effects. Generality is achieved by recourse to the theory of algebraic effects, a development of Moggi's monadic theory of computational effects that emphasises the operations causing the effects at hand and their equational theory. The key observation is that annotation effects can be identified with operation symbols. We develop an annotated version of Levy's Call-by-Push-Value language with a kind of computations for every effect set; it can be thought of as a sequential, annotated intermediate language. We develop a range of validated optimisations (i.e., equivalences), generalising many existing ones and adding new ones. We classify these optimisations as structural, algebraic, or abstract: structural optimisations always hold; algebraic ones depend on the effect theory at hand; and abstract ones depend on the global nature of that theory (we give modularly-checkable sufficient conditions for their validity).}},
  url = {https://doi.org/10.1145/2103656.2103698},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup105,
  title = {Subjective auxiliary state for coarse-grained concurrency}},
  author = {Ley-Wild, Ruy and Nanevski, Aleksandar}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {From Owicki-Gries' Resource Invariants and Jones' Rely/Guarantee to modern variants based on Separation Logic, axiomatic logics for concurrency require auxiliary state to explicitly relate the effect of all threads to the global invariant on the shared resource. Unfortunately, auxiliary state gives the proof of an individual thread access to the auxiliaries of all other threads. This makes proofs sensitive to the global context, which prevents local reasoning and compositionality.To tame this historical difficulty of auxiliary state, we propose subjective auxiliary state, whereby each thread is verified using a self view (i.e., the thread's effect on the shared resource) and an other view (i.e., the collective effect of all the other threads). Subjectivity generalizes auxiliary state from stacks and heaps to user-chosen partial commutative monoids, which can eliminate the dependence on the global thread structure.We employ subjectivity to formulate Subjective Concurrent Separation Logic as a combination of subjective auxiliary state and Concurrent Separation Logic. The logic yields simple, compositional proofs of coarse-grained concurrent programs that use auxiliary state, and scales to support higher-order recursive procedures that can themselves fork new threads. We prove the soundness of the logic with a novel denotational semantics of action trees and a definition of safety using rely/guarantee transitions over a large subjective footprint. We have mechanized the denotational semantics, logic, metatheory, and a number of examples by a shallow embedding in Coq.}},
  url = {https://doi.org/10.1145/2429069.2429134},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup106,
  title = {Conservation cores: reducing the energy of mature computations}},
  author = {Venkatesh, Ganesh and Sampson, Jack and Goulding, Nathan and Garcia, Saturnino and Bryksin, Vladyslav and Lugo-Martinez, Jose and Swanson, Steven and Taylor, Michael Bedford}},
  year = {2010}},
  journal = {Proceedings of the Fifteenth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Growing transistor counts, limited power budgets, and the breakdown of voltage scaling are currently conspiring to create a utilization wall that limits the fraction of a chip that can run at full speed at one time. In this regime, specialized, energy-efficient processors can increase parallelism by reducing the per-computation power requirements and allowing more computations to execute under the same power budget. To pursue this goal, this paper introduces conservation cores. Conservation cores, or c-cores, are specialized processors that focus on reducing energy and energy-delay instead of increasing performance. This focus on energy makes c-cores an excellent match for many applications that would be poor candidates for hardware acceleration (e.g., irregular integer codes). We present a toolchain for automatically synthesizing c-cores from application source code and demonstrate that they can significantly reduce energy and energy-delay for a wide range of applications. The c-cores support patching, a form of targeted reconfigurability, that allows them to adapt to new versions of the software they target. Our results show that conservation cores can reduce energy consumption by up to 16.0x for functions and by up to 2.1x for whole applications, while patching can extend the useful lifetime of individual c-cores to match that of conventional processors.}},
  url = {https://doi.org/10.1145/1736020.1736044},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup107,
  title = {Mitosis compiler: an infrastructure for speculative threading based on pre-computation slices}},
  author = {Qui\~{n}},
  year = {2005}},
  journal = {Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Speculative parallelization can provide significant sources of additional thread-level parallelism, especially for irregular applications that are hard to parallelize by conventional approaches. In this paper, we present the Mitosis compiler, which partitions applications into speculative threads, with special emphasis on applications for which conventional parallelizing approaches fail.The management of inter-thread data dependences is crucial for the performance of the system. The Mitosis framework uses a pure software approach to predict/compute the thread's input values. This software approach is based on the use of pre-computation slices (p-slices), which are built by the Mitosis compiler and added at the beginning of the speculative thread. P-slices must compute thread input values accurately but they do not need to guarantee correctness, since the underlying architecture can detect and recover from misspeculations. This allows the compiler to use aggressive/unsafe optimizations to significantly reduce their overhead. The most important optimizations included in the Mitosis compiler and presented in this paper are branch pruning, memory and register dependence speculation, and early thread squashing.Performance evaluation of Mitosis compiler/architecture shows an average speedup of 2.2.}},
  url = {https://doi.org/10.1145/1065010.1065043},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup108,
  title = {Computation migration: enhancing locality for distributed-memory parallel systems}},
  author = {Hsieh, Wilson C. and Wang, Paul and Weihl, William E.}},
  year = {1993}},
  journal = {Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We describe computation migration, a new technique that is based on compile-time program transformations, for accesing remote data in a distributed-memory parallel system. In contrast with RPC-style access, where the access is performed remotely, and with data migration, where the data is moved so that it is local, computation migration moves part of the current thread to the processor where the data resides. The access is performed at the remote processor, and the migrated thread portion continues to run on that same processor; this makes subsequent accesses in the thread portion local.We describe an implementation of computation migration that consists of two parts: an implementation that migrates single activation frames, and a high-level language annotation that allows a programmer to express when migration is desired. We performed experiments using two applications; these experiments demonstrate that computation migration is a valuable alternative to RPC and data migration.}},
  url = {https://doi.org/10.1145/155332.155357},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup109,
  title = {Orchestrating interactions among parallel computations}},
  author = {Graham, Susan L. and Lucco, Steven and Sharp, Oliver}},
  year = {1993}},
  journal = {Proceedings of the ACM SIGPLAN 1993 Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many parallel programs contain multiple sub-computations, each with distinct communication and load balancing requirements. The traditional approach to compiling such programs is to impose a processor synchronization barrier between sub-computations, optimizing each as a separate entity. This paper develops a methodology for managing the interactions among sub-computations, avoiding strict synchronization where concurrent or pipelined relationships are possible.Our approach to compiling parallel programs has two components: symbolic data access analysis and adaptive runtime support. We summarize the data access behavior of sub-computations (such as loop nests) and split them to expose concurrency and pipelining opportunities. The split transformation has been incorporated into an extended FORTRAN compiler, which outputs a FORTRAN 77 program augmented with calls to library routines written in C and a coarse-grained dataflow graph summarizing the exposed parallelism.The compiler encodes symbolic information, including loop bounds and communication requirements, for an adaptive runtime system, which uses runtime information to improve the scheduling efficiency of irregular sub-computations. The runtime system incorporates algorithms that allocate processing resources to concurrently executing sub-computations and choose communication granularity. We have demonstrated that these dynamic techniques substantially improve performance on a range of production applications including climate modeling and x-ray tomography, expecially when large numbers of processors are available.}},
  url = {https://doi.org/10.1145/155090.155100},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup110,
  title = {Incremental computation of dominator trees}},
  author = {Sreedhar, Vugranam C. and Gao, Guang R. and Lee, Yong-fong}},
  year = {1995}},
  journal = {Papers from the 1995 ACM SIGPLAN Workshop on Intermediate Representations}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Data flow analysis based on an incremental approach may require that the dominator tree be correctly maintained at all times. Previous solutions to the problem of incrementally maintaining dominator trees were restricted to reducible flowgraphs. In this paper we present a new algorithm for incrementally maintaining the dominator tree of an arbitrary flowgraph, either reducible or irreducible, based on a program representation called the DJ-graph. For the case where an edge is inserted, our algorithm is also faster than previous approaches (in the worst case). For the deletion case, our algorithm is likely to run fast on the average cases.}},
  url = {https://doi.org/10.1145/202529.202531},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup111,
  title = {Monads in action}},
  author = {Filinski, Andrzej}},
  year = {2010}},
  journal = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In functional programming, monadic characterizations of computational effects are normally understood denotationally: they describe how an effectful program can be systematically expanded or translated into a larger, pure program, which can then be evaluated according to an effect-free semantics. Any effect-specific operations expressible in the monad are also given purely functional definitions, but these definitions are only directly executable in the context of an already translated program. This approach thus takes an inherently Church-style view of effects: the nominal meaning of every effectful term in the program depends crucially on its type.We present here a complementary, operational view of monadic effects, in which an effect definition directly induces an imperative behavior of the new operations expressible in the monad. This behavior is formalized as additional operational rules for only the new constructs; it does not require any structural changes to the evaluation judgment. Specifically, we give a small-step operational semantics of a prototypical functional language supporting programmer-definable, layered effects, and show how this semantics naturally supports reasoning by familiar syntactic techniques, such as showing soundness of a Curry-style effect-type system by the progress+preservation method.}},
  url = {https://doi.org/10.1145/1706299.1706354},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup112,
  title = {Generalized partial computation for a lazy functional language}},
  author = {Takano, Akihiko}},
  year = {1991}},
  journal = {Proceedings of the 1991 ACM SIGPLAN Symposium on Partial Evaluation and Semantics-Based Program Manipulation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {},
  url = {https://doi.org/10.1145/115865.115867},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup113,
  title = {How to print floating-point numbers accurately}},
  author = {Steele, Guy L. and White, Jon L.}},
  year = {1990}},
  journal = {Proceedings of the ACM SIGPLAN 1990 Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present algorithms for accurately converting floating-point numbers to decimal representation. The key idea is to carry along with the computation an explicit representation of the required rounding accuracy.We begin with the simpler problem of converting fixed-point fractions. A modification of the well-known algorithm for radix-conversion of fixed-point fractions by multiplication explicitly determines when to terminate the conversion process; a variable number of digits are produced. The algorithm has these properties: No information is lost; the original fraction can be recovered from the output by rounding.No “garbage digits” are produced.The output is correctly rounded.It is never necessary to propagate carries on rounding.We then derive two algorithms for free-formal output of floating-point numbers. The first simply scales the given floating-point number to an appropriate fractional range and then applies the algorithm for fractions. This is quite fast and simple to code but has inaccuracies stemming from round-off errors and oversimplification. The second algorithm guarantees mathematical accuracy by using multiple-precision integer arithmetic and handling special cases. Both algorithms produce no more digits than necessary (intuitively, the “1.3 prints as 1.2999999” problem does not occur).Finally, we modify the free-format conversion algorithm for use in fixed-format applications. Information may be lost if the fixed format provides too few digit positions, but the output is always correctly rounded. On the other hand, no “garbage digits” are ever produced, even if the fixed format specifies too many digit positions (intuitively, the “4/3 prints as 1.333333328366279602” problem does not occur).}},
  url = {https://doi.org/10.1145/93542.93559},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup114,
  title = {Verifying the correctness of compiler transformations on basic blocks using abstract interpretation}},
  author = {McNerney, Timothy S.}},
  year = {1991}},
  journal = {Proceedings of the 1991 ACM SIGPLAN Symposium on Partial Evaluation and Semantics-Based Program Manipulation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {},
  url = {https://doi.org/10.1145/115865.115877},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup115,
  title = {Restoring consistent global states of distributed computations}},
  author = {Goldberg, Arthur P. and Gopal, Ajei and Lowry, Andy and Strom, Rob}},
  year = {1991}},
  journal = {Proceedings of the 1991 ACM/ONR Workshop on Parallel and Distributed Debugging}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {},
  url = {https://doi.org/10.1145/122759.122772},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup116,
  title = {Slot games: a quantitative model of computation}},
  author = {Ghica, Dan R.}},
  year = {2005}},
  journal = {Proceedings of the 32nd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a games-based denotational semantics for a quantitative analysis of programming languages. We define a Hyland-Ong-style games framework called slot games, which consists of HO games augmented with a new action called token. We develop a slot-game model for the language Idealised Concurrent Algol by instrumenting the strategies in its HO game model with token actions. We show that the slot-game model is a denotational semantics induced by a notion of observation formalised in the operational theory of improvement of Sands, and we give a full abstraction result. A quantitative analysis of programs has many potential applications, from compiler optimisations to resource-constrained execution and static performance profiling. We illustrate several such applications with putative examples that would be nevertheless difficult, if not impossible, to handle using known operational techniques.}},
  url = {https://doi.org/10.1145/1040305.1040313},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup117,
  title = {Acute: high-level programming language design for distributed computation}},
  author = {Sewell, Peter and Leifer, James J. and Wansbrough, Keith and Nardelli, Francesco Zappa and Allen-Williams, Mair and Habouzit, Pierre and Vafeiadis, Viktor}},
  year = {2005}},
  journal = {Proceedings of the Tenth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Existing languages provide good support for typeful programming of standalone programs. In a distributed system, however, there may be interaction between multiple instances of many distinct programs, sharing some (but not necessarily all) of their module structure, and with some instances rebuilt with new versions of certain modules as time goes on. In this paper we discuss programminglanguage support for such systems, focussing on their typing and naming issues.We describe an experimental language, Acute, which extends an ML core to support distributed development, deployment, and execution, allowing type-safe interaction between separately-built programs. The main features are: (1) type-safe marshalling of arbitrary values; (2) type names that are generated (freshly and by hashing) to ensure that type equality tests suffice to protect the invariants of abstract types, across the entire distributed system; (3) expression-level names generated to ensure that name equality tests suffice for type-safety of associated values, e.g. values carried on named channels; (4) controlled dynamic rebinding of marshalled values to local resources; and (5) thunkification of threads and mutexes to support computation mobility.These features are a large part of what is needed for typeful distributed programming. They are a relatively lightweight extension of ML, should be efficiently implementable, and are expressive enough to enable a wide variety of distributed infrastructure layers to be written as simple library code above the byte-string network and persistent store APIs. This disentangles the language runtime from communication intricacies. This paper highlights the main design choices in Acute. It is supported by a full language definition (of typing, compilation, and operational semantics), by a prototype implementation, and by example distribution libraries.}},
  url = {https://doi.org/10.1145/1086365.1086370},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup118,
  title = {A RISC architecture for symbolic computation}},
  author = {Kieburtz, Richard B.}},
  year = {1987}},
  journal = {Proceedings of the Second International Conference on Architectual Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The G-machine is a language-directed processor architecture designed to support graph reduction as a model of computation. It can carry out lazy evaluation of functional language programs and can evaluate programs in which logical variables are used. To support these language features, the abstract machine requires tagged memory and executes some rather complex instructions, such as to evaluate a function application.This paper explores an implementation of the G-machine as a high performance RISC architecture. Complex instructions can be represented by RISC code without experiencing a large expansion of code volume. The instruction pipeline is discussed in some detail. The processor is intended to be integrated into a standard, 32-bit memory architecture. Tagged memory is supported by aggregating data with tags in a cache.}},
  url = {https://doi.org/10.1145/36206.36197},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup119,
  title = {An introduction to object-based reflective concurrent computation}},
  author = {Yonezawa, A. and Watanabe, T.}},
  year = {1988}},
  journal = {Proceedings of the 1988 ACM SIGPLAN Workshop on Object-Based Concurrent Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {},
  url = {https://doi.org/10.1145/67386.67399},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup120,
  title = {Just do it: simple monadic equational reasoning}},
  author = {Gibbons, Jeremy and Hinze, Ralf}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {One of the appeals of pure functional programming is that it is so amenable to equational reasoning. One of the problems of pure functional programming is that it rules out computational effects. Moggi and Wadler showed how to get round this problem by using monads to encapsulate the effects, leading in essence to a phase distinction - a pure functional evaluation yielding an impure imperative computation. Still, it has not been clear how to reconcile that phase distinction with the continuing appeal of functional programming; does the impure imperative part become inaccessible to equational reasoning? We think not; and to back that up, we present a simple axiomatic approach to reasoning about programs with computational effects.}},
  url = {https://doi.org/10.1145/2034773.2034777},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup121,
  title = {Selective interpretation as a technique for debugging computationally intensive programs}},
  author = {Chase, B. B. and Hood, R. T.}},
  year = {1987}},
  journal = {Papers of the Symposium on Interpreters and Interpretive Techniques}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {As part of Rice University's project to build a programming environment for scientific software, we have built a facility for program execution that solves some of the problems inherent in debugging large, computationally intensive programs. By their very nature such programs do not lend themselves to full-scale interpretation. In moderation however, interpretation can be extremely useful during the debugging process. In addition to discussing the particular benefits that we expect from interpretation, this paper addresses how interpretive techniques can be effectively used in conjunction with the execution of compiled code. The same implementation technique that permits interpretation to be incorporated as part of execution will also permit the execution facility to be used for debugging parallel programs running on a remote machine.}},
  url = {https://doi.org/10.1145/29650.29662},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup122,
  title = {INC: a language for incremental computations}},
  author = {Yellin, D. and Strom, R.}},
  year = {1988}},
  journal = {Proceedings of the ACM SIGPLAN 1988 Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {},
  url = {https://doi.org/10.1145/53990.54002},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup123,
  title = {Compiling Fortran 8x array features for the connection machine computer system}},
  author = {Albert, Eugene and Knobe, Kathleen and Lukas, Joan D. and Steele, Guy L.}},
  year = {1988}},
  journal = {Proceedings of the ACM/SIGPLAN Conference on Parallel Programming: Experience with Applications, Languages and Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The Connection Machine® computer system supports a data parallel programming style, making it a natural target architecture for Fortran 8x array constructs. The Connection Machine Fortran compiler generates VAX code that performs scalar operations and directs the Connection Machine to perform array operations. The Connection Machine virtual processor mechanism supports elemental operations on very large arrays. Most array operators and intrinsic functions map into single instructions or short instruction sequences. Noncontiguous array sections, array-valued subscripts, and parallel constructs such as WHERE and FORALL are also readily accommodated on the Connection Machine. In addition to such customary optimizations as common subexpression elimination, the CM Fortran compiler minimizes data motion for aligning array operations, minimizes transfers between the Connection Machine and the VAX and minimizes context switching for masked computations.}},
  url = {https://doi.org/10.1145/62115.62121},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup124,
  title = {The geometry of parallelism: classical, probabilistic, and quantum effects}},
  author = {Dal Lago, Ugo and Faggian, Claudia and Valiron, Beno\^{\i}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We introduce a Geometry of Interaction model for higher-order quantum computation, and prove its adequacy for a fully fledged quantum programming language in which entanglement, duplication, and recursion are all available. This model is an instance of a new framework which captures not only quantum but also classical and probabilistic computation. Its main feature is the ability to model commutative effects in a parallel setting. Our model comes with a multi-token machine, a proof net system, and a -style language. Being based on a multi-token machine equipped with a memory, it has a concrete nature which makes it well suited for building low-level operational descriptions of higher-order languages.}},
  url = {https://doi.org/10.1145/3009837.3009859},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup125,
  title = {Parallel inclusion-based points-to analysis}},
  author = {M\'{e}},
  year = {2010}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Inclusion-based points-to analysis provides a good trade-off between precision of results and speed of analysis, and it has been incorporated into several production compilers including gcc. There is an extensive literature on how to speed up this algorithm using heuristics such as detecting and collapsing cycles of pointer-equivalent variables. This paper describes a complementary approach based on exploiting parallelism. Our implementation exploits two key insights. First, we show that inclusion-based points-to analysis can be formulated entirely in terms of graphs and graph rewrite rules. This exposes the amorphous data-parallelism in this algorithm and makes it easier to develop a parallel implementation. Second, we show that this graph-theoretic formulation reveals certain key properties of the algorithm that can be exploited to obtain an efficient parallel implementation. Our parallel implementation achieves a scaling of up to 3x on a 8-core machine for a suite of ten large C programs. For all but the smallest benchmarks, the parallel analysis outperforms a state-of-the-art, highly optimized, serial implementation of the same algorithm. To the best of our knowledge, this is the first parallel implementation of a points-to analysis.}},
  url = {https://doi.org/10.1145/1869459.1869495},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup126,
  title = {Nomadic pict: correct communication infrastructure for mobile computation}},
  author = {Unyapoth, Asis and Sewell, Peter}},
  year = {2001}},
  journal = {Proceedings of the 28th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper addresses the design and verification of infrastructure for mobile computation. In particular, we study language primitives for communication between mobile agents. They can be classi ed into two groups. At a low level there are location dependent primitives that require a programmer to know the current site of a mobile agent in order to communicate with it. At a high level there are location independent primitives that allow communication with a mobile agent irrespective of any migrations. Implementation of the high level requires delicate distributed infrastructure algorithms. In earlier work with Wojciechowski and Pierce we made the two levels precise as process calculi, allowing such algorithms to be expressed as encodings of the high level into the low level; we built NOMADIC PICT, a distributed programming language for experimenting with such encodings. In this paper we turn to semantics, giving a de nition of the core language and proving correctness of an example infrastructure. This requires novel techniques: we develop equivalences that take migration into account, and reasoning principles for agents that are temporarily immobile (eg. waiting on a lock elsewhere in the system).}},
  url = {https://doi.org/10.1145/360204.360214},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup127,
  title = {Optimistic active messages: a mechanism for scheduling communication with computation}},
  author = {Wallach, Deborah A. and Hsieh, Wilson C. and Johnson, Kirk L. and Kaashoek, M. Frans and Weihl, William E.}},
  year = {1995}},
  journal = {Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Low-overhead message passing is critical to the performance of many applications. Active Messages reduce the software overhead for message handling: messages are run as handlers instead of as threads, which avoids the overhead of thread management and the unnecessary data copying of other communication models. Scheduling the execution of Active Messages is typically done by disabling and enabling interrupts, or by polling the network. This primitive scheduling control, combined with the fact that handlers are not schedulable entities, puts severe restrictions on the code that can be run in a message handler. This paper describes a new software mechanism, Optimistic Active Messages (OAM), that eliminates these restrictions; OAMs allow arbitrary user code to execute in handlers, and also allow handlers to block. Despite this gain in expressiveness, OAMs perform as well as Active Messages.We used OAM as the base for an RPC system, Optimistic RPC (ORPC), for the Thinking Machines CM-5 multiprocessor; it consists of an optimized thread package and a stub compiler that hides communication details from the programmer. ORPC is 1.5 to 5 times faster than traditional RPC (TRPC) for small messages and performs as well as Active Messages (AM). Applications that primarily communicate using large data transfers or are fairly coarse-grained perform equally well, independent of whether AMs, ORPCs, or TRPCs are used. For applications that send many short messages, however, the ORPC and AM implementations are up to three times faster than the TRPC implementations. Using ORPC, programmers obtain the benefits of well-proven programming abstractions such as threads, mutexes, and condition variables, do not have to be concerned with communication details, and yet obtain nearly the performance of hand-coded Active Message programs.}},
  url = {https://doi.org/10.1145/209936.209959},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup128,
  title = {Parametric effect monads and semantics of effect systems}},
  author = {Katsumata, Shin-ya}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We study fundamental properties of a generalisation of monad called parametric effect monad, and apply it to the interpretation of general effect systems whose effects have sequential composition operators. We show that parametric effect monads admit analogues of the structures and concepts that exist for monads, such as Kleisli triples, the state monad and the continuation monad, Plotkin and Power's algebraic operations, and the categorical ┬┬-lifting. We also show a systematic method to generate both effects and a parametric effect monad from a monad morphism. Finally, we introduce two effect systems with explicit and implicit subeffecting, and discuss their denotational semantics and the soundness of effect systems.}},
  url = {https://doi.org/10.1145/2535838.2535846},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup129,
  title = {Efficient computation of flow insensitive interprocedural summary information}},
  author = {Cooper, Keith D. and Kennedy, Ken}},
  year = {1984}},
  journal = {Proceedings of the 1984 SIGPLAN Symposium on Compiler Construction}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {},
  url = {https://doi.org/10.1145/502874.502898},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup130,
  title = {Distributed merge trees}},
  author = {Morozov, Dmitriy and Weber, Gunther}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Improved simulations and sensors are producing datasets whose increasing complexity exhausts our ability to visualize and comprehend them directly. To cope with this problem, we can detect and extract significant features in the data and use them as the basis for subsequent analysis. Topological methods are valuable in this context because they provide robust and general feature definitions.As the growth of serial computational power has stalled, data analysis is becoming increasingly dependent on massively parallel machines. To satisfy the computational demand created by complex datasets, algorithms need to effectively utilize these computer architectures. The main strength of topological methods, their emphasis on global information, turns into an obstacle during parallelization.We present two approaches to alleviate this problem. We develop a distributed representation of the merge tree that avoids computing the global tree on a single processor and lets us parallelize subsequent queries. To account for the increasing number of cores per processor, we develop a new data structure that lets us take advantage of multiple shared-memory cores to parallelize the work on a single node. Finally, we present experiments that illustrate the strengths of our approach as well as help identify future challenges.}},
  url = {https://doi.org/10.1145/2442516.2442526},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup131,
  title = {Efficient computation of LALR(1) look-ahead sets}},
  author = {DeRemer, Frank and Pennello, Thomas J.}},
  year = {1979}},
  journal = {SIGPLAN Not.}},
  tipo = {Article}},
  publisher = {Association for Computing Machinery}},
  abstract = {We define two relations that capture the essential structure of the problem of computing LALR(1) look-ahead sets, and present an efficient algorithm to compute the sets in time linear in the size of the relations. In particular, for a PASCAL grammar, our algorithm performs less than 20\% of the set unions performed by a popular-compiler (YACC).}},
  url = {https://doi.org/10.1145/872732.806968},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup132,
  title = {Object views: language support for intelligent object caching in parallel and distributed computations}},
  author = {Lipkind, Ilya and Pechtchanski, Igor and Karamcheti, Vijay}},
  year = {1999}},
  journal = {Proceedings of the 14th ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Object-based parallel and distributed applications are becoming increasingly popular, driven by the programmability advantages of component technology and a flat shared-object space. However, the flat shared-object space introduces a performance challenge: applications that rely on the transparent coherent caching of objects achieve high performance only on tightly coupled parallel machines. In distributed environments, the overheads of object caching force application designers to choose other solutions. Consequently, most applications sacrifice programmability, relying instead on either the explicit coherence management of cached objects, or on vastly different middleware abstractions such as multicast and events.In this paper, we describe object views — language support for efficient object caching in parallel and distributed computations. Object views specify restrictions on how computation threads can use an object, providing the underlying implementation with information about the potential side effects of object access, and thereby enabling construction of scalable, low-overhead caching protocols customized to application requirements. We present extensions to the Java programming language for expressing object views, and describe the design and implementation of a translator and run-time system for executing view-augmented Java programs on a distributed cluster of workstations. Experimental results based on a shared whiteboard application demonstrate that view-based object caching can achieve performance superior to multicast- and event-based implementations, while retaining essentially a shared object interface.}},
  url = {https://doi.org/10.1145/320384.320433},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup133,
  title = {Algebraic Effects, Linearity, and Quantum Programming Languages}},
  author = {Staton, Sam}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We develop a new framework of algebraic theories with linear parameters, and use it to analyze the equational reasoning principles of quantum computing and quantum programming languages. We use the framework as follows: we present a new elementary algebraic theory of quantum computation, built from unitary gates and measurement;we provide a completeness theorem or the elementary algebraic theory by relating it with a model from operator algebra; we extract an equational theory for a quantum programming language from the algebraic theory;we compare quantum computation with other local notions of computation by investigating variations on the algebraic theory.}},
  url = {https://doi.org/10.1145/2676726.2676999},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup134,
  title = {Recursive definitions of partial functions and their computations}},
  author = {Cadiou, J. M. and Manna, Zohar}},
  year = {1972}},
  journal = {Proceedings of ACM Conference on Proving Assertions about Programs}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The object of this paper is to present a syntactic and semantic model for recursive definitions, and to study the relation between their computed functions and their fixpoints. The recursive definitions that we consider are syntactic generalizations of those introduced in [2] by Kleene and in [5] by McCarthy.Each recursive definition yields two classes of fixpoint partial functions (“fixpoints over D υ {ω}},
  url = {https://doi.org/10.1145/800235.807072},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup135,
  title = {A type system for higher-order modules}},
  author = {Dreyer, Derek and Crary, Karl and Harper, Robert}},
  year = {2003}},
  journal = {Proceedings of the 30th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a type theory for higher-order modules that accounts for many central issues in module system design, including translucency, applicativity, generativity, and modules as first-class values. Our type system harmonizes design elements from previous work, resulting in a simple, economical account of modular programming. The main unifying principle is the treatment of abstraction mechanisms as computational effects. Our language is the first to provide a complete and practical formalization of all of these critical issues in module system design.}},
  url = {https://doi.org/10.1145/604131.604151},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup136,
  title = {A sound and complete abstraction for reasoning about parallel prefix sums}},
  author = {Chong, Nathan and Donaldson, Alastair F. and Ketema, Jeroen}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Prefix sums are key building blocks in the implementation of many concurrent software applications, and recently much work has gone into efficiently implementing prefix sums to run on massively parallel graphics processing units (GPUs). Because they lie at the heart of many GPU-accelerated applications, the correctness of prefix sum implementations is of prime importance.We introduce a novel abstraction, the interval of summations, that allows scalable reasoning about implementations of prefix sums. We present this abstraction as a monoid, and prove a soundness and completeness result showing that a generic sequential prefix sum implementation is correct for an array of length $n$ if and only if it computes the correct result for a specific test case when instantiated with the interval of summations monoid. This allows correctness to be established by running a single test where the input and result require O(n lg(n)) space. This improves upon an existing result by Sheeran where the input requires O(n lg(n)) space and the result O(n2 lg(n)) space, and is more feasible for large n than a method by Voigtlaender that uses O(n) space for the input and result but requires running O(n2) tests. We then extend our abstraction and results to the context of data-parallel programs, developing an automated verification method for GPU implementations of prefix sums. Our method uses static verification to prove that a generic prefix sum implementation is data race-free, after which functional correctness of the implementation can be determined by running a single test case under the interval of summations abstraction.We present an experimental evaluation using four different prefix sum algorithms, showing that our method is highly automatic, scales to large thread counts, and significantly outperforms Voigtlaender's method when applied to large arrays.}},
  url = {https://doi.org/10.1145/2535838.2535882},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup137,
  title = {Incremental inference for probabilistic programs}},
  author = {Cusumano-Towner, Marco and Bichsel, Benjamin and Gehr, Timon and Vechev, Martin and Mansinghka, Vikash K.}},
  year = {2018}},
  journal = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a novel approach for approximate sampling in probabilistic programs based on incremental inference. The key idea is to adapt the samples for a program P into samples for a program Q, thereby avoiding the expensive sampling computation for program Q. To enable incremental inference in probabilistic programming, our work: (i) introduces the concept of a trace translator which adapts samples from P into samples of Q, (ii) phrases this translation approach in the context of sequential Monte Carlo (SMC), which gives theoretical guarantees that the adapted samples converge to the distribution induced by Q, and (iii) shows how to obtain a concrete trace translator by establishing a correspondence between the random choices of the two probabilistic programs. We implemented our approach in two different probabilistic programming systems and showed that, compared to methods that sample the program Q from scratch, incremental inference can lead to orders of magnitude increase in efficiency, depending on how closely related P and Q are.}},
  url = {https://doi.org/10.1145/3192366.3192399},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup138,
  title = {A functional programmer's guide to homotopy type theory}},
  author = {Licata, Dan}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Dependent type theories are functional programming languages with types rich enough to do computer-checked mathematics and software verification. Homotopy type theory is a recent area of work that connects dependent type theory to the mathematical disciplines of homotopy theory and higher-dimensional category theory. From a programming point of view, these connections have revealed that all types in dependent type theory support a certain generic program that had not previously been exploited. Specifically, each type can be equipped with computationally relevant witnesses of equality of elements of that type, and all types support a generic program that transports elements along these equalities. One mechanism for equipping types with non-trivial witnesses of equality is Voevodsky’s univalence axiom, which implies that equality of types themselves is witnessed by type isomorphism. Another is higher inductive types, an extended datatype schema that allows identifications between different datatype constructors. While these new mechanisms were originally formulated as axiomatic extensions of type theory, recent work has investigated their computational meaning, leading to the development of new programming languages that better support them. In this talk, I will illustrate what univalence and higher inductive types mean in programming terms. I will also discuss how studying some related semantic settings can reveal additional structure on types; for example, moving from groupoids (categories where all maps are invertible) to general categories yields an account of coercions instead of equalities. Overall, I hope to convey some of the beauty and richness of these connections between disciplines, which we are just beginning to understand.}},
  url = {https://doi.org/10.1145/2951913.2976748},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup139,
  title = {Alphonse: incremental computation as a programming abstraction}},
  author = {Hoover, Roger}},
  year = {1992}},
  journal = {Proceedings of the ACM SIGPLAN 1992 Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Alphonse is a program transformation system that uses dynamic dependency analysis and incremental computation techniques to automatically generate efficient dynamic implementations from simple exhaustive imperative program specifications.}},
  url = {https://doi.org/10.1145/143095.143139},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup140,
  title = {APT: a data structure for optimal control dependence computation}},
  author = {Pingali, Keshav and Bilardi, Gianfranco}},
  year = {1995}},
  journal = {Proceedings of the ACM SIGPLAN 1995 Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The control dependence relation is used extensively in restructuring compilers. This relation is usually represented using the control dependence graph; unfortunately, the size of this data structure can be quadratic in the size of the program, even for some structured programs. In this paper, we introduce a data structure called the augmented post-dominator tree (APT) which is constructed in space and time proportional to the size of the program, and which can answer control dependence queries in time proportional to the size of the output. Therefore, APT is an optimal representation of control dependence. We also show that using APT, we can compute SSA graphs, as well as sparse dataflow evaluator graphs, in time proportional to the size of the program. Finally, we put APT in perspective by showing that it can be viewed as a factored representation of control dependence graph in which filtered search is used to answer queries.}},
  url = {https://doi.org/10.1145/207110.207114},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup141,
  title = {Abstraction and invariance for algebraically indexed types}},
  author = {Atkey, Robert and Johann, Patricia and Kennedy, Andrew}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Reynolds' relational parametricity provides a powerful way to reason about programs in terms of invariance under changes of data representation. A dazzling array of applications of Reynolds' theory exists, exploiting invariance to yield "free theorems", non-inhabitation results, and encodings of algebraic datatypes. Outside computer science, invariance is a common theme running through many areas of mathematics and physics. For example, the area of a triangle is unaltered by rotation or flipping. If we scale a triangle, then we scale its area, maintaining an invariant relationship between the two. The transformations under which properties are invariant are often organised into groups, with the algebraic structure reflecting the composability and invertibility of transformations.In this paper, we investigate programming languages whose types are indexed by algebraic structures such as groups of geometric transformations. Other examples include types indexed by principals--for information flow security--and types indexed by distances--for analysis of analytic uniform continuity properties. Following Reynolds, we prove a general Abstraction Theorem that covers all these instances. Consequences of our Abstraction Theorem include free theorems expressing invariance properties of programs, type isomorphisms based on invariance properties, and non-definability results indicating when certain algebraically indexed types are uninhabited or only inhabited by trivial programs. We have fully formalised our framework and most examples in Coq.}},
  url = {https://doi.org/10.1145/2429069.2429082},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup142,
  title = {Cryptographically sound implementations for typed information-flow security}},
  author = {Fournet, C\'{e}},
  year = {2008}},
  journal = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In language-based security, confidentiality and integrity policies conveniently specify the permitted flows of information between different parts of a program with diverse levels of trust. These policies enable a simple treatment of security, and they can often be verified by typing. However, their enforcement in concrete systems involves delicate compilation issues.We consider cryptographic enforcement mechanisms for imperative programs with untrusted components. Such programs may represent, for instance, distributed systems connected by some untrusted network. In source programs, security depends on an abstract access-control policy for reading and writing the shared memory. In their implementations, shared memory is unprotected and security depends instead on encryption and signing.We build a translation from well-typed source programs and policies to cryptographic implementations. To establish its correctness, we develop a type system for the target language. Our typing rules enforce a correct usage of cryptographic primitives against active adversaries; from an information-flow viewpoint, they capture controlled forms of robust declassification and endorsement. We showtype soundness for a variant of the non-interference property, then show that our translation preserves typability.We rely on concrete primitives and hypotheses for cryptography, stated in terms of probabilistic polynomial-time algorithms and games. We model these primitives as commands in our target language. Thus, we develop a uniform language-based model of security, ranging from computational non-interference for probabilistic programs down to standard cryptographic hypotheses.}},
  url = {https://doi.org/10.1145/1328438.1328478},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup143,
  title = {POP-PL: a patient-oriented prescription programming language}},
  author = {Florence, Spencer P. and Fetscher, Bruke and Flatt, Matthew and Temps, William H. and Kiguradze, Tina and West, Dennis P. and Niznik, Charlotte and Yarnold, Paul R. and Findler, Robert Bruce and Belknap, Steven M.}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Medical professionals have long used algorithmic thinking to describe and implement health care processes without the benefit of the conceptual framework provided by a programming language. Instead, medical algorithms are expressed using English, flowcharts, or data tables. This results in prescriptions that are difficult to understand, hard to debug, and awkward to reuse. This paper reports on the design and evaluation of a domain-specific programming language, POP-PL for expressing medical algorithms. The design draws on the experience of researchers in two disciplines, programming languages and medicine. The language is based around the idea that programs and humans have complementary strengths, that when combined can make for safer, more accurate performance of prescriptions. We implemented a prototype of our language and evaluated its design by writing prescriptions in the new language and administering a usability survey to medical professionals. This formative evaluation suggests that medical prescriptions can be conveyed by a programming language's mode of expression and provides useful information for refining the language. Analysis of the survey results suggests that medical professionals can understand and correctly modify programs in POP-PL.}},
  url = {https://doi.org/10.1145/2814204.2814221},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup144,
  title = {Complexity analysis and algorithm design for reorganizing data to minimize non-coalesced memory accesses on GPU}},
  author = {Wu, Bo and Zhao, Zhijia and Zhang, Eddy Zheng and Jiang, Yunlian and Shen, Xipeng}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The performance of Graphic Processing Units (GPU) is sensitive to irregular memory references. Some recent work shows the promise of data reorganization for eliminating non-coalesced memory accesses that are caused by irregular references. However, all previous studies have employed simple, heuristic methods to determine the new data layouts to create. As a result, they either do not provide any performance guarantee or are effective to only some limited scenarios. This paper contributes a fundamental study to the problem. It systematically analyzes the inherent complexity of the problem in various settings, and for the first time, proves that the problem is NP-complete. It then points out the limitations of existing techniques and reveals that in practice, the essence for designing an appropriate data reorganization algorithm can be reduced to a tradeoff among space, time, and complexity. Based on that insight, it develops two new data reorganization algorithms to overcome the limitations of previous methods. Experiments show that an assembly composed of the new algorithms and a previous algorithm can circumvent the inherent complexity in finding optimal data layouts, making it feasible to minimize non-coalesced memory accesses for a variety of irregular applications and settings that are beyond the reach of existing techniques.}},
  url = {https://doi.org/10.1145/2442516.2442523},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup145,
  title = {Dijkstra monads for free}},
  author = {Ahman, Danel and Hri\c{t}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Dijkstra monads enable a dependent type theory to be enhanced with support for specifying and verifying effectful code via weakest preconditions. Together with their closely related counterparts, Hoare monads, they provide the basis on which verification tools like F*, Hoare Type Theory (HTT), and Ynot are built. We show that Dijkstra monads can be derived "for free" by applying a continuation-passing style (CPS) translation to the standard monadic definitions of the underlying computational effects. Automatically deriving Dijkstra monads in this way provides a correct-by-construction and efficient way of reasoning about user-defined effects in dependent type theories. We demonstrate these ideas in EMF*, a new dependently typed calculus, validating it via both formal proof and a prototype implementation within F*. Besides equipping F* with a more uniform and extensible effect system, EMF* enables a novel mixture of intrinsic and extrinsic proofs within F*.}},
  url = {https://doi.org/10.1145/3009837.3009878},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup146,
  title = {An Architecture Supporting Formal and Compositional Binary Analysis}},
  author = {McMahan, Joseph and Christensen, Michael and Nichols, Lawton and Roesch, Jared and Guo, Sung-Yee and Hardekopf, Ben and Sherwood, Timothy}},
  year = {2017}},
  journal = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Building a trustworthy life-critical embedded system requires deep reasoning about the potential effects that sequences of machine instructions can have on full system operation. Rather than trying to analyze complete binaries and the countless ways their instructions can interact with one another --- memory, side effects, control registers, implicit state, etc. --- we explore a new approach. We propose an architecture controlled by a thin computational layer designed to tightly correspond with the lambda calculus, drawing on principles of functional programming to bring the assembly much closer to myriad reasoning frameworks, such as the Coq proof assistant. This approach allows assembly-level verified versions of critical code to operate safely in tandem with arbitrary code, including imperative and unverified system components, without the need for large supporting trusted computing bases. We demonstrate that this computational layer can be built in such a way as to simultaneously provide full programmability and compact, precise, and complete semantics, while still using hardware resources comparable to normal embedded systems. To demonstrate the practicality of this approach, our FPGA-implemented prototype runs an embedded medical application which monitors and treats life-threatening arrhythmias. Though the system integrates untrusted and imperative components, our architecture allows for the formal verification of multiple properties of the end-to-end system, including a proof of correctness of the assembly-level implementation of the core algorithm, the integrity of trusted data via a non-interference proof, and a guarantee that our prototype meets critical timing requirements.}},
  url = {https://doi.org/10.1145/3037697.3037733},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup147,
  title = {Applying quantitative semantics to higher-order quantum computing}},
  author = {Pagani, Michele and Selinger, Peter and Valiron, Beno\^{\i}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Finding a denotational semantics for higher order quantum computation is a long-standing problem in the semantics of quantum programming languages. Most past approaches to this problem fell short in one way or another, either limiting the language to an unusably small finitary fragment, or giving up important features of quantum physics such as entanglement. In this paper, we propose a denotational semantics for a quantum lambda calculus with recursion and an infinite data type, using constructions from quantitative semantics of linear logic.}},
  url = {https://doi.org/10.1145/2535838.2535879},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup148,
  title = {A logical account of pspace}},
  author = {Gaboardi, Marco and Marion, Jean-Yves and Della Rocca, Simona Ronchi}},
  year = {2008}},
  journal = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We propose a characterization of PSPACE by means of atype assignment for an extension of lambda calculus with a conditional construction. The type assignment STAB is an extension of STA, a type assignment for lambda-calculus inspired by Lafont's Soft Linear Logic.We extend STA by means of a ground type and terms for booleans. The key point is that the elimination rule for booleans is managed in an additive way. Thus, we are able to program polynomial time Alternating Turing Machines. Conversely, we introduce a call-by-name evaluation machine in order tocompute programs in polynomial space. As far as we know, this is the first characterization of PSPACE which is based on lambda calculusand light logics.}},
  url = {https://doi.org/10.1145/1328438.1328456},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup149,
  title = {First-class effect reflection for effect-guided programming}},
  author = {Long, Yuheng and Liu, Yu David and Rajan, Hridesh}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper introduces a novel type-and-effect calculus, first-class effects, where the computational effect of an expression can be programmatically reflected, passed around as values, and analyzed at run time. A broad range of designs "hard-coded" in existing effect-guided analyses — from thread scheduling, version-consistent software updating, to data zeroing — can be naturally supported through the programming abstractions. The core technical development is a type system with a number of features, including a hybrid type system that integrates static and dynamic effect analyses, a refinement type system to verify application-specific effect management properties, a double-bounded type system that computes both over-approximation of effects and their under-approximation. We introduce and establish a notion of soundness called trace consistency, defined in terms of how the effect and trace correspond. The property sheds foundational insight on "good" first-class effect programming.}},
  url = {https://doi.org/10.1145/2983990.2984037},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup150,
  title = {A syntactic approach to program transformations}},
  author = {Ariola, Zena M.}},
  year = {1991}},
  journal = {Proceedings of the 1991 ACM SIGPLAN Symposium on Partial Evaluation and Semantics-Based Program Manipulation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {},
  url = {https://doi.org/10.1145/115865.115878},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup151,
  title = {High-performance genomic analysis framework with in-memory computing}},
  author = {Li, Xueqi and Tan, Guangming and Wang, Bingchen and Sun, Ninghui}},
  year = {2018}},
  journal = {Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this paper, we propose an in-memory computing framework (called GPF) that provides a set of genomic formats, APIs and a fast genomic engine for large-scale genomic data processing. Our GPF comprises two main components: (1) scalable genomic data formats and API. (2) an advanced execution engine that supports efficient compression of genomic data and eliminates redundancies in the execution engine of our GPF. We further present both system and algorithm-specific implementations for users to build genomic analysis pipeline without any acquaintance of Spark parallel programming. To test the performance of GPF, we built a WGS pipeline on top of our GPF as a test case. Our experimental data indicate that GPF completes Whole-Genome-Sequencing (WGS) analysis of 146.9G bases Human Platinum Genome in running time of 24 minutes, with over 50\% parallel efficiency when used on 2048 CPU cores. Together, our GPF framework provides a fast and general engine for large-scale genomic data processing which supports in-memory computing.}},
  url = {https://doi.org/10.1145/3178487.3178511},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup152,
  title = {A design and verification methodology for secure isolated regions}},
  author = {Sinha, Rohit and Costa, Manuel and Lal, Akash and Lopes, Nuno P. and Rajamani, Sriram and Seshia, Sanjit A. and Vaswani, Kapil}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Hardware support for isolated execution (such as Intel SGX) enables development of applications that keep their code and data confidential even while running in a hostile or compromised host. However, automatically verifying that such applications satisfy confidentiality remains challenging. We present a methodology for designing such applications in a way that enables certifying their confidentiality. Our methodology consists of forcing the application to communicate with the external world through a narrow interface, compiling it with runtime checks that aid verification, and linking it with a small runtime that implements the narrow interface. The runtime includes services such as secure communication channels and memory management. We formalize this restriction on the application as Information Release Confinement (IRC), and we show that it allows us to decompose the task of proving confidentiality into (a) one-time, human-assisted functional verification of the runtime to ensure that it does not leak secrets, (b) automatic verification of the application's machine code to ensure that it satisfies IRC and does not directly read or corrupt the runtime's internal state. We present /CONFIDENTIAL: a verifier for IRC that is modular, automatic, and keeps our compiler out of the trusted computing base. Our evaluation suggests that the methodology scales to real-world applications.}},
  url = {https://doi.org/10.1145/2908080.2908113},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup153,
  title = {Optimizing N-dimensional, winograd-based convolution for manycore CPUs}},
  author = {Jia, Zhen and Zlateski, Aleksandar and Durand, Fredo and Li, Kai}},
  year = {2018}},
  journal = {Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Recent work on Winograd-based convolution allows for a great reduction of computational complexity, but existing implementations are limited to 2D data and a single kernel size of 3 by 3. They can achieve only slightly better, and often worse performance than better optimized, direct convolution implementations. We propose and implement an algorithm for N-dimensional Winograd-based convolution that allows arbitrary kernel sizes and is optimized for manycore CPUs. Our algorithm achieves high hardware utilization through a series of optimizations. Our experiments show that on modern ConvNets, our optimized implementation, is on average more than 3 x, and sometimes 8 x faster than other state-of-the-art CPU implementations on an Intel Xeon Phi manycore processors. Moreover, our implementation on the Xeon Phi achieves competitive performance for 2D ConvNets and superior performance for 3D ConvNets, compared with the best GPU implementations.}},
  url = {https://doi.org/10.1145/3178487.3178496},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup154,
  title = {Gloss: Seamless Live Reconfiguration and Reoptimization of Stream Programs}},
  author = {Rajadurai, Sumanaruban and Bosboom, Jeffrey and Wong, Weng-Fai and Amarasinghe, Saman}},
  year = {2018}},
  journal = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {An important class of applications computes on long-running or infinite streams of data, often with known fixed data rates. The latter is referred to as synchronous data flow ~(SDF) streams. These stream applications need to run on clusters or the cloud due to the high performance requirement. Further, they require live reconfiguration and reoptimization for various reasons such as hardware maintenance, elastic computation, or to respond to fluctuations in resources or application workload. However, reconfiguration and reoptimization without downtime while accurately preserving program state in a distributed environment is difficult. In this paper, we introduce Gloss, a suite of compiler and runtime techniques for live reconfiguration of distributed stream programs. Gloss, for the first time, avoids periods of zero throughput during the reconfiguration of both stateless and stateful SDF based stream programs. Furthermore, unlike other systems, Gloss globally reoptimizes and completely recompiles the program during reconfiguration. This permits it to reoptimize the application for entirely new configurations that it may not have encountered before. All these Gloss operations happen in-situ, requiring no extra hardware resources. We show how Gloss allows stream programs to reconfigure and reoptimize with no downtime and minimal overhead, and demonstrate the wider applicability of it via a variety of experiments.}},
  url = {https://doi.org/10.1145/3173162.3173170},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup155,
  title = {Functional pearl: getting a quick fix on comonads}},
  author = {Foner, Kenneth}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A piece of functional programming folklore due to Piponi provides L\"{o}},
  url = {https://doi.org/10.1145/2804302.2804310},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup156,
  title = {High performance model based image reconstruction}},
  author = {Wang, Xiao and Sabne, Amit and Kisner, Sherman and Raghunathan, Anand and Bouman, Charles and Midkiff, Samuel}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Computed Tomography (CT) Image Reconstruction is an important technique used in a wide range of applications, ranging from explosive detection, medical imaging to scientific imaging. Among available reconstruction methods, Model Based Iterative Reconstruction (MBIR) produces higher quality images and allows for the use of more general CT scanner geometries than is possible with more commonly used methods. The high computational cost of MBIR, however, often makes it impractical in applications for which it would otherwise be ideal. This paper describes a new MBIR implementation that significantly reduces the computational cost of MBIR while retaining its benefits. It describes a novel organization of the scanner data into super-voxels (SV) that, combined with a super-voxel buffer (SVB), dramatically increase locality and prefetching, enable parallelism across SVs and lead to an average speedup of 187 on 20 cores.}},
  url = {https://doi.org/10.1145/2851141.2851163},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup157,
  title = {Type-directed automatic incrementalization}},
  author = {Chen, Yan and Dunfield, Jana and Acar, Umut A.}},
  year = {2012}},
  journal = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Application data often changes slowly or incrementally over time. Since incremental changes to input often result in only small changes in output, it is often feasible to respond to such changes asymptotically more efficiently than by re-running the whole computation. Traditionally, realizing such asymptotic efficiency improvements requires designing problem-specific algorithms known as dynamic or incremental algorithms, which are often significantly more complicated than conventional algorithms to design, analyze, implement, and use. A long-standing open problem is to develop techniques that automatically transform conventional programs so that they correctly and efficiently respond to incremental changes.In this paper, we describe a significant step towards solving the problem of automatic incrementalization: a programming language and a compiler that can, given a few type annotations describing what can change over time, compile a conventional program that assumes its data to be static (unchanging over time) to an incremental program. Based on recent advances in self-adjusting computation, including a theoretical proposal for translating purely functional programs to self-adjusting programs, we develop techniques for translating conventional Standard ML programs to self-adjusting programs. By extending the Standard ML language, we design a fully featured programming language with higher-order features, a module system, and a powerful type system, and implement a compiler for this language. The resulting programming language, LML, enables translating conventional programs decorated with simple type annotations into incremental programs that can respond to changes in their data correctly and efficiently.We evaluate the effectiveness of our approach by considering a range of benchmarks involving lists, vectors, and matrices, as well as a ray tracer. For these benchmarks, our compiler incrementalizes existing code with only trivial amounts of annotation. The resulting programs are often asymptotically more efficient, leading to orders of magnitude speedups in practice.}},
  url = {https://doi.org/10.1145/2254064.2254100},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup158,
  title = {Garbage collecting the world: one car at a time}},
  author = {Hudson, Richard L. and Morrison, Ron and Moss, J. Eliot B. and Munro, David S.}},
  year = {1997}},
  journal = {Proceedings of the 12th ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A new garbage collection algorithm for distributed object systems, called DMOS (Distributed. Mature Object Space), is presented. It is derived from two previous algorithms, MOS (Mature Object Space), sometimes called the train algorithm, and PMOS (Persistent Mature Object Space). The contribution of DMOS is that it provides the following unique combination of properties for a distributed collector: safety, completeness, non-disruptiveness, incrementality, and scalability. Furthermore, the DMOS collector is non-blocking and does not use global tracing.}},
  url = {https://doi.org/10.1145/263698.264353},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup159,
  title = {A peta-scalable CPU-GPU algorithm for global atmospheric simulations}},
  author = {Yang, Chao and Xue, Wei and Fu, Haohuan and Gan, Lin and Li, Linfeng and Xu, Yangtong and Lu, Yutong and Sun, Jiachang and Yang, Guangwen and Zheng, Weimin}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Developing highly scalable algorithms for global atmospheric modeling is becoming increasingly important as scientists inquire to understand behaviors of the global atmosphere at extreme scales. Nowadays, heterogeneous architecture based on both processors and accelerators is becoming an important solution for large-scale computing. However, large-scale simulation of the global atmosphere brings a severe challenge to the development of highly scalable algorithms that fit well into state-of-the-art heterogeneous systems. Although successes have been made on GPU-accelerated computing in some top-level applications, studies on fully exploiting heterogeneous architectures in global atmospheric modeling are still very less to be seen, due in large part to both the computational difficulties of the mathematical models and the requirement of high accuracy for long term simulations.In this paper, we propose a peta-scalable hybrid algorithm that is successfully applied in a cubed-sphere shallow-water model in global atmospheric simulations. We employ an adjustable partition between CPUs and GPUs to achieve a balanced utilization of the entire hybrid system, and present a pipe-flow scheme to conduct conflict-free inter-node communication on the cubed-sphere geometry and to maximize communication-computation overlap. Systematic optimizations for multithreading on both GPU and CPU sides are performed to enhance computing throughput and improve memory efficiency. Our experiments demonstrate nearly ideal strong and weak scalabilities on up to 3,750 nodes of the Tianhe-1A. The largest run sustains a performance of 0.8 Pflops in double precision (32\% of the peak performance), using 45,000 CPU cores and 3,750 GPUs.}},
  url = {https://doi.org/10.1145/2442516.2442518},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup160,
  title = {'Cause I'm strong enough: Reasoning about consistency choices in distributed systems}},
  author = {Gotsman, Alexey and Yang, Hongseok and Ferreira, Carla and Najafzadeh, Mahsa and Shapiro, Marc}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Large-scale distributed systems often rely on replicated databases that allow a programmer to request different data consistency guarantees for different operations, and thereby control their performance. Using such databases is far from trivial: requesting stronger consistency in too many places may hurt performance, and requesting it in too few places may violate correctness. To help programmers in this task, we propose the first proof rule for establishing that a particular choice of consistency guarantees for various operations on a replicated database is enough to ensure the preservation of a given data integrity invariant. Our rule is modular: it allows reasoning about the behaviour of every operation separately under some assumption on the behaviour of other operations. This leads to simple reasoning, which we have automated in an SMT-based tool. We present a nontrivial proof of soundness of our rule and illustrate its use on several examples.}},
  url = {https://doi.org/10.1145/2837614.2837625},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup161,
  title = {Constructive Galois connections: taming the Galois connection framework for mechanized metatheory}},
  author = {Darais, David and Van Horn, David}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Galois connections are a foundational tool for structuring abstraction in semantics and their use lies at the heart of the theory of abstract interpretation. Yet, mechanization of Galois connections remains limited to restricted modes of use, preventing their general application in mechanized metatheory and certified programming. This paper presents constructive Galois connections, a variant of Galois connections that is effective both on paper and in proof assistants; is complete with respect to a large subset of classical Galois connections; and enables more general reasoning principles, including the "calculational" style advocated by Cousot. To design constructive Galois connection we identify a restricted mode of use of classical ones which is both general and amenable to mechanization in dependently-typed functional programming languages. Crucial to our metatheory is the addition of monadic structure to Galois connections to control a "specification effect". Effectful calculations may reason classically, while pure calculations have extractable computational content. Explicitly moving between the worlds of specification and implementation is enabled by our metatheory. To validate our approach, we provide two case studies in mechanizing existing proofs from the literature: one uses calculational abstract interpretation to design a static analyzer, the other forms a semantic basis for gradual typing. Both mechanized proofs closely follow their original paper-and-pencil counterparts, employ reasoning principles not captured by previous mechanization approaches, support the extraction of verified algorithms, and are novel.}},
  url = {https://doi.org/10.1145/2951913.2951934},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup162,
  title = {Contextual effects for version-consistent dynamic software updating and safe concurrent programming}},
  author = {Neamtiu, Iulian and Hicks, Michael and Foster, Jeffrey S. and Pratikakis, Polyvios}},
  year = {2008}},
  journal = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents a generalization of standard effect systems that we call contextual effects. A traditional effect system computes the effect of an expression e. Our system additionally computes the effects of the computational context in which e occurs. More specifically, we computethe effect of the computation that has already occurred(the prior effect) and the effect of the computation yet to take place (the future effect).Contextual effects are useful when the past or future computation of the program is relevant at various program points. We present two substantial examples. First, we show how prior and future effects can be used to enforce transactional version consistency(TVC), a novel correctness property for dynamic software updates. TV Censures that programmer-designated transactional code blocks appear to execute entirely at the same code version, even if a dynamic update occurs in the middle of the block. Second, we show how future effects can be used in the analysis of multi-threaded programs to find thread-shared locations. This is an essential step in applications such as data race detection.}},
  url = {https://doi.org/10.1145/1328438.1328447},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup163,
  title = {On Characterizing the Data Access Complexity of Programs}},
  author = {Elango, Venmugil and Rastello, Fabrice and Pouchet, Louis-No\"{e}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Technology trends will cause data movement to account for the majority of energy expenditure and execution time on emerging computers. Therefore, computational complexity will no longer be a sufficient metric for comparing algorithms, and a fundamental characterization of data access complexity will be increasingly important. The problem of developing lower bounds for data access complexity has been modeled using the formalism of Hong and Kung's red/blue pebble game for computational directed acyclic graphs (CDAGs). However, previously developed approaches to lower bounds analysis for the red/blue pebble game are very limited in effectiveness when applied to CDAGs of real programs, with computations comprised of multiple sub-computations with differing DAG structure. We address this problem by developing an approach for effectively composing lower bounds based on graph decomposition. We also develop a static analysis algorithm to derive the asymptotic data-access lower bounds of programs, as a function of the problem size and cache size.}},
  url = {https://doi.org/10.1145/2676726.2677010},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup164,
  title = {Isoefficiency in Practice: Configuring and Understanding the Performance of Task-based Applications}},
  author = {Shudler, Sergei and Calotoiu, Alexandru and Hoefler, Torsten and Wolf, Felix}},
  year = {2017}},
  journal = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Task-based programming offers an elegant way to express units of computation and the dependencies among them, making it easier to distribute the computational load evenly across multiple cores. However, this separation of problem decomposition and parallelism requires a sufficiently large input problem to achieve satisfactory efficiency on a given number of cores. Unfortunately, finding a good match between input size and core count usually requires significant experimentation, which is expensive and sometimes even impractical. In this paper, we propose an automated empirical method for finding the isoefficiency function of a task-based program, binding efficiency, core count, and the input size in one analytical expression. This allows the latter two to be adjusted according to given (realistic) efficiency objectives. Moreover, we not only find (i) the actual isoefficiency function but also (ii) the function one would yield if the program execution was free of resource contention and (iii) an upper bound that could only be reached if the program was able to maintain its average parallelism throughout its execution. The difference between the three helps to explain low efficiency, and in particular, it helps to differentiate between resource contention and structural conflicts related to task dependencies or scheduling. The insights gained can be used to co-design programs and shared system resources.}},
  url = {https://doi.org/10.1145/3018743.3018770},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup165,
  title = {Fast polyhedra abstract domain}},
  author = {Singh, Gagandeep and P\"{u}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Numerical abstract domains are an important ingredient of modern static analyzers used for verifying critical program properties (e.g., absence of buffer overflow or memory safety). Among the many numerical domains introduced over the years, Polyhedra is the most expressive one, but also the most expensive: it has worst-case exponential space and time complexity. As a consequence, static analysis with the Polyhedra domain is thought to be impractical when applied to large scale, real world programs. In this paper, we present a new approach and a complete implementation for speeding up Polyhedra domain analysis. Our approach does not lose precision, and for many practical cases, is orders of magnitude faster than state-of-the-art solutions. The key insight underlying our work is that polyhedra arising during analysis can usually be kept decomposed, thus considerably reducing the overall complexity. We first present the theory underlying our approach, which identifies the interaction between partitions of variables and domain operators. Based on the theory we develop new algorithms for these operators that work with decomposed polyhedra. We implemented these algorithms using the same interface as existing libraries, thus enabling static analyzers to use our implementation with little effort. In our evaluation, we analyze large benchmarks from the popular software verification competition, including Linux device drivers with over 50K lines of code. Our experimental results demonstrate massive gains in both space and time: we show end-to-end speedups of two to five orders of magnitude compared to state-of-the-art Polyhedra implementations as well as significant memory gains, on all larger benchmarks. In fact, in many cases our analysis terminates in seconds where prior code runs out of memory or times out after 4 hours. We believe this work is an important step in making the Polyhedra abstract domain both feasible and practically usable for handling large, real-world programs.}},
  url = {https://doi.org/10.1145/3009837.3009885},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup166,
  title = {Higher Inductive Types as Homotopy-Initial Algebras}},
  author = {Sojakova, Kristina}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Homotopy Type Theory is a new field of mathematics based on the recently-discovered correspondence between Martin-L\"{o}},
  url = {https://doi.org/10.1145/2676726.2676983},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup167,
  title = {Higher-order ghost state}},
  author = {Jung, Ralf and Krebbers, Robbert and Birkedal, Lars and Dreyer, Derek}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The development of concurrent separation logic (CSL) has sparked a long line of work on modular verification of sophisticated concurrent programs. Two of the most important features supported by several existing extensions to CSL are higher-order quantification and custom ghost state. However, none of the logics that support both of these features reap the full potential of their combination. In particular, none of them provide general support for a feature we dub "higher-order ghost state": the ability to store arbitrary higher-order separation-logic predicates in ghost variables. In this paper, we propose higher-order ghost state as a interesting and useful extension to CSL, which we formalize in the framework of Jung et al.'s recently developed Iris logic. To justify its soundness, we develop a novel algebraic structure called CMRAs ("cameras"), which can be thought of as "step-indexed partial commutative monoids". Finally, we show that Iris proofs utilizing higher-order ghost state can be effectively formalized in Coq, and discuss the challenges we faced in formalizing them.}},
  url = {https://doi.org/10.1145/2951913.2951943},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup168,
  title = {Typed syntactic meta-programming}},
  author = {Devriese, Dominique and Piessens, Frank}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a novel set of meta-programming primitives for use in a dependently-typed functional language. The types of our meta-programs provide strong and precise guarantees about their termination, correctness and completeness. Our system supports type-safe construction and analysis of terms, types and typing contexts. Unlike alternative approaches, they are written in the same style as normal programs and use the language's standard functional computational model. We formalise the new meta-programming primitives, implement them as an extension of Agda, and provide evidence of usefulness by means of two compelling applications in the fields of datatype-generic programming and proof tactics.}},
  url = {https://doi.org/10.1145/2500365.2500575},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup169,
  title = {Compositional semantics for composable continuations: from abortive to delimited control}},
  author = {Downen, Paul and Ariola, Zena M.}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Parigot's λμ-calculus, a system for computational reasoning about classical proofs, serves as a foundation for control operations embodied by operators like Scheme's callcc. We demonstrate that the call-by-value theory of the λμ-calculus contains a latent theory of delimited control, and that a known variant of λμ which unshackles the syntax yields a calculus of composable continuations from the existing constructs and rules for classical control. To relate to the various formulations of control effects, and to continuation-passing style, we use a form of compositional program transformations which preserves the underlying structure of equational theories, contexts, and substitution. Finally, we generalize the call-by-name and call-by-value theories of the λμ-calculus by giving a single parametric theory that encompasses both, allowing us to generate a call-by-need instance that defines a calculus of classical and delimited control with lazy evaluation and sharing.}},
  url = {https://doi.org/10.1145/2628136.2628147},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup170,
  title = {Simplifying reductions}},
  author = {Gautam and Rajopadhye, S.}},
  year = {2006}},
  journal = {Conference Record of the 33rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present optimization techniques for high level equational programs that are generalizations of affine control loops (ACLs). Significant parts of the SpecFP and PerfectClub benchmarks are ACLs. They often contain reductions: associative and commutative operators applied to a collection of values. They also often exhibit reuse: intermediate values computed or used at different index points being identical. We develop various techniques to automatically exploit reuse to simplify the computational complexity of evaluating reductions. Finally, we present an algorithm for the optimal application of such simplifications resulting in an equivalent specification with minimum complexity.}},
  url = {https://doi.org/10.1145/1111037.1111041},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup171,
  title = {SaberLDA: Sparsity-Aware Learning of Topic Models on GPUs}},
  author = {Li, Kaiwei and Chen, Jianfei and Chen, Wenguang and Zhu, Jun}},
  year = {2017}},
  journal = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Latent Dirichlet Allocation (LDA) is a popular tool for analyzing discrete count data such as text and images. Applications require LDA to handle both large datasets and a large number of topics. Though distributed CPU systems have been used, GPU-based systems have emerged as a promising alternative because of the high computational power and memory bandwidth of GPUs. However, existing GPU-based LDA systems cannot support a large number of topics because they use algorithms on dense data structures whose time and space complexity is linear to the number of topics.In this paper, we propose SaberLDA, a GPU-based LDA system that implements a sparsity-aware algorithm to achieve sublinear time complexity and scales well to learn a large number of topics. To address the challenges introduced by sparsity, we propose a novel data layout, a new warp-based sampling kernel, and an efficient sparse count matrix updating algorithm that improves locality, makes efficient utilization of GPU warps, and reduces memory consumption. Experiments show that SaberLDA can learn from billions-token-scale data with up to 10,000 topics, which is almost two orders of magnitude larger than that of the previous GPU-based systems. With a single GPU card, SaberLDA is able to learn 10,000 topics from a dataset of billions of tokens in a few hours, which is only achievable with clusters with tens of machines before.}},
  url = {https://doi.org/10.1145/3037697.3037740},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup172,
  title = {Stochastic invariants for probabilistic termination}},
  author = {Chatterjee, Krishnendu and Novotn\'{y}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Termination is one of the basic liveness properties, and we study the termination problem for probabilistic programs with real-valued variables. Previous works focused on the qualitative problem that asks whether an input program terminates with probability 1 (almost-sure termination). A powerful approach for this qualitative problem is the notion of ranking supermartingales with respect to a given set of invariants. The quantitative problem (probabilistic termination) asks for bounds on the termination probability, and this problem has not been addressed yet. A fundamental and conceptual drawback of the existing approaches to address probabilistic termination is that even though the supermartingales consider the probabilistic behaviour of the programs, the invariants are obtained completely ignoring the probabilistic aspect (i.e., the invariants are obtained considering all behaviours with no information about the probability). In this work we address the probabilistic termination problem for linear-arithmetic probabilistic programs with nondeterminism. We formally define the notion of stochastic invariants, which are constraints along with a probability bound that the constraints hold. We introduce a concept of repulsing supermartingales. First, we show that repulsing supermartingales can be used to obtain bounds on the probability of the stochastic invariants. Second, we show the effectiveness of repulsing supermartingales in the following three ways: (1) With a combination of ranking and repulsing supermartingales we can compute lower bounds on the probability of termination; (2) repulsing supermartingales provide witnesses for refutation of almost-sure termination; and (3) with a combination of ranking and repulsing supermartingales we can establish persistence properties of probabilistic programs. Along with our conceptual contributions, we establish the following computational results: First, the synthesis of a stochastic invariant which supports some ranking supermartingale and at the same time admits a repulsing supermartingale can be achieved via reduction to the existential first-order theory of reals, which generalizes existing results from the non-probabilistic setting. Second, given a program with "strict invariants" (e.g., obtained via abstract interpretation) and a stochastic invariant, we can check in polynomial time whether there exists a linear repulsing supermartingale w.r.t. the stochastic invariant (via reduction to LP). We also present experimental evaluation of our approach on academic examples.}},
  url = {https://doi.org/10.1145/3009837.3009873},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup173,
  title = {Coming to terms with quantified reasoning}},
  author = {Kov\'{a}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The theory of finite term algebras provides a natural framework to describe the semantics of functional languages. The ability to efficiently reason about term algebras is essential to automate program analysis and verification for functional or imperative programs over inductively defined data types such as lists and trees. However, as the theory of finite term algebras is not finitely axiomatizable, reasoning about quantified properties over term algebras is challenging. In this paper we address full first-order reasoning about properties of programs manipulating term algebras, and describe two approaches for doing so by using first-order theorem proving. Our first method is a conservative extension of the theory of term alge- bras using a finite number of statements, while our second method relies on extending the superposition calculus of first-order theorem provers with additional inference rules. We implemented our work in the first-order theorem prover Vampire and evaluated it on a large number of inductive datatype benchmarks, as well as game theory constraints. Our experimental results show that our methods are able to find proofs for many hard problems previously unsolved by state-of-the-art methods. We also show that Vampire implementing our methods outperforms existing SMT solvers able to deal with inductive data types.}},
  url = {https://doi.org/10.1145/3009837.3009887},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup174,
  title = {Monitoring refinement via symbolic reasoning}},
  author = {Emmi, Michael and Enea, Constantin and Hamza, Jad}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Efficient implementations of concurrent objects such as semaphores, locks, and atomic collections are essential to modern computing. Programming such objects is error prone: in minimizing the synchronization overhead between concurrent object invocations, one risks the conformance to reference implementations — or in formal terms, one risks violating observational refinement. Precisely testing this refinement even within a single execution is intractable, limiting existing approaches to executions with very few object invocations. We develop scalable and effective algorithms for detecting refinement violations. Our algorithms are founded on incremental, symbolic reasoning, and exploit foundational insights into the refinement-checking problem. Our approach is sound, in that we detect only actual violations, and scales far beyond existing violation-detection algorithms. Empirically, we find that our approach is practically complete, in that we detect the violations arising in actual executions.}},
  url = {https://doi.org/10.1145/2737924.2737983},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup175,
  title = {Reasoning with the HERMIT: tool support for equational reasoning on GHC core programs}},
  author = {Farmer, Andrew and Sculthorpe, Neil and Gill, Andy}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A benefit of pure functional programming is that it encourages equational reasoning. However, the Haskell language has lacked direct tool support for such reasoning. Consequently, reasoning about Haskell programs is either performed manually, or in another language that does provide tool support (e.g. Agda or Coq). HERMIT is a Haskell-specific toolkit designed to support equational reasoning and user-guided program transformation, and to do so as part of the GHC compilation pipeline. This paper describes HERMIT's recently developed support for equational reasoning, and presents two case studies of HERMIT usage: checking that type-class laws hold for specific instance declarations, and mechanising textbook equational reasoning.}},
  url = {https://doi.org/10.1145/2804302.2804303},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup176,
  title = {A Multicore Path to Connectomics-on-Demand}},
  author = {Matveev, Alexander and Meirovitch, Yaron and Saribekyan, Hayk and Jakubiuk, Wiktor and Kaler, Tim and Odor, Gergely and Budden, David and Zlateski, Aleksandar and Shavit, Nir}},
  year = {2017}},
  journal = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The current design trend in large scale machine learning is to use distributed clusters of CPUs and GPUs with MapReduce-style programming. Some have been led to believe that this type of horizontal scaling can reduce or even eliminate the need for traditional algorithm development, careful parallelization, and performance engineering. This paper is a case study showing the contrary: that the benefits of algorithms, parallelization, and performance engineering, can sometimes be so vast that it is possible to solve "cluster-scale" problems on a single commodity multicore machine.Connectomics is an emerging area of neurobiology that uses cutting edge machine learning and image processing to extract brain connectivity graphs from electron microscopy images. It has long been assumed that the processing of connectomics data will require mass storage, farms of CPU/GPUs, and will take months (if not years) of processing time. We present a high-throughput connectomics-on-demand system that runs on a multicore machine with less than 100 cores and extracts connectomes at the terabyte per hour pace of modern electron microscopes.}},
  url = {https://doi.org/10.1145/3018743.3018766},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup177,
  title = {The transactional memory / garbage collection analogy}},
  author = {Grossman, Dan}},
  year = {2007}},
  journal = {Proceedings of the 22nd Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This essay presents remarkable similarities between transactional memory and garbage collection. The connections are fascinating in their own right, and they let us better understand one technology by thinking about the corresponding issues for the other.}},
  url = {https://doi.org/10.1145/1297027.1297080},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup178,
  title = {Fresh-register automata}},
  author = {Tzevelekos, Nikos}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {What is a basic automata-theoretic model of computation with names and fresh-name generation? We introduce Fresh-Register Automata (FRA), a new class of automata which operate on an infinite alphabet of names and use a finite number of registers to store fresh names, and to compare incoming names with previously stored ones. These finite machines extend Kaminski and Francez's Finite-Memory Automata by being able to recognise globally fresh inputs, that is, names fresh in the whole current run. We examine the expressivity of FRA's both from the aspect of accepted languages and of bisimulation equivalence. We establish primary properties and connections between automata of this kind, and answer key decidability questions. As a demonstrating example, we express the theory of the pi-calculus in FRA's and characterise bisimulation equivalence by an appropriate, and decidable in the finitary case, notion in these automata.}},
  url = {https://doi.org/10.1145/1926385.1926420},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup179,
  title = {Consistency analysis of decision-making programs}},
  author = {Chaudhuri, Swarat and Farzan, Azadeh and Kincaid, Zachary}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Applications in many areas of computing make discrete decisions under uncertainty, for reasons such as limited numerical precision in calculations and errors in sensor-derived inputs. As a result, individual decisions made by such programs may be nondeterministic, and lead to contradictory decisions at different points of an execution. This means that an otherwise correct program may execute along paths, that it would not follow under its ideal semantics, violating essential program invariants on the way. A program is said to be consistent if it does not suffer from this problem despite uncertainty in decisions.In this paper, we present a sound, automatic program analysis for verifying that a program is consistent in this sense. Our analysis proves that each decision made along a program execution is consistent with the decisions made earlier in the execution. The proof is done by generating an invariant that abstracts the set of all decisions made along executions that end at a program location l, then verifying, using a fixpoint constraint-solver, that no contradiction can be derived when these decisions are combined with new decisions made at l.We evaluate our analysis on a collection of programs implementing algorithms in computational geometry. Consistency is known to be a critical, frequently-violated, and thoroughly studied correctness property in geometry, but ours is the first attempt at automated verification of consistency of geometric algorithms. Our benchmark suite consists of implementations of convex hull computation, triangulation, and point location algorithms. On almost all examples that are not consistent (with two exceptions), our analysis is able to verify consistency within a few minutes.}},
  url = {https://doi.org/10.1145/2535838.2535858},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup180,
  title = {Hoare-style reasoning with (algebraic) continuations}},
  author = {Delbianco, Germ\'{a}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Continuations are programming abstractions that allow for manipulating the "future" of a computation. Amongst their many applications, they enable implementing unstructured program flow through higher-order control operators such as callcc. In this paper we develop a Hoare-style logic for the verification of programs with higher-order control, in the presence of dynamic state. This is done by designing a dependent type theory with first class callcc and abort operators, where pre- and postconditions of programs are tracked through types. Our operators are algebraic in the sense of Plotkin and Power, and Jaskelioff, to reduce the annotation burden and enable verification by symbolic evaluation. We illustrate working with the logic by verifying a number of characteristic examples.}},
  url = {https://doi.org/10.1145/2500365.2500593},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup181,
  title = {Abstracting abstract control}},
  author = {Glaze, Dionna and Van Horn, David}},
  year = {2014}},
  journal = {Proceedings of the 10th ACM Symposium on Dynamic Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The strength of a dynamic language is also its weakness: run-time flexibility comes at the cost of compile-time predictability. Many of the hallmarks of dynamic languages such as closures, continuations, various forms of reflection, and a lack of static types make many programmers rejoice, while compiler writers, tool developers, and verification engineers lament. The dynamism of these features simply confounds statically reasoning about programs that use them. Consequently, static analyses for dynamic languages are few, far between, and seldom sound.The "abstracting abstract machines" (AAM) approach to constructing static analyses has recently been proposed as a method to ameliorate the difficulty of designing analyses for such language features. The approach, so called because it derives a function for the sound and computable approximation of program behavior starting from the abstract machine semantics of a language, provides a viable approach to dynamic language analysis since all that is required is a machine description of the interpreter.The AAM recipe as originally described produces finite state abstractions: the behavior of a program is approximated as a finite state machine. Such a model is inherently imprecise when it comes to reasoning about the control stack of the interpreter: a finite state machine cannot faithfully represent a stack. Recent advances have shown that higher-order programs can be approximated with pushdown systems. However, such models, founded in automata theory, either breakdown or require significant engineering in the face of dynamic language features that inspect or modify the control stack.In this paper, we tackle the problem of bringing pushdown flow analysis to the domain of dynamic language features. We revise the abstracting abstract machines technique to target the stronger computational model of pushdown systems. In place of automata theory, we use only abstract machines and memoization. As case studies, we show the technique applies to a language with closures, garbage collection, stack-inspection, and first-class composable continuations.}},
  url = {https://doi.org/10.1145/2661088.2661098},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup182,
  title = {A theory of changes for higher-order languages: incrementalizing λ-calculi by static differentiation}},
  author = {Cai, Yufei and Giarrusso, Paolo G. and Rendel, Tillmann and Ostermann, Klaus}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {If the result of an expensive computation is invalidated by a small change to the input, the old result should be updated incrementally instead of reexecuting the whole computation. We incrementalize programs through their derivative. A derivative maps changes in the program's input directly to changes in the program's output, without reexecuting the original program. We present a program transformation taking programs to their derivatives, which is fully static and automatic, supports first-class functions, and produces derivatives amenable to standard optimization.We prove the program transformation correct in Agda for a family of simply-typed λ-calculi, parameterized by base types and primitives. A precise interface specifies what is required to incrementalize the chosen primitives.We investigate performance by a case study: We implement in Scala the program transformation, a plugin and improve performance of a nontrivial program by orders of magnitude.}},
  url = {https://doi.org/10.1145/2594291.2594304},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup183,
  title = {Deriving divide-and-conquer dynamic programming algorithms using solver-aided transformations}},
  author = {Itzhaky, Shachar and Singh, Rohit and Solar-Lezama, Armando and Yessenov, Kuat and Lu, Yongquan and Leiserson, Charles and Chowdhury, Rezaul}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We introduce a framework allowing domain experts to manipulate computational terms in the interest of deriving better, more efficient implementations.It employs deductive reasoning to generate provably correct efficient implementations from a very high-level specification of an algorithm, and inductive constraint-based synthesis to improve automation. Semantic information is encoded into program terms through the use of refinement types. In this paper, we develop the technique in the context of a system called Bellmania that uses solver-aided tactics to derive parallel divide-and-conquer implementations of dynamic programming algorithms that have better locality and are significantly more efficient than traditional loop-based implementations. Bellmania includes a high-level language for specifying dynamic programming algorithms and a calculus that facilitates gradual transformation of these specifications into efficient implementations. These transformations formalize the divide-and conquer technique; a visualization interface helps users to interactively guide the process, while an SMT-based back-end verifies each step and takes care of low-level reasoning required for parallelism. We have used the system to generate provably correct implementations of several algorithms, including some important algorithms from computational biology, and show that the performance is comparable to that of the best manually optimized code.}},
  url = {https://doi.org/10.1145/2983990.2983993},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup184,
  title = {Closed type families with overlapping equations}},
  author = {Eisenberg, Richard A. and Vytiniotis, Dimitrios and Peyton Jones, Simon and Weirich, Stephanie}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Open, type-level functions are a recent innovation in Haskell that move Haskell towards the expressiveness of dependent types, while retaining the look and feel of a practical programming language. This paper shows how to increase expressiveness still further, by adding closed type functions whose equations may overlap, and may have non-linear patterns over an open type universe. Although practically useful and simple to implement, these features go beyond conventional dependent type theory in some respects, and have a subtle metatheory.}},
  url = {https://doi.org/10.1145/2535838.2535856},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup185,
  title = {The sequential semantics of producer effect systems}},
  author = {Tate, Ross}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Effects are fundamental to programming languages. Even the lambda calculus has effects, and consequently the two famous evaluation strategies produce different semantics. As such, much research has been done to improve our understanding of effects. Since Moggi introduced monads for his computational lambda calculus, further generalizations have been designed to formalize increasingly complex computational effects, such as indexed monads followed by layered monads followed by parameterized monads. This succession prompted us to determine the most general formalization possible. In searching for this formalization we came across many surprises, such as the insufficiencies of arrows, as well as many unexpected insights, such as the importance of considering an effect as a small component of a whole system rather than just an isolated feature. In this paper we present our semantic formalization for producer effect systems, which we call a productor, and prove its maximal generality by focusing on only sequential composition of effectful computations, consequently guaranteeing that the existing monadic techniques are specializations of productors.}},
  url = {https://doi.org/10.1145/2429069.2429074},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup186,
  title = {Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines}},
  author = {Ragan-Kelley, Jonathan and Barnes, Connelly and Adams, Andrew and Paris, Sylvain and Durand, Fr\'{e}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Image processing pipelines combine the challenges of stencil computations and stream programs. They are composed of large graphs of different stencil stages, as well as complex reductions, and stages with global or data-dependent access patterns. Because of their complex structure, the performance difference between a naive implementation of a pipeline and an optimized one is often an order of magnitude. Efficient implementations require optimization of both parallelism and locality, but due to the nature of stencils, there is a fundamental tension between parallelism, locality, and introducing redundant recomputation of shared values.We present a systematic model of the tradeoff space fundamental to stencil pipelines, a schedule representation which describes concrete points in this space for each stage in an image processing pipeline, and an optimizing compiler for the Halide image processing language that synthesizes high performance implementations from a Halide algorithm and a schedule. Combining this compiler with stochastic search over the space of schedules enables terse, composable programs to achieve state-of-the-art performance on a wide range of real image processing pipelines, and across different hardware architectures, including multicores with SIMD, and heterogeneous CPU+GPU execution. From simple Halide programs written in a few hours, we demonstrate performance up to 5x faster than hand-tuned C, intrinsics, and CUDA implementations optimized by experts over weeks or months, for image processing applications beyond the reach of past automatic compilers.}},
  url = {https://doi.org/10.1145/2491956.2462176},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup187,
  title = {Speculate: discovering conditional equations and inequalities about black-box functions by reasoning from test results}},
  author = {Braquehais, Rudy and Runciman, Colin}},
  year = {2017}},
  journal = {Proceedings of the 10th ACM SIGPLAN International Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents Speculate, a tool that automatically conjectures laws involving conditional equations and inequalities about Haskell functions. Speculate enumerates expressions involving a given collection of Haskell functions, testing to separate those expressions into apparent equivalence classes. Expressions in the same equivalence class are used to conjecture equations. Representative expressions of different equivalence classes are used to conjecture conditional equations and inequalities. Speculate uses lightweight equational reasoning based on term rewriting to discard redundant laws and to avoid needless testing. Several applications demonstrate the effectiveness of Speculate.}},
  url = {https://doi.org/10.1145/3122955.3122961},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup188,
  title = {In-Memory Data Parallel Processor}},
  author = {Fujiki, Daichi and Mahlke, Scott and Das, Reetuparna}},
  year = {2018}},
  journal = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Recent developments in Non-Volatile Memories (NVMs) have opened up a new horizon for in-memory computing. Despite the significant performance gain offered by computational NVMs, previous works have relied on manual mapping of specialized kernels to the memory arrays, making it infeasible to execute more general workloads. We combat this problem by proposing a programmable in-memory processor architecture and data-parallel programming framework. The efficiency of the proposed in-memory processor comes from two sources: massive parallelism and reduction in data movement. A compact instruction set provides generalized computation capabilities for the memory array. The proposed programming framework seeks to leverage the underlying parallelism in the hardware by merging the concepts of data-flow and vector processing. To facilitate in-memory programming, we develop a compilation framework that takes a TensorFlow input and generates code for our in-memory processor. Our results demonstrate 7.5x speedup over a multi-core CPU server for a set of applications from Parsec and 763x speedup over a server-class GPU for a set of Rodinia benchmarks.}},
  url = {https://doi.org/10.1145/3173162.3173171},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup189,
  title = {Farms, pipes, streams and reforestation: reasoning about structured parallel processes using types and hylomorphisms}},
  author = {Castro, David and Hammond, Kevin and Sarkar, Susmit}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The increasing importance of parallelism has motivated the creation of better abstractions for writing parallel software, including structured parallelism using nested algorithmic skeletons. Such approaches provide high-level abstractions that avoid common problems, such as race conditions, and often allow strong cost models to be defined. However, choosing a combination of algorithmic skeletons that yields good parallel speedups for a program on some specific parallel architecture remains a difficult task. In order to achieve this, it is necessary to simultaneously reason both about the costs of different parallel structures and about the semantic equivalences between them. This paper presents a new type-based mechanism that enables strong static reasoning about these properties. We exploit well-known properties of a very general recursion pattern, hylomorphisms, and give a denotational semantics for structured parallel processes in terms of these hylomorphisms. Using our approach, it is possible to determine formally whether it is possible to introduce a desired parallel structure into a program without altering its functional behaviour, and also to choose a version of that parallel structure that minimises some given cost model.}},
  url = {https://doi.org/10.1145/2951913.2951920},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup190,
  title = {It's Time for a New Old Language}},
  author = {Steele, Guy L.}},
  year = {2017}},
  journal = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The most popular programming language in computer science has no compiler or interpreter. Its definition is not written down in any one place. It has changed a lot over the decades, and those changes have introduced ambiguities and inconsistencies. Today, dozens of variations are in use, and its complexity has reached the point where it needs to be re-explained, at least in part, every time it is used. Much effort has been spent in hand-translating between this language and other languages that do have compilers. The language is quite amenable to parallel computation, but this fact has gone unexploited.In this talk we will summarize the history of the language, highlight the variations and some of the problems that have arisen, and propose specific solutions. We suggest that it is high time that this language be given a complete formal specification, and that compilers, IDEs, and proof-checkers be created to support it, so that all the best tools and techniques of our trade may be applied to it also.}},
  url = {https://doi.org/10.1145/3018743.3018773},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup191,
  title = {Numerical computing on the web: benchmarking for the future}},
  author = {Herrera, David and Chen, Hanfeng and Lavoie, Erick and Hendren, Laurie}},
  year = {2018}},
  journal = {Proceedings of the 14th ACM SIGPLAN International Symposium on Dynamic Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Recent advances in execution environments for JavaScript and WebAssembly that run on a broad range of devices, from workstations and mobile phones to IoT devices, provide new opportunities for portable and web-based numerical computing. Indeed, numerous numerical libraries and applications are emerging on the web, including Tensorflow.js, JSMapReduce, and the NLG Protein Viewer. This paper evaluates the current performance of numerical computing on the web, including both JavaScript and WebAssembly, over a wide range of devices from workstations to IoT devices. We developed a new benchmarking approach, which allowed us to perform centralized benchmarking, including benchmarking on mobile and IoT devices. Using this approach we performed four performance studies using the Ostrich benchmark suite, a collection of numerical programs representing the numerical dwarf categories identified by Colella. We studied the performance evolution of JavaScript, the relative performance of WebAssembly, the performance of server-side Node.js, and a comprehensive performance showdown for a wide range of devices.}},
  url = {https://doi.org/10.1145/3276945.3276968},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup192,
  title = {Cache-oblivious wavefront: improving parallelism of recursive dynamic programming algorithms without losing cache-efficiency}},
  author = {Tang, Yuan and You, Ronghui and Kan, Haibin and Tithi, Jesmin Jahan and Ganapathi, Pramod and Chowdhury, Rezaul A.}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {State-of-the-art cache-oblivious parallel algorithms for dynamic programming (DP) problems usually guarantee asymptotically optimal cache performance without any tuning of cache parameters, but they often fail to exploit the theoretically best parallelism at the same time. While these algorithms achieve cache-optimality through the use of a recursive divide-and-conquer (DAC) strategy, scheduling tasks at the granularity of task dependency introduces artificial dependencies in addition to those arising from the defining recurrence equations. We removed the artificial dependency by scheduling tasks ready for execution as soon as all its real dependency constraints are satisfied, while preserving the cache-optimality by inheriting the DAC strategy. We applied our approach to a set of widely known dynamic programming problems, such as Floyd-Warshall's All-Pairs Shortest Paths, Stencil, and LCS. Theoretical analyses show that our techniques improve the span of 2-way DAC-based Floyd Warshall's algorithm on an $n$ node graph from $Thn^2n$ to $Thn$, stencil computations on a $d$-dimensional hypercubic grid of width $w$ for $h$ time steps from $Th(d^2 h) w^ (d+2) - 1$ to $Thh$, and LCS on two sequences of length $n$ each from $Thn^_2 3$ to $Thn$. In each case, the total work and cache complexity remain asymptotically optimal. Experimental measurements exhibit a $3$ - $5$ times improvement in absolute running time, $10$ - $20$ times improvement in burdened span by Cilkview, and approximately the same L1/L2 cache misses by PAPI.}},
  url = {https://doi.org/10.1145/2688500.2688514},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup193,
  title = {Mechanized meta-reasoning using a hybrid HOAS/de bruijn representation and reflection}},
  author = {Hickey, Jason and Nogin, Aleksey and Yu, Xin and Kopylov, Alexei}},
  year = {2006}},
  journal = {Proceedings of the Eleventh ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We investigate the development of a general-purpose framework for mechanized reasoning about the meta-theory of programming languages. In order to provide a standard, uniform account of a programming language, we propose to define it as a logic in a logical framework, using the same mechanisms for definition, reasoning, and automation that are available to other logics. Then, in order to reason about the language's meta-theory, we use reflection to inject the programming language into (usually richer and more expressive) meta-theory.One of the key features of our approach is that structure of the language is preserved when it is reflected, including variables, meta-variables, and binding structure. This allows the structure of proofs to be preserved as well, and there is a one-to-one map from proof steps in the original programming logic to proof steps in the reflected logic. The act of reflecting a language is automated; all definitions, theorems, and proofs are preserved by the transformation and all the key lemmas (such as proof and structural induction) are automatically derived.The principal representation used by the reflected logic is higher-order abstract syntax (HOAS). However, reasoning about terms in HOAS can be awkward in some cases, especially for variables. For this reason, we define a computationally equivalent variable-free de Bruijn representation that is interchangeable with the HOAS in all contexts. The de Bruijn representation inherits the properties of substitution and alpha-equality from the logical framework, and it is not complicated by administrative issues like variable renumbering.We further develop the concepts and principles of proofs, provability, and structural and proof induction. This work is fully implemented in the MetaPRL theorem prover. We illustrate with an application to F&lt;: as defined in the POPLmark challenge.}},
  url = {https://doi.org/10.1145/1159803.1159826},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup194,
  title = {Self-adjusting stack machines}},
  author = {Hammer, Matthew A. and Neis, Georg and Chen, Yan and Acar, Umut A.}},
  year = {2011}},
  journal = {Proceedings of the 2011 ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Self-adjusting computation offers a language-based approach to writing programs that automatically respond to dynamically changing data. Recent work made significant progress in developing sound semantics and associated implementations of self-adjusting computation for high-level, functional languages. These techniques, however, do not address issues that arise for low-level languages, i.e., stack-based imperative languages that lack strong type systems and automatic memory management. In this paper, we describe techniques for self-adjusting computation which are suitable for low-level languages. Necessarily, we take a different approach than previous work: instead of starting with a high-level language with additional primitives to support self-adjusting computation, we start with a low-level intermediate language, whose semantics is given by a stack-based abstract machine. We prove that this semantics is sound: it always updates computations in a way that is consistent with full reevaluation. We give a compiler and runtime system for the intermediate language used by our abstract machine. We present an empirical evaluation that shows that our approach is efficient in practice, and performs favorably compared to prior proposals.}},
  url = {https://doi.org/10.1145/2048066.2048124},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup195,
  title = {i3QL: language-integrated live data views}},
  author = {Mitschke, Ralf and Erdweg, Sebastian and K\"{o}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {An incremental computation updates its result based on a change to its input, which is often an order of magnitude faster than a recomputation from scratch. In particular, incrementalization can make expensive computations feasible for settings that require short feedback cycles, such as interactive systems, IDEs, or (soft) real-time systems.This paper presents i3QL, a general-purpose programming language for specifying incremental computations. i3QL provides a declarative SQL-like syntax and is based on incremental versions of operators from relational algebra, enriched with support for general recursion. We integrated i3QL into Scala as a library, which enables programmers to use regular Scala code for non-incremental subcomputations of an i3QL query and to easily integrate incremental computations into larger software projects. To improve performance, i3QL optimizes user-defined queries by applying algebraic laws and partial evaluation. We describe the design and implementation of i3QL and its optimizations, demonstrate its applicability, and evaluate its performance.}},
  url = {https://doi.org/10.1145/2660193.2660242},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup196,
  title = {Programming monads operationally with Unimo}},
  author = {Lin, Chuan-kai}},
  year = {2006}},
  journal = {Proceedings of the Eleventh ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Monads are widely used in Haskell for modeling computational effects, but defining monads remains a daunting challenge. Since every part of a monad's definition depends on its computational effects, programmers cannot leverage the common behavior of all monads easily and thus must build from scratch each monad that models a new computational effect.I propose the Unimo framework which allows programmers to define monads and monad transformers in a modular manner. Unimo contains a heavily parameterized observer function which enforces the monad laws, and programmers define a monad by invoking the observer function with arguments that specify the computational effects of the monad. Since Unimo provides the common behavior of all monads in a reusable form, programmers no longer need to rebuild the semantic boilerplate for each monad and can instead focus on the more interesting and rewarding task of modeling the desired computational effects.}},
  url = {https://doi.org/10.1145/1159803.1159840},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup197,
  title = {Information effects}},
  author = {James, Roshan P. and Sabry, Amr}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Computation is a physical process which, like all other physical processes, is fundamentally reversible. From the notion of type isomorphisms, we derive a typed, universal, and reversible computational model in which information is treated as a linear resource that can neither be duplicated nor erased. We use this model as a semantic foundation for computation and show that the "gap" between conventional irreversible computation and logically reversible computation can be captured by a type-and-effect system. Our type-and-effect system is structured as an arrow metalanguage that exposes creation and erasure of information as explicit effect operations. Irreversible computations arise from interactions with an implicit information environment, thus making them a derived notion, much like open systems in Physics. We sketch several applications which can benefit from an explicit treatment of information effects, such as quantitative information-flow security and differential privacy.}},
  url = {https://doi.org/10.1145/2103656.2103667},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup198,
  title = {Combining effects and coeffects via grading}},
  author = {Gaboardi, Marco and Katsumata, Shin-ya and Orchard, Dominic and Breuvart, Flavien and Uustalu, Tarmo}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Effects and coeffects are two general, complementary aspects of program behaviour. They roughly correspond to computations which change the execution context (effects) versus computations which make demands on the context (coeffects). Effectful features include partiality, non-determinism, input-output, state, and exceptions. Coeffectful features include resource demands, variable access, notions of linearity, and data input requirements. The effectful or coeffectful behaviour of a program can be captured and described via type-based analyses, with fine grained information provided by monoidal effect annotations and semiring coeffects. Various recent work has proposed models for such typed calculi in terms of graded (strong) monads for effects and graded (monoidal) comonads for coeffects. Effects and coeffects have been studied separately so far, but in practice many computations are both effectful and coeffectful, e.g., possibly throwing exceptions but with resource requirements. To remedy this, we introduce a new general calculus with a combined effect-coeffect system. This can describe both the changes and requirements that a program has on its context, as well as interactions between these effectful and coeffectful features of computation. The effect-coeffect system has a denotational model in terms of effect-graded monads and coeffect-graded comonads where interaction is expressed via the novel concept of graded distributive laws. This graded semantics unifies the syntactic type theory with the denotational model. We show that our calculus can be instantiated to describe in a natural way various different kinds of interaction between a program and its evaluation context.}},
  url = {https://doi.org/10.1145/2951913.2951939},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup199,
  title = {High-level separation logic for low-level code}},
  author = {Jensen, Jonas B. and Benton, Nick and Kennedy, Andrew}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Separation logic is a powerful tool for reasoning about structured, imperative programs that manipulate pointers. However, its application to unstructured, lower-level languages such as assembly language or machine code remains challenging. In this paper we describe a separation logic tailored for this purpose that we have applied to x86 machine-code programs.The logic is built from an assertion logic on machine states over which we construct a specification logic that encapsulates uses of frames and step indexing. The traditional notion of Hoare triple is not applicable directly to unstructured machine code, where code and data are mixed together and programs do not in general run to completion, so instead we adopt a continuation-passing style of specification with preconditions alone. Nevertheless, the range of primitives provided by the specification logic, which include a higher-order frame connective, a novel read-only frame connective, and a 'later' modality, support the definition of derived forms to support structured-programming-style reasoning for common cases, in which standard rules for Hoare triples are derived as lemmas. Furthermore, our encoding of scoped assembly-language labels lets us give definitions and proof rules for powerful assembly-language 'macros' such as while loops, conditionals and procedures.We have applied the framework to a model of sequential x86 machine code built entirely within the Coq proof assistant, including tactic support based on computational reflection.}},
  url = {https://doi.org/10.1145/2429069.2429105},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup200,
  title = {HorseIR: bringing array programming languages together with database query processing}},
  author = {Chen, Hanfeng and D'silva, Joseph Vinish and Chen, Hongji and Kemme, Bettina and Hendren, Laurie}},
  year = {2018}},
  journal = {Proceedings of the 14th ACM SIGPLAN International Symposium on Dynamic Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Relational database management systems (RDBMS) are operationally similar to a dynamic language processor. They take SQL queries as input, dynamically generate an optimized execution plan, and then execute it. In recent decades, the emergence of in-memory databases with columnar storage, which use array-like storage structures, has shifted the focus on optimizations from the traditional I/O bottleneck to CPU and memory. However, database research so far has primarily focused on CPU cache optimizations. The similarity in the computational characteristics of such database workloads and array programming language optimizations are largely unexplored. We believe that these database implementations can benefit from merging database optimizations with dynamic array-based programming language approaches. Therefore, in this paper, we propose a novel approach to optimize database query execution using a new array-based intermediate representation, HorseIR, that resides between database queries and compiled code. Furthermore, we provide a translator to generate HorseIR from database execution plans and a compiler that optimizes HorseIR and generates efficient code. We compare HorseIR with the MonetDB RDBMS, by testing standard SQL queries, and show how our approach and compiler optimizations improve the runtime of complex queries.}},
  url = {https://doi.org/10.1145/3276945.3276951},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup201,
  title = {Efficient support for irregular applications on distributed-memory machines}},
  author = {Mukherjee, Shubhendu S. and Sharma, Shamik D. and Hill, Mark D. and Larus, James R. and Rogers, Anne and Saltz, Joel}},
  year = {1995}},
  journal = {Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Irregular computation problems underlie many important scientific applications. Although these problems are computationally expensive, and so would seem appropriate for parallel machines, their irregular and unpredictable run-time behavior makes this type of parallel program difficult to write and adversely affects run-time performance.This paper explores three issues—partitioning, mutual exclusion, and data transfer—crucial to the efficient execution of irregular problems on distributed-memory machines. Unlike previous work, we studied the same programs running in three alternative systems on the same hardware base (a Thinking Machines CM-5): the CHAOS irregular application library, Transparent Shared Memory (TSM), and eXtensible Shared Memory (XSM). CHAOS and XSM performed equivalently for all three applications. Both systems were somewhat (13\%) to significantly faster (991\%) than TSM.}},
  url = {https://doi.org/10.1145/209936.209945},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup202,
  title = {Graspan: A Single-machine Disk-based Graph System for Interprocedural Static Analyses of Large-scale Systems Code}},
  author = {Wang, Kai and Hussain, Aftab and Zuo, Zhiqiang and Xu, Guoqing and Amiri Sani, Ardalan}},
  year = {2017}},
  journal = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {There is more than a decade-long history of using static analysis to find bugs in systems such as Linux. Most of the existing static analyses developed for these systems are simple checkers that find bugs based on pattern matching. Despite the presence of many sophisticated interprocedural analyses, few of them have been employed to improve checkers for systems code due to their complex implementations and poor scalability. In this paper, we revisit the scalability problem of interprocedural static analysis from a "Big Data" perspective. That is, we turn sophisticated code analysis into Big Data analytics and leverage novel data processing techniques to solve this traditional programming language problem. We develop Graspan, a disk-based parallel graph system that uses an edge-pair centric computation model to compute dynamic transitive closures on very large program graphs.We implement context-sensitive pointer/alias and dataflow analyses on Graspan. An evaluation of these analyses on large codebases such as Linux shows that their Graspan implementations scale to millions of lines of code and are much simpler than their original implementations. Moreover, we show that these analyses can be used to augment the existing checkers; these augmented checkers uncovered 132 new NULL pointer bugs and 1308 unnecessary NULL tests in Linux 4.4.0-rc5, PostgreSQL 8.3.9, and Apache httpd 2.2.18.}},
  url = {https://doi.org/10.1145/3037697.3037744},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup203,
  title = {Quantifying the test effectiveness of Algol 68 programs}},
  author = {Hennell, M. A. and Hedley, D. and Woodward, M. R.}},
  year = {1977}},
  journal = {Proceedings of the Strathclyde ALGOL 68 Conference}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this paper we describe how a software testbed is used to determine the quality of testing of Algol 68 programs. The system monitors the execution of an Algol 68 program and obtains a run-time execution history. This history is then compared with the results of a static analysis and three levels of testing are calculated. The third level is an extension to Algol 68 of a method originally devised for analysing Fortran IV programs. The system has been used extensively in the coordination stage of the NAG Algol 68 numerical algorithms library and we quote some results obtained from an analysis of stringent tests on these library routines.}},
  url = {https://doi.org/10.1145/800238.807140},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup204,
  title = {Patterns and statistical analysis for understanding reduced resource computing}},
  author = {Rinard, Martin and Hoffmann, Henry and Misailovic, Sasa and Sidiroglou, Stelios}},
  year = {2010}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present several general, broadly applicable mechanisms that enable computations to execute with reduced resources, typically at the cost of some loss in the accuracy of the result they produce.We identify several general computational patterns that interact well with these resource reduction mechanisms, present a concrete manifestation of these patterns in the form of simple model programs, perform simulationbased explorations of the quantitative consequences of applying these mechanisms to our model programs, and relate the model computations (and their interaction with the resource reduction mechanisms) to more complex benchmark applications drawn from a variety of fields.}},
  url = {https://doi.org/10.1145/1869459.1869525},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup205,
  title = {Terra: a multi-stage language for high-performance computing}},
  author = {DeVito, Zachary and Hegarty, James and Aiken, Alex and Hanrahan, Pat and Vitek, Jan}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {High-performance computing applications, such as auto-tuners and domain-specific languages, rely on generative programming techniques to achieve high performance and portability. However, these systems are often implemented in multiple disparate languages and perform code generation in a separate process from program execution, making certain optimizations difficult to engineer. We leverage a popular scripting language, Lua, to stage the execution of a novel low-level language, Terra. Users can implement optimizations in the high-level language, and use built-in constructs to generate and execute high-performance Terra code. To simplify meta-programming, Lua and Terra share the same lexical environment, but, to ensure performance, Terra code can execute independently of Lua's runtime. We evaluate our design by reimplementing existing multi-language systems entirely in Terra. Our Terra-based auto-tuner for BLAS routines performs within 20\% of ATLAS, and our DSL for stencil computations runs 2.3x faster than hand-written C.}},
  url = {https://doi.org/10.1145/2491956.2462166},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup206,
  title = {Orchestrating dynamic analyses of distributed processes for full-stack JavaScript programs}},
  author = {Christophe, Laurent and De Roover, Coen and Boix, Elisa Gonzalez and De Meuter, Wolfgang}},
  year = {2018}},
  journal = {Proceedings of the 17th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Dynamic analyses are commonly implemented by instrumenting the program under analysis. Examples of such analyses for JavaScript range from checkers of user- defined invariants to concolic testers. For a full-stack JavaScript program, these analyses would benefit from reasoning about the state of the client-side and server-side processes it is comprised of. Lifting a dynamic analysis so that it supports full-stack programs can be challenging. It involves distributed communication to maintain the analysis state across all processes, which has to be deadlock-free. In this paper, we advocate maintaining distributed analysis state in a centralized analysis process instead — which is communicated with from the processes under analysis. The approach is supported by a dynamic analysis platform that provides abstractions for this communication. We evaluate the approach through a case study. We use the platform to build a distributed origin analysis, capable of tracking the expressions from which values originate from across process boundaries, and deploy it on collaborative drawing application. The results show that our approach greatly simplifies the lifting process at the cost of a computational overhead. We deem this overhead acceptable for analyses intended for use at development time.}},
  url = {https://doi.org/10.1145/3278122.3278135},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup207,
  title = {Well-typed music does not sound wrong (experience report)}},
  author = {Szamozvancev, Dmitrij and Gale, Michael B.}},
  year = {2017}},
  journal = {Proceedings of the 10th ACM SIGPLAN International Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Music description and generation are popular use cases for Haskell, ranging from live coding libraries to automatic harmonisation systems. Some approaches use probabilistic methods, others build on the theory of Western music composition, but there has been little work done on checking the correctness of musical pieces in terms of voice leading, harmony, and structure. Haskell's recent additions to the type-system now enable us to perform such analysis statically. We present our experience of implementing a type-level model of classical music and an accompanying EDSL which enforce the rules of classical music at compile-time, turning composition mistakes into compiler errors. Along the way, we discuss the strengths and limitations of doing this in Haskell and demonstrate that the type system of the language is fully capable of expressing non-trivial and practical logic specific to a particular domain.}},
  url = {https://doi.org/10.1145/3122955.3122964},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup208,
  title = {Probabilistic relational reasoning for differential privacy}},
  author = {Barthe, Gilles and K\"{o}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Differential privacy is a notion of confidentiality that protects the privacy of individuals while allowing useful computations on their private data. Deriving differential privacy guarantees for real programs is a difficult and error-prone task that calls for principled approaches and tool support. Approaches based on linear types and static analysis have recently emerged; however, an increasing number of programs achieve privacy using techniques that cannot be analyzed by these approaches. Examples include programs that aim for weaker, approximate differential privacy guarantees, programs that use the Exponential mechanism, and randomized programs that achieve differential privacy without using any standard mechanism. Providing support for reasoning about the privacy of such programs has been an open problem.We report on CertiPriv, a machine-checked framework for reasoning about differential privacy built on top of the Coq proof assistant. The central component of CertiPriv is a quantitative extension of a probabilistic relational Hoare logic that enables one to derive differential privacy guarantees for programs from first principles. We demonstrate the expressiveness of CertiPriv using a number of examples whose formal analysis is out of the reach of previous techniques. In particular, we provide the first machine-checked proofs of correctness of the Laplacian and Exponential mechanisms and of the privacy of randomized and streaming algorithms from the recent literature.}},
  url = {https://doi.org/10.1145/2103656.2103670},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup209,
  title = {Fusing effectful comprehensions}},
  author = {Saarikivi, Olli and Veanes, Margus and Mytkowicz, Todd and Musuvathi, Madan}},
  year = {2017}},
  journal = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {List comprehensions provide a powerful abstraction mechanism for expressing computations over ordered collections of data declaratively without having to use explicit iteration constructs. This paper puts forth effectful comprehensions as an elegant way to describe list comprehensions that incorporate loop-carried state. This is motivated by operations such as compression/decompression and serialization/deserialization that are common in log/data processing pipelines and require loop-carried state when processing an input stream of data. We build on the underlying theory of symbolic transducers to fuse pipelines of effectful comprehensions into a single representation, from which efficient code can be generated. Using background theory reasoning with an SMT solver, our fusion and subsequent reachability based branch elimination algorithms can significantly reduce the complexity of the fused pipelines. Our implementation shows significant speedups over reasonable hand-written code (3.4\texttimes{}},
  url = {https://doi.org/10.1145/3062341.3062362},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup210,
  title = {Automatic dataflow model extraction from modal real-time stream processing applications}},
  author = {Geuns, Stefan J. and Hausmans, Joost P.H.M. and Bekooij, Marco J.G.}},
  year = {2013}},
  journal = {Proceedings of the 14th ACM SIGPLAN/SIGBED Conference on Languages, Compilers and Tools for Embedded Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many real-time stream processing applications are initially described as a sequential application containing while-loops, which execute for an unknown number of iterations. These modal applications have to be executed in parallel on an MPSoC system in order to meet their real-time throughput constraints. However, no suitable approach exists that can automatically derive a temporal analysis model from a sequential specification containing while- loops with an unknown number of iterations.This paper introduces an approach to the automatic generation of a Structured Variable-rate Phased Dataflow (SVPDF) model from a sequential specification of a modal application. The real-time requirements of an application can be analyzed despite the presence of while-loops with an unknown number of iterations. It is shown that an algorithm that has a polynomial time computational complexity can be applied on the generated SVPDF model to determine whether a throughput constraint can be met. The enabler for the automatic generation of an SVPDF model is the decoupling of synchronization between tasks that contain different while-loops. A DVB-T radio transceiver illustrates the derivation of the SVPDF model.}},
  url = {https://doi.org/10.1145/2491899.2465561},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup211,
  title = {Reasoning about the POSIX file system: local update and global pathnames}},
  author = {Ntzik, Gian and Gardner, Philippa}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We introduce a program logic for specifying a core sequential subset of the POSIX file system and for reasoning abstractly about client programs working with the file system. The challenge is to reason about the combination of local directory update and global pathname traversal (including '..' and symbolic links) which may overlap the directories being updated. Existing reasoning techniques are either based on first-order logic and do not scale, or on separation logic and can only handle linear pathnames (no '..' or symbolic links). We introduce fusion logic for reasoning about local update and global pathname traversal, introducing a novel effect frame rule to propagate the effect of a local update on overlapping pathnames. We apply our reasoning to the standard recursive remove utility (rm -r), discovering bugs in well-known implementations.}},
  url = {https://doi.org/10.1145/2814270.2814306},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup212,
  title = {Deciding kCFA is complete for EXPTIME}},
  author = {Van Horn, David and Mairson, Harry G.}},
  year = {2008}},
  journal = {Proceedings of the 13th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We give an exact characterization of the computational complexity of the kCFA hierarchy. For any k &gt; 0, we prove that the control flow decision problem is complete for deterministic exponential time. This theorem validates empirical observations that such control flow analysis is intractable. It also provides more general insight into the complexity of abstract interpretation.}},
  url = {https://doi.org/10.1145/1411204.1411243},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup213,
  title = {GPU code optimization using abstract kernel emulation and sensitivity analysis}},
  author = {Hong, Changwan and Sukumaran-Rajam, Aravind and Kim, Jinsung and Rawat, Prashant Singh and Krishnamoorthy, Sriram and Pouchet, Louis-No\"{e}},
  year = {2018}},
  journal = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this paper, we develop an approach to GPU kernel optimization by focusing on identification of bottleneck resources and determining optimization parameters that can alleviate the bottleneck. Performance modeling for GPUs is done by abstract kernel emulation along with latency/gap modeling of resources. Sensitivity analysis with respect to resource latency/gap parameters is used to predict the bottleneck resource for a given kernel's execution. The utility of the bottleneck analysis is demonstrated in two contexts: 1) Coupling the new bottleneck-driven optimization strategy with the OpenTuner auto-tuner: experimental results on all kernels from the Rodinia suite and GPU tensor contraction kernels from the NWChem computational chemistry suite demonstrate effectiveness. 2) Manual code optimization: two case studies illustrate the use of the bottleneck analysis to iteratively improve the performance of code from state-of-the-art domain-specific code generators.}},
  url = {https://doi.org/10.1145/3192366.3192397},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup214,
  title = {Higher-order functional reactive programming in bounded space}},
  author = {Krishnaswami, Neelakantan R. and Benton, Nick and Hoffmann, Jan}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Functional reactive programming (FRP) is an elegant and successful approach to programming reactive systems declaratively. The high levels of abstraction and expressivity that make FRP attractive as a programming model do, however, often lead to programs whose resource usage is excessive and hard to predict. In this paper, we address the problem of space leaks in discrete-time functional reactive programs. We present a functional reactive programming language that statically bounds the size of the dataflow graph a reactive program creates, while still permitting use of higher-order functions and higher-type streams such as streams of streams. We achieve this with a novel linear type theory that both controls allocation and ensures that all recursive definitions are well-founded.We also give a denotational semantics for our language by combining recent work on metric spaces for the interpretation of higher-order causal functions with length-space models of space-bounded computation. The resulting category is doubly closed and hence forms a model of the logic of bunched implications.}},
  url = {https://doi.org/10.1145/2103656.2103665},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup215,
  title = {Register optimizations for stencils on GPUs}},
  author = {Rawat, Prashant Singh and Rastello, Fabrice and Sukumaran-Rajam, Aravind and Pouchet, Louis-No\"{e}},
  year = {2018}},
  journal = {Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The recent advent of compute-intensive GPU architecture has allowed application developers to explore high-order 3D stencils for better computational accuracy. A common optimization strategy for such stencils is to expose sufficient data reuse by means such as loop unrolling, with the expectation of register-level reuse. However, the resulting code is often highly constrained by register pressure. While current state-of-the-art register allocators are satisfactory for most applications, they are unable to effectively manage register pressure for such complex high-order stencils, resulting in sub-optimal code with a large number of register spills. In this paper, we develop a statement reordering framework that models stencil computations as a DAG of trees with shared leaves, and adapts an optimal scheduling algorithm for minimizing register usage for expression trees. The effectiveness of the approach is demonstrated through experimental results on a range of stencils extracted from application codes.}},
  url = {https://doi.org/10.1145/3178487.3178500},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup216,
  title = {Post-compiler software optimization for reducing energy}},
  author = {Schulte, Eric and Dorn, Jonathan and Harding, Stephen and Forrest, Stephanie and Weimer, Westley}},
  year = {2014}},
  journal = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Modern compilers typically optimize for executable size and speed, rarely exploring non-functional properties such as power efficiency. These properties are often hardware-specific, time-intensive to optimize, and may not be amenable to standard dataflow optimizations. We present a general post-compilation approach called Genetic Optimization Algorithm (GOA), which targets measurable non-functional aspects of software execution in programs that compile to x86 assembly. GOA combines insights from profile-guided optimization, superoptimization, evolutionary computation and mutational robustness. GOA searches for program variants that retain required functional behavior while improving non-functional behavior, using characteristic workloads and predictive modeling to guide the search. The resulting optimizations are validated using physical performance measurements and a larger held-out test suite. Our experimental results on PARSEC benchmark programs show average energy reductions of 20\%, both for a large AMD system and a small Intel system, while maintaining program functionality on target workloads.}},
  url = {https://doi.org/10.1145/2541940.2541980},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup217,
  title = {Communication-avoiding parallel minimum cuts and connected components}},
  author = {Gianinazzi, Lukas and Kalvoda, Pavel and De Palma, Alessandro and Besta, Maciej and Hoefler, Torsten}},
  year = {2018}},
  journal = {Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present novel scalable parallel algorithms for finding global minimum cuts and connected components, which are important and fundamental problems in graph processing. To take advantage of future massively parallel architectures, our algorithms are communication-avoiding: they reduce the costs of communication across the network and the cache hierarchy. The fundamental technique underlying our work is the randomized sparsification of a graph: removing a fraction of graph edges, deriving a solution for such a sparsified graph, and using the result to obtain a solution for the original input. We design and implement sparsification with O(1) synchronization steps. Our global minimum cut algorithm decreases communication costs and computation compared to the state-of-the-art, while our connected components algorithm incurs few cache misses and synchronization steps. We validate our approach by evaluating MPI implementations of the algorithms on a petascale supercomputer. We also provide an approximate variant of the minimum cut algorithm and show that it approximates the exact solutions well while using a fraction of cores in a fraction of time.}},
  url = {https://doi.org/10.1145/3178487.3178504},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup218,
  title = {CALOREE: Learning Control for Predictable Latency and Low Energy}},
  author = {Mishra, Nikita and Imes, Connor and Lafferty, John D. and Hoffmann, Henry}},
  year = {2018}},
  journal = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many modern computing systems must provide reliable latency with minimal energy. Two central challenges arise when allocating system resources to meet these conflicting goals: (1) complexity modern hardware exposes diverse resources with complicated interactions and (2) dynamics latency must be maintained despite unpredictable changes in operating environment or input. Machine learning accurately models the latency of complex, interacting resources, but does not address system dynamics; control theory adjusts to dynamic changes, but struggles with complex resource interaction. We therefore propose CALOREE, a resource manager that learns key control parameters to meet latency requirements with minimal energy in complex, dynamic en- vironments. CALOREE breaks resource allocation into two sub-tasks: learning how interacting resources affect speedup, and controlling speedup to meet latency requirements with minimal energy. CALOREE deines a general control system whose parameters are customized by a learning framework while maintaining control-theoretic formal guarantees that the latency goal will be met. We test CALOREE's ability to deliver reliable latency on heterogeneous ARM big.LITTLE architectures in both single and multi-application scenarios. Compared to the best prior learning and control solutions, CALOREE reduces deadline misses by 60\% and energy consumption by 13\%.}},
  url = {https://doi.org/10.1145/3173162.3173184},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup219,
  title = {Relational cost analysis}},
  author = {\c{C}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Establishing quantitative bounds on the execution cost of programs is essential in many areas of computer science such as complexity analysis, compiler optimizations, security and privacy. Techniques based on program analysis, type systems and abstract interpretation are well-studied, but methods for analyzing how the execution costs of two programs compare to each other have not received attention. Naively combining the worst and best case execution costs of the two programs does not work well in many cases because such analysis forgets the similarities between the programs or the inputs. In this work, we propose a relational cost analysis technique that is capable of establishing precise bounds on the difference in the execution cost of two programs by making use of relational properties of programs and inputs. We develop , a refinement type and effect system for a higher-order functional language with recursion and subtyping. The key novelty of our technique is the combination of relational refinements with two modes of typing-relational typing for reasoning about similar computations/inputs and unary typing for reasoning about unrelated computations/inputs. This combination allows us to analyze the execution cost difference of two programs more precisely than a naive non-relational approach. We prove our type system sound using a semantic model based on step-indexed unary and binary logical relations accounting for non-relational and relational reasoning principles with their respective costs. We demonstrate the precision and generality of our technique through examples.}},
  url = {https://doi.org/10.1145/3009837.3009858},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup220,
  title = {Neurosurgeon: Collaborative Intelligence Between the Cloud and Mobile Edge}},
  author = {Kang, Yiping and Hauswald, Johann and Gao, Cao and Rovinski, Austin and Mudge, Trevor and Mars, Jason and Tang, Lingjia}},
  year = {2017}},
  journal = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The computation for today's intelligent personal assistants such as Apple Siri, Google Now, and Microsoft Cortana, is performed in the cloud. This cloud-only approach requires significant amounts of data to be sent to the cloud over the wireless network and puts significant computational pressure on the datacenter. However, as the computational resources in mobile devices become more powerful and energy efficient, questions arise as to whether this cloud-only processing is desirable moving forward, and what are the implications of pushing some or all of this compute to the mobile devices on the edge.In this paper, we examine the status quo approach of cloud-only processing and investigate computation partitioning strategies that effectively leverage both the cycles in the cloud and on the mobile device to achieve low latency, low energy consumption, and high datacenter throughput for this class of intelligent applications. Our study uses 8 intelligent applications spanning computer vision, speech, and natural language domains, all employing state-of-the-art Deep Neural Networks (DNNs) as the core machine learning technique. We find that given the characteristics of DNN algorithms, a fine-grained, layer-level computation partitioning strategy based on the data and computation variations of each layer within a DNN has significant latency and energy advantages over the status quo approach.Using this insight, we design Neurosurgeon, a lightweight scheduler to automatically partition DNN computation between mobile devices and datacenters at the granularity of neural network layers. Neurosurgeon does not require per-application profiling. It adapts to various DNN architectures, hardware platforms, wireless networks, and server load levels, intelligently partitioning computation for best latency or best mobile energy. We evaluate Neurosurgeon on a state-of-the-art mobile development platform and show that it improves end-to-end latency by 3.1X on average and up to 40.7X, reduces mobile energy consumption by 59.5\% on average and up to 94.7\%, and improves datacenter throughput by 1.5X on average and up to 6.7X.}},
  url = {https://doi.org/10.1145/3037697.3037698},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup221,
  title = {A semantic account of metric preservation}},
  author = {Azevedo de Amorim, Arthur and Gaboardi, Marco and Hsu, Justin and Katsumata, Shin-ya and Cherigui, Ikram}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Program sensitivity measures how robust a program is to small changes in its input, and is a fundamental notion in domains ranging from differential privacy to cyber-physical systems. A natural way to formalize program sensitivity is in terms of metrics on the input and output spaces, requiring that an r-sensitive function map inputs that are at distance d to outputs that are at distance at most r · d. Program sensitivity is thus an analogue of Lipschitz continuity for programs. Reed and Pierce introduced Fuzz, a functional language with a linear type system that can express program sensitivity. They show soundness operationally, in the form of a metric preservation property. Inspired by their work, we study program sensitivity and metric preservation from a denotational point of view. In particular, we introduce metric CPOs, a novel semantic structure for reasoning about computation on metric spaces, by endowing CPOs with a compatible notion of distance. This structure is useful for reasoning about metric properties of programs, and specifically about program sensitivity. We demonstrate metric CPOs by giving a model for the deterministic fragment of Fuzz.}},
  url = {https://doi.org/10.1145/3009837.3009890},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup222,
  title = {Dependent partitioning}},
  author = {Treichler, Sean and Bauer, Michael and Sharma, Rahul and Slaughter, Elliott and Aiken, Alex}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A key problem in parallel programming is how data is partitioned: divided into subsets that can be operated on in parallel and, in distributed memory machines, spread across multiple address spaces. We present a dependent partitioning framework that allows an application to concisely describe relationships between partitions. Applications first establish independent partitions, which may contain arbitrary subsets of application data, permitting the expression of arbitrary application-specific data distributions. Dependent partitions are then derived from these using the dependent partitioning operations provided by the framework. By directly capturing inter-partition relationships, our framework can soundly and precisely reason about programs to perform important program analyses crucial to ensuring correctness and achieving good performance. As an example of the reasoning made possible, we present a static analysis that discharges most consistency checks on partitioned data during compilation. We describe an implementation of our framework within Regent, a language designed for the Legion programming model. The use of dependent partitioning constructs results in a 86-96\% decrease in the lines of code required to describe the partitioning, eliminates many of the expensive dynamic checks required for soundness by the current Regent partitioning implementation, and speeds up the computation of partitions by 2.6-12.7X even on a single thread. Additionally, we show that a distributed implementation incorporated into the the Legion runtime system allows partitioning of data sets that are too large to fit on a single node and yields a further 29X speedup of partitioning operations on 64 nodes.}},
  url = {https://doi.org/10.1145/2983990.2984016},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup223,
  title = {Free delivery (functional pearl)}},
  author = {Gibbons, Jeremy}},
  year = {2016}},
  journal = {Proceedings of the 9th International Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Remote procedure calls are computationally expensive, because network round-trips take several orders of magnitude longer than local interactions. One common technique for amortizing this cost is to batch together multiple independent requests into one compound request. Batching requests amounts to serializing the abstract syntax tree of a small program, in order to transmit it and run it remotely. The standard representation for abstract syntax is to use free monads; we show that free applicative functors are actually a better choice of representation for this scenario.}},
  url = {https://doi.org/10.1145/2976002.2976005},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup224,
  title = {Supporting High Performance Molecular Dynamics in Virtualized Clusters using IOMMU, SR-IOV, and GPUDirect}},
  author = {Younge, Andrew J. and Walters, John Paul and Crago, Stephen P. and Fox, Geoffrey C.}},
  year = {2015}},
  journal = {Proceedings of the 11th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Cloud Infrastructure-as-a-Service paradigms have recently shown their utility for a vast array of computational problems, ranging from advanced web service architectures to high throughput computing. However, many scientific computing applications have been slow to adapt to virtualized cloud frameworks. This is due to performance impacts of virtualization technologies, coupled with the lack of advanced hardware support necessary for running many high performance scientific applications at scale.By using KVM virtual machines that leverage both Nvidia GPUs and InfiniBand, we show that molecular dynamics simulations with LAMMPS and HOOMD run at near-native speeds. This experiment also illustrates how virtualized environments can support the latest parallel computing paradigms, including both MPI+CUDA and new GPUDirect RDMA functionality. Specific findings show initial promise in scaling of such applications to larger production deployments targeting large scale computational workloads.}},
  url = {https://doi.org/10.1145/2731186.2731194},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup225,
  title = {Unifying refinement and hoare-style reasoning in a logic for higher-order concurrency}},
  author = {Turon, Aaron and Dreyer, Derek and Birkedal, Lars}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Modular programming and modular verification go hand in hand, but most existing logics for concurrency ignore two crucial forms of modularity: *higher-order functions*, which are essential for building reusable components, and *granularity abstraction*, a key technique for hiding the intricacies of fine-grained concurrent data structures from the clients of those data structures. In this paper, we present CaReSL, the first logic to support the use of granularity abstraction for modular verification of higher-order concurrent programs. After motivating the features of CaReSL through a variety of illustrative examples, we demonstrate its effectiveness by using it to tackle a significant case study: the first formal proof of (partial) correctness for Hendler et al.'s "flat combining" algorithm.}},
  url = {https://doi.org/10.1145/2500365.2500600},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup226,
  title = {The Architectural Implications of Autonomous Driving: Constraints and Acceleration}},
  author = {Lin, Shih-Chieh and Zhang, Yunqi and Hsu, Chang-Hong and Skach, Matt and Haque, Md E. and Tang, Lingjia and Mars, Jason}},
  year = {2018}},
  journal = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Autonomous driving systems have attracted a significant amount of interest recently, and many industry leaders, such as Google, Uber, Tesla, and Mobileye, have invested a large amount of capital and engineering power on developing such systems. Building autonomous driving systems is particularly challenging due to stringent performance requirements in terms of both making the safe operational decisions and finishing processing at real-time. Despite the recent advancements in technology, such systems are still largely under experimentation and architecting end-to-end autonomous driving systems remains an open research question. To investigate this question, we first present and formalize the design constraints for building an autonomous driving system in terms of performance, predictability, storage, thermal and power. We then build an end-to-end autonomous driving system using state-of-the-art award-winning algorithms to understand the design trade-offs for building such systems. In our real-system characterization, we identify three computational bottlenecks, which conventional multicore CPUs are incapable of processing under the identified design constraints. To meet these constraints, we accelerate these algorithms using three accelerator platforms including GPUs, FPGAs, and ASICs, which can reduce the tail latency of the system by 169x, 10x, and 93x respectively. With accelerator-based designs, we are able to build an end-to-end autonomous driving system that meets all the design constraints, and explore the trade-offs among performance, power and the higher accuracy enabled by higher resolution cameras.}},
  url = {https://doi.org/10.1145/3173162.3173191},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup227,
  title = {Compositionality in scenario-aware dataflow: a rendezvous perspective}},
  author = {Skelin, Mladen and Geilen, Marc}},
  year = {2018}},
  journal = {Proceedings of the 19th ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Finite-state machine-based scenario-aware dataflow (FSM-SADF) is a dynamic dataflow model of computation that combines streaming data and finite-state control. For the most part, it preserves the determinism of its underlying synchronous dataflow (SDF) concurrency model and only when necessary introduces the non-deterministic variation in terms of scenarios that are represented by SDF graphs. This puts FSM-SADF in a sweet spot in the trade-off space between expressiveness and analyzability. However, FSM-SADF supports no notion of compositionality, which hampers its usability in modeling and consequent analysis of large systems. In this work we propose a compositional semantics for FSM-SADF that overcomes this problem. We base the semantics of the composition on standard composition of processes with rendezvous communication in the style of CCS or CSP at the control level and the parallel, serial and feedback composition of SDF graphs at the dataflow level. We evaluate the approach on a case study from the multimedia domain.}},
  url = {https://doi.org/10.1145/3211332.3211339},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup228,
  title = {Architecture support for disciplined approximate programming}},
  author = {Esmaeilzadeh, Hadi and Sampson, Adrian and Ceze, Luis and Burger, Doug}},
  year = {2012}},
  journal = {Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Disciplined approximate programming lets programmers declare which parts of a program can be computed approximately and consequently at a lower energy cost. The compiler proves statically that all approximate computation is properly isolated from precise computation. The hardware is then free to selectively apply approximate storage and approximate computation with no need to perform dynamic correctness checks.In this paper, we propose an efficient mapping of disciplined approximate programming onto hardware. We describe an ISA extension that provides approximate operations and storage, which give the hardware freedom to save energy at the cost of accuracy. We then propose Truffle, a microarchitecture design that efficiently supports the ISA extensions. The basis of our design is dual-voltage operation, with a high voltage for precise operations and a low voltage for approximate operations. The key aspect of the microarchitecture is its dependence on the instruction stream to determine when to use the low voltage. We evaluate the power savings potential of in-order and out-of-order Truffle configurations and explore the resulting quality of service degradation. We evaluate several applications and demonstrate energy savings up to 43\%.}},
  url = {https://doi.org/10.1145/2150976.2151008},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup229,
  title = {Verification of parameterized concurrent programs by modular reasoning about data and control}},
  author = {Farzan, Azadeh and Kincaid, Zachary}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this paper, we consider the problem of verifying thread-state properties of multithreaded programs in which the number of active threads cannot be statically bounded. Our approach is based on decomposing the task into two modules, where one reasons about data and the other reasons about control. The data module computes thread-state invariants (e.g., linear constraints over global variables and local variables of one thread) using the thread interference information computed by the control module. The control module computes a representation of thread interference, as an incrementally constructed data flow graph, using the data invariants provided by the data module. These invariants are used to rule out patterns of thread interference that can not occur in a real program execution. The two modules are incorporated into a feedback loop, so that the abstractions of data and interference are iteratively coarsened as the algorithm progresses (that is, they become weaker) until a fixed point is reached. Our approach is sound and terminating, and applicable to programs with infinite state (e.g., unbounded integers) and unboundedly many threads. The verification method presented in this paper has been implemented into a tool, called Duet. We demonstrate the effectiveness of our technique by verifying properties of a selection of Linux device drivers using Duet, and also compare Duet with previous work on verification of parameterized Boolean program using the Boolean abstractions of these drivers.}},
  url = {https://doi.org/10.1145/2103656.2103693},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup230,
  title = {Tales from the jungle}},
  author = {Sewell, Peter}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We rely on a computational infrastructure that is a densely interwined mass of software and hardware: programming languages, network protocols, operating systems, and processors. It has accumulated great complexity, from a combination of engineering design decisions, contingent historical choices, and sheer scale, yet it is defined at best by prose specifications, or, all too often, just by the common implementations. Can we do better? More specifically, can we apply rigorous methods to this mainstream infrastructure, taking the accumulated complexity seriously, and if we do, does it help? My colleagues and I have looked at these questions in several contexts: the TCP/IP network protocols with their Sockets API; programming language design, including the Java module system and the C11/C++11 concurrency model; the hardware concurrency behaviour of x86, IBM POWER, and ARM multiprocessors; and compilation of concurrent code.In this talk I will draw some lessons from what did and did not succeed, looking especially at the empirical nature of some of the work, at the social process of engagement with the various different communities, and at the mathematical and software tools we used. Domain-specific modelling languages (based on functional programming ideas) and proof assistants were invaluable for working with the large and loose specifications involved: idioms within HOL4 for TCP, our Ott tool for programming language specification, and Owens's Lem tool for portable semantic definitions, with HOL4, Isabelle, and Coq, for the relaxed-memory concurrency semantics work. Our experience with these suggests something of what is needed to make full-scale rigorous semantics a commonplace reality.}},
  url = {https://doi.org/10.1145/2364527.2364566},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup231,
  title = {QWIRE: a core language for quantum circuits}},
  author = {Paykin, Jennifer and Rand, Robert and Zdancewic, Steve}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper introduces QWIRE (``choir''), a language for defining quantum circuits and an interface for manipulating them inside of an arbitrary classical host language. QWIRE is minimal---it contains only a few primitives---and sound with respect to the physical properties entailed by quantum mechanics. At the same time, QWIRE is expressive and highly modular due to its relationship with the host language, mirroring the QRAM model of computation that places a quantum computer (controlled by circuits) alongside a classical computer (controlled by the host language). We present QWIRE along with its type system and operational semantics, which we prove is safe and strongly normalizing whenever the host language is. We give circuits a denotational semantics in terms of density matrices. Throughout, we investigate examples that demonstrate the expressive power of QWIRE, including extensions to the host language that (1) expose a general analysis framework for circuits, and (2) provide dependent types.}},
  url = {https://doi.org/10.1145/3009837.3009894},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup232,
  title = {Clearing the clouds: a study of emerging scale-out workloads on modern hardware}},
  author = {Ferdman, Michael and Adileh, Almutaz and Kocberber, Onur and Volos, Stavros and Alisafaee, Mohammad and Jevdjic, Djordje and Kaynak, Cansu and Popescu, Adrian Daniel and Ailamaki, Anastasia and Falsafi, Babak}},
  year = {2012}},
  journal = {Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Emerging scale-out workloads require extensive amounts of computational resources. However, data centers using modern server hardware face physical constraints in space and power, limiting further expansion and calling for improvements in the computational density per server and in the per-operation energy. Continuing to improve the computational resources of the cloud while staying within physical constraints mandates optimizing server efficiency to ensure that server hardware closely matches the needs of scale-out workloads.In this work, we introduce CloudSuite, a benchmark suite of emerging scale-out workloads. We use performance counters on modern servers to study scale-out workloads, finding that today's predominant processor micro-architecture is inefficient for running these workloads. We find that inefficiency comes from the mismatch between the workload needs and modern processors, particularly in the organization of instruction and data memory systems and the processor core micro-architecture. Moreover, while today's predominant micro-architecture is inefficient when executing scale-out workloads, we find that continuing the current trends will further exacerbate the inefficiency in the future. In this work, we identify the key micro-architectural needs of scale-out workloads, calling for a change in the trajectory of server processors that would lead to improved computational density and power efficiency in data centers.}},
  url = {https://doi.org/10.1145/2150976.2150982},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup233,
  title = {An Event-Triggered Programmable Prefetcher for Irregular Workloads}},
  author = {Ainsworth, Sam and Jones, Timothy M.}},
  year = {2018}},
  journal = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many modern workloads compute on large amounts of data, often with irregular memory accesses. Current architectures perform poorly for these workloads, as existing prefetching techniques cannot capture the memory access patterns; these applications end up heavily memory-bound as a result. Although a number of techniques exist to explicitly configure a prefetcher with traversal patterns, gaining significant speedups, they do not generalise beyond their target data structures. Instead, we propose an event-triggered programmable prefetcher combining the flexibility of a general-purpose computational unit with an event-based programming model, along with compiler techniques to automatically generate events from the original source code with annotations. This allows more complex fetching decisions to be made, without needing to stall when intermediate results are required. Using our programmable prefetching system, combined with small prefetch kernels extracted from applications, we achieve an average 3.0x speedup in simulation for a variety of graph, database and HPC workloads.}},
  url = {https://doi.org/10.1145/3173162.3173189},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup234,
  title = {Tapir: Embedding Fork-Join Parallelism into LLVM's Intermediate Representation}},
  author = {Schardl, Tao B. and Moses, William S. and Leiserson, Charles E.}},
  year = {2017}},
  journal = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper explores how fork-join parallelism, as supported by concurrency platforms such as Cilk and OpenMP, can be embedded into a compiler's intermediate representation (IR). Mainstream compilers typically treat parallel linguistic constructs as syntactic sugar for function calls into a parallel runtime. These calls prevent the compiler from performing optimizations across parallel control constructs. Remedying this situation is generally thought to require an extensive reworking of compiler analyses and code transformations to handle parallel semantics.Tapir is a compiler IR that represents logically parallel tasks asymmetrically in the program's control flow graph. Tapir allows the compiler to optimize across parallel control constructs with only minor changes to its existing analyses and code transformations. To prototype Tapir in the LLVM compiler, for example, we added or modified about 6000 lines of LLVM's 4-million-line codebase. Tapir enables LLVM's existing compiler optimizations for serial code -- including loop-invariant-code motion, common-subexpression elimination, and tail-recursion elimination -- to work with parallel control constructs such as spawning and parallel loops. Tapir also supports parallel optimizations such as loop scheduling.}},
  url = {https://doi.org/10.1145/3018743.3018758},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup235,
  title = {Isomorphisms of generic recursive polynomial types}},
  author = {Fiore, Marcelo}},
  year = {2004}},
  journal = {Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper gives the first decidability results on type isomorphism for recursive types, establishing the explicit decidability of type isomorphism for the type theory of sums and products over an inhabited generic recursive polynomial type. The technical development provides connections between themes in programming-language theory (type isomorphism) and computational algebra (Gr\"{o}},
  url = {https://doi.org/10.1145/964001.964008},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup236,
  title = {Experience report: type-checking polymorphic units for astrophysics research in Haskell}},
  author = {Muranushi, Takayuki and Eisenberg, Richard A.}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM SIGPLAN Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many of the bugs in scientific programs have their roots in mistreatment of physical dimensions, via erroneous expressions in the quantity calculus. Now that the type system in the Glasgow Haskell Compiler is rich enough to support type-level integers and other promoted datatypes, we can type-check the quantity calculus in Haskell. In addition to basic dimension-aware arithmetic and unit conversions, our units library features an extensible system of dimensions and units, a notion of dimensions apart from that of units, and unit polymorphism designed to describe the laws of physics. We demonstrate the utility of units by writing an astrophysics research paper. This work is free of unit concerns because every quantity expression in the paper is rigorously type-checked.}},
  url = {https://doi.org/10.1145/2633357.2633362},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup237,
  title = {Unifiers as equivalences: proof-relevant unification of dependently typed data}},
  author = {Cockx, Jesper and Devriese, Dominique and Piessens, Frank}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Dependently typed languages such as Agda, Coq and Idris use a syntactic first-order unification algorithm to check definitions by dependent pattern matching. However, these algorithms don’t adequately consider the types of the terms being unified, leading to various unintended results. As a consequence, they require ad hoc restrictions to preserve soundness, but this makes them very hard to prove correct, modify, or extend. This paper proposes a framework for reasoning formally about unification in a dependently typed setting. In this framework, unification rules compute not just a unifier but also a corresponding correctness proof in the form of an equivalence between two sets of equations. By rephrasing the standard unification rules in a proof-relevant manner, they are guaranteed to preserve soundness of the theory. In addition, it enables us to safely add new rules that can exploit the dependencies between the types of equations. Using our framework, we reimplemented the unification algorithm used by Agda. As a result, we were able to replace previous ad hoc restrictions with formally verified unification rules, fixing a number of bugs in the process. We are convinced this will also enable the addition of new and interesting unification rules in the future, without compromising soundness along the way.}},
  url = {https://doi.org/10.1145/2951913.2951917},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup238,
  title = {Symbolic reasoning for automatic signal placement}},
  author = {Ferles, Kostas and Van Geffen, Jacob and Dillig, Isil and Smaragdakis, Yannis}},
  year = {2018}},
  journal = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Explicit signaling between threads is a perennial cause of bugs in concurrent programs. While there are several run-time techniques to automatically notify threads upon the availability of some shared resource, such techniques are not widely-adopted due to their run-time overhead. This paper proposes a new solution based on static analysis for automatically generating a performant explicit-signal program from its corresponding implicit-signal implementation. The key idea is to generate verification conditions that allow us to minimize the number of required signals and unnecessary context switches, while guaranteeing semantic equivalence between the source and target programs. We have implemented our method in a tool called Expresso and evaluate it on challenging benchmarks from prior papers and open-source software. Expresso-generated code significantly outperforms past automatic signaling mechanisms (avg. 1.56x speedup) and closely matches the performance of hand-optimized explicit-signal code.}},
  url = {https://doi.org/10.1145/3192366.3192395},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup239,
  title = {High performance Fortran for highly irregular problems}},
  author = {Hu, Y. Charlie and Johnsson, S. Lennart and Teng, Shang-Hua}},
  year = {1997}},
  journal = {Proceedings of the Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a general data parallel formulation for highly irregular problems in High Performance Fortran (HPF). Our formulation consists of(1) a method for linearizing irregular data structures (2) a data parallel implementation (in HPF) of graph partitioning algorithms applied to the linearized data structure, (3) techniques for expressing irregular communication and nonuniform computations associated with the elements of linearized data structures.We demonstrate and evaluate our formulation on a parallel, hierarchical N--body method for the evaluation of potentials and forces of nonuniform particle distributions. Our experimental results demonstrate that efficient data parallel (HPF) implementations of highly nonuniform problems are feasible with the proper language/compiler/runtime support. Our data parallel N--body code provides a much needed "benchmark" code for evaluating and improving HPF compilers.}},
  url = {https://doi.org/10.1145/263764.263769},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup240,
  title = {Efficient execution of recursive programs on commodity vector hardware}},
  author = {Ren, Bin and Jo, Youngjoon and Krishnamoorthy, Sriram and Agrawal, Kunal and Kulkarni, Milind}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The pursuit of computational efficiency has led to the proliferation of throughput-oriented hardware, from GPUs to increasingly wide vector units on commodity processors and accelerators. This hardware is designed to efficiently execute data-parallel computations in a vectorized manner. However, many algorithms are more naturally expressed as divide-and-conquer, recursive, task-parallel computations. In the absence of data parallelism, it seems that such algorithms are not well suited to throughput-oriented architectures. This paper presents a set of novel code transformations that expose the data parallelism latent in recursive, task-parallel programs. These transformations facilitate straightforward vectorization of task-parallel programs on commodity hardware. We also present scheduling policies that maintain high utilization of vector resources while limiting space usage. Across several task-parallel benchmarks, we demonstrate both efficient vector resource utilization and substantial speedup on chips using Intel’s SSE4.2 vector units, as well as accelerators using Intel’s AVX512 units.}},
  url = {https://doi.org/10.1145/2737924.2738004},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup241,
  title = {Model-based Iterative CT Image Reconstruction on GPUs}},
  author = {Sabne, Amit and Wang, Xiao and Kisner, Sherman J. and Bouman, Charles A. and Raghunathan, Anand and Midkiff, Samuel P.}},
  year = {2017}},
  journal = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Computed Tomography (CT) Image Reconstruction is an important technique used in a variety of domains, including medical imaging, electron microscopy, non-destructive testing and transportation security. Model-based Iterative Reconstruction (MBIR) using Iterative Coordinate Descent (ICD) is a CT algorithm that produces state-of-the-art results in terms of image quality. However, MBIR is highly computationally intensive and challenging to parallelize, and has traditionally been viewed as impractical in applications where reconstruction time is critical. We present the first GPU-based algorithm for ICD-based MBIR. The algorithm leverages the recently-proposed concept of SuperVoxels, and efficiently exploits the three levels of parallelism available in MBIR to better utilize the GPU hardware resources. We also explore data layout transformations to obtain more coalesced accesses and several GPU-specific optimizations for MBIR that boost performance. Across a suite of 3200 test cases, our GPU implementation obtains a geometric mean speedup of 4.43X over a state-of-the-art multi-core implementation on a 16-core iso-power CPU.}},
  url = {https://doi.org/10.1145/3018743.3018765},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup242,
  title = {The hardness of cache conscious data placement}},
  author = {Petrank, Erez and Rawitz, Dror}},
  year = {2002}},
  journal = {Proceedings of the 29th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The growing gap between the speed of memory access and cache access has made cache misses an influential factor in program efficiency. Much effort has been spent recently on reducing the number of cache misses during program run. This effort includes wise rearranging of program code, cache-conscious data placement, and algorithmic modifications that improve the program cache behavior. In this work we investigate the complexity of finding the optimal placement of objects (or code) in the memory, in the sense that this placement reduces the cache misses to the minimum. We show that this problem is one of the toughest amongst the interesting algorithmic problems in computer science. In particular, suppose one is given a sequence of memory accesses and one has to place the data in the memory so as to minimize the number of cache misses for this sequence. We show that if P ≠ NP, then one cannot efficiently approximate the optimal solution even up to a very liberal approximation ratio. Thus, this problem joins the small family of extremely inapproximable optimization problems. The other two famous members in this family are minimum coloring and maximum clique.}},
  url = {https://doi.org/10.1145/503272.503283},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup243,
  title = {Helium: lifting high-performance stencil kernels from stripped x86 binaries to halide DSL code}},
  author = {Mendis, Charith and Bosboom, Jeffrey and Wu, Kevin and Kamil, Shoaib and Ragan-Kelley, Jonathan and Paris, Sylvain and Zhao, Qin and Amarasinghe, Saman}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Highly optimized programs are prone to bit rot, where performance quickly becomes suboptimal in the face of new hardware and compiler techniques. In this paper we show how to automatically lift performance-critical stencil kernels from a stripped x86 binary and generate the corresponding code in the high-level domain-specific language Halide. Using Halide’s state-of-the-art optimizations targeting current hardware, we show that new optimized versions of these kernels can replace the originals to rejuvenate the application for newer hardware. The original optimized code for kernels in stripped binaries is nearly impossible to analyze statically. Instead, we rely on dynamic traces to regenerate the kernels. We perform buffer structure reconstruction to identify input, intermediate and output buffer shapes. We abstract from a forest of concrete dependency trees which contain absolute memory addresses to symbolic trees suitable for high-level code generation. This is done by canonicalizing trees, clustering them based on structure, inferring higher-dimensional buffer accesses and finally by solving a set of linear equations based on buffer accesses to lift them up to simple, high-level expressions. Helium can handle highly optimized, complex stencil kernels with input-dependent conditionals. We lift seven kernels from Adobe Photoshop giving a 75\% performance improvement, four kernels from IrfanView, leading to 4.97\texttimes{}},
  url = {https://doi.org/10.1145/2737924.2737974},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup244,
  title = {Reasoning about nondeterminism in programs}},
  author = {Cook, Byron and Koskinen, Eric}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Branching-time temporal logics (e.g. CTL, CTL*, modal mu-calculus) allow us to ask sophisticated questions about the nondeterminism that appears in systems. Applications of this type of reasoning include planning, games, security analysis, disproving, precondition synthesis, environment synthesis, etc. Unfortunately, existing automatic branching-time verification tools have limitations that have traditionally restricted their applicability (e.g. push-down systems only, universal path quantifiers only, etc).In this paper we introduce an automation strategy that lifts many of these previous restrictions. Our method works reliably for properties with non-trivial mixtures of universal and existential modal operators. Furthermore, our approach is designed to support (possibly infinite-state) programs.The basis of our approach is the observation that existential reasoning can be reduced to universal reasoning if the system's state-space is appropriately restricted. This restriction on the state-space must meet a constraint derived from recent work on proving non-termination. The observation leads to a new route for implementation based on existing tools. To demonstrate the practical viability of our approach, we report on the results applying our preliminary implementation to a set of benchmarks drawn from the Windows operating system, the PostgreSQL database server, SoftUpdates patching system, as well as other hand-crafted examples.}},
  url = {https://doi.org/10.1145/2491956.2491969},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup245,
  title = {Distributed Halide}},
  author = {Denniston, Tyler and Kamil, Shoaib and Amarasinghe, Saman}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many image processing tasks are naturally expressed as a pipeline of small computational kernels known as stencils. Halide is a popular domain-specific language and compiler designed to implement image processing algorithms. Halide uses simple language constructs to express what to compute and a separate scheduling co-language for expressing when and where to perform the computation. This approach has demonstrated performance comparable to or better than hand-optimized code. Until now, however, Halide has been restricted to parallel shared memory execution, limiting its performance for memory-bandwidth-bound pipelines or large-scale image processing tasks.We present an extension to Halide to support distributed-memory parallel execution of complex stencil pipelines. These extensions compose with the existing scheduling constructs in Halide, allowing expression of complex computation and communication strategies. Existing Halide applications can be distributed with minimal changes, allowing programmers to explore the tradeoff between recomputation and communication with little effort. Approximately 10 new of lines code are needed even for a 200 line, 99 stage application. On nine image processing benchmarks, our extensions give up to a 1.4\texttimes{}},
  url = {https://doi.org/10.1145/2851141.2851157},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup246,
  title = {Program synthesis using conflict-driven learning}},
  author = {Feng, Yu and Martins, Ruben and Bastani, Osbert and Dillig, Isil}},
  year = {2018}},
  journal = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We propose a new conflict-driven program synthesis technique that is capable of learning from past mistakes. Given a spurious program that violates the desired specification, our synthesis algorithm identifies the root cause of the conflict and learns new lemmas that can prevent similar mistakes in the future. Specifically, we introduce the notion of equivalence modulo conflict and show how this idea can be used to learn useful lemmas that allow the synthesizer to prune large parts of the search space. We have implemented a general-purpose CDCL-style program synthesizer called Neo and evaluate it in two different application domains, namely data wrangling in R and functional programming over lists. Our experiments demonstrate the substantial benefits of conflict-driven learning and show that Neo outperforms two state-of-the-art synthesis tools, Morpheus and Deepcoder, that target these respective domains.}},
  url = {https://doi.org/10.1145/3192366.3192382},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup247,
  title = {Practical probabilistic programming with monads}},
  author = {undefinedcibior, Adam and Ghahramani, Zoubin and Gordon, Andrew D.}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The machine learning community has recently shown a lot of interest in practical probabilistic programming systems that target the problem of Bayesian inference. Such systems come in different forms, but they all express probabilistic models as computational processes using syntax resembling programming languages. In the functional programming community monads are known to offer a convenient and elegant abstraction for programming with probability distributions, but their use is often limited to very simple inference problems. We show that it is possible to use the monad abstraction to construct probabilistic models for machine learning, while still offering good performance of inference in challenging models. We use a GADT as an underlying representation of a probability distribution and apply Sequential Monte Carlo-based methods to achieve efficient inference. We define a formal semantics via measure theory. We demonstrate a clean and elegant implementation that achieves performance comparable with Anglican, a state-of-the-art probabilistic programming system.}},
  url = {https://doi.org/10.1145/2804302.2804317},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup248,
  title = {Systems biology, models, and concurrency}},
  author = {Fontana, Walter}},
  year = {2008}},
  journal = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Models will play a central role in the representation, storage, manipulation, and communication of knowledge in systems biology. Models capable of fulfilling such a role will likely differ from the all familiar styles deployed with great success in the physical sciences. Molecular systems at the basis of cellular decision processes are concurrent and combinatorial. Their behavior is as much constrained by relationships of causality between molecular interactions as it is by chemical kinetics. Understanding how such systems give rise to coherent behavior and designing effective interventions to fight disease will require a notion of model that is akin to the concept of program in computer science. I will discuss recent progress in implementing a platform and tools for formal analysis that bring us closer to this vision. Protein interactions are represented by means of rules expressed in a formal language that captures a very simple, yet effective and biologically meaningful level of abstraction. Models, then, are collections of rules operating on an initial set of agents, in complete analogy to rules of organic chemical reactions. I will describe tools for analyzing and navigating rule collections as well as exploring their dynamics. We draw on concepts familiar to computer science, especially event structures, and adapt them to biological needs with the goal of formalizing the notion of "pathway". The challenges are many, but a road map for the future is discernible. Computer science will play a central role in providing an additional foundational layer, both theoretical and practical, that neither physics nor chemistry can offer on their own in the future definition of the biological sciences.}},
  url = {https://doi.org/10.1145/1328438.1328439},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup249,
  title = {Freer monads, more extensible effects}},
  author = {Kiselyov, Oleg and Ishii, Hiromi}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a rational reconstruction of extensible effects, the recently proposed alternative to monad transformers, as the confluence of efforts to make effectful computations compose. Free monads and then extensible effects emerge from the straightforward term representation of an effectful computation, as more and more boilerplate is abstracted away. The generalization process further leads to freer monads, constructed without the Functor constraint. The continuation exposed in freer monads can then be represented as an efficient type-aligned data structure. The end result is the algorithmically efficient extensible effects library, which is not only more comprehensible but also faster than earlier implementations. As an illustration of the new library, we show three surprisingly simple applications: non-determinism with committed choice (LogicT), catching IO exceptions in the presence of other effects, and the semi-automatic management of file handles and other resources through monadic regions. We extensively use and promote the new sort of `laziness', which underlies the left Kan extension: instead of performing an operation, keep its operands and pretend it is done.}},
  url = {https://doi.org/10.1145/2804302.2804319},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup250,
  title = {Optimizing the Four-Index Integral Transform Using Data Movement Lower Bounds Analysis}},
  author = {Rajbhandari, Samyam and Rastello, Fabrice and Kowalski, Karol and Krishnamoorthy, Sriram and Sadayappan, P.}},
  year = {2017}},
  journal = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The four-index integral transform is a fundamental and computationally demanding calculation used in many computational chemistry suites such as NWChem. It transforms a four-dimensional tensor from one basis to another. This transformation is most efficiently implemented as a sequence of four tensor contractions that each contract a four- dimensional tensor with a two-dimensional transformation matrix. Differing degrees of permutation symmetry in the intermediate and final tensors in the sequence of contractions cause intermediate tensors to be much larger than the final tensor and limit the number of electronic states in the modeled systems.Loop fusion, in conjunction with tiling, can be very effective in reducing the total space requirement, as well as data movement. However, the large number of possible choices for loop fusion and tiling, and data/computation distribution across a parallel system, make it challenging to develop an optimized parallel implementation for the four-index integral transform. We develop a novel approach to address this problem, using lower bounds modeling of data movement complexity. We establish relationships between available aggregate physical memory in a parallel computer system and ineffective fusion configurations, enabling their pruning and consequent identification of effective choices and a characterization of optimality criteria. This work has resulted in the development of a significantly improved implementation of the four-index transform that enables higher performance and the ability to model larger electronic systems than the current implementation in the NWChem quantum chemistry software suite.}},
  url = {https://doi.org/10.1145/3018743.3018771},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup251,
  title = {Distributed garbage collection for general graphs}},
  author = {Brandt, Steven R. and Krishnan, Hari and Busch, Costas and Sharma, Gokarna}},
  year = {2018}},
  journal = {Proceedings of the 2018 ACM SIGPLAN International Symposium on Memory Management}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We propose a scalable, cycle-collecting, decentralized, reference counting garbage collector with partial tracing. The algorithm is based on the Brownbridge system but uses four different types of references to label edges. Memory usage is O (log n) bits per node, where n is the number of nodes in the graph. The algorithm assumes an asynchronous network model with a reliable reordering channel. It collects garbage in O (E a ) time, where E a is the number of edges in the in- duced subgraph. The algorithm uses termination detection to manage the distributed computation, a unique identifier to break the symmetry among multiple collectors, and a transaction-based approach when multiple collectors conflict. Unlike existing algorithms, ours is not centralized, does not require barriers, does not require migration of nodes, does not require back-pointers on every edge, and is stable against concurrent mutation.}},
  url = {https://doi.org/10.1145/3210563.3210572},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup252,
  title = {Elaborating evaluation-order polymorphism}},
  author = {Dunfield, Jana}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We classify programming languages according to evaluation order: each language fixes one evaluation order as the default, making it transparent to program in that evaluation order, and troublesome to program in the other. This paper develops a type system that is impartial with respect to evaluation order. Evaluation order is implicit in terms, and explicit in types, with by-value and by-name versions of type connectives. A form of intersection type quantifies over evaluation orders, describing code that is agnostic over (that is, polymorphic in) evaluation order. By allowing such generic code, programs can express the by-value and by-name versions of a computation without code duplication. We also formulate a type system that only has by-value connectives, plus a type that generalizes the difference between by-value and by-name connectives: it is either a suspension (by name) or a "no-op" (by value). We show a straightforward encoding of the impartial type system into the more economical one. Then we define an elaboration from the economical language to a call-by-value semantics, and prove that elaborating a well-typed source program, where evaluation order is implicit, produces a well-typed target program where evaluation order is explicit. We also prove a simulation between evaluation of the target program and reductions (either by-value or by-name) in the source program. Finally, we prove that typing, elaboration, and evaluation are faithful to the type annotations given in the source program: if the programmer only writes by-value types, no by-name reductions can occur at run time.}},
  url = {https://doi.org/10.1145/2784731.2784744},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup253,
  title = {Programming and reasoning with algebraic effects and dependent types}},
  author = {Brady, Edwin}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {One often cited benefit of pure functional programming is that pure code is easier to test and reason about, both formally and informally. However, real programs have side-effects including state management, exceptions and interactions with the outside world. Haskell solves this problem using monads to capture details of possibly side-effecting computations --- it provides monads for capturing state, I/O, exceptions, non-determinism, libraries for practical purposes such as CGI and parsing, and many others, as well as monad transformers for combining multiple effects.Unfortunately, useful as monads are, they do not compose very well. Monad transformers can quickly become unwieldy when there are lots of effects to manage, leading to a temptation in larger programs to combine everything into one coarse-grained state and exception monad. In this paper I describe an alternative approach based on handling algebraic effects, implemented in the IDRIS programming language. I show how to describe side effecting computations, how to write programs which compose multiple fine-grained effects, and how, using dependent types, we can use this approach to reason about states in effectful programs.}},
  url = {https://doi.org/10.1145/2500365.2500581},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup254,
  title = {Python: the full monty}},
  author = {Politz, Joe Gibbs and Martinez, Alejandro and Milano, Mae and Warren, Sumner and Patterson, Daniel and Li, Junsong and Chitipothu, Anand and Krishnamurthi, Shriram}},
  year = {2013}},
  journal = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a small-step operational semantics for the Python programming language. We present both a core language for Python, suitable for tools and proofs, and a translation process for converting Python source to this core. We have tested the composition of translation and evaluation of the core for conformance with the primary Python implementation, thereby giving confidence in the fidelity of the semantics. We briefly report on the engineering of these components. Finally, we examine subtle aspects of the language, identifying scope as a pervasive concern that even impacts features that might be considered orthogonal.}},
  url = {https://doi.org/10.1145/2509136.2509536},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup255,
  title = {First-class runtime generation of high-performance types using exotypes}},
  author = {DeVito, Zachary and Ritchie, Daniel and Fisher, Matt and Aiken, Alex and Hanrahan, Pat}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We introduce exotypes, user-defined types that combine the flexibility of meta-object protocols in dynamically-typed languages with the performance control of low-level languages. Like objects in dynamic languages, exotypes are defined programmatically at run-time, allowing behavior based on external data such as a database schema. To achieve high performance, we use staged programming to define the behavior of an exotype during a runtime compilation step and implement exotypes in Terra, a low-level staged programming language.We show how exotype constructors compose, and use exotypes to implement high-performance libraries for serialization, dynamic assembly, automatic differentiation, and probabilistic programming. Each exotype achieves expressiveness similar to libraries written in dynamically-typed languages but implements optimizations that exceed the performance of existing libraries written in low-level statically-typed languages. Though each implementation is significantly shorter, our serialization library is 11 times faster than Kryo, and our dynamic assembler is 3--20 times faster than Google's Chrome assembler.}},
  url = {https://doi.org/10.1145/2594291.2594307},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup256,
  title = {Programming with time: cyber-physical programming with impromptu}},
  author = {Sorensen, Andrew and Gardner, Henry}},
  year = {2010}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The act of computer programming is generally considered to be temporally removed from a computer program's execution. In this paper we discuss the idea of programming as an activity that takes place within the temporal bounds of a real-time computational process and its interactions with the physical world. We ground these ideas within the con- text of livecoding -- a live audiovisual performance practice. We then describe how the development of the programming environment "Impromptu" has addressed our ideas of programming with time and the notion of the programmer as an agent in a cyber-physical system.}},
  url = {https://doi.org/10.1145/1869459.1869526},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup257,
  title = {TIC: a scalable model checking based approach to WCET estimation}},
  author = {Metta, Ravindra and Becker, Martin and Bokil, Prasad and Chakraborty, Samarjit and Venkatesh, R}},
  year = {2016}},
  journal = {Proceedings of the 17th ACM SIGPLAN/SIGBED Conference on Languages, Compilers, Tools, and Theory for Embedded Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The application of Model Checking to compute WCET has not been explored as much as Integer Linear Programming (ILP), primarily because model checkers fail to scale for complex programs. These programs have loops with large or unknown bounds, leading to a state space explosion that model checkers cannot handle. To overcome this, we have developed a technique, TIC, that employs slicing, loop acceleration and over-approximation on time-annotated source code, enabling Model Checking to scale better for WCET computation. Further, our approach is parametric, so that the user can make a trade-off between the tightness of WCET estimate and the analysis time. We conducted experiments on the M\"{a}},
  url = {https://doi.org/10.1145/2907950.2907961},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup258,
  title = {On the complexity of equivalence of specifications of infinite objects}},
  author = {Endrullis, J\"{o}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We study the complexity of deciding the equality of infinite objects specified by systems of equations, and of infinite objects specified by λ-terms. For equational specifications there are several natural notions of equality: equality in all models, equality of the sets of solutions, and equality of normal forms for productive specifications. For λ-terms we investigate B\"{o}},
  url = {https://doi.org/10.1145/2364527.2364551},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup259,
  title = {FIDEX: filtering spreadsheet data using examples}},
  author = {Wang, Xinyu and Gulwani, Sumit and Singh, Rishabh}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Data filtering in spreadsheets is a common problem faced by millions of end-users. The task of data filtering requires a computational model that can separate intended positive and negative string instances. We present a system, FIDEX, that can efficiently learn desired data filtering expressions from a small set of positive and negative string examples. There are two key ideas of our approach. First, we design an expressive DSL to represent disjunctive filter expressions needed for several real-world data filtering tasks. Second, we develop an efficient synthesis algorithm for incrementally learning consistent filter expressions in the DSL from very few positive and negative examples. A DAG-based data structure is used to succinctly represent a large number of filter expressions, and two corresponding operators are defined for algorithmically handling positive and negative examples, namely, the intersection and subtraction operators. FIDEX is able to learn data filters for 452 out of 460 real-world data filtering tasks in real time (0.22s), using only 2.2 positive string instances and 2.7 negative string instances on average.}},
  url = {https://doi.org/10.1145/2983990.2984030},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup260,
  title = {The complexity of interaction}},
  author = {Gimenez, St\'{e}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this paper, we analyze the complexity of functional programs written in the interaction-net computation model, an asynchronous, parallel and confluent model that generalizes linear-logic proof nets. Employing user-defined sized and scheduled types, we certify concrete time, space and space-time complexity bounds for both sequential and parallel reductions of interaction-net programs by suitably assigning complexity potentials to typed nodes. The relevance of this approach is illustrated on archetypal programming examples. The provided analysis is precise, compositional and is, in theory, not restricted to particular complexity classes.}},
  url = {https://doi.org/10.1145/2837614.2837646},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup261,
  title = {Leveraging Weighted Automata in Compositional Reasoning about Concurrent Probabilistic Systems}},
  author = {He, Fei and Gao, Xiaowei and Wang, Bow-Yaw and Zhang, Lijun}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We propose the first sound and complete learning-based compositional verification technique for probabilistic safety properties on concurrent systems where each component is an Markov decision process. Different from previous works, weighted assumptions are introduced to attain completeness of our framework. Since weighted assumptions can be implicitly represented by multi-terminal binary decision diagrams (MTBDD's), we give an L*-based learning algorithm for MTBDD's to infer weighted assumptions. Experimental results suggest promising outlooks for our compositional technique.}},
  url = {https://doi.org/10.1145/2676726.2676998},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup262,
  title = {MAERI: Enabling Flexible Dataflow Mapping over DNN Accelerators via Reconfigurable Interconnects}},
  author = {Kwon, Hyoukjun and Samajdar, Ananda and Krishna, Tushar}},
  year = {2018}},
  journal = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Deep neural networks (DNN) have demonstrated highly promising results across computer vision and speech recognition, and are becoming foundational for ubiquitous AI. The computational complexity of these algorithms and a need for high energy-efficiency has led to a surge in research on hardware accelerators. \% for this paradigm. To reduce the latency and energy costs of accessing DRAM, most DNN accelerators are spatial in nature, with hundreds of processing elements (PE) operating in parallel and communicating with each other directly. DNNs are evolving at a rapid rate, and it is common to have convolution, recurrent, pooling, and fully-connected layers with varying input and filter sizes in the most recent topologies.They may be dense or sparse. They can also be partitioned in myriad ways (within and across layers) to exploit data reuse (weights and intermediate outputs). All of the above can lead to different dataflow patterns within the accelerator substrate. Unfortunately, most DNN accelerators support only fixed dataflow patterns internally as they perform a careful co-design of the PEs and the network-on-chip (NoC). In fact, the majority of them are only optimized for traffic within a convolutional layer. This makes it challenging to map arbitrary dataflows on the fabric efficiently, and can lead to underutilization of the available compute resources. DNN accelerators need to be programmable to enable mass deployment. For them to be programmable, they need to be configurable internally to support the various dataflow patterns that could be mapped over them. To address this need, we present MAERI, which is a DNN accelerator built with a set of modular and configurable building blocks that can easily support myriad DNN partitions and mappings by appropriately configuring tiny switches. MAERI provides 8-459\% better utilization across multiple dataflow mappings over baselines with rigid NoC fabrics.}},
  url = {https://doi.org/10.1145/3173162.3173176},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup263,
  title = {Verifying bit-manipulations of floating-point}},
  author = {Lee, Wonyeol and Sharma, Rahul and Aiken, Alex}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Reasoning about floating-point is difficult and becomes only more so if there is an interplay between floating-point and bit-level operations. Even though real-world floating-point libraries use implementations that have such mixed computations, no systematic technique to verify the correctness of the implementations of such computations is known. In this paper, we present the first general technique for verifying the correctness of mixed binaries, which combines abstraction, analytical optimization, and testing. The technique provides a method to compute an error bound of a given implementation with respect to its mathematical specification. We apply our technique to Intel's implementations of transcendental functions and prove formal error bounds for these widely used routines.}},
  url = {https://doi.org/10.1145/2908080.2908107},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup264,
  title = {Portable performance on heterogeneous architectures}},
  author = {Phothilimthana, Phitchaya Mangpo and Ansel, Jason and Ragan-Kelley, Jonathan and Amarasinghe, Saman}},
  year = {2013}},
  journal = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Trends in both consumer and high performance computing are bringing not only more cores, but also increased heterogeneity among the computational resources within a single machine. In many machines, one of the greatest computational resources is now their graphics coprocessors (GPUs), not just their primary CPUs. But GPU programming and memory models differ dramatically from conventional CPUs, and the relative performance characteristics of the different processors vary widely between machines. Different processors within a system often perform best with different algorithms and memory usage patterns, and achieving the best overall performance may require mapping portions of programs across all types of resources in the machine.To address the problem of efficiently programming machines with increasingly heterogeneous computational resources, we propose a programming model in which the best mapping of programs to processors and memories is determined empirically. Programs define choices in how their individual algorithms may work, and the compiler generates further choices in how they can map to CPU and GPU processors and memory systems. These choices are given to an empirical autotuning framework that allows the space of possible implementations to be searched at installation time. The rich choice space allows the autotuner to construct poly-algorithms that combine many different algorithmic techniques, using both the CPU and the GPU, to obtain better performance than any one technique alone. Experimental results show that algorithmic changes, and the varied use of both CPUs and GPUs, are necessary to obtain up to a 16.5x speedup over using a single program configuration for all architectures.}},
  url = {https://doi.org/10.1145/2451116.2451162},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup265,
  title = {SC-Haskell: Sequential Consistency in Languages That Minimize Mutable Shared Heap}},
  author = {Vollmer, Michael and Scott, Ryan G. and Musuvathi, Madanlal and Newton, Ryan R.}},
  year = {2017}},
  journal = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A core, but often neglected, aspect of a programming language design is its memory (consistency) model. Sequential consistency~(SC) is the most intuitive memory model for programmers as it guarantees sequential composition of instructions and provides a simple abstraction of shared memory as a single global store with atomic read and writes. Unfortunately, SC is widely considered to be impractical due to its associated performance overheads. Perhaps contrary to popular opinion, this paper demonstrates that SC is achievable with acceptable performance overheads for mainstream languages that minimize mutable shared heap. In particular, we modify the Glasgow Haskell Compiler to insert fences on all writes to shared mutable memory accessed in nonfunctional parts of the program. For a benchmark suite containing 1,279 programs, SC adds a geomean overhead of less than 0.4\% on an x86 machine. The efficiency of SC arises primarily due to the isolation provided by the Haskell type system between purely functional and thread-local imperative computations on the one hand, and imperative computations on the global heap on the other. We show how to use new programming idioms to further reduce the SC overhead; these create a virtuous cycle of less overhead and even stronger semantic guarantees (static data-race freedom).}},
  url = {https://doi.org/10.1145/3018743.3018746},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup266,
  title = {Statistical debugging for real-world performance problems}},
  author = {Song, Linhai and Lu, Shan}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Design and implementation defects that lead to inefficient computation widely exist in software. These defects are difficult to avoid and discover. They lead to severe performance degradation and energy waste during production runs, and are becoming increasingly critical with the meager increase of single-core hardware performance and the increasing concerns about energy constraints. Effective tools that diagnose performance problems and point out the inefficiency root cause are sorely needed.The state of the art of performance diagnosis is preliminary. Profiling can identify the functions that consume the most computation resources, but can neither identify the ones that waste the most resources nor explain why. Performance-bug detectors can identify specific type of inefficient computation, but are not suited for diagnosing general performance problems. Effective failure diagnosis techniques, such as statistical debugging, have been proposed for functional bugs. However, whether they work for performance problems is still an open question.In this paper, we first conduct an empirical study to understand how performance problems are observed and reported by real-world users. Our study shows that statistical debugging is a natural fit for diagnosing performance problems, which are often observed through comparison-based approaches and reported together with both good and bad inputs. We then thoroughly investigate different design points in statistical debugging, including three different predicates and two different types of statistical models, to understand which design point works the best for performance diagnosis. Finally, we study how some unique nature of performance bugs allows sampling techniques to lower the overhead of run-time performance diagnosis without extending the diagnosis latency.}},
  url = {https://doi.org/10.1145/2660193.2660234},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup267,
  title = {Optimizing Ackermann's function by incrementalization}},
  author = {Liu, Yanhong A. and Stoller, Scott D.}},
  year = {2003}},
  journal = {Proceedings of the 2003 ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Based Program Manipulation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper describes a formal derivation of an optimized Ackermann's function following a general and systematic method based on incrementalization. The method identifies an appropriate input increment operation and computes the function by repeatedly performing an incremental computation at the step of the increment. This eliminates repeated subcomputations in executions that follow the straightforward recursive definition of Ackermann's function, yielding an optimized program that is drastically faster and takes extremely little space. This case study uniquely shows the power and limitation of the incrementalization method, as well as both the iterative and recursive nature of computation underlying the optimized Ackermann's function.}},
  url = {https://doi.org/10.1145/777388.777398},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup268,
  title = {Inductive reasoning about effectful data types}},
  author = {Filinski, Andrzej and St\o{}},
  year = {2007}},
  journal = {Proceedings of the 12th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a pair of reasoning principles, definition and proof by rigid induction, which can be seen as proper generalizations of lazy-datatype induction to monadic effects other than partiality. We further show how these principles can be integrated into logical-relations arguments, and obtain as a particular instance a general and principled proof that the success-stream and failure-continuation models of backtracking are equivalent. As another application, we present a monadic model of general search trees, not necessarily traversed depth-first. The results are applicable to both lazy and eager languages, and we emphasize this by presenting most examples in both Haskell and SML.}},
  url = {https://doi.org/10.1145/1291151.1291168},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup269,
  title = {Elaboration on functional dependencies: functional dependencies are dead, long live functional dependencies!}},
  author = {Karachalias, Georgios and Schrijvers, Tom}},
  year = {2017}},
  journal = {Proceedings of the 10th ACM SIGPLAN International Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Functional dependencies are a popular extension to Haskell's type-class system because they provide fine-grained control over type inference, resolve ambiguities and even enable type-level computations. Unfortunately, several aspects of Haskell's functional dependencies are ill-understood. In particular, the GHC compiler does not properly enforce the functional dependency property, and rejects well-typed programs because it does not know how to elaborate them into its core language, System FC. This paper presents a novel formalization of functional dependencies that addresses these issues: We explicitly capture the functional dependency property in the type system, in the form of explicit type equalities. We also provide a type inference algorithm and an accompanying elaboration strategy which allows all well-typed programs to be elaborated into System FC.}},
  url = {https://doi.org/10.1145/3122955.3122966},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup270,
  title = {Accelerating program analyses by cross-program training}},
  author = {Kulkarni, Sulekha and Mangal, Ravi and Zhang, Xin and Naik, Mayur}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Practical programs share large modules of code. However, many program analyses are ineffective at reusing analysis results for shared code across programs. We present POLYMER, an analysis optimizer to address this problem. POLYMER runs the analysis offline on a corpus of training programs and learns analysis facts over shared code. It prunes the learnt facts to eliminate intermediate computations and then reuses these pruned facts to accelerate the analysis of other programs that share code with the training corpus. We have implemented POLYMER to accelerate analyses specified in Datalog, and apply it to optimize two analyses for Java programs: a call-graph analysis that is flow- and context-insensitive, and a points-to analysis that is flow- and context-sensitive. We evaluate the resulting analyses on ten programs from the DaCapo suite that share the JDK library. POLYMER achieves average speedups of 2.6\texttimes{}},
  url = {https://doi.org/10.1145/2983990.2984023},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup271,
  title = {GPU multisplit}},
  author = {Ashkiani, Saman and Davidson, Andrew and Meyer, Ulrich and Owens, John D.}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Multisplit is a broadly useful parallel primitive that permutes its input data into contiguous buckets or bins, where the function that categorizes an element into a bucket is provided by the programmer. Due to the lack of an efficient multisplit on GPUs, programmers often choose to implement multisplit with a sort. However, sort does more work than necessary to implement multisplit, and is thus inefficient. In this work, we provide a parallel model and multiple implementations for the multisplit problem. Our principal focus is multisplit for a small number of buckets. In our implementations, we exploit the computational hierarchy of the GPU to perform most of the work locally, with minimal usage of global operations. We also use warp-synchronous programming models to avoid branch divergence and reduce memory usage, as well as hierarchical reordering of input elements to achieve better coalescing of global memory accesses. On an NVIDIA K40c GPU, for key-only (key-value) multisplit, we demonstrate a 3.0-6.7x (4.4-8.0x) speedup over radix sort, and achieve a peak throughput of 10.0 G keys/s.}},
  url = {https://doi.org/10.1145/2851141.2851169},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup272,
  title = {Application-aware management of parallel simulation collections}},
  author = {Yau, Siu and Karamcheti, Vijay and Zorin, Denis and Damevski, Kostadin and Parker, Steven G.}},
  year = {2009}},
  journal = {Proceedings of the 14th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents a system deployed on parallel clusters to manage a collection of parallel simulations that make up a computational study. It explores how such a system can extend traditional parallel job scheduling and resource allocation techniques to incorporate knowledge specific to the study.Using a UINTAH-based helium gas simulation code (ARCHES) and the SimX system for multi-experiment computational studies, this paper demonstrates that, by using application-specific knowledge in resource allocation and scheduling decisions, one can reduce the run time of a computational study from over 20 hours to under 4.5 hours on a 32-processor cluster, and from almost 11 hours to just over 3.5 hours on a 64-processor cluster.}},
  url = {https://doi.org/10.1145/1504176.1504184},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup273,
  title = {Quipper: a scalable quantum programming language}},
  author = {Green, Alexander S. and Lumsdaine, Peter LeFanu and Ross, Neil J. and Selinger, Peter and Valiron, Beno\^{\i}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The field of quantum algorithms is vibrant. Still, there is currently a lack of programming languages for describing quantum computation on a practical scale, i.e., not just at the level of toy problems. We address this issue by introducing Quipper, a scalable, expressive, functional, higher-order quantum programming language. Quipper has been used to program a diverse set of non-trivial quantum algorithms, and can generate quantum gate representations using trillions of gates. It is geared towards a model of computation that uses a classical computer to control a quantum device, but is not dependent on any particular model of quantum hardware. Quipper has proven effective and easy to use, and opens the door towards using formal methods to analyze quantum algorithms.}},
  url = {https://doi.org/10.1145/2491956.2462177},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup274,
  title = {Shadow state encoding for efficient monitoring of block-level properties}},
  author = {Vorobyov, Kostyantyn and Signoles, Julien and Kosmatov, Nikolai}},
  year = {2017}},
  journal = {Proceedings of the 2017 ACM SIGPLAN International Symposium on Memory Management}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Memory shadowing associates addresses from an application's memory to values stored in a disjoint memory space called shadow memory. At runtime shadow values store metadata about application memory locations they are mapped to. Shadow state encodings -- the structure of shadow values and their interpretation -- vary across different tools. Encodings used by the state-of-the-art monitoring tools have been proven useful for tracking memory at a byte-level, but cannot address properties related to memory block boundaries. Tracking block boundaries is however crucial for spatial memory safety analysis, where a spatial violation such as out-of-bounds access, may dereference an allocated location belonging to an adjacent block or a different struct member. This paper describes two novel shadow state encodings which capture block-boundary-related properties. These encodings have been implemented in E-ACSL - a runtime verification tool for C programs. Initial experiments involving checking validity of pointer and array accesses in computationally intensive runs of programs selected from SPEC CPU benchmarks demonstrate runtime and memory overheads comparable to state-of-the-art memory debuggers.}},
  url = {https://doi.org/10.1145/3092255.3092269},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup275,
  title = {System FC with explicit kind equality}},
  author = {Weirich, Stephanie and Hsu, Justin and Eisenberg, Richard A.}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {System FC, the core language of the Glasgow Haskell Compiler, is an explicitly-typed variant of System F with first-class type equality proofs called coercions. This extensible proof system forms the foundation for type system extensions such as type families (type-level functions) and Generalized Algebraic Datatypes (GADTs). Such features, in conjunction with kind polymorphism and datatype promotion, support expressive compile-time reasoning.However, the core language lacks explicit kind equality proofs. As a result, type-level computation does not have access to kind-level functions or promoted GADTs, the type-level analogues to expression-level features that have been so useful. In this paper, we eliminate such discrepancies by introducing kind equalities to System FC. Our approach is based on dependent type systems with heterogeneous equality and the "Type-in-Type" axiom, yet it preserves the metatheoretic properties of FC. In particular, type checking is simple, decidable and syntax directed. We prove the preservation and progress theorems for the extended language.}},
  url = {https://doi.org/10.1145/2500365.2500599},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup276,
  title = {Advanced automata-based algorithms for program termination checking}},
  author = {Chen, Yu-Fang and Heizmann, Matthias and Leng\'{a}},
  year = {2018}},
  journal = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In 2014, Heizmann et al. proposed a novel framework for program termination analysis. The analysis starts with a termination proof of a sample path. The path is generalized to a B\"{u}},
  url = {https://doi.org/10.1145/3192366.3192405},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup277,
  title = {Certified symbolic management of financial multi-party contracts}},
  author = {Bahr, Patrick and Berthold, Jost and Elsman, Martin}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Domain-specific languages (DSLs) for complex financial contracts are in practical use in many banks and financial institutions today. Given the level of automation and pervasiveness of software in the sector, the financial domain is immensely sensitive to software bugs. At the same time, there is an increasing need to analyse (and report on) the interaction between multiple parties. In this paper, we present a multi-party contract language that rigorously relegates any artefacts of simulation and computation from its core, which leads to favourable algebraic properties, and therefore allows for formalising domain-specific analyses and transformations using a proof assistant. At the centre of our formalisation is a simple denotational semantics independent of any stochastic aspects. Based on this semantics, we devise certified contract analyses and transformations. In particular, we give a type system, with an accompanying type inference procedure, that statically ensures that contracts follow the principle of causality. Moreover, we devise a reduction semantics that allows us to evolve contracts over time, in accordance with the denotational semantics. From the verified Coq definitions, we automatically extract a Haskell implementation of an embedded contract DSL along with the formally verified contract management functionality. This approach opens a road map towards more reliable contract management software, including the possibility of analysing contracts based on symbolic instead of numeric methods.}},
  url = {https://doi.org/10.1145/2784731.2784747},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup278,
  title = {Integrating Linear and Dependent Types}},
  author = {Krishnaswami, Neelakantan R. and Pradic, Pierre and Benton, Nick}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this paper, we show how to integrate linear types with type dependency, by extending the linear/non-linear calculus of Benton to support type dependency. Next, we give an application of this calculus by giving a proof-theoretic account of imperative programming, which requires extending the calculus with computationally irrelevant quantification, proof irrelevance, and a monad of computations. We show the soundness of our theory by giving a realizability model in the style of Nuprl, which permits us to validate not only the beta-laws for each type, but also the eta-laws. These extensions permit us to decompose Hoare triples into a collection of simpler type-theoretic connectives, yielding a rich equational theory for dependently-typed higher-order imperative programs. Furthermore, both the type theory and its model are relatively simple, even when all of the extensions are considered.}},
  url = {https://doi.org/10.1145/2676726.2676969},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup279,
  title = {PLUTO+: near-complete modeling of affine transformations for parallelism and locality}},
  author = {Acharya, Aravind and Bondhugula, Uday}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Affine transformations have proven to be very powerful for loop restructuring due to their ability to model a very wide range of transformations. A single multi-dimensional affine function can represent a long and complex sequence of simpler transformations. Existing affine transformation frameworks like the Pluto algorithm, that include a cost function for modern multicore architectures where coarse-grained parallelism and locality are crucial, consider only a sub-space of transformations to avoid a combinatorial explosion in finding the transformations. The ensuing practical trade-offs lead to the exclusion of certain useful transformations, in particular, transformation compositions involving loop reversals and loop skewing by negative factors. In this paper, we propose an approach to address this limitation by modeling a much larger space of affine transformations in conjunction with the Pluto algorithm's cost function. We perform an experimental evaluation of both, the effect on compilation time, and performance of generated codes. The evaluation shows that our new framework, Pluto+, provides no degradation in performance in any of the Polybench benchmarks. For Lattice Boltzmann Method (LBM) codes with periodic boundary conditions, it provides a mean speedup of 1.33x over Pluto. We also show that Pluto+ does not increase compile times significantly. Experimental results on Polybench show that Pluto+ increases overall polyhedral source-to-source optimization time only by 15\%. In cases where it improves execution time significantly, it increased polyhedral optimization time only by 2.04x.}},
  url = {https://doi.org/10.1145/2688500.2688512},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup280,
  title = {Verified decision procedures for MSO on words based on derivatives of regular expressions}},
  author = {Traytel, Dmitriy and Nipkow, Tobias}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Monadic second-order logic on finite words (MSO) is a decidable yet expressive logic into which many decision problems can be encoded. Since MSO formulas correspond to regular languages, equivalence of MSO formulas can be reduced to the equivalence of some regular structures (e.g. automata). This paper presents a verified functional decision procedure for MSO formulas that is not based on automata but on regular expressions. Functional languages are ideally suited for this task: regular expressions are data types and functions on them are defined by pattern matching and recursion and are verified by structural induction.Decision procedures for regular expression equivalence have been formalized before, usually based on Brzozowski derivatives. Yet, for a straightforward embedding of MSO formulas into regular expressions an extension of regular expressions with a projection operation is required. We prove total correctness and completeness of an equivalence checker for regular expressions extended in that way. We also define a language-preserving translation of formulas into regular expressions with respect to two different semantics of MSO. Our results have been formalized and verified in the theorem prover Isabelle. Using Isabelle's code generation facility, this yields purely functional, formally verified programs that decide equivalence of MSO formulas.}},
  url = {https://doi.org/10.1145/2500365.2500612},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup281,
  title = {Wild control operators}},
  author = {Barker, Chris}},
  year = {2009}},
  journal = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Linguists seek to understand the semantics of expressions in human languages. Certainly there are many natural language expressions--operators in the wild, so to speak--that control evaluation in ways that are familiar from programming languages: just think of the natural-language counterparts of if, unless, while, etc. But in general, how well-behaved are control operators found in the wild? Can we always understand them in terms of familiar programming constructs, or do they go significantly beyond the expressive power of programming languages?As an example where operators from a programming langauge can provide an insightful analysis of a natural language construction, consider the difference in meaning between the following two sentences:(1) a. \&amp; John only drinks PERRIER. (emphasis on Perrier)b. John only DRINKS Perrier. (emphasis on drinks)The first sentence entails that John never drinks, say, Evian, but the second sentence entails instead that John never does anything (relevant) with Perrier except drink it. I will suggest that we can understand this difference by expressing the meanings of these sentences in terms of Sitaram's fcontrol and run operators (variants on throw and catch).But not all wild operators are so easily captured. I will discuss in some detail the meaning of the word same in English as it occurs in the following sentence:(2) \&amp; John and Bill read the same book.This sentence has a prominent interpretation on which it means (roughly) 'there is some book x such that John read x and Bill read x. I provide a preliminary analysis based on Danvy and Filinski's shift and reset. However, the shift reset approach does not generalize to the full range of related sentences in English. I give a more general solution expressed in a Type Logical Grammar (a certain kind of substructural logic) with explicit continuations. But even this is inadequate: I go on to discuss additional, only slightly less ordinary uses of same that remain untamed by any known compositional semantics.}},
  url = {https://doi.org/10.1145/1480881.1480901},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup282,
  title = {Paravirtual Remote I/O}},
  author = {Kuperman, Yossi and Moscovici, Eyal and Nider, Joel and Ladelsky, Razya and Gordon, Abel and Tsafrir, Dan}},
  year = {2016}},
  journal = {Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The traditional "trap and emulate" I/O paravirtualization model conveniently allows for I/O interposition, yet it inherently incurs costly guest-host context switches. The newer "sidecore" model eliminates this overhead by dedicating host (side)cores to poll the relevant guest memory regions and react accordingly without context switching. But the dedication of sidecores on each host might be wasteful when I/O activity is low, or it might not provide enough computational power when I/O activity is high. We propose to alleviate this problem at rack scale by consolidating the dedicated sidecores spread across several hosts onto one server. The hypervisor is then effectively split into two parts: the local hypervisor that hosts the VMs, and the remote hypervisor that processes their paravirtual I/O. We call this model vRIO---paraVirtual Remote I/O. We find that by increasing the latency somewhat, it provides comparable throughput with fewer sidecores and superior throughput with the same number of sidecores as compared to the state of the art. vRIO additionally constitutes a new, cost-effective way to consolidate I/O devices (on the remote hypervisor) while supporting efficient programmable I/O interposition.}},
  url = {https://doi.org/10.1145/2872362.2872378},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup283,
  title = {Learning nominal automata}},
  author = {Moerman, Joshua and Sammartino, Matteo and Silva, Alexandra and Klin, Bartek and Szynwelski, Micha\l{}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present an Angluin-style algorithm to learn nominal automata, which are acceptors of languages over infinite (structured) alphabets. The abstract approach we take allows us to seamlessly extend known variations of the algorithm to this new setting. In particular we can learn a subclass of nominal non-deterministic automata. An implementation using a recently developed Haskell library for nominal computation is provided for preliminary experiments.}},
  url = {https://doi.org/10.1145/3009837.3009879},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup284,
  title = {Language support for dynamic, hierarchical data partitioning}},
  author = {Treichler, Sean and Bauer, Michael and Aiken, Alex}},
  year = {2013}},
  journal = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Applications written for distributed-memory parallel architectures must partition their data to enable parallel execution. As memory hierarchies become deeper, it is increasingly necessary that the data partitioning also be hierarchical to match. Current language proposals perform this hierarchical partitioning statically, which excludes many important applications where the appropriate partitioning is itself data dependent and so must be computed dynamically. We describe Legion, a region-based programming system, where each region may be partitioned into subregions. Partitions are computed dynamically and are fully programmable. The division of data need not be disjoint and subregions of a region may overlap, or alias one another. Computations use regions with certain privileges (e.g., expressing that a computation uses a region read-only) and data coherence (e.g., expressing that the computation need only be atomic with respect to other operations on the region), which can be controlled on a per-region (or subregion) basis.We present the novel aspects of the Legion design, in particular the combination of static and dynamic checks used to enforce soundness. We give an extended example illustrating how Legion can express computations with dynamically determined relationships between computations and data partitions. We prove the soundness of Legion's type system, and show Legion type checking improves performance by up to 71\% by eliding provably safe memory checks. In particular, we show that the dynamic checks to detect aliasing at runtime at the region granularity have negligible overhead. We report results for three real-world applications running on distributed memory machines, achieving up to 62.5X speedup on 96 GPUs on the Keeneland supercomputer.}},
  url = {https://doi.org/10.1145/2509136.2509545},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup285,
  title = {Resolving and exploiting the k-CFA paradox: illuminating functional vs. object-oriented program analysis}},
  author = {Might, Matthew and Smaragdakis, Yannis and Van Horn, David}},
  year = {2010}},
  journal = {Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Low-level program analysis is a fundamental problem, taking the shape of "flow analysis" in functional languages and "points-to" analysis in imperative and object-oriented languages. Despite the similarities, the vocabulary and results in the two communities remain largely distinct, with limited cross-understanding. One of the few links is Shivers's k-CFA work, which has advanced the concept of "context-sensitive analysis" and is widely known in both communities.Recent results indicate that the relationship between the functional and object-oriented incarnations of k-CFA is not as well understood as thought. Van Horn and Mairson proved k-CFA for k ≥ 1 to be EXPTIME-complete; hence, no polynomial-time algorithm can exist. Yet, there are several polynomial-time formulations of context-sensitive points-to analyses in object-oriented languages. Thus, it seems that functional k-CFA may actually be a profoundly different analysis from object-oriented k-CFA. We resolve this paradox by showing that the exact same specification of k-CFA is polynomial-time for object-oriented languages yet exponential-time for functional ones: objects and closures are subtly different, in a way that interacts crucially with context-sensitivity and complexity. This illumination leads to an immediate payoff: by projecting the object-oriented treatment of objects onto closures, we derive a polynomial-time hierarchy of context-sensitive CFAs for functional programs.}},
  url = {https://doi.org/10.1145/1806596.1806631},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup286,
  title = {Simple, Accurate, Analytical Time Modeling and Optimal Tile Size Selection for GPGPU Stencils}},
  author = {Prajapati, Nirmal and Ranasinghe, Waruna and Rajopadhye, Sanjay and Andonov, Rumen and Djidjev, Hristo and Grosser, Tobias}},
  year = {2017}},
  journal = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Stencil computations are an important class of compute and data intensive programs that occur widely in scientific and engineeringapplications. A number of tools use sophisticated tiling, parallelization, and memory mapping strategies, and generate code that relies on vendor-supplied compilers. This code has a number of parameters, such as tile sizes, that are then tuned via empirical exploration.We develop a model that guides such a choice. Our model is a simple set of analytical functions that predict the execution time of the generated code. It is deliberately optimistic, since tile sizes and, moreover, the optimistic assumptions are intended to enable we are targeting modeling and parameter selections yielding highly tuned codes.We experimentally validate the model on a number of 2D and 3D stencil codes, and show that the root mean square error in the execution time is less than 10\% for the subset of the codes that achieve performance within 20\% of the best. Furthermore, based on using our model, we are able to predict tile sizes that achieve a further improvement of 9\% on average.}},
  url = {https://doi.org/10.1145/3018743.3018744},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup287,
  title = {Diagnosing the causes and severity of one-sided message contention}},
  author = {Tallent, Nathan R. and Vishnu, Abhinav and Van Dam, Hubertus and Daily, Jeff and Kerbyson, Darren J. and Hoisie, Adolfy}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Two trends suggest network contention for one-sided messages is poised to become a performance problem that concerns application developers: an increased interest in one-sided programming models and a rising ratio of hardware threads to network injection bandwidth. Often it is difficult to reason about when one-sided tasks decrease or increase network contention. We present effective and portable techniques for diagnosing the causes and severity of one-sided message contention. To detect that a message is affected by contention, we maintain statistics representing instantaneous network resource demand. Using lightweight measurement and modeling, we identify the portion of a message's latency that is due to contention and whether contention occurs at the initiator or target. We attribute these metrics to program statements in their full static and dynamic context. We characterize contention for an important computational chemistry benchmark on InfiniBand, Cray Aries, and IBM Blue Gene/Q interconnects. We pinpoint the sources of contention, estimate their severity, and show that when message delivery time deviates from an ideal model, there are other messages contending for the same network links. With a small change to the benchmark, we reduce contention by 50\% and improve total runtime by 20\%.}},
  url = {https://doi.org/10.1145/2688500.2688516},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup288,
  title = {Grammar-aware Parallelization for Scalable XPath Querying}},
  author = {Jiang, Lin and Zhao, Zhijia}},
  year = {2017}},
  journal = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Semi-structured data emerge in many domains, especially in web analytics and business intelligence. However, querying such data is inherently sequential due to the nested structure of input data. Existing solutions pessimistically enumerate all execution paths to circumvent dependencies, yielding sub-optimal performance and limited scalability.This paper presents GAP, a parallelization scheme that, for the first time, leverages the grammar of the input data to boost the parallelization efficiency. GAP leverages static analysis to infer feasible execution paths for specific con- texts based on the grammar of the semi-structured data. It can eliminate unnecessary paths without compromising the correctness. In the absence of a pre-defined grammar, GAP switches into a speculative execution mode and takes potentially incomplete grammar extracted either from prior inputs. Together, the dual-mode GAP reduces the execution paths from all paths to a minimum, therefore maximizing the parallelization efficiency and scalability. The benefits of path elimination go beyond reducing extra computation -- it also enables the use of more efficient data structures, which further improves the efficiency. An evaluation on a large set of standard benchmarks with diverse queries shows that GAP yields significant efficiency increase and boosts the speedup of the state-of-the-art from 2.9X to 17.6X on a 20-core ma- chine for a set of 200 queries.}},
  url = {https://doi.org/10.1145/3018743.3018772},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup289,
  title = {Modular reasoning about concurrent higher-order imperative programs}},
  author = {Birkedal, Lars}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {},
  url = {https://doi.org/10.1145/2535838.2537849},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup290,
  title = {Handlers in action}},
  author = {Kammar, Ohad and Lindley, Sam and Oury, Nicolas}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Plotkin and Pretnar's handlers for algebraic effects occupy a sweet spot in the design space of abstractions for effectful computation. By separating effect signatures from their implementation, algebraic effects provide a high degree of modularity, allowing programmers to express effectful programs independently of the concrete interpretation of their effects. A handler is an interpretation of the effects of an algebraic computation. The handler abstraction adapts well to multiple settings: pure or impure, strict or lazy, static types or dynamic types. This is a position paper whose main aim is to popularise the handler abstraction. We give a gentle introduction to its use, a collection of illustrative examples, and a straightforward operational semantics. We describe our Haskell implementation of handlers in detail, outline the ideas behind our OCaml, SML, and Racket implementations, and present experimental results comparing handlers with existing code.}},
  url = {https://doi.org/10.1145/2500365.2500590},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup291,
  title = {Prototyping Fortran-90 compilers for massively parallel machines}},
  author = {Chen, Marina and Cowie, James}},
  year = {1992}},
  journal = {Proceedings of the ACM SIGPLAN 1992 Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Massively parallel architectures, and the languages used to program them, are among both the most difficult and the most rapidly-changing subjects for compilation. This has created a demand for new compiler prototyping technologies that allow novel style of compilation and optimization to be tested in a reasonable amount of time.Using formal specification techniques, we have produced a data-parallel Fortran-90 subset compiler for Thinking Machines' Connection Machine/2 and Connection Machine/5. The prototype produces code from initial Fortran-90 benchmarks demonstrating sustained performance superior to hand-coded Lisp and competitive with Thinking Machines' CM Fortran compiler. This paper presents some new specification techniques necessary to construct competitive, easily retargetable prototype compilers.}},
  url = {https://doi.org/10.1145/143095.143122},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup292,
  title = {Disjunctive normal forms and local exceptions}},
  author = {Beffara, Emmanuel and Danos, Vincent}},
  year = {2003}},
  journal = {Proceedings of the Eighth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {All classical ?-terms typable with disjunctive normal forms are shown to share a common computational behavior: they implement a local exception handling mechanism whose exact workings depend on the tautology. Equivalent and more efficient control combinators are described through a specialized sequent calculus and shown to be correct.}},
  url = {https://doi.org/10.1145/944705.944724},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup293,
  title = {Modular reasoning for deterministic parallelism}},
  author = {Dodds, Mike and Jagannathan, Suresh and Parkinson, Matthew J.}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Weaving a concurrency control protocol into a program is difficult and error-prone. One way to alleviate this burden is deterministic parallelism. In this well-studied approach to parallelisation, a sequential program is annotated with sections that can execute concurrently, with automatically injected control constructs used to ensure observable behaviour consistent with the original program.This paper examines the formal specification and verification of these constructs. Our high-level specification defines the conditions necessary for correct execution; these conditions reflect program dependencies necessary to ensure deterministic behaviour. We connect the high-level specification used by clients of the library with the low-level library implementation, to prove that a client's requirements for determinism are enforced. Significantly, we can reason about program and library correctness without breaking abstraction boundaries.To achieve this, we use concurrent abstract predicates, based on separation logic, to encapsulate racy behaviour in the library's implementation. To allow generic specifications of libraries that can be instantiated by client programs, we extend the logic with higher-order parameters and quantification. We show that our high-level specification abstracts the details of deterministic parallelism by verifying two different low-level implementations of the library.}},
  url = {https://doi.org/10.1145/1926385.1926416},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup294,
  title = {Views: compositional reasoning for concurrent programs}},
  author = {Dinsdale-Young, Thomas and Birkedal, Lars and Gardner, Philippa and Parkinson, Matthew and Yang, Hongseok}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Compositional abstractions underly many reasoning principles for concurrent programs: the concurrent environment is abstracted in order to reason about a thread in isolation; and these abstractions are composed to reason about a program consisting of many threads. For instance, separation logic uses formulae that describe part of the state, abstracting the rest; when two threads use disjoint state, their specifications can be composed with the separating conjunction. Type systems abstract the state to the types of variables; threads may be composed when they agree on the types of shared variables.In this paper, we present the "Concurrent Views Framework", a metatheory of concurrent reasoning principles. The theory is parameterised by an abstraction of state with a notion of composition, which we call views. The metatheory is remarkably simple, but highly applicable: the rely-guarantee method, concurrent separation logic, concurrent abstract predicates, type systems for recursive references and for unique pointers, and even an adaptation of the Owicki-Gries method can all be seen as instances of the Concurrent Views Framework. Moreover, our metatheory proves each of these systems is sound without requiring induction on the operational semantics.}},
  url = {https://doi.org/10.1145/2429069.2429104},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup295,
  title = {Equality of streams is a Π0 over 2-complete problem}},
  author = {Ro\c{s}},
  year = {2006}},
  journal = {Proceedings of the Eleventh ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper gives a precise characterization for the complexity of the problem of proving equal two streams defined with a finite number of equations: Π0 over 2. Since the Π 0 over 2 class includes properly both the reursively enumerable and the corecursively enumerable classes, this result implies that neither the set of pairs of equal streams nor the set of pairs of unequal streams is recursively enumerable. Consequently, one can find no algorithm for determining equality of streams, as well as no algorithm for determining inequality of streams. In particular, there is no complete proof system for equality of streams and no complete system for inequality of streams.}},
  url = {https://doi.org/10.1145/1159803.1159827},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup296,
  title = {Streaming irregular arrays}},
  author = {Clifton-Everest, Robert and McDonell, Trevor L. and Chakravarty, Manuel M. T. and Keller, Gabriele}},
  year = {2017}},
  journal = {Proceedings of the 10th ACM SIGPLAN International Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Previous work has demonstrated that it is possible to generate efficient and highly parallel code for multicore CPUs and GPUs from combinator-based array languages for a range of applications. That work, however, has been limited to operating on flat, rectangular structures without any facilities for irregularity or nesting.In this paper, we show that even a limited form of nesting provides substantial benefits both in terms of the expressiveness of the language (increasing modularity and providing support for simple irregular structures) and the portability of the code (increasing portability across resource-constrained devices, such as GPUs). Specifically, we generalise Blelloch's flattening transformation along two lines: (1) we explicitly distinguish between definitely regular and potentially irregular computations; and (2) we handle multidimensional arrays. We demonstrate the utility of this generalisation by an extension of the embedded array language Accelerate to include irregular streams of multidimensional arrays. We discuss code generation, optimisation, and irregular stream scheduling as well as a range of benchmarks on both multicore CPUs and GPUs.}},
  url = {https://doi.org/10.1145/3122955.3122971},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup297,
  title = {Relating complexity and precision in control flow analysis}},
  author = {Van Horn, David and Mairson, Harry G.}},
  year = {2007}},
  journal = {Proceedings of the 12th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We analyze the computational complexity of kCFA, a hierarchy of control flow analyses that determine which functions may be applied at a given call-site. This hierarchy specifies related decision problems, quite apart from any algorithms that may implement their solutions. We identify a simple decision problem answered by this analysis and prove that in the 0CFA case, the problem is complete for polynomial time. The proof is based on a nonstandard, symmetric implementation of Boolean logic within multiplicative linear logic (MLL). We also identify a simpler version of 0CFA related to η-expansion, and prove that it is complete for logarithmic space, using arguments based on computing paths and permutations.For any fixed k&gt;0, it is known that kCFA (and the analogous decision problem) can be computed in time exponential in the program size. For k=1, we show that the decision problem is NP-hard, and sketch why this remains true for larger fixed values of k. The proof technique depends on using the approximation of CFA as an essentially nondeterministic computing mechanism, as distinct from the exactness of normalization. When k=n, so that the "depth" of the control flow analysis grows linearly in the program length, we show that the decision problem is complete for exponential time.In addition, we sketch how the analysis presented here may be extended naturally to languages with control operators. All of the insights presented give clear examples of how straightforward observations about linearity, and linear logic, may in turn be used to give a greater understanding of functional programming and program analysis.}},
  url = {https://doi.org/10.1145/1291151.1291166},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup298,
  title = {The intensional content of Rice's theorem}},
  author = {Asperti, Andrea}},
  year = {2008}},
  journal = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The proofs of major results of Computability Theory like Rice, Rice-Shapiro or Kleene's fixed point theorem hidemore information of what is usually expressed in theirrespective statements. We make this information explicit, allowing to state stronger, complexity theoretic-versions of all these theorems. In particular, we replace the notion of extensional set of indices of programs, by a set of indices of programs having not only the same extensional behavior but also similar complexity (Complexity Clique). We prove, under very weak complexity assumptions, that any recursive Complexity Clique is trivial, and any r.e. Complexity Clique is an extensional set (and thus satisfies Rice-Shapiro conditions). This allows, for instance, to use Rice's argument to prove that the property of having polynomial complexity is not decidable, and to use Rice-Shapiro to conclude that it is not even semi-decidable. We conclude the paper with a discussion of "complexity-theoretic" versions of Kleene's Fixed Point Theorem.}},
  url = {https://doi.org/10.1145/1328438.1328455},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup299,
  title = {Access control in a core calculus of dependency}},
  author = {Abadi, Mart\'{\i}},
  year = {2006}},
  journal = {Proceedings of the Eleventh ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The Dependency Core Calculus (DCC) is an extension of the computational lambda calculus that was designed in order to capture the notion of dependency that arises in information-flow control, partial evaluation, and other programming-language settings. We show that, unexpectedly, DCC can also be used as a calculus for access control in distributed systems. Initiating the study of DCC from this perspective, we explore some of its appealing properties.}},
  url = {https://doi.org/10.1145/1159803.1159839},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup300,
  title = {Structures for structural recursion}},
  author = {Downen, Paul and Johnson-Freyd, Philip and Ariola, Zena M.}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Our goal is to develop co-induction from our understanding of induction, putting them on level ground as equal partners for reasoning about programs. We investigate several structures which represent well-founded forms of recursion in programs. These simple structures encapsulate reasoning by primitive and noetherian induction principles, and can be composed together to form complex recursion schemes for programs operating over a wide class of data and co-data types. At its heart, this study is guided by duality: each structure for recursion has a dual form, giving perfectly symmetric pairs of equal and opposite data and co-data types for representing recursion in programs. Duality is brought out through a framework presented in sequent style, which inherently includes control effects that are interpreted logically as classical reasoning principles. To accommodate the presence of effects, we give a calculus parameterized by a notion of strategy, which is strongly normalizing for a wide range of strategies. We also present a more traditional calculus for representing effect-free functional programs, but at the cost of losing some of the founding dualities.}},
  url = {https://doi.org/10.1145/2784731.2784762},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup301,
  title = {Typed closure conversion for the calculus of constructions}},
  author = {Bowman, William J. and Ahmed, Amal}},
  year = {2018}},
  journal = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Dependently typed languages such as Coq are used to specify and verify the full functional correctness of source programs. Type-preserving compilation can be used to preserve these specifications and proofs of correctness through compilation into the generated target-language programs. Unfortunately, type-preserving compilation of dependent types is hard. In essence, the problem is that dependent type systems are designed around high-level compositional abstractions to decide type checking, but compilation interferes with the type-system rules for reasoning about run-time terms. We develop a type-preserving closure-conversion translation from the Calculus of Constructions (CC) with strong dependent pairs (Σ types)—a subset of the core language of Coq—to a type-safe, dependently typed compiler intermediate language named CC-CC. The central challenge in this work is how to translate the source type-system rules for reasoning about functions into target type-system rules for reasoning about closures. To justify these rules, we prove soundness of CC-CC by giving a model in CC. In addition to type preservation, we prove correctness of separate compilation.}},
  url = {https://doi.org/10.1145/3192366.3192372},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup302,
  title = {Sirius: An Open End-to-End Voice and Vision Personal Assistant and Its Implications for Future Warehouse Scale Computers}},
  author = {Hauswald, Johann and Laurenzano, Michael A. and Zhang, Yunqi and Li, Cheng and Rovinski, Austin and Khurana, Arjun and Dreslinski, Ronald G. and Mudge, Trevor and Petrucci, Vinicius and Tang, Lingjia and Mars, Jason}},
  year = {2015}},
  journal = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {As user demand scales for intelligent personal assistants (IPAs) such as Apple's Siri, Google's Google Now, and Microsoft's Cortana, we are approaching the computational limits of current datacenter architectures. It is an open question how future server architectures should evolve to enable this emerging class of applications, and the lack of an open-source IPA workload is an obstacle in addressing this question. In this paper, we present the design of Sirius, an open end-to-end IPA web-service application that accepts queries in the form of voice and images, and responds with natural language. We then use this workload to investigate the implications of four points in the design space of future accelerator-based server architectures spanning traditional CPUs, GPUs, manycore throughput co-processors, and FPGAs.To investigate future server designs for Sirius, we decompose Sirius into a suite of 7 benchmarks (Sirius Suite) comprising the computationally intensive bottlenecks of Sirius. We port Sirius Suite to a spectrum of accelerator platforms and use the performance and power trade-offs across these platforms to perform a total cost of ownership (TCO) analysis of various server design points. In our study, we find that accelerators are critical for the future scalability of IPA services. Our results show that GPU- and FPGA-accelerated servers improve the query latency on average by 10x and 16x. For a given throughput, GPU- and FPGA-accelerated servers can reduce the TCO of datacenters by 2.6x and 1.4x, respectively.}},
  url = {https://doi.org/10.1145/2694344.2694347},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup303,
  title = {Accelerating CUDA graph algorithms at maximum warp}},
  author = {Hong, Sungpack and Kim, Sang Kyun and Oguntebi, Tayo and Olukotun, Kunle}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Graphs are powerful data representations favored in many computational domains. Modern GPUs have recently shown promising results in accelerating computationally challenging graph problems but their performance suffered heavily when the graph structure is highly irregular, as most real-world graphs tend to be. In this study, we first observe that the poor performance is caused by work imbalance and is an artifact of a discrepancy between the GPU programming model and the underlying GPU architecture.We then propose a novel virtual warp-centric programming method that exposes the traits of underlying GPU architectures to users. Our method significantly improves the performance of applications with heavily imbalanced workloads, and enables trade-offs between workload imbalance and ALU underutilization for fine-tuning the performance. Our evaluation reveals that our method exhibits up to 9x speedup over previous GPU algorithms and 12x over single thread CPU execution on irregular graphs. When properly configured, it also yields up to 30\% improvement over previous GPU algorithms on regular graphs. In addition to performance gains on graph algorithms, our programming method achieves 1.3x to 15.1x speedup on a set of GPU benchmark applications. Our study also confirms that the performance gap between GPUs and other multi-threaded CPU graph implementations is primarily due to the large difference in memory bandwidth.}},
  url = {https://doi.org/10.1145/1941553.1941590},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup304,
  title = {PuDianNao: A Polyvalent Machine Learning Accelerator}},
  author = {Liu, Daofu and Chen, Tianshi and Liu, Shaoli and Zhou, Jinhong and Zhou, Shengyuan and Teman, Olivier and Feng, Xiaobing and Zhou, Xuehai and Chen, Yunji}},
  year = {2015}},
  journal = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Machine Learning (ML) techniques are pervasive tools in various emerging commercial applications, but have to be accommodated by powerful computer systems to process very large data. Although general-purpose CPUs and GPUs have provided straightforward solutions, their energy-efficiencies are limited due to their excessive supports for flexibility. Hardware accelerators may achieve better energy-efficiencies, but each accelerator often accommodates only a single ML technique (family). According to the famous No-Free-Lunch theorem in the ML domain, however, an ML technique performs well on a dataset may perform poorly on another dataset, which implies that such accelerator may sometimes lead to poor learning accuracy. Even if regardless of the learning accuracy, such accelerator can still become inapplicable simply because the concrete ML task is altered, or the user chooses another ML technique.In this study, we present an ML accelerator called PuDianNao, which accommodates seven representative ML techniques, including k-means, k-nearest neighbors, naive bayes, support vector machine, linear regression, classification tree, and deep neural network. Benefited from our thorough analysis on computational primitives and locality properties of different ML techniques, PuDianNao can perform up to 1056 GOP/s (e.g., additions and multiplications) in an area of 3.51 mm^2, and consumes 596 mW only. Compared with the NVIDIA K20M GPU (28nm process), PuDianNao (65nm process) is 1.20x faster, and can reduce the energy by 128.41x.}},
  url = {https://doi.org/10.1145/2694344.2694358},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup305,
  title = {The impact of higher-order state and control effects on local relational reasoning}},
  author = {Dreyer, Derek and Neis, Georg and Birkedal, Lars}},
  year = {2010}},
  journal = {Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Reasoning about program equivalence is one of the oldest problems in semantics. In recent years, useful techniques have been developed, based on bisimulations and logical relations, for reasoning about equivalence in the setting of increasingly realistic languages - languages nearly as complex as ML or Haskell. Much of the recent work in this direction has considered the interesting representation independence principles enabled by the use of local state, but it is also important to understand the principles that powerful features like higher-order state and control effects disable. This latter topic has been broached extensively within the framework of game semantics, resulting in what Abramsky dubbed the "semantic cube": fully abstract game-semantic characterizations of various axes in the design space of ML-like languages. But when it comes to reasoning about many actual examples, game semantics does not yet supply a useful technique for proving equivalences.In this paper, we marry the aspirations of the semantic cube to the powerful proof method of step-indexed Kripke logical relations. Building on recent work of Ahmed, Dreyer, and Rossberg, we define the first fully abstract logical relation for an ML-like language with recursive types, abstract types, general references and call/cc. We then show how, under orthogonal restrictions to the expressive power our language - namely, the restriction to first-order state and/or the removal of call/cc - we can enhance the proving power of our possible-worlds model in correspondingly orthogonal ways, and we demonstrate this proving power on a range of interesting examples. Central to our story is the use of state transition systems to model the way in which properties of local state evolve over time.}},
  url = {https://doi.org/10.1145/1863543.1863566},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup306,
  title = {Barrier elision for production parallel programs}},
  author = {Chabbi, Milind and Lavrijsen, Wim and de Jong, Wibe and Sen, Koushik and Mellor-Crummey, John and Iancu, Costin}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Large scientific code bases are often composed of several layers of runtime libraries, implemented in multiple programming languages. In such situation, programmers often choose conservative synchronization patterns leading to suboptimal performance. In this paper, we present context-sensitive dynamic optimizations that elide barriers redundant during the program execution. In our technique, we perform data race detection alongside the program to identify redundant barriers in their calling contexts; after an initial learning, we start eliding all future instances of barriers occurring in the same calling context. We present an automatic on-the-fly optimization and a multi-pass guided optimization. We apply our techniques to NWChem--a 6 million line computational chemistry code written in C/C++/Fortran that uses several runtime libraries such as Global Arrays, ComEx, DMAPP, and MPI. Our technique elides a surprisingly high fraction of barriers (as many as 63\%) in production runs. This redundancy elimination translates to application speedups as high as 14\% on 2048 cores. Our techniques also provided valuable insight about the application behavior, later used by NWChem developers. Overall, we demonstrate the value of holistic context-sensitive analyses that consider the domain science in conjunction with the associated runtime software stack.}},
  url = {https://doi.org/10.1145/2688500.2688502},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup307,
  title = {Verification of a cryptographic primitive: SHA-256 (abstract)}},
  author = {Appel, Andrew W.}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A full formal machine-checked verification of a C program: the OpenSSL implementation of SHA-256. This is an interactive proof of functional correctness in the Coq proof assistant, using the Verifiable C program logic. Verifiable C is a separation logic for the C language, proved sound w.r.t. the operational semantics for C, connected to the CompCert verified optimizing C compiler.}},
  url = {https://doi.org/10.1145/2737924.2774972},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup308,
  title = {An effective theory of type refinements}},
  author = {Mandelbaum, Yitzhak and Walker, David and Harper, Robert}},
  year = {2003}},
  journal = {Proceedings of the Eighth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We develop an explicit two level system that allows programmers to reason about the behavior of effectful programs. The first level is an ordinary ML-style type system, which confers standard properties on program behavior. The second level is a conservative extension of the first that uses a logic of type refinements to check more precise properties of program behavior. Our logic is a fragment of intuitionistic linear logic, which gives programmers the ability to reason locally about changes of program state. We provide a generic resource semantics for our logic as well as a sound, decidable, syntactic refinement-checking system. We also prove that refinements give rise to an optimization principle for programs. Finally, we illustrate the power of our system through a number of examples.}},
  url = {https://doi.org/10.1145/944705.944725},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup309,
  title = {Expressing and verifying probabilistic assertions}},
  author = {Sampson, Adrian and Panchekha, Pavel and Mytkowicz, Todd and McKinley, Kathryn S. and Grossman, Dan and Ceze, Luis}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Traditional assertions express correctness properties that must hold on every program execution. However, many applications have probabilistic outcomes and consequently their correctness properties are also probabilistic (e.g., they identify faces in images, consume sensor data, or run on unreliable hardware). Traditional assertions do not capture these correctness properties. This paper proposes that programmers express probabilistic correctness properties with probabilistic assertions and describes a new probabilistic evaluation approach to efficiently verify these assertions. Probabilistic assertions are Boolean expressions that express the probability that a property will be true in a given execution rather than asserting that the property must always be true. Given either specific inputs or distributions on the input space, probabilistic evaluation verifies probabilistic assertions by first performing distribution extraction to represent the program as a Bayesian network. Probabilistic evaluation then uses statistical properties to simplify this representation to efficiently compute assertion probabilities directly or with sampling. Our approach is a mix of both static and dynamic analysis: distribution extraction statically builds and optimizes the Bayesian network representation and sampling dynamically interprets this representation. We implement our approach in a tool called Mayhap for C and C++ programs. We evaluate expressiveness, correctness, and performance of Mayhap on programs that use sensors, perform approximate computation, and obfuscate data for privacy. Our case studies demonstrate that probabilistic assertions describe useful correctness properties and that Mayhap efficiently verifies them.}},
  url = {https://doi.org/10.1145/2594291.2594294},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup310,
  title = {Evaluation of architectural support for global address-based communication in large-scale parallel machines}},
  author = {Krishnamurthy, Arvind and Schauser, Klaus E. and Scheiman, Chris J. and Wang, Randolph Y. and Culler, David E. and Yelick, Katherine}},
  year = {1996}},
  journal = {Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Large-scale parallel machines are incorporating increasingly sophisticated architectural support for user-level messaging and global memory access. We provide a systematic evaluation of a broad spectrum of current design alternatives based on our implementations of a global address language on the Thinking Machines CM-5, Intel Paragon, Meiko CS-2, Cray T3D, and Berkeley NOW. This evaluation includes a range of compilation strategies that make varying use of the network processor; each is optimized for the target architecture and the particular strategy. We analyze a family of interacting issues that determine the performance trade-offs in each implementation, quantify the resulting latency, overhead, and bandwidth of the global access operations, and demonstrate the effects on application performance.}},
  url = {https://doi.org/10.1145/237090.237147},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup311,
  title = {Automated reasoning for web page layout}},
  author = {Panchekha, Pavel and Torlak, Emina}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Web pages define their appearance using Cascading Style Sheets, a modular language for layout of tree-structured documents. In principle, using CSS is easy: the developer specifies declarative constraints on the layout of an HTML document (such as the positioning of nodes in the HTML tree), and the browser solves the constraints to produce a box-based rendering of that document. In practice, however, the subtleties of CSS semantics make it difficult to develop stylesheets that produce the intended layout across different user preferences and browser settings. This paper presents the first mechanized formalization of a substantial fragment of the CSS semantics. This formalization is equipped with an efficient reduction to the theory of quantifier-free linear real arithmetic, enabling effective automated reasoning about CSS stylesheets and their behavior. We implement this reduction in Cassius, a solver-aided framework for building semantics-aware tools for CSS. To demonstrate the utility of Cassius, we prototype new tools for automated verification, debugging, and synthesis of CSS code. We show that these tools work on fragments of real-world websites, and that Cassius is a practical first step toward solver-aided programming for the web.}},
  url = {https://doi.org/10.1145/2983990.2984010},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup312,
  title = {Compiling without continuations}},
  author = {Maurer, Luke and Downen, Paul and Ariola, Zena M. and Peyton Jones, Simon}},
  year = {2017}},
  journal = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many fields of study in compilers give rise to the concept of a join point—a place where different execution paths come together. Join points are often treated as functions or continuations, but we believe it is time to study them in their own right. We show that adding join points to a direct-style functional intermediate language is a simple but powerful change that allows new optimizations to be performed, including a significant improvement to list fusion. Finally, we report on recent work on adding join points to the intermediate language of the Glasgow Haskell Compiler.}},
  url = {https://doi.org/10.1145/3062341.3062380},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup313,
  title = {A linear-time algorithm for computing the memory access sequence in data-parallel programs}},
  author = {Kennedy, Ken and Nedeljkovic, Nenad and Sethi, Ajay}},
  year = {1995}},
  journal = {Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Data-parallel languages, such as High Performance Fortran, are widely regarded as a promising means for writing portable programs for distributed-memory machines. Novel features of these languages call for the development of new techniques in both compilers and run-time systems. In this paper, we present an improved algorithm for finding the local memory access sequence in computations involving regular sections of arrays with cyclic(k) distributions. After establishing the fact that regular section indices correspond to elements of an integer lattice, we show how to find a lattice basis that allows for simple and fast enumeration of memory accesses. The complexity of our algorithm is shown to be lower than that of the previous solution for the same problem. In addition, the experimental results demonstrate the efficiency of our method in practice.}},
  url = {https://doi.org/10.1145/209936.209948},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup314,
  title = {Optimality and inefficiency: what isn't a cost model of the lambda calculus?}},
  author = {Lawall, Julia L. and Mairson, Harry G.}},
  year = {1996}},
  journal = {Proceedings of the First ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We investigate the computational efficiency of the sharing graphs of Lamping [Lam90], Gonthier, Abadi, and Lévy [GAL92], and Asperti [Asp94], designed to effect so-called optimal evaluation, with the goal of reconciling optimality, efficiency, and the clarification of reasonable cost models for the λ-calculus. Do these graphs suggest reasonable cost models for the λ-calculus? If they are optimal, are they efficient?We present a brief survey of these optimal evaluators, identifying their common characteristics, as well as their shared failures. We give a lower bound on the efficiency of sharing graphs by identifying a class of λ-terms that are normalizable in Θ(n) time, and require Θ(n) "fan interactions," but require Ω(2n) bookkeeping steps. For [GAL92], we analyze this anomaly in terms of the dynamic maintenance of deBruijn indices for intermediate terms. We give another lower bound showing that sharing graphs can do Ω(2n) work (via fan interactions) on graphs that have no β-redexes. Finally, we criticize a proposed cost model for λ-calculus given by Frandsen and Sturtivant [FS91], showing by example that the model does not take account of the size of intermediate forms. Our example is a term requiring Θ(2n) steps while having proposed cost Θ(n). We propose some cost models that both reflect this parameter, and simultaneously reconcile key concepts from optimal reduction.}},
  url = {https://doi.org/10.1145/232627.232639},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup315,
  title = {Verifying quantitative reliability for programs that execute on unreliable hardware}},
  author = {Carbin, Michael and Misailovic, Sasa and Rinard, Martin C.}},
  year = {2013}},
  journal = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Emerging high-performance architectures are anticipated to contain unreliable components that may exhibit soft errors, which silently corrupt the results of computations. Full detection and masking of soft errors is challenging, expensive, and, for some applications, unnecessary. For example, approximate computing applications (such as multimedia processing, machine learning, and big data analytics) can often naturally tolerate soft errors.We present Rely a programming language that enables developers to reason about the quantitative reliability of an application -- namely, the probability that it produces the correct result when executed on unreliable hardware. Rely allows developers to specify the reliability requirements for each value that a function produces.We present a static quantitative reliability analysis that verifies quantitative requirements on the reliability of an application, enabling a developer to perform sound and verified reliability engineering. The analysis takes a Rely program with a reliability specification and a hardware specification that characterizes the reliability of the underlying hardware components and verifies that the program satisfies its reliability specification when executed on the underlying unreliable hardware platform. We demonstrate the application of quantitative reliability analysis on six computations implemented in Rely.}},
  url = {https://doi.org/10.1145/2509136.2509546},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup316,
  title = {User-guided program reasoning using Bayesian inference}},
  author = {Raghothaman, Mukund and Kulkarni, Sulekha and Heo, Kihong and Naik, Mayur}},
  year = {2018}},
  journal = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Program analyses necessarily make approximations that often lead them to report true alarms interspersed with many false alarms. We propose a new approach to leverage user feedback to guide program analyses towards true alarms and away from false alarms. Our approach associates each alarm with a confidence value by performing Bayesian inference on a probabilistic model derived from the analysis rules. In each iteration, the user inspects the alarm with the highest confidence and labels its ground truth, and the approach recomputes the confidences of the remaining alarms given this feedback. It thereby maximizes the return on the effort by the user in inspecting each alarm. We have implemented our approach in a tool named Bingo for program analyses expressed in Datalog. Experiments with real users and two sophisticated analyses---a static datarace analysis for Java programs and a static taint analysis for Android apps---show significant improvements on a range of metrics, including false alarm rates and number of bugs found.}},
  url = {https://doi.org/10.1145/3192366.3192417},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup317,
  title = {Iris: Monoids and Invariants as an Orthogonal Basis for Concurrent Reasoning}},
  author = {Jung, Ralf and Swasey, David and Sieczkowski, Filip and Svendsen, Kasper and Turon, Aaron and Birkedal, Lars and Dreyer, Derek}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present Iris, a concurrent separation logic with a simple premise: monoids and invariants are all you need. Partial commutative monoids enable us to express---and invariants enable us to enforce---user-defined *protocols* on shared state, which are at the conceptual core of most recent program logics for concurrency. Furthermore, through a novel extension of the concept of a *view shift*, Iris supports the encoding of *logically atomic specifications*, i.e., Hoare-style specs that permit the client of an operation to treat the operation essentially as if it were atomic, even if it is not.}},
  url = {https://doi.org/10.1145/2676726.2676980},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup318,
  title = {Serialization sets: a dynamic dependence-based parallel execution model}},
  author = {Allen, Matthew D. and Sridharan, Srinath and Sohi, Gurindar S.}},
  year = {2009}},
  journal = {Proceedings of the 14th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper proposes a new parallel execution model where programmers augment a sequential program with pieces of code called serializers that dynamically map computational operations into serialization sets of dependent operations. A runtime system executes operations in the same serialization set in program order, and may concurrently execute operations in different sets. Because serialization sets establish a logical ordering on all operations, the resulting parallel execution is predictable and deterministic.We describe the API and design of Prometheus, a C++ library that implements the serialization set abstraction through compile-time template instantiation and a runtime support library. We evaluate a set of parallel programs running on the x86_64 and SPARC-V9 instruction sets and study their performance on multicore, symmetric multiprocessor, and ccNUMA parallel machines. By contrast with conventional parallel execution models, we find that Prometheus programs are significantly easier to write, test, and debug, and their parallel execution achieves comparable performance.}},
  url = {https://doi.org/10.1145/1504176.1504190},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup319,
  title = {Cooperative reasoning for preemptive execution}},
  author = {Yi, Jaeheon and Sadowski, Caitlin and Flanagan, Cormac}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We propose a cooperative methodology for multithreaded software, where threads use traditional synchronization idioms such as locks, but additionally document each point of potential thread interference with a "yield" annotation. Under this methodology, code between two successive yield annotations forms a serializable transaction that is amenable to sequential reasoning. This methodology reduces the burden of reasoning about thread interleavings by indicating only those interference points that matter. We present experimental results showing that very few yield annotations are required, typically one or two per thousand lines of code. We also present dynamic analysis algorithms for detecting cooperability violations, where thread interference is not documented by a yield, and for yield annotation inference for legacy software.}},
  url = {https://doi.org/10.1145/1941553.1941575},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup320,
  title = {Extensible effects: an alternative to monad transformers}},
  author = {Kiselyov, Oleg and Sabry, Amr and Swords, Cameron}},
  year = {2013}},
  journal = {Proceedings of the 2013 ACM SIGPLAN Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We design and implement a library that solves the long-standing problem of combining effects without imposing restrictions on their interactions (such as static ordering). Effects arise from interactions between a client and an effect handler (interpreter); interactions may vary throughout the program and dynamically adapt to execution conditions. Existing code that relies on monad transformers may be used with our library with minor changes, gaining efficiency over long monad stacks. In addition, our library has greater expressiveness, allowing for practical idioms that are inefficient, cumbersome, or outright impossible with monad transformers.Our alternative to a monad transformer stack is a single monad, for the coroutine-like communication of a client with its handler. Its type reflects possible requests, i.e., possible effects of a computation. To support arbitrary effects and their combinations, requests are values of an extensible union type, which allows adding and, notably, subtracting summands. Extending and, upon handling, shrinking of the union of possible requests is reflected in its type, yielding a type-and-effect system for Haskell. The library is lightweight, generalizing the extensible exception handling to other effects and accurately tracking them in types.}},
  url = {https://doi.org/10.1145/2503778.2503791},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup321,
  title = {Minimization of symbolic automata}},
  author = {D'Antoni, Loris and Veanes, Margus}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Symbolic Automata extend classical automata by using symbolic alphabets instead of finite ones. Most of the classical automata algorithms rely on the alphabet being finite, and generalizing them to the symbolic setting is not a trivial task. In this paper we study the problem of minimizing symbolic automata. We formally define and prove the basic properties of minimality in the symbolic setting, and lift classical minimization algorithms (Huffman-Moore's and Hopcroft's algorithms) to symbolic automata. While Hopcroft's algorithm is the fastest known algorithm for DFA minimization, we show how, in the presence of symbolic alphabets, it can incur an exponential blowup. To address this issue, we introduce a new algorithm that fully benefits from the symbolic representation of the alphabet and does not suffer from the exponential blowup. We provide comprehensive performance evaluation of all the algorithms over large benchmarks and against existing state-of-the-art implementations. The experiments show how the new symbolic algorithm is faster than previous implementations.}},
  url = {https://doi.org/10.1145/2535838.2535849},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup322,
  title = {Specialization of inductively sequential functional logic programs}},
  author = {Alpuente, Mar\'{\i}},
  year = {1999}},
  journal = {Proceedings of the Fourth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Functional logic languages combine the operational principles of the most important declarative programming paradigms, namely functional and logic programming. Inductively sequential programs admit the definition of optimal computation strategies and are the basis of several recent (lazy) functional logic languages. In this paper, we define a partial evaluator for inductively sequential functional logic programs. We prove strong correctness of this partial evaluator and show that the nice properties of inductively sequential programs carry over to the specialization process and the specialized programs. In particular, the structure of the programs is preserved by the specialization process. This is in contrast to other partial evaluation methods for functional logic programs which can destroy the original program structure. Finally, we present some experiments which highlight the practical advantages of our approach.}},
  url = {https://doi.org/10.1145/317636.317910},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup323,
  title = {Local rely-guarantee reasoning}},
  author = {Feng, Xinyu}},
  year = {2009}},
  journal = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Rely-Guarantee reasoning is a well-known method for verification of shared-variable concurrent programs. However, it is difficult for users to define rely/guarantee conditions, which specify threads' behaviors over the whole program state. Recent efforts to combine Separation Logic with Rely-Guarantee reasoning have made it possible to hide thread-local resources, but the shared resources still need to be globally known and specified. This greatly limits the reuse of verified program modules.In this paper, we propose LRG, a new Rely-Guarantee-based logic that brings local reasoning and information hiding to concurrency verification. Our logic, for the first time, supports a frame rule over rely/guarantee conditions so that specifications of program modules only need to talk about the resources used locally, and the verified modules can be reused in different threads without redoing the proof. Moreover, we introduce a new hiding rule to hide the resources shared by a subset of threads from the rest in the system. The support of information hiding not only improves the modularity of Rely-Guarantee reasoning, but also enables the sharing of dynamically allocated resources, which requires adjustment of rely/guarantee conditions.}},
  url = {https://doi.org/10.1145/1480881.1480922},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup324,
  title = {The linearity Monad}},
  author = {Paykin, Jennifer and Zdancewic, Steve}},
  year = {2017}},
  journal = {Proceedings of the 10th ACM SIGPLAN International Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We introduce a technique for programming with domain-specific linear languages using the monad that arises from the theory of linear/non-linear logic. In this work we interpret the linear/non-linear model as a simple, effectful linear language embedded inside an existing non-linear host language. We implement a modular framework for defining these linear EDSLs in Haskell, allowing both shallow and deep embeddings. To demonstrate the effectiveness of the framework and the linearity monad, we implement languages for file handles, mutable arrays, session types, and quantum computing.}},
  url = {https://doi.org/10.1145/3122955.3122965},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup325,
  title = {Consolidation of queries with user-defined functions}},
  author = {Sousa, Marcelo and Dillig, Isil and Vytiniotis, Dimitrios and Dillig, Thomas and Gkantsidis, Christos}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Motivated by streaming and data analytics scenarios where many queries operate on the same data and perform similar computations, we propose program consolidation for merging multiple user-defined functions (UDFs) that operate on the same input. Program consolidation exploits common computations between UDFs to generate an equivalent optimized function whose execution cost is often much smaller (and never greater) than the sum of the costs of executing each function individually. We present a sound consolidation calculus and an effective algorithm for consolidating multiple UDFs. Our approach is purely static and uses symbolic SMT-based techniques to identify shared or redundant computations. We have implemented the proposed technique on top of the Naiad data processing system. Our experiments show that our algorithm dramatically improves overall job completion time when executing user-defined filters that operate on the same data and perform similar computations.}},
  url = {https://doi.org/10.1145/2594291.2594305},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup326,
  title = {Relaxed-memory concurrency and verified compilation}},
  author = {undefinedev\v{c}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this paper, we consider the semantic design and verified compilation of a C-like programming language for concurrent shared-memory computation above x86 multiprocessors. The design of such a language is made surprisingly subtle by several factors: the relaxed-memory behaviour of the hardware, the effects of compiler optimisation on concurrent code, the need to support high-performance concurrent algorithms, and the desire for a reasonably simple programming model. In turn, this complexity makes verified (or verifying) compilation both essential and challenging.We define a concurrent relaxed-memory semantics for ClightTSO, an extension of CompCert's Clight in which the processor's memory model is exposed for high-performance code. We discuss a strategy for verifying compilation from ClightTSO to x86, which we validate with correctness proofs (building on CompCert) for the most interesting compiler phases.}},
  url = {https://doi.org/10.1145/1926385.1926393},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup327,
  title = {Compiler-assisted detection of transient memory errors}},
  author = {Tavarageri, Sanket and Krishnamoorthy, Sriram and Sadayappan, P.}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The probability of bit flips in hardware memory systems is projected to increase significantly as memory systems continue to scale in size and complexity. Effective hardware-based error detection and correction require that the complete data path, involving all parts of the memory system, be protected with sufficient redundancy. First, this may be costly to employ on commodity computing platforms, and second, even on high-end systems, protection against multi-bit errors may be lacking. Therefore, augmenting hardware error detection schemes with software techniques is of considerable interest.In this paper, we consider software-level mechanisms to comprehensively detect transient memory faults. We develop novel compile-time algorithms to instrument application programs with checksum computation codes to detect memory errors. Unlike prior approaches that employ checksums on computational and architectural states, our scheme verifies every data access and works by tracking variables as they are produced and consumed. Experimental evaluation demonstrates that the proposed comprehensive error detection solution is viable as a completely software-only scheme. We also demonstrate that with limited hardware support, overheads of error detection can be further reduced.}},
  url = {https://doi.org/10.1145/2594291.2594298},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup328,
  title = {Symbolic Algorithms for Language Equivalence and Kleene Algebra with Tests}},
  author = {Pous, Damien}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We propose algorithms for checking language equivalence of finite automata over a large alphabet. We use symbolic automata, where the transition function is compactly represented using (multi-terminal) binary decision diagrams (BDD). The key idea consists in computing a bisimulation by exploring reachable pairs symbolically, so as to avoid redundancies. This idea can be combined with already existing optimisations, and we show in particular a nice integration with the disjoint sets forest data-structure from Hopcroft and Karp's standard algorithm.Then we consider Kleene algebra with tests (KAT), an algebraic theory that can be used for verification in various domains ranging from compiler optimisation to network programming analysis. This theory is decidable by reduction to language equivalence of automata on guarded strings, a particular kind of automata that have exponentially large alphabets. We propose several methods allowing to construct symbolic automata out of KAT expressions, based either on Brzozowski's derivatives or on standard automata constructions.All in all, this results in efficient algorithms for deciding equivalence of KAT expressions.}},
  url = {https://doi.org/10.1145/2676726.2677007},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup329,
  title = {Coupling proofs are probabilistic product programs}},
  author = {Barthe, Gilles and Gr\'{e}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Couplings are a powerful mathematical tool for reasoning about pairs of probabilistic processes. Recent developments in formal verification identify a close connection between couplings and pRHL, a relational program logic motivated by applications to provable security, enabling formal construction of couplings from the probability theory literature. However, existing work using pRHL merely shows existence of a coupling and does not give a way to prove quantitative properties about the coupling, needed to reason about mixing and convergence of probabilistic processes. Furthermore, pRHL is inherently incomplete, and is not able to capture some advanced forms of couplings such as shift couplings. We address both problems as follows. First, we define an extension of pRHL, called x-pRHL, which explicitly constructs the coupling in a pRHL derivation in the form of a probabilistic product program that simulates two correlated runs of the original program. Existing verification tools for probabilistic programs can then be directly applied to the probabilistic product to prove quantitative properties of the coupling. Second, we equip x-pRHL with a new rule for while loops, where reasoning can freely mix synchronized and unsynchronized loop iterations. Our proof rule can capture examples of shift couplings, and the logic is relatively complete for deterministic programs. We show soundness of x-PRHL and use it to analyze two classes of examples. First, we verify rapid mixing using different tools from coupling: standard coupling, shift coupling, and path coupling, a compositional principle for combining local couplings into a global coupling. Second, we verify (approximate) equivalence between a source and an optimized program for several instances of loop optimizations from the literature.}},
  url = {https://doi.org/10.1145/3009837.3009896},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup330,
  title = {Compiling Markov chain Monte Carlo algorithms for probabilistic modeling}},
  author = {Huang, Daniel and Tristan, Jean-Baptiste and Morrisett, Greg}},
  year = {2017}},
  journal = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The problem of probabilistic modeling and inference, at a high-level, can be viewed as constructing a (model, query, inference) tuple, where an inference algorithm implements a query on a model. Notably, the derivation of inference algorithms can be a difficult and error-prone task. Hence, researchers have explored how ideas from probabilistic programming can be applied. In the context of constructing these tuples, probabilistic programming can be seen as taking a language-based approach to probabilistic modeling and inference. For instance, by using (1) appropriate languages for expressing models and queries and (2) devising inference techniques that operate on encodings of models (and queries) as program expressions, the task of inference can be automated. In this paper, we describe a compiler that transforms a probabilistic model written in a restricted modeling language and a query for posterior samples given observed data into a Markov Chain Monte Carlo (MCMC) inference algorithm that implements the query. The compiler uses a sequence of intermediate languages (ILs) that guide it in gradually and successively refining a declarative specification of a probabilistic model and the query into an executable MCMC inference algorithm. The compilation strategy produces composable MCMC algorithms for execution on a CPU or GPU.}},
  url = {https://doi.org/10.1145/3062341.3062375},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup331,
  title = {A program optimization for automatic database result caching}},
  author = {Scully, Ziv and Chlipala, Adam}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Most popular Web applications rely on persistent databases based on languages like SQL for declarative specification of data models and the operations that read and modify them. As applications scale up in user base, they often face challenges responding quickly enough to the high volume of requests. A common aid is caching of database results in the application's memory space, taking advantage of program-specific knowledge of which caching schemes are sound and useful, embodied in handwritten modifications that make the program less maintainable. These modifications also require nontrivial reasoning about the read-write dependencies across operations. In this paper, we present a compiler optimization that automatically adds sound SQL caching to Web applications coded in the Ur/Web domain-specific functional language, with no modifications required to source code. We use a custom cache implementation that supports concurrent operations without compromising the transactional semantics of the database abstraction. Through experiments with microbenchmarks and production Ur/Web applications, we show that our optimization in many cases enables an easy doubling or more of an application's throughput, requiring nothing more than passing an extra command-line flag to the compiler.}},
  url = {https://doi.org/10.1145/3009837.3009891},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup332,
  title = {Probabilistic coherence spaces are fully abstract for probabilistic PCF}},
  author = {Ehrhard, Thomas and Tasson, Christine and Pagani, Michele}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Probabilistic coherence spaces (PCoh) yield a semantics of higher-order probabilistic computation, interpreting types as convex sets and programs as power series. We prove that the equality of interpretations in Pcoh characterizes the operational indistinguishability of programs in PCF with a random primitive.This is the first result of full abstraction for a semantics of probabilistic PCF. The key ingredient relies on the regularity of power series.Along the way to the theorem, we design a weighted intersection type assignment system giving a logical presentation of PCoh.}},
  url = {https://doi.org/10.1145/2535838.2535865},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup333,
  title = {User-controllable coherence for high performance shared memory multiprocessors}},
  author = {McCurdy, Collin and Fischer, Charles}},
  year = {2003}},
  journal = {Proceedings of the Ninth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In programming high performance applications, shared address-space platforms are preferable for fine-grained computation, while distributed address-space platforms are more suitable for coarse-grained computation. However, currently only distributed address-space systems scale beyond the low hundreds of processors. In this paper we introduce a hybrid architecture that allows users to trade off local memory usage for coherence communication, making possible larger-scale shared memory architectures. We introduce a programming model and examine possible implementations of hardware mechanisms, evaluating some of the trade-offs inherent in each. Preliminary experiments on an application with particularly fine-grained communication requirements indicate that effective placement of directives can reduce coherence communication by more than a factor of 10 for 64 processors.}},
  url = {https://doi.org/10.1145/781498.781507},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup334,
  title = {Hardware-Assisted Secure Resource Accounting under a Vulnerable Hypervisor}},
  author = {Jin, Seongwook and Seol, Jinho and Huh, Jaehyuk and Maeng, Seungryoul}},
  year = {2015}},
  journal = {Proceedings of the 11th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {With the proliferation of cloud computing to outsource computation in remote servers, the accountability of computational resources has emerged as an important new challenge for both cloud users and providers. Among the cloud resources, CPU and memory are difficult to verify their actual allocation, since the current virtualization techniques attempt to hide the discrepancy between physical and virtual allocations for the two resources. This paper proposes an online verifiable resource accounting technique for CPU and memory allocation for cloud computing. Unlike prior approaches for cloud resource accounting, the proposed accounting mechanism, called Hardware-assisted Resource Accounting (HRA), uses the hardware support for system management mode (SMM) and virtualization to provide secure resource accounting, even if the hypervisor is compromised. Using a secure isolated execution support of SMM, this study investigates two aspects of verifiable resource accounting for cloud systems. First, this paper presents how the hardware-assisted SMM and virtualization techniques can be used to implement the secure resource accounting mechanism even under a compromised hypervisor. Second, the paper investigates a sample-based resource accounting technique to minimize performance overheads. Using a statistical random sampling method, the technique estimates the overall CPU and memory allocation status with 99\%~100\% accuracies and performance degradations of 0.1\%~0.5\%.}},
  url = {https://doi.org/10.1145/2731186.2731203},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup335,
  title = {The geometry of types}},
  author = {Dal lago, Ugo and Petit, Barbara}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We show that time complexity analysis of higher-order functional programs can be effectively reduced to an arguably simpler (although computationally equivalent) verification problem, namely checking first-order inequalities for validity. This is done by giving an efficient inference algorithm for linear dependent types which, given a PCF term, produces in output both a linear dependent type and a cost expression for the term, together with a set of proof obligations. Actually, the output type judgement is derivable iff all proof obligations are valid. This, coupled with the already known relative completeness of linear dependent types, ensures that no information is lost, i.e., that there are no false positives or negatives. Moreover, the procedure reflects the difficulty of the original problem: simple PCF terms give rise to sets of proof obligations which are easy to solve. The latter can then be put in a format suitable for automatic or semi-automatic verification by external solvers. Ongoing experimental evaluation has produced encouraging results, which are briefly presented in the paper.}},
  url = {https://doi.org/10.1145/2429069.2429090},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup336,
  title = {Bigraphs and transitions}},
  author = {Jensen, Ole H\o{}},
  year = {2003}},
  journal = {Proceedings of the 30th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A bigraphical reactive system (BRS) involves bigraphs, in which the nesting of nodes represents locality, independently of the edges connecting them. BRSs represent a wide variety of calculi for mobility, including λ-calculus and ambient calculus. A labelled transition system (LTS) for each BRS is here derived uniformly, adapting previous work of Leifer and Milner, so that under certain conditions the resulting bisimilarity is automatically a congruence. For an asynchronous λ-calculus, this LTS and its bisimilarity agree closely with the standard.}},
  url = {https://doi.org/10.1145/604131.604135},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup337,
  title = {Generating performance portable code using rewrite rules: from high-level functional expressions to high-performance OpenCL code}},
  author = {Steuwer, Michel and Fensch, Christian and Lindley, Sam and Dubach, Christophe}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Computers have become increasingly complex with the emergence of heterogeneous hardware combining multicore CPUs and GPUs. These parallel systems exhibit tremendous computational power at the cost of increased programming effort resulting in a tension between performance and code portability. Typically, code is either tuned in a low-level imperative language using hardware-specific optimizations to achieve maximum performance or is written in a high-level, possibly functional, language to achieve portability at the expense of performance. We propose a novel approach aiming to combine high-level programming, code portability, and high-performance. Starting from a high-level functional expression we apply a simple set of rewrite rules to transform it into a low-level functional representation, close to the OpenCL programming model, from which OpenCL code is generated. Our rewrite rules define a space of possible implementations which we automatically explore to generate hardware-specific OpenCL implementations. We formalize our system with a core dependently-typed lambda-calculus along with a denotational semantics which we use to prove the correctness of the rewrite rules. We test our design in practice by implementing a compiler which generates high performance imperative OpenCL code. Our experiments show that we can automatically derive hardware-specific implementations from simple functional high-level algorithmic expressions offering performance on a par with highly tuned code for multicore CPUs and GPUs written by experts.}},
  url = {https://doi.org/10.1145/2784731.2784754},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup338,
  title = {A general constraint-centric scheduling framework for spatial architectures}},
  author = {Nowatzki, Tony and Sartin-Tarm, Michael and De Carli, Lorenzo and Sankaralingam, Karthikeyan and Estan, Cristian and Robatmili, Behnam}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Specialized execution using spatial architectures provides energy efficient computation, but requires effective algorithms for spatially scheduling the computation. Generally, this has been solved with architecture-specific heuristics, an approach which suffers from poor compiler/architect productivity, lack of insight on optimality, and inhibits migration of techniques between architectures.Our goal is to develop a scheduling framework usable for all spatial architectures. To this end, we expresses spatial scheduling as a constraint satisfaction problem using Integer Linear Programming (ILP). We observe that architecture primitives and scheduler responsibilities can be related through five abstractions: placement of computation, routing of data, managing event timing, managing resource utilization, and forming the optimization objectives. We encode these responsibilities as 20 general ILP constraints, which are used to create schedulers for the disparate TRIPS, DySER, and PLUG architectures. Our results show that a general declarative approach using ILP is implementable, practical, and typically matches or outperforms specialized schedulers.}},
  url = {https://doi.org/10.1145/2491956.2462163},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup339,
  title = {Automating string processing in spreadsheets using input-output examples}},
  author = {Gulwani, Sumit}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We describe the design of a string programming/expression language that supports restricted forms of regular expressions, conditionals and loops. The language is expressive enough to represent a wide variety of string manipulation tasks that end-users struggle with. We describe an algorithm based on several novel concepts for synthesizing a desired program in this language from input-output examples. The synthesis algorithm is very efficient taking a fraction of a second for various benchmark examples. The synthesis algorithm is interactive and has several desirable features: it can rank multiple solutions and has fast convergence, it can detect noise in the user input, and it supports an active interaction model wherein the user is prompted to provide outputs on inputs that may have multiple computational interpretations.The algorithm has been implemented as an interactive add-in for Microsoft Excel spreadsheet system. The prototype tool has met the golden test - it has synthesized part of itself, and has been used to solve problems beyond author's imagination.}},
  url = {https://doi.org/10.1145/1926385.1926423},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup340,
  title = {Bridging the gap between deep learning and sparse matrix format selection}},
  author = {Zhao, Yue and Li, Jiajia and Liao, Chunhua and Shen, Xipeng}},
  year = {2018}},
  journal = {Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This work presents a systematic exploration on the promise and special challenges of deep learning for sparse matrix format selection---a problem of determining the best storage format for a matrix to maximize the performance of Sparse Matrix Vector Multiplication (SpMV). It describes how to effectively bridge the gap between deep learning and the special needs of the pillar HPC problem through a set of techniques on matrix representations, deep learning structure, and cross-architecture model migrations. The new solution cuts format selection errors by two thirds, and improves SpMV performance by 1.73X on average over the state of the art.}},
  url = {https://doi.org/10.1145/3178487.3178495},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup341,
  title = {Type specialisation for imperative languages}},
  author = {Dussart, Dirk and Hughes, John and Thiemann, Peter}},
  year = {1997}},
  journal = {Proceedings of the Second ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We extend type specialisation to a computational lambda calculus with first-class references. The resulting specialiser has been used to specialise a self-interpreter for this typed computational lambda calculus optimally. Furthermore, this specialiser can perform operations on references at specialisation time, when possible.}},
  url = {https://doi.org/10.1145/258948.258968},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup342,
  title = {Sneaking around concatMap: efficient combinators for dynamic programming}},
  author = {H\"{o}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a framework of dynamic programming combinators that provides a high-level environment to describe the recursions typical of dynamic programming over sequence data in a style very similar to algebraic dynamic programming (ADP). Using a combination of type-level programming and stream fusion leads to a substantial increase in performance, without sacrificing much of the convenience and theoretical underpinnings of ADP.We draw examples from the field of computational biology, more specifically RNA secondary structure prediction, to demonstrate how to use these combinators and what differences exist between this library, ADP, and other approaches.The final version of the combinator library allows writing algorithms with performance close to hand-optimized C code.}},
  url = {https://doi.org/10.1145/2364527.2364559},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup343,
  title = {Adventures in time and space}},
  author = {Danner, Norman and Royer, James S.}},
  year = {2006}},
  journal = {Conference Record of the 33rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper investigates what is essentially a call-by-value version of PCF under a complexity-theoretically motivated type system. The programming formalism, ATR1, has its first-order programs characterize the poly-time computable functions, and its second-order programs characterize the type-2 basic feasible functionals of Mehlhorn and of Cook and Urquhart. (The ATR1-types are confined to levels 0, 1, and 2.) The type system comes in two parts, one that primarily restricts the sizes of values of expressions and a second that primarily restricts the time required to evaluate expressions. The size-restricted part is motivated by Bellantoni and Cook's and Leivant's implicit characterizations of poly-time. The time-restricting part is an affine version of Barber and Plotkin's DILL. Two semantics are constructed for ATR1. The first is a pruning of the na\"{\i}},
  url = {https://doi.org/10.1145/1111037.1111053},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup344,
  title = {Functional differentiation of computer programs}},
  author = {Karczmarczuk, Jerzy}},
  year = {1998}},
  journal = {Proceedings of the Third ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present two purely functional implementations of the computational differentiation tools -- the well known numeric (not symbolic!) techniques which permit to compute pointwise derivatives of functions defined by computer programs economically and exactly. We show how the co-recursive (lazy) algorithm formulation permits to construct in a transparent and elegant manner the entire infinite tower of derivatives of higher order for any expressions present in the program, and we present a purely functional variant of the reverse (or adjoint) mode of computational differentiation, using a chain of delayed evaluations represented by closures. Some concrete applications are also discussed.}},
  url = {https://doi.org/10.1145/289423.289442},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup345,
  title = {Specifying concurrent objects}},
  author = {Kr\"{a}},
  year = {1988}},
  journal = {Proceedings of the 1988 ACM SIGPLAN Workshop on Object-Based Concurrent Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The object-oriented style of programming (OOP) is gaining increasing importance as a practical technique for organizing large designs and programs. Another striking aspect of OOP is its potential for concurrent and distributed applications which is based on the fact that objects may coexist in time and concurrently may exchange information by message passing. Up to now, however, there is only little progress in the development of formal computation models that exploit the full power of concurrency inherent in the OOP paradigm. (One of the few exceptions is the actor model [1].) As a consequence, there are not many OOP languages such as POOL [2] or Trellis/Owl [7] that provide mechanisms allowing the designer to keep control of how objects are dynamically created, how they interact in a distributed environment, and how objects can be allocated on a network of processors.Our Model. We propose a model of concurrent objects that integrates ideas from three different research areas to cope with what we think is the substance of the object-oriented paradigm: 1) Cardelli's work on a formal semantics of multiple inheritance [3] influenced our way of thinking about object decomposition and inheritance relationships between them; in particular, we adopt his idea to base the decomposition of objects on labeled Cartesian products and disjoint sums. 2) The algebraic foundation of abstract data types provides a representation independent notion of data structures and encapsulation concepts. 3) To support a notion of distributed state and concurrent state changes, we use high level Petri nets. They describe dynamic behavior of concurrent systems in a way that is at the same time formal and visual. It is easy to see that record and variant type constructors can be handled in an algebraic framework, if taking some care of variants. Moreover, in [5] and [6] we have shown that algebraic and Petri net based specification techniques can be unified in a conceptually and semantically coherent framework.In the sequel we give two examples to explain this combination of concepts. Similar to [4], we distinguish between functions to denote object attributes and methods to denote state changing operations: we use Cardelli's notation of variants and records, and an intuitive notation for signatures and conditional axioms. We allow methods to operate concurrently on an object's attributes, provided that they do not share any attribute of that object. Variant objects. Variants are taken to model sequential objects. The disjoint cases of a variant are used to describe the set of distinct states in which a variant object can be observed. All methods defined on a variant object mutually exclude each other and affect a specific state transition. For example, the simple object module SENDER specified below might form a component in the specification of an alternating bit protocol: The axioms both specify constraints to the applicability of methods and the effect a method invocation has on the state of an instance. For example, method get-ack is only applicable if an instance, say S, is in the state labeled snt: the effect of get-ack either is that S is in state ack (in which a new message can be accepted) or in state rdy (in which the old message is to be resent). Like Cardelli, we assume basic operations as-L to extract the contents of a variant object with a particular label L.In place of the axioms, we better could have used high level Petri nets1 to visualize the dynamic behavior of sender objects: Remark: The dependency between the control bits B, B' and the label L involved in method get-ack must be specified by an axiom. The new method is omitted. Record objects. More interesting behavioral aspects are associated with record objects as we view their attributes as potential candidates for concurrent manipulation. To achieve this, we conceptually treat record objects as distributed data structures by splitting them along their decomposition in attributes. For each method we have to indicate which attributes it needs to access. Then, concurrency arises when methods access different attributes of a particular object.In the specification below we discuss some aspects of a host in a communications network. It is composed of different, partly independent subsystems such as a server providing diverse client services, input and output buffers to store incoming and outgoing messages, a sender, and a receiver which are responsible message transmission to other hosts. For simplification we give meaning only to a few of the conceivable methods. Discussion. The above Petri net reflects the behavioral concepts we are interested in as follows: 1) Independently accessible local data appear as distinct data tokens: 2) mutual exclusion between methods is manifested in terms of conflicting accesses to such tokens: 3) sequential dependencies are expressed by a producer-consumer relationship w.r.t. particular tokens (cf. the previous example); 4) concurrency arises when methods access different tokens; and (what has not been shown) 5) object interaction is expressed as participation in common method invocations. Conclusions. The underlying theory of Petri nets offers concepts for reasoning about lifeness and safeness of a behavior specification and provides an appropriate notion of nonsequential observations of method executions while many-sorted partial algebras provide a precise meaning of the local data operated on. Inheritance mechanism have not been discussed here as we are not yet sure whether our initial ideas to extend Cardelli's structural approach to behavior specifications really will hold.}},
  url = {https://doi.org/10.1145/67386.67432},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup346,
  title = {Safe futures for Java}},
  author = {Welc, Adam and Jagannathan, Suresh and Hosking, Antony}},
  year = {2005}},
  journal = {Proceedings of the 20th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A future is a simple and elegant abstraction that allows concurrency to be expressed often through a relatively small rewrite of a sequential program. In the absence of side-effects, futures serve as benign annotations that mark potentially concurrent regions of code. Unfortunately, when computation relies heavily on mutation as is the case in Java, its meaning is less clear, and much of its intended simplicity lost.This paper explores the definition and implementation of safe futures for Java. One can think of safe futures as truly transparent annotations on method calls, which designate opportunities for concurrency. Serial programs can be made concurrent simply by replacing standard method calls with future invocations. Most significantly, even though some parts of the program are executed concurrently and may indeed operate on shared data, the semblance of serial execution is nonetheless preserved. Thus, program reasoning is simplified since data dependencies present in a sequential program are not violated in a version augmented with safe futures.Besides presenting a programming model and API for safe futures, we formalize the safety conditions that must be satisfied to ensure equivalence between a sequential Java program and its future-annotated counterpart. A detailed implementation study is also provided. Our implementation exploits techniques such as object versioning and task revocation to guarantee necessary safety conditions. We also present an extensive experimental evaluation of our implementation to quantify overheads and limitations. Our experiments indicate that for programs with modest mutation rates on shared data, applications can use futures to profitably exploit parallelism, without sacrificing safety.}},
  url = {https://doi.org/10.1145/1094811.1094845},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup347,
  title = {Agda-curious? an exploration of programming with dependent types}},
  author = {McBride, Conor Thomas}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {I explore programming with the dependently typed functional language, AGDA. I present the progress which AGDA has made, demonstrate its usage in a small development, reflect critically on the state of the art, and speculate about the way ahead. I do not seek to persuade you to adopt AGDA as your primary tool for systems development, but argue that AGDA stimulates new useful ways to think about programming problems and deserves not just curiosity but interest, support and contribution.}},
  url = {https://doi.org/10.1145/2364527.2364529},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup348,
  title = {Deciding equivalence with sums and the empty type}},
  author = {Scherer, Gabriel}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The logical technique of focusing can be applied to the λ-calculus; in a simple type system with atomic types and negative type formers (functions, products, the unit type), its normal forms coincide with βη-normal forms. Introducing a saturation phase gives a notion of quasi-normal forms in presence of positive types (sum types and the empty type). This rich structure let us prove the decidability of βη-equivalence in presence of the empty type, the fact that it coincides with contextual equivalence, and with set-theoretic equality in all finite models.}},
  url = {https://doi.org/10.1145/3009837.3009901},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup349,
  title = {Homotopical patch theory}},
  author = {Angiuli, Carlo and Morehouse, Edward and Licata, Daniel R. and Harper, Robert}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Homotopy type theory is an extension of Martin-L\"{o}},
  url = {https://doi.org/10.1145/2628136.2628158},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup350,
  title = {CURD: a dynamic CUDA race detector}},
  author = {Peng, Yuanfeng and Grover, Vinod and Devietti, Joseph}},
  year = {2018}},
  journal = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {As GPUs have become an integral part of nearly every pro- cessor, GPU programming has become increasingly popular. GPU programming requires a combination of extreme levels of parallelism and low-level programming, making it easy for concurrency bugs such as data races to arise. These con- currency bugs can be extremely subtle and di cult to debug due to the massive numbers of threads running concurrently on a modern GPU. While some tools exist to detect data races in GPU pro- grams, they are often prohibitively slow or focused only on a small class of data races in shared memory. Compared to prior work, our race detector, CURD, can detect data races precisely on both shared and global memory, selects an appropriate race detection algorithm based on the synchronization used in a program, and utilizes efficient compiler instrumentation to reduce performance overheads. Across 53 benchmarks, we find that using CURD incurs an aver- age slowdown of just 2.88x over native execution. CURD is 2.1x faster than Nvidia’s CUDA-Racecheck race detector, de- spite detecting a much broader class of races. CURD finds 35 races across our benchmarks, including bugs in established benchmark suites and in sample programs from Nvidia.}},
  url = {https://doi.org/10.1145/3192366.3192368},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup351,
  title = {Progress guarantee for parallel programs via bounded lock-freedom}},
  author = {Petrank, Erez and Musuvathi, Madanlal and Steesngaard, Bjarne}},
  year = {2009}},
  journal = {Proceedings of the 30th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Parallel platforms are becoming ubiquitous with modern computing systems. Many parallel applications attempt to avoid locks in order to achieve high responsiveness, aid scalability, and avoid deadlocks and livelocks. However, avoiding the use of system locks does not guarantee that no locks are actually used, because progress inhibitors may occur in subtle ways through various program structures. Notions of progress guarantee such as lock-freedom, wait-freedom, and obstruction-freedom have been proposed in the literature to provide various levels of progress guarantees.In this paper we formalize the notions of progress guarantees using linear temporal logic (LTL). We concentrate on lock-freedom and propose a variant of it denoted bounded lock-freedom, which is more suitable for guaranteeing progress in practical systems. We use this formal definition to build a tool that checks if a concurrent program is bounded lock-free for a given bound. We then study the interaction between programs with progress guarantees and the underlying system (e.g., compilers, runtimes, operating systems, and hardware platforms). We propose a means to argue that an underlying system supports lock-freedom. A composition theorem asserts that bounded lock-free algorithms running on bounded lock-free supporting systems retain bounded lock-freedom for the composed execution.}},
  url = {https://doi.org/10.1145/1542476.1542493},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup352,
  title = {FreshML: programming with binders made simple}},
  author = {Shinwell, Mark R. and Pitts, Andrew M. and Gabbay, Murdoch J.}},
  year = {2003}},
  journal = {Proceedings of the Eighth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {FreshML extends ML with elegant and practical constructs for declaring and manipulating syntactical data involving statically scoped binding operations. User-declared FreshML datatypes involving binders are concrete, in the sense that values of these types can be deconstructed by matching against patterns naming bound variables explicitly. This may have the computational effect of swapping bound names with freshly generated ones; previous work on FreshML used a complicated static type system inferring information about the 'freshness' of names for expressions in order to tame this effect. The main contribution of this paper is to show (perhaps surprisingly) that a standard type system without freshness inference, coupled with a conventional treatment of fresh name generation, suffices for FreshML's crucial correctness property that values of datatypes involving binders are operationally equivalent if and only if they represent a-equivalent pieces of object-level syntax. This is established via a novel denotational semantics. FreshML without static freshness inference is no more impure than ML and experience with it shows that it supports a programming style pleasingly close to informal practice when it comes to dealing with object-level syntax modulo a-equivalence.}},
  url = {https://doi.org/10.1145/944705.944729},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup353,
  title = {An SSA-based algorithm for optimal speculative code motion under an execution profile}},
  author = {Zhou, Hucheng and Chen, Wenguang and Chow, Fred}},
  year = {2011}},
  journal = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {To derive maximum optimization benefits from partial redundancy elimination (PRE),it is necessary to go beyond its safety constraint. Algorithms for optimal speculative code motion have been developed based on the application of minimum cut to flow networks formed out of the control flow graph. These previous techniques did not take advantage of the SSA form, which is a popular program representation widely used in modern-day compilers. We have developed the MC-SSAPRE algorithm that enables an SSA-based compiler to take full advantage of SSA to perform optimal speculative code motion efficiently when an execution profile is available. Our work shows that it is possible to form flow networks out of SSA graphs, and the min-cut technique can be applied equally well on these flow networks to find the optimal code placement. We provide proofs of the correctness and computational and lifetime optimality of MC-SSAPRE. We analyze its time complexity to show its efficiency advantage. We have implemented MC-SSAPRE in the open-sourced Path64 compiler. Our experimental data based on the full SPEC CPU2006 Benchmark Suite show that MC-SSAPRE can further improve program performance over traditional SSAPRE, and that our sparse approach to the problem does result in smaller problem sizes.}},
  url = {https://doi.org/10.1145/1993498.1993510},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup354,
  title = {On the bright side of type classes: instance arguments in Agda}},
  author = {Devriese, Dominique and Piessens, Frank}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present instance arguments: an alternative to type classes and related features in the dependently typed, purely functional programming language/proof assistant Agda. They are a new, general type of function arguments, resolved from call-site scope in a type-directed way. The mechanism is inspired by both Scala's implicits and Agda's existing implicit arguments, but differs from both in important ways. Our mechanism is designed and implemented for Agda, but our design choices can be applied to other programming languages as well.Like Scala's implicits, we do not provide a separate structure for type classes and their instances, but instead rely on Agda's standard dependently typed records, so that standard language mechanisms provide features that are missing or expensive in other proposals. Like Scala, we support the equivalent of local instances. Unlike Scala, functions taking our new arguments are first-class citizens and can be abstracted over and manipulated in standard ways. Compared to other proposals, we avoid the pitfall of introducing a separate type-level computational model through the instance search mechanism. All values in scope are automatically candidates for instance resolution. A final novelty of our approach is that existing Agda libraries using records gain the benefits of type classes without any modification.We discuss our implementation in Agda (to be part of Agda 2.2.12) and we use monads as an example to show how it allows existing concepts in the Agda standard library to be used in a similar way as corresponding Haskell code using type classes. We also demonstrate and discuss equivalents and alternatives to some advanced type class-related patterns from the literature and some new patterns specific to our system.}},
  url = {https://doi.org/10.1145/2034773.2034796},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup355,
  title = {Linear dependent types for differential privacy}},
  author = {Gaboardi, Marco and Haeberlen, Andreas and Hsu, Justin and Narayan, Arjun and Pierce, Benjamin C.}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Differential privacy offers a way to answer queries about sensitive information while providing strong, provable privacy guarantees, ensuring that the presence or absence of a single individual in the database has a negligible statistical effect on the query's result. Proving that a given query has this property involves establishing a bound on the query's sensitivity---how much its result can change when a single record is added or removed.A variety of tools have been developed for certifying that a given query differentially private. In one approach, Reed and Pierce [34] proposed a functional programming language, Fuzz, for writing differentially private queries. Fuzz uses linear types to track sensitivity and a probability monad to express randomized computation; it guarantees that any program with a certain type is differentially private. Fuzz can successfully verify many useful queries. However, it fails when the sensitivity analysis depends on values that are not known statically.We present DFuzz, an extension of Fuzz with a combination of linear indexed types and lightweight dependent types. This combination allows a richer sensitivity analysis that is able to certify a larger class of queries as differentially private, including ones whose sensitivity depends on runtime information. As in Fuzz, the differential privacy guarantee follows directly from the soundness theorem of the type system. We demonstrate the enhanced expressivity of DFuzz by certifying differential privacy for a broad class of iterative algorithms that could not be typed previously.}},
  url = {https://doi.org/10.1145/2429069.2429113},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup356,
  title = {Abstract Symbolic Automata: Mixed syntactic/semantic similarity analysis of executables}},
  author = {Dalla Preda, Mila and Giacobazzi, Roberto and Lakhotia, Arun and Mastroeni, Isabella}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We introduce a model for mixed syntactic/semantic approximation of programs based on symbolic finite automata (SFA). The edges of SFA are labeled by predicates whose semantics specifies the denotations that are allowed by the edge. We introduce the notion of abstract symbolic finite automaton (ASFA) where approximation is made by abstract interpretation of symbolic finite automata, acting both at syntactic (predicate) and semantic (denotation) level. We investigate in the details how the syntactic and semantic abstractions of SFA relate to each other and contribute to the determination of the recognized language. Then we introduce a family of transformations for simplifying ASFA. We apply this model to prove properties of commonly used tools for similarity analysis of binary executables. Following the structure of their control flow graphs, disassembled binary executables are represented as (concrete) SFA, where states are program points and predicates represent the (possibly infinite) I/O semantics of each basic block in a constraint form. Known tools for binary code analysis are viewed as specific choices of symbolic and semantic abstractions in our framework, making symbolic finite automata and their abstract interpretations a unifying model for comparing and reasoning about soundness and completeness of analyses of low-level code.}},
  url = {https://doi.org/10.1145/2676726.2676986},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup357,
  title = {Ode on a random urn (functional pearl)}},
  author = {Lampropoulos, Leonidas and Spector-Zabusky, Antal and Foner, Kenneth}},
  year = {2017}},
  journal = {Proceedings of the 10th ACM SIGPLAN International Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present the urn, a simple tree-based data structure that supports sampling from and updating discrete probability distributions in logarithmic time. We avoid the usual complexity of traditional self-balancing binary search trees by not keeping values in a specific order. Instead, we keep the tree maximally balanced at all times using a single machine word of overhead: its size. Urns provide an alternative interface for the frequency combinator from the QuickCheck library that allows for asymptotically more efficient sampling from dynamically-updated distributions. They also facilitate backtracking in property-based random testing, and can be applied to such complex examples from the literature as generating well-typed lambda terms or information flow machine states, demonstrating significant speedups.}},
  url = {https://doi.org/10.1145/3122955.3122959},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup358,
  title = {Checking NFA equivalence with bisimulations up to congruence}},
  author = {Bonchi, Filippo and Pous, Damien}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We introduce bisimulation up to congruence as a technique for proving language equivalence of non-deterministic finite automata. Exploiting this technique, we devise an optimisation of the classical algorithm by Hopcroft and Karp. We compare our approach to the recently introduced antichain algorithms, by analysing and relating the two underlying coinductive proof methods. We give concrete examples where we exponentially improve over antichains; experimental results moreover show non negligible improvements.}},
  url = {https://doi.org/10.1145/2429069.2429124},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup359,
  title = {Staging with control: type-safe multi-stage programming with control operators}},
  author = {Oishi, Junpei and Kameyama, Yukiyoshi}},
  year = {2017}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Staging allows a programmer to write domain-specific, custom code generators. Ideally, a programming language for staging provides all necessary features for staging, and at the same time, gives static guarantee for the safety properties of generated code including well typedness and well scopedness. We address this classic problem for the language with control operators, which allow code optimizations in a modular and compact way. Specifically, we design a staged programming language with the expressive control operators shift0 and reset0, which let us express, for instance, multi-layer let-insertion, while keeping the static guarantee of well typedness and well scopedness. For this purpose, we extend our earlier work on refined environment classifiers which were introduced for the staging language with state. We show that our language is expressive enough to express interesting code generation techniques, and that the type system enjoys type soundness. We also mention a type inference algorithm for our language under reasonable restriction.}},
  url = {https://doi.org/10.1145/3136040.3136049},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup360,
  title = {Discovering affine equalities using random interpretation}},
  author = {Gulwani, Sumit and Necula, George C.}},
  year = {2003}},
  journal = {Proceedings of the 30th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a new polynomial-time randomized algorithm for discovering affine equalities involving variables in a program. The key idea of the algorithm is to execute a code fragment on a few random inputs, but in such a way that all paths are covered on each run. This makes it possible to rule out invalid relationships even with very few runs.The algorithm is based on two main techniques. First, both branches of a conditional are executed on each run and at joint points we perform an affine combination of the joining states. Secondly, in the branches of an equality conditional we adjust the data values on the fly to reflect the truth value of the guarding boolean expression. This increases the number of affine equalities that the analysis discovers.The algorithm is simpler to implement than alternative deterministic versions, has better computational complexity, and has an extremely small probability of error for even a small number of runs. This algorithm is an example of how randomization can provide a trade-off between the cost and complexity of program analysis, and a small probability of unsoundness.}},
  url = {https://doi.org/10.1145/604131.604138},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup361,
  title = {Higher-Order Approximate Relational Refinement Types for Mechanism Design and Differential Privacy}},
  author = {Barthe, Gilles and Gaboardi, Marco and Gallego Arias, Emilio Jes\'{u}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Mechanism design is the study of algorithm design where the inputs to the algorithm are controlled by strategic agents, who must be incentivized to faithfully report them. Unlike typical programmatic properties, it is not sufficient for algorithms to merely satisfy the property, incentive properties are only useful if the strategic agents also believe this fact.Verification is an attractive way to convince agents that the incentive properties actually hold, but mechanism design poses several unique challenges: interesting properties can be sophisticated relational properties of probabilistic computations involving expected values, and mechanisms may rely on other probabilistic properties, like differential privacy, to achieve their goals.We introduce a relational refinement type system, called HOARe2, for verifying mechanism design and differential privacy. We show that HOARe2 is sound w.r.t. a denotational semantics, and correctly models (epsilon,delta)-differential privacy; moreover, we show that it subsumes DFuzz, an existing linear dependent type system for differential privacy. Finally, we develop an SMT-based implementation of HOARe2 and use it to verify challenging examples of mechanism design, including auctions and aggregative games, and new proposed examples from differential privacy.}},
  url = {https://doi.org/10.1145/2676726.2677000},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup362,
  title = {Ordered vs. unordered: a comparison of parallelism and work-efficiency in irregular algorithms}},
  author = {Hassaan, Muhammad Amber and Burtscher, Martin and Pingali, Keshav}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Outside of computational science, most problems are formulated in terms of irregular data structures such as graphs, trees and sets. Unfortunately, we understand relatively little about the structure of parallelism and locality in irregular algorithms. In this paper, we study multiple algorithms for four such problems: discrete-event simulation, single-source shortest path, breadth-first search, and minimal spanning trees. We show that the algorithms can be classified into two categories that we call unordered and ordered, and demonstrate experimentally that there is a trade-off between parallelism and work efficiency: unordered algorithms usually have more parallelism than their ordered counterparts for the same problem, but they may also perform more work. Nevertheless, our experimental results show that unordered algorithms typically lead to more scalable implementations, demonstrating that less work-efficient irregular algorithms may be better for parallel execution.}},
  url = {https://doi.org/10.1145/1941553.1941557},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup363,
  title = {KISS: keep it simple and sequential}},
  author = {Qadeer, Shaz and Wu, Dinghao}},
  year = {2004}},
  journal = {Proceedings of the ACM SIGPLAN 2004 Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The design of concurrent programs is error-prone due to the interaction between concurrently executing threads. Traditional automated techniques for finding errors in concurrent programs, such as model checking, explore all possible thread interleavings. Since the number of thread interleavings increases exponentially with the number of threads, such analyses have high computational complexity. In this paper, we present a novel analysis technique for concurrent programs that avoids this exponential complexity. Our analysis transforms a concurrent program into a sequential program that simulates the execution of a large subset of the behaviors of the concurrent program. The sequential program is then analyzed by a tool that only needs to understand the semantics of sequential execution. Our technique never reports false errors but may miss errors. We have implemented the technique in KISS, an automated checker for multithreaded C programs, and obtained promising initial results by using KISS to detect race conditions in Windows device drivers.}},
  url = {https://doi.org/10.1145/996841.996845},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup364,
  title = {Alembic: automatic locality extraction via migration}},
  author = {Holt, Brandon and Briggs, Preston and Ceze, Luis and Oskin, Mark}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Partitioned Global Address Space (PGAS) environments simplify writing parallel code for clusters because they make data movement implicit - dereferencing global pointers automatically moves data around. However, it does not free the programmer from needing to reason about locality - poor placement of data can lead to excessive and even unnecessary communication. For this reason, modern PGAS languages such as X10, Chapel, and UPC allow programmers to express data-layout constraints and explicitly move computation. This places an extra burden on the programmer, and is less effective for applications with limited or data-dependent locality (e.g., graph analytics).This paper proposes Alembic, a new static analysis that frees programmers from having to manually move computation to exploit locality in PGAS programs. It works by determining regions of code that access the same cluster node, then transforming the code to migrate parts of the execution to increase the proportion of accesses to local data. We implement the analysis and transformation for C++ in LLVM and show that in irregular application kernels, Alembic can achieve 82\% of the performance of hand-tuned communication (for comparison, na\"{\i}},
  url = {https://doi.org/10.1145/2660193.2660194},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup365,
  title = {Non-linear loop invariant generation using Gr\"{o}},
  author = {Sankaranarayanan, Sriram and Sipma, Henny B. and Manna, Zohar}},
  year = {2004}},
  journal = {Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a new technique for the generation of non-linear (algebraic) invariants of a program. Our technique uses the theory of ideals over polynomial rings to reduce the non-linear invariant generation problem to a numerical constraint solving problem. So far, the literature on invariant generation has been focussed on the construction of linear invariants for linear programs. Consequently, there has been little progress toward non-linear invariant generation. In this paper, we demonstrate a technique that encodes the conditions for a given template assertion being an invariant into a set of constraints, such that all the solutions to these constraints correspond to non-linear (algebraic) loop invariants of the program. We discuss some trade-offs between the completeness of the technique and the tractability of the constraint-solving problem generated. The application of the technique is demonstrated on a few examples.}},
  url = {https://doi.org/10.1145/964001.964028},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup366,
  title = {A model of cooperative threads}},
  author = {Abadi, Martin and Plotkin, Gordon}},
  year = {2009}},
  journal = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We develop a model of concurrent imperative programming with threads. We focus on a small imperative language with cooperative threads which execute without interruption until they terminate or explicitly yield control. We define and study a trace-based denotational semantics for this language; this semantics is fully abstract but mathematically elementary. We also give an equational theory for the computational effects that underlie the language, including thread spawning. We then analyze threads in terms of the free algebra monad for this theory.}},
  url = {https://doi.org/10.1145/1480881.1480887},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup367,
  title = {The chemical approach to typestate-oriented programming}},
  author = {Crafa, Silvia and Padovani, Luca}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We study a novel approach to typestate-oriented programming based on the chemical metaphor: state and operations on objects are molecules of messages and state transformations are chemical reactions. This approach allows us to investigate typestate in an inherently concurrent setting, whereby objects can be accessed and modified concurrently by several processes, each potentially changing only part of their state. We introduce a simple behavioral type theory to express in a uniform way both the private and the public interfaces of objects, to describe and enforce structured object protocols consisting of possibilities, prohibitions, and obligations, and to control object sharing.}},
  url = {https://doi.org/10.1145/2814270.2814287},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup368,
  title = {DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning}},
  author = {Chen, Tianshi and Du, Zidong and Sun, Ninghui and Wang, Jia and Wu, Chengyong and Chen, Yunji and Temam, Olivier}},
  year = {2014}},
  journal = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Machine-Learning tasks are becoming pervasive in a broad range of domains, and in a broad range of systems (from embedded systems to data centers). At the same time, a small set of machine-learning algorithms (especially Convolutional and Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications. As architectures evolve towards heterogeneous multi-cores composed of a mix of cores and accelerators, a machine-learning accelerator can achieve the rare combination of efficiency (due to the small number of target algorithms) and broad application scope.Until now, most machine-learning accelerator designs have focused on efficiently implementing the computational part of the algorithms. However, recent state-of-the-art CNNs and DNNs are characterized by their large size. In this study, we design an accelerator for large-scale CNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and energy.We show that it is possible to design an accelerator with a high throughput, capable of performing 452 GOP/s (key NN operations such as synaptic weight multiplications and neurons outputs additions) in a small footprint of 3.02 mm2 and 485 mW; compared to a 128-bit 2GHz SIMD processor, the accelerator is 117.87x faster, and it can reduce the total energy by 21.08x. The accelerator characteristics are obtained after layout at 65 nm. Such a high throughput in a small footprint can open up the usage of state-of-the-art machine-learning algorithms in a broad set of systems and for a broad set of applications.}},
  url = {https://doi.org/10.1145/2541940.2541967},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup369,
  title = {Diderot: a parallel DSL for image analysis and visualization}},
  author = {Chiw, Charisee and Kindlmann, Gordon and Reppy, John and Samuels, Lamont and Seltzer, Nick}},
  year = {2012}},
  journal = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Research scientists and medical professionals use imaging technology, such as computed tomography (CT) and magnetic resonance imaging (MRI) to measure a wide variety of biological and physical objects. The increasing sophistication of imaging technology creates demand for equally sophisticated computational techniques to analyze and visualize the image data. Analysis and visualization codes are often crafted for a specific experiment or set of images, thus imaging scientists need support for quickly developing codes that are reliable, robust, and efficient.In this paper, we present the design and implementation of Diderot, which is a parallel domain-specific language for biomedical image analysis and visualization. Diderot supports a high-level model of computation that is based on continuous tensor fields. These tensor fields are reconstructed from discrete image data using separable convolution kernels, but may also be defined by applying higher-order operations, such as differentiation (∇). Early experiments demonstrate that Diderot provides both a high-level concise notation for image analysis and visualization algorithms, as well as high sequential and parallel performance.}},
  url = {https://doi.org/10.1145/2254064.2254079},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup370,
  title = {Decomposition instead of self-composition for proving the absence of timing channels}},
  author = {Antonopoulos, Timos and Gazzillo, Paul and Hicks, Michael and Koskinen, Eric and Terauchi, Tachio and Wei, Shiyi}},
  year = {2017}},
  journal = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a novel approach to proving the absence of timing channels. The idea is to partition the program's execution traces in such a way that each partition component is checked for timing attack resilience by a time complexity analysis and that per-component resilience implies the resilience of the whole program. We construct a partition by splitting the program traces at secret-independent branches. This ensures that any pair of traces with the same public input has a component containing both traces. Crucially, the per-component checks can be normal safety properties expressed in terms of a single execution. Our approach is thus in contrast to prior approaches, such as self-composition, that aim to reason about multiple (k≥ 2) executions at once. We formalize the above as an approach called quotient partitioning, generalized to any k-safety property, and prove it to be sound. A key feature of our approach is a demand-driven partitioning strategy that uses a regex-like notion called trails to identify sets of execution traces, particularly those influenced by tainted (or secret) data. We have applied our technique in a prototype implementation tool called Blazer, based on WALA, PPL, and the brics automaton library. We have proved timing-channel freedom of (or synthesized an attack specification for) 24 programs written in Java bytecode, including 6 classic examples from the literature and 6 examples extracted from the DARPA STAC challenge problems.}},
  url = {https://doi.org/10.1145/3062341.3062378},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup371,
  title = {Tigr: Transforming Irregular Graphs for GPU-Friendly Graph Processing}},
  author = {Nodehi Sabet, Amir Hossein and Qiu, Junqiao and Zhao, Zhijia}},
  year = {2018}},
  journal = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Graph analytics delivers deep knowledge by processing large volumes of highly connected data. In real-world graphs, the degree distribution tends to follow the power law -- a small portion of nodes own a large number of neighbors. The high irregularity of degree distribution acts as a major barrier to their efficient processing on GPU architectures, which are primarily designed for accelerating computations on regular data with SIMD executions. Existing solutions to the inefficiency of GPU-based graph analytics either modify the graph programming abstraction or rely on changes to the low-level thread execution models. The former requires more programming efforts for designing and maintaining graph analytics; while the latter couples with the underlying architectures, making it difficult to adapt as architectures quickly evolve. Unlike prior efforts, this work proposes to address the above fundamental problem at its origin -- the irregular graph data itself. It raises a critical question in irregular graph processing: Is it possible to transform irregular graphs into more regular ones such that the graphs can be processed more efficiently on GPU-like architectures, yet still producing the same results? Inspired by the question, this work introduces Tigr -- a graph transformation framework that can effectively reduce the irregularity of real-world graphs with correctness guarantees for a wide range of graph analytics. To make the transformations practical, Tigr features a lightweight virtual transformation scheme, which can substantially reduce the costs of graph transformations, while preserving the benefits of reduced irregularity. Evaluation on Tigr-based GPU graph processing shows significant and consistent speedup over the state-of-the-art GPU graph processing frameworks for a spectrum of irregular graphs.}},
  url = {https://doi.org/10.1145/3173162.3173180},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup372,
  title = {Traveling forward in time to newer operating systems using ShadowReboot}},
  author = {Yamada, Hiroshi and Kono, Kenji}},
  year = {2013}},
  journal = {Proceedings of the 9th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Operating system (OS) reboots are an essential part of updating kernels and applications on laptops and desktop PCs. Long downtime during OS reboots severely disrupts users' computational activities. This long disruption discourages the users from conducting OS reboots, failing to enforce them to conduct software updates. This paper presents ShadowReboot, a virtual machine monitor (VMM)-based approach that shortens downtime of OS reboots in software updates. ShadowReboot conceals OS reboot activities from user's applications by spawning a VM dedicated to an OS reboot and systematically producing the rebooted state where the updated kernel and applications are ready for use. ShadowReboot provides an illusion to the users that the guest OS travels forward in time to the rebooted state. ShadowReboot offers the following advantages. It can be used to apply patches to the kernels and even system configuration updates. Next, it does not require any special patch requiring detailed knowledge about the target kernels. Lastly, it does not require any target kernel modification. We implemented a prototype in VirtualBox 4.0.10 OSE. Our experimental results show that ShadowReboot successfully updated software on unmodified commodity OS kernels and shortened the downtime of commodity OS reboots on five Linux distributions (Fedora, Ubuntu, Gentoo, Cent, and SUSE) by 91 to 98\%.}},
  url = {https://doi.org/10.1145/2451512.2451536},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup373,
  title = {CUDAlign: using GPU to accelerate the comparison of megabase genomic sequences}},
  author = {Sandes, Edans Flavius O. and de Melo, Alba Cristina M.A.}},
  year = {2010}},
  journal = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Biological sequence comparison is a very important operation in Bioinformatics. Even though there do exist exact methods to compare biological sequences, these methods are often neglected due to their quadratic time and space complexity. In order to accelerate these methods, many GPU algorithms were proposed in the literature. Nevertheless, all of them restrict the size of the smallest sequence in such a way that Megabase genome comparison is prevented. In this paper, we propose and evaluate CUDAlign, a GPU algorithm that is able to compare Megabase biological sequences with an exact Smith-Waterman affine gap variant. CUDAlign was implemented in CUDA and tested in two GPU boards, separately. For real sequences whose size range from 1MBP (Megabase Pairs) to 47MBP, a close to uniform GCUPS (Giga Cells Updates per Second) was obtained, showing the potential scalability of our approach. Also, CUDAlign was able to compare the human chromosome 21 and the chimpanzee chromosome 22. This operation took 21 hours on GeForce GTX 280, resulting in a peak performance of 20.375 GCUPS. As far as we know, this is the first time such huge chromosomes are compared with an exact method.}},
  url = {https://doi.org/10.1145/1693453.1693473},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup374,
  title = {Dynamic knobs for responsive power-aware computing}},
  author = {Hoffmann, Henry and Sidiroglou, Stelios and Carbin, Michael and Misailovic, Sasa and Agarwal, Anant and Rinard, Martin}},
  year = {2011}},
  journal = {Proceedings of the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present PowerDial, a system for dynamically adapting application behavior to execute successfully in the face of load and power fluctuations. PowerDial transforms static configuration parameters into dynamic knobs that the PowerDial control system can manipulate to dynamically trade off the accuracy of the computation in return for reductions in the computational resources that the application requires to produce its results. These reductions translate directly into performance improvements and power savings.Our experimental results show that PowerDial can enable our benchmark applications to execute responsively in the face of power caps that would otherwise significantly impair responsiveness. They also show that PowerDial can significantly reduce the number of machines required to service intermittent load spikes, enabling reductions in power and capital costs.}},
  url = {https://doi.org/10.1145/1950365.1950390},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup375,
  title = {Set-based pre-processing for points-to analysis}},
  author = {Smaragdakis, Yannis and Balatsouras, George and Kastrinis, George}},
  year = {2013}},
  journal = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present set-based pre-analysis: a virtually universal optimization technique for flow-insensitive points-to analysis. Points-to analysis computes a static abstraction of how object values flow through a program's variables. Set-based pre-analysis relies on the observation that much of this reasoning can take place at the set level rather than the value level. Computing constraints at the set level results in significant optimization opportunities: we can rewrite the input program into a simplified form with the same essential points-to properties. This rewrite results in removing both local variables and instructions, thus simplifying the subsequent value-based points-to computation. Effectively, set-based pre-analysis puts the program in a normal form optimized for points-to analysis. Compared to other techniques for off-line optimization of points-to analyses in the literature, the new elements of our approach are the ability to eliminate statements, and not just variables, as well as its modularity: set-based pre-analysis can be performed on the input just once, e.g., allowing the pre-optimization of libraries that are subsequently reused many times and for different analyses. In experiments with Java programs, set-based pre-analysis eliminates 30\% of the program's local variables and 30\% or more of computed context-sensitive points-to facts, over a wide set of benchmarks and analyses, resulting in a ~20\% average speedup (max: 110\%, median: 18\%).}},
  url = {https://doi.org/10.1145/2509136.2509524},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup376,
  title = {Cantor meets Scott: semantic foundations for probabilistic networks}},
  author = {Smolka, Steffen and Kumar, Praveen and Foster, Nate and Kozen, Dexter and Silva, Alexandra}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {ProbNetKAT is a probabilistic extension of NetKAT with a denotational semantics based on Markov kernels. The language is expressive enough to generate continuous distributions, which raises the question of how to compute effectively in the language. This paper gives an new characterization of ProbNetKAT’s semantics using domain theory, which provides the foundation needed to build a practical implementation. We show how to use the semantics to approximate the behavior of arbitrary ProbNetKAT programs using distributions with finite support. We develop a prototype implementation and show how to use it to solve a variety of problems including characterizing the expected congestion induced by different routing schemes and reasoning probabilistically about reachability in a network.}},
  url = {https://doi.org/10.1145/3009837.3009843},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup377,
  title = {Communication avoiding successive band reduction}},
  author = {Ballard, Grey and Demmel, James and Knight, Nicholas}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The running time of an algorithm depends on both arithmetic and communication (i.e., data movement) costs, and the relative costs of communication are growing over time. In this work, we present both theoretical and practical results for tridiagonalizing a symmetric band matrix: we present an algorithm that asymptotically reduces communication, and we show that it indeed performs well in practice.The tridiagonalization of a symmetric band matrix is a key kernel in solving the symmetric eigenvalue problem for both full and band matrices. In order to preserve sparsity, tridiagonalization routines use annihilate-and-chase procedures that previously have suffered from poor data locality. We improve data locality by reorganizing the computation, asymptotically reducing communication costs compared to existing algorithms. Our sequential implementation demonstrates that avoiding communication improves runtime even at the expense of extra arithmetic: we observe a 2x speedup over Intel MKL while doing 43\% more floating point operations.Our parallel implementation targets shared-memory multicore platforms. It uses pipelined parallelism and a static scheduler while retaining the locality properties of the sequential algorithm. Due to lightweight synchronization and effective data reuse, we see 9.5x scaling over our serial code and up to 6x speedup over the PLASMA library, comparing parallel performance on a ten-core processor.}},
  url = {https://doi.org/10.1145/2145816.2145822},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup378,
  title = {Generative unbinding of names}},
  author = {Pitts, Andrew M. and Shinwell, Mark R.}},
  year = {2007}},
  journal = {Proceedings of the 34th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper is concerned with a programming language construct for typed name binding that enforces αequivalence. It proves a new result about what operations on names can co-exist with this construct. The particular form of typed name binding studied is that used by the FreshML family of languages. Its characteristic feature is that a name binding is represented by an abstract (name,value)-pair that may only be deconstructed via the generation of fresh bound names. In FreshML the only observation one can make of names is to test whether or not they are equal. This restricted amount of observation was thought necessary to ensure that there is no observable difference between αequivalent name binders. Yet from an algorithmic point of view it would be desirable to allow other operations and relations on names, such as a total ordering. This paper shows that, contrary to expectations, one may add not just ordering, but almost any relation or numerical function on names without disturbing the fundamental correctness result about this form of typed name binding (that object-level αequivalence precisely corresponds to contextual equivalence at the programming meta-level), so long as one takes the state of dynamically created names into account.}},
  url = {https://doi.org/10.1145/1190216.1190232},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup379,
  title = {Invariants of quantum programs: characterisations and generation}},
  author = {Ying, Mingsheng and Ying, Shenggang and Wu, Xiaodi}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Program invariant is a fundamental notion widely used in program verification and analysis. The aim of this paper is twofold: (i) find an appropriate definition of invariants for quantum programs; and (ii) develop an effective technique of invariant generation for verification and analysis of quantum programs. Interestingly, the notion of invariant can be defined for quantum programs in two different ways -- additive invariants and multiplicative invariants -- corresponding to two interpretations of implication in a continuous valued logic: the Lukasiewicz implication and the Godel implication. It is shown that both of them can be used to establish partial correctness of quantum programs. The problem of generating additive invariants of quantum programs is addressed by reducing it to an SDP (Semidefinite Programming) problem. This approach is applied with an SDP solver to generate invariants of two important quantum algorithms -- quantum walk and quantum Metropolis sampling. Our examples show that the generated invariants can be used to verify correctness of these algorithms and are helpful in optimising quantum Metropolis sampling. To our knowledge, this paper is the first attempt to define the notion of invariant and to develop a method of invariant generation for quantum programs.}},
  url = {https://doi.org/10.1145/3009837.3009840},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup380,
  title = {Using Butterfly-Patterned Partial Sums to Draw from Discrete Distributions}},
  author = {Steele, Guy L. and Tristan, Jean-Baptiste}},
  year = {2017}},
  journal = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We describe a SIMD technique for drawing values from multiple discrete distributions, such as sampling from the random variables of a mixture model, that avoids computing a complete table of partial sums of the relative probabilities. A table of alternate ("butterfly-patterned") form is faster to compute, making better use of coalesced memory accesses; from this table, complete partial sums are computed on the fly during a binary search. Measurements using CUDA 7.5 on an NVIDIA Titan Black GPU show that this technique makes an entire machine-learning application that uses a Latent Dirichlet Allocation topic model with 1024 topics about about 13\% faster (when using single-precision floating-point data) or about 35\% faster (when using double-precision floating-point data) than doing a straightforward matrix transposition after using coalesced accesses.}},
  url = {https://doi.org/10.1145/3018743.3018757},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup381,
  title = {A model-learner pattern for bayesian reasoning}},
  author = {Gordon, Andrew D. and Aizatulin, Mihhail and Borgstrom, Johannes and Claret, Guillaume and Graepel, Thore and Nori, Aditya V. and Rajamani, Sriram K. and Russo, Claudio}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A Bayesian model is based on a pair of probability distributions, known as the prior and sampling distributions. A wide range of fundamental machine learning tasks, including regression, classification, clustering, and many others, can all be seen as Bayesian models. We propose a new probabilistic programming abstraction, a typed Bayesian model, which is based on a pair of probabilistic expressions for the prior and sampling distributions. A sampler for a model is an algorithm to compute synthetic data from its sampling distribution, while a learner for a model is an algorithm for probabilistic inference on the model. Models, samplers, and learners form a generic programming pattern for model-based inference. They support the uniform expression of common tasks including model testing, and generic compositions such as mixture models, evidence-based model averaging, and mixtures of experts. A formal semantics supports reasoning about model equivalence and implementation correctness. By developing a series of examples and three learner implementations based on exact inference, factor graphs, and Markov chain Monte Carlo, we demonstrate the broad applicability of this new programming pattern.}},
  url = {https://doi.org/10.1145/2429069.2429119},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup382,
  title = {On coinductive equivalences for higher-order probabilistic functional programs}},
  author = {Dal Lago, Ugo and Sangiorgi, Davide and Alberti, Michele}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We study bisimulation and context equivalence in a probabilistic lambda-calculus. The contributions of this paper are threefold. Firstly we show a technique for proving congruence of probabilistic applicative bisimilarity. While the technique follows Howe's method, some of the technicalities are quite different, relying on non-trivial "disentangling" properties for sets of real numbers. Secondly we show that, while bisimilarity is in general strictly finer than context equivalence, coincidence between the two relations is attained on pure lambda-terms. The resulting equality is that induced by Levy-Longo trees, generally accepted as the finest extensional equivalence on pure lambda-terms under a lazy regime. Finally, we derive a coinductive characterisation of context equivalence on the whole probabilistic language, via an extension in which terms akin to distributions may appear in redex position. Another motivation for the extension is that its operational semantics allows us to experiment with a different congruence technique, namely that of logical bisimilarity.}},
  url = {https://doi.org/10.1145/2535838.2535872},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup383,
  title = {Time-warp: lightweight abort minimization in transactional memory}},
  author = {Diegues, Nuno and Romano, Paolo}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The notion of permissiveness in Transactional Memory (TM) translates to only aborting a transaction when it cannot be accepted in any history that guarantees correctness criterion. This property is neglected by most TMs, which, in order to maximize implementation's efficiency, resort to aborting transactions under overly conservative conditions. In this paper we seek to identify a sweet spot between permissiveness and efficiency by introducing the Time-Warp Multi-version algorithm (TWM). TWM is based on the key idea of allowing an update transaction that has performed stale reads (i.e., missed the writes of concurrently committed transactions) to be serialized by committing it in the past, which we call a time-warp commit. At its core, TWM uses a novel, lightweight validation mechanism with little computational overheads. TWM also guarantees that read-only transactions can never be aborted. Further, TWM guarantees Virtual World Consistency, a safety property that is deemed as particularly relevant in the context of TM. We demonstrate the practicality of this approach through an extensive experimental study, where we compare TWM with four other TMs, and show an average performance improvement of 65\% in high concurrency scenarios.}},
  url = {https://doi.org/10.1145/2555243.2555259},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup384,
  title = {Hybrid top-down and bottom-up interprocedural analysis}},
  author = {Zhang, Xin and Mangal, Ravi and Naik, Mayur and Yang, Hongseok}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Interprocedural static analyses are broadly classified into top-down and bottom-up, depending upon how they compute, instantiate, and reuse procedure summaries. Both kinds of analyses are challenging to scale: top-down analyses are hindered by ineffective reuse of summaries whereas bottom-up analyses are hindered by inefficient computation and instantiation of summaries. This paper presents a hybrid approach Swift that combines top-down and bottom-up analyses in a manner that gains their benefits without suffering their drawbacks. Swift is general in that it is parametrized by the top-down and bottom-up analyses it combines. We show an instantiation of Swift on a type-state analysis and evaluate it on a suite of 12 Java programs of size 60-250 KLOC each. Swift outperforms both conventional approaches, finishing on all the programs while both of those approaches fail on the larger programs.}},
  url = {https://doi.org/10.1145/2594291.2594328},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup385,
  title = {Adaptive data parallelism for internet clients on heterogeneous platforms}},
  author = {Pignotti, Alessandro and Welc, Adam and Mathiske, Bernd}},
  year = {2012}},
  journal = {Proceedings of the 8th Symposium on Dynamic Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Today's Internet is long past static web pages filled with HTML-formatted text sprinkled with an occasional image or animation. We have entered an era of Rich Internet Applications executed locally on Internet clients such as web browsers: games, physics engines, image rendering, photo editing, etc. Yet today's languages used to program Internet clients have limited ability to tap to the computational capabilities of the underlying, often heterogeneous, platforms. In this paper we present how a Domain Specific Language(DSL) can be integrated into ActionScript, one of the most popular scripting languages used to program Internet clients and a close cousin of JavaScript. We demonstrate how our DSL, called ASDP (ActionScript Data Parallel), can be used to enable data parallelism for existing sequential programs. We also present a prototype of a system where data parallel workloads can be executed on either CPU or a GPU, with the runtime system transparently selecting the best processing unit, depending on the type of workload as well as the architecture and current load of the execution platform. We evaluate performance of our system on a variety of benchmarks, representing different types of workloads: physics, image processing, scientific computing and cryptography.}},
  url = {https://doi.org/10.1145/2384577.2384585},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup386,
  title = {Reasoning about multiple related abstractions with MultiStar}},
  author = {van Staden, Stephan and Calcagno, Cristiano}},
  year = {2010}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Encapsulated abstractions are fundamental in object-oriented programming. A single class may employ multiple abstractions to achieve its purpose. Such abstractions are often related and combined in disciplined ways. This paper explores ways to express, verify and rely on logical relationships between abstractions. It introduces two general specification mechanisms: export clauses for relating abstractions in individual classes, and axiom clauses for relating abstractions in a class and all its descendants. MultiStar, an automatic verification tool based on separation logic and abstract predicate families, implements these mechanisms in a multiple inheritance setting. Several verified examples illustrate MultiStar's underlying logic. To demonstrate the flexibility of our approach, we also used MultiStar to verify the core iterator hierarchy of a popular data structure library.}},
  url = {https://doi.org/10.1145/1869459.1869501},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup387,
  title = {Abstraction refinement guided by a learnt probabilistic model}},
  author = {Grigore, Radu and Yang, Hongseok}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The core challenge in designing an effective static program analysis is to find a good program abstraction -- one that retains only details relevant to a given query. In this paper, we present a new approach for automatically finding such an abstraction. Our approach uses a pessimistic strategy, which can optionally use guidance from a probabilistic model. Our approach applies to parametric static analyses implemented in Datalog, and is based on counterexample-guided abstraction refinement. For each untried abstraction, our probabilistic model provides a probability of success, while the size of the abstraction provides an estimate of its cost in terms of analysis time. Combining these two metrics, probability and cost, our refinement algorithm picks an optimal abstraction. Our probabilistic model is a variant of the Erdos--Renyi random graph model, and it is tunable by what we call hyperparameters. We present a method to learn good values for these hyperparameters, by observing past runs of the analysis on an existing codebase. We evaluate our approach on an object sensitive pointer analysis for Java programs, with two client analyses (PolySite and Downcast).}},
  url = {https://doi.org/10.1145/2837614.2837663},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup388,
  title = {Persistent pointer information}},
  author = {Xiao, Xiao and Zhang, Qirun and Zhou, Jinguo and Zhang, Charles}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Pointer information, indispensable for static analysis tools, is expensive to compute and query. We provide a query-efficient persistence technique, Pestrie, to mitigate the costly computation and slow querying of precise pointer information. Leveraging equivalence and hub properties, Pestrie can compress pointer information and answers pointer related queries very efficiently. The experiment shows that Pestrie produces 10.5X and 17.5X smaller persistent files than the traditional bitmap and BDD encodings. Meanwhile, Pestrie is 2.9X to 123.6X faster than traditional demand-driven approaches for serving points-to related queries.}},
  url = {https://doi.org/10.1145/2594291.2594314},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup389,
  title = {The LOFAR correlator: implementation and performance analysis}},
  author = {Romein, John W. and Broekema, P. Chris and Mol, Jan David and van Nieuwpoort, Rob V.}},
  year = {2010}},
  journal = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {LOFAR is the first of a new generation of radio telescopes.Rather than using expensive dishes, it forms a distributed sensor network that combines the signals from many thousands of simple antennas. Its revolutionary design allows observations in a frequency range that has hardly been studied before.Another novel feature of LOFAR is the elaborate use of software to process data, where traditional telescopes use customized hardware. This dramatically increases flexibility and substantially reduces costs, but the high processing and bandwidth requirements compel the use of a supercomputer. The antenna signals are centrally combined, filtered, optionally beam-formed, and correlated by an IBM Blue Gene/P.This paper describes the implementation of the so-called correlator. To meet the real-time requirements, the application is highly optimized, and reaches exceptionally high computational and I/O efficiencies. Additionally, we study the scalability of the system, and show that it scales well beyond the requirements. The optimizations allows us to use only half the planned amount of resources, and process 50\% more telescope data, significantly improving the effectiveness of the entire telescope.}},
  url = {https://doi.org/10.1145/1693453.1693477},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup390,
  title = {Resilient X10: efficient failure-aware programming}},
  author = {Cunningham, David and Grove, David and Herta, Benjamin and Iyengar, Arun and Kawachiya, Kiyokuni and Murata, Hiroki and Saraswat, Vijay and Takeuchi, Mikio and Tardieu, Olivier}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Scale-out programs run on multiple processes in a cluster. In scale-out systems, processes can fail. Computations using traditional libraries such as MPI fail when any component process fails. The advent of Map Reduce, Resilient Data Sets and MillWheel has shown dramatic improvements in productivity are possible when a high-level programming framework handles scale-out and resilience automatically.We are concerned with the development of general-purpose languages that support resilient programming. In this paper we show how the X10 language and implementation can be extended to support resilience. In Resilient X10, places may fail asynchronously, causing loss of the data and tasks at the failed place. Failure is exposed through exceptions. We identify a {em Happens Before Invariance Principle}},
  url = {https://doi.org/10.1145/2555243.2555248},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup391,
  title = {StreamQRE: modular specification and efficient evaluation of quantitative queries over streaming data}},
  author = {Mamouras, Konstantinos and Raghothaman, Mukund and Alur, Rajeev and Ives, Zachary G. and Khanna, Sanjeev}},
  year = {2017}},
  journal = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Real-time decision making in emerging IoT applications typically relies on computing quantitative summaries of large data streams in an efficient and incremental manner. To simplify the task of programming the desired logic, we propose StreamQRE, which provides natural and high-level constructs for processing streaming data. Our language has a novel integration of linguistic constructs from two distinct programming paradigms: streaming extensions of relational query languages and quantitative extensions of regular expressions. The former allows the programmer to employ relational constructs to partition the input data by keys and to integrate data streams from different sources, while the latter can be used to exploit the logical hierarchy in the input stream for modular specifications. We first present the core language with a small set of combinators, formal semantics, and a decidable type system. We then show how to express a number of common patterns with illustrative examples. Our compilation algorithm translates the high-level query into a streaming algorithm with precise complexity bounds on per-item processing time and total memory footprint. We also show how to integrate approximation algorithms into our framework. We report on an implementation in Java, and evaluate it with respect to existing high-performance engines for processing streaming data. Our experimental evaluation shows that (1) StreamQRE allows more natural and succinct specification of queries compared to existing frameworks, (2) the throughput of our implementation is higher than comparable systems (for example, two-to-four times greater than RxJava), and (3) the approximation algorithms supported by our implementation can lead to substantial memory savings.}},
  url = {https://doi.org/10.1145/3062341.3062369},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup392,
  title = {VYRD: verifYing concurrent programs by runtime refinement-violation detection}},
  author = {Elmas, Tayfun and Tasiran, Serdar and Qadeer, Shaz}},
  year = {2005}},
  journal = {Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a runtime technique for checking that a concurrently-accessed data structure implementation, such as a file system or the storage management module of a database, conforms to an executable specification that contains an atomic method per data structure operation. The specification can be provided separately or a non-concurrent, "atomized" interpretation of the implementation can serve as the specification. The technique consists of two phases. In the first phase, the implementation is instrumented in order to record information into a log during execution. In the second, a separate verification thread uses the logged information to drive an instance of the specification and to check whether the logged execution conforms to it. We paid special attention to the general applicability and scalability of the techniques and to minimizing their concurrency and performance impact. The result is a lightweight verification method that provides a significant improvement over testing for concurrent programs.We formalize conformance to a specification using the notion of refinement: Each trace of the implementation must be equivalent to some trace of the specification. Among the novel features of our work are two variations on the definition of refinement appropriate for runtime checking: I/O and "view" refinement. These definitions were motivated by our experience with two industrial-scale concurrent data structure implementations: the Boxwood project, a B-link tree data structure built on a novel storage infrastructure [10] and the Scan file system [9]. I/O and view refinement checking were implemented as a verification tool named VRYD (VerifYing concurrent programs by Runtime Refinement-violation Detection). VYRD was applied to the verification of Boxwood, Java class libraries, and, previously, to the Scan filesystem. It was able to detect previously unnoticed subtle concurrency bugs in Boxwood and the Scan file system, and the known bugs in the Java class libraries and manually constructed examples. Experimental results indicate that our techniques have modest computational cost.}},
  url = {https://doi.org/10.1145/1065010.1065015},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup393,
  title = {Denotational cost semantics for functional languages with inductive types}},
  author = {Danner, Norman and Licata, Daniel R. and Ramyaa, Ramyaa}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A central method for analyzing the asymptotic complexity of a functional program is to extract and then solve a recurrence that expresses evaluation cost in terms of input size. The relevant notion of input size is often specific to a datatype, with measures including the length of a list, the maximum element in a list, and the height of a tree. In this work, we give a formal account of the extraction of cost and size recurrences from higher-order functional programs over inductive datatypes. Our approach allows a wide range of programmer-specified notions of size, and ensures that the extracted recurrences correctly predict evaluation cost. To extract a recurrence from a program, we first make costs explicit by applying a monadic translation from the source language to a complexity language, and then abstract datatype values as sizes. Size abstraction can be done semantically, working in models of the complexity language, or syntactically, by adding rules to a preorder judgement. We give several different models of the complexity language, which support different notions of size. Additionally, we prove by a logical relations argument that recurrences extracted by this process are upper bounds for evaluation cost; the proof is entirely syntactic and therefore applies to all of the models we consider.}},
  url = {https://doi.org/10.1145/2784731.2784749},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup394,
  title = {The essence of Reynolds}},
  author = {Brookes, Stephen and O'Hearn, Peter W. and Reddy, Uday}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {John Reynolds (1935-2013) was a pioneer of programming languages research. In this paper we pay tribute to the man, his ideas, and his influence.}},
  url = {https://doi.org/10.1145/2535838.2537851},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup395,
  title = {The virtues of conflict: analysing modern concurrency}},
  author = {Narayanaswamy, Ganesh and Joshi, Saurabh and Kroening, Daniel}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Modern shared memory multiprocessors permit reordering of memory operations for performance reasons. These reorderings are often a source of subtle bugs in programs written for such architectures. Traditional approaches to verify weak memory programs often rely on interleaving semantics, which is prone to state space explosion, and thus severely limits the scalability of the analysis. In recent times, there has been a renewed interest in modelling dynamic executions of weak memory programs using partial orders. However, such an approach typically requires ad-hoc mechanisms to correctly capture the data and control-flow choices/conflicts present in real-world programs. In this work, we propose a novel, conflict-aware, composable, truly concurrent semantics for programs written using C/C++ for modern weak memory architectures. We exploit our symbolic semantics based on general event structures to build an efficient decision procedure that detects assertion violations in bounded multi-threaded programs. Using a large, representative set of benchmarks, we show that our conflict-aware semantics outperforms the state-of-the-art partial-order based approaches.}},
  url = {https://doi.org/10.1145/2851141.2851165},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup396,
  title = {Mapping spiking neural networks onto a manycore neuromorphic architecture}},
  author = {Lin, Chit-Kwan and Wild, Andreas and Chinya, Gautham N. and Lin, Tsung-Han and Davies, Mike and Wang, Hong}},
  year = {2018}},
  journal = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a compiler for Loihi, a novel manycore neuromorphic processor that features a programmable, on-chip learning engine for training and executing spiking neural networks (SNNs). An SNN is distinguished from other neural networks in that (1) its independent computing units, or "neurons", communicate with others only through spike messages; and (2) each neuron evaluates local learning rules, which are functions of spike arrival and departure timings, to modify its local state. The collective neuronal state dynamics of an SNN form a nonlinear dynamical system that can be cast as an unconventional model of computation. To realize such an SNN on Loihi requires each constituent neuron to locally store and independently update its own spike timing information. However, each Loihi core has limited resources for this purpose and these must be shared by neurons assigned to the same core. In this work, we present a compiler for Loihi that maps the neurons of an SNN onto and across Loihi's cores efficiently. We show that a poor neuron-to-core mapping can incur significant energy costs and address this with a greedy algorithm that compiles SNNs onto Loihi in a power-efficient manner. In so doing, we highlight the need for further development of compilers for this new, emerging class of architectures.}},
  url = {https://doi.org/10.1145/3192366.3192371},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup397,
  title = {Automatic refunctionalization to a language with copattern matching: with applications to the expression problem}},
  author = {Rendel, Tillmann and Trieflinger, Julia and Ostermann, Klaus}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Defunctionalization and refunctionalization establish a correspondence between first-class functions and pattern matching, but the correspondence is not symmetric: Not all uses of pattern matching can be automatically refunctionalized to uses of higher-order functions. To remedy this asymmetry, we generalize from first-class functions to arbitrary codata. This leads us to full defunctionalization and refunctionalization between a codata language based on copattern matching and a data language based on pattern matching. We observe how programs can be written as matrices so that they are modularly extensible in one dimension but not the other. In this representation, defunctionalization and refunctionalization correspond to matrix transposition which effectively changes the dimension of extensibility a program supports. This suggests applications to the expression problem.}},
  url = {https://doi.org/10.1145/2784731.2784763},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup398,
  title = {Coherent explicit dictionary application for Haskell}},
  author = {Winant, Thomas and Devriese, Dominique}},
  year = {2018}},
  journal = {Proceedings of the 11th ACM SIGPLAN International Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Type classes are one of Haskell's most popular features and extend its type system with ad-hoc polymorphism. Since their conception, there were useful features that could not be offered because of the desire to offer two correctness properties: coherence and global uniqueness of instances. Coherence essentially guarantees that program semantics are independent from type-checker internals. Global uniqueness of instances is relied upon by libraries for enforcing, for example, that a single order relation is used for all manipulations of an ordered binary tree. The features that could not be offered include explicit dictionary application and local instances, which would be highly useful in practice. In this paper, we propose a new design for offering explicit dictionary application, without compromising coherence and global uniqueness. We introduce a novel criterion based on GHC's type argument roles to decide when a dictionary application is safe with respect to global uniqueness of instances. We preserve coherence by detecting potential sources of incoherence, and prove it formally. Moreover, our solution makes it possible to use local dictionaries. In addition to developing our ideas formally, we have implemented a working prototype in GHC.}},
  url = {https://doi.org/10.1145/3242744.3242752},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup399,
  title = {yaSpMV: yet another SpMV framework on GPUs}},
  author = {Yan, Shengen and Li, Chao and Zhang, Yunquan and Zhou, Huiyang}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {SpMV is a key linear algebra algorithm and has been widely used in many important application domains. As a result, numerous attempts have been made to optimize SpMV on GPUs to leverage their massive computational throughput. Although the previous work has shown impressive progress, load imbalance and high memory bandwidth remain the critical performance bottlenecks for SpMV. In this paper, we present our novel solutions to these problems. First, we devise a new SpMV format, called blocked compressed common coordinate (BCCOO), which uses bit flags to store the row indices in a blocked common coordinate (COO) format so as to alleviate the bandwidth problem. We further improve this format by partitioning the matrix into vertical slices to enhance the cache hit rates when accessing the vector to be multiplied. Second, we revisit the segmented scan approach for SpMV to address the load imbalance problem. We propose a highly efficient matrix-based segmented sum/scan for SpMV and further improve it by eliminating global synchronization. Then, we introduce an auto-tuning framework to choose optimization parameters based on the characteristics of input sparse matrices and target hardware platforms. Our experimental results on GTX680 GPUs and GTX480 GPUs show that our proposed framework achieves significant performance improvement over the vendor tuned CUSPARSE V5.0 (up to 229\% and 65\% on average on GTX680 GPUs, up to 150\% and 42\% on average on GTX480 GPUs) and some most recently proposed schemes (e.g., up to 195\% and 70\% on average over clSpMV on GTX680 GPUs, up to 162\% and 40\% on average over clSpMV on GTX480 GPUs).}},
  url = {https://doi.org/10.1145/2555243.2555255},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup400,
  title = {Analysis of recursively parallel programs}},
  author = {Bouajjani, Ahmed and Emmi, Michael}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We propose a general formal model of isolated hierarchical parallel computations, and identify several fragments to match the concurrency constructs present in real-world programming languages such as Cilk and X10. By associating fundamental formal models (vector addition systems with recursive transitions) to each fragment, we provide a common platform for exposing the relative difficulties of algorithmic reasoning. For each case we measure the complexity of deciding state-reachability for finite-data recursive programs, and propose algorithms for the decidable cases. The complexities which include PTIME, NP, EXPSPACE, and 2EXPTIME contrast with undecidable state-reachability for recursive multi-threaded programs.}},
  url = {https://doi.org/10.1145/2103656.2103681},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup401,
  title = {Guarded impredicative polymorphism}},
  author = {Serrano, Alejandro and Hage, Jurriaan and Vytiniotis, Dimitrios and Peyton Jones, Simon}},
  year = {2018}},
  journal = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The design space for type systems that support impredicative instantiation is extremely complicated. One needs to strike a balance between expressiveness, simplicity for both the end programmer and the type system implementor, and how easily the system can be integrated with other advanced type system concepts. In this paper, we propose a new point in the design space, which we call guarded impredicativity. Its key idea is that impredicative instantiation in an application is allowed for type variables that occur under a type constructor. The resulting type system has a clean declarative specification — making it easy for programmers to predict what will type and what will not —, allows for a smooth integration with GHC’s OutsideIn(X) constraint solving framework, while giving up very little in terms of expressiveness compared to systems like HMF, HML, FPH and MLF. We give a sound and complete inference algorithm, and prove a principal type property for our system.}},
  url = {https://doi.org/10.1145/3192366.3192389},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup402,
  title = {Sound input filter generation for integer overflow errors}},
  author = {Long, Fan and Sidiroglou-Douskos, Stelios and Kim, Deokhwan and Rinard, Martin}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a system, SIFT, for generating input filters that nullify integer overflow errors associated with critical program sites such as memory allocation or block copy sites. SIFT uses a static pro- gram analysis to generate filters that discard inputs that may trigger integer overflow errors in the computations of the sizes of allocated memory blocks or the number of copied bytes in block copy operations. Unlike all previous techniques of which we are aware, SIFT is sound -- if an input passes the filter, it will not trigger an integer overflow error at any analyzed site. Our results show that SIFT successfully analyzes (and therefore generates sound input filters for) 56 out of 58 memory allocation and block memory copy sites in analyzed input processing modules from five applications (VLC, Dillo, Swfdec, Swftools, and GIMP). These nullified errors include six known integer overflow vulnerabilities. Our results also show that applying these filters to 62895 real-world inputs produces no false positives. The analysis and filter generation times are all less than a second.}},
  url = {https://doi.org/10.1145/2535838.2535888},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup403,
  title = {Memoryful geometry of Interaction II: recursion and adequacy}},
  author = {Muroya, Koko and Hoshino, Naohiko and Hasuo, Ichiro}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A general framework of Memoryful Geometry of Interaction (mGoI) is introduced recently by the authors. It provides a sound translation of lambda-terms (on the high-level) to their realizations by stream transducers (on the low-level), where the internal states of the latter (called memories) are exploited for accommodating algebraic effects of Plotkin and Power. The translation is compositional, hence ``denotational,'' where transducers are inductively composed using an adaptation of Barbosa's coalgebraic component calculus. In the current paper we extend the mGoI framework and provide a systematic treatment of recursion---an essential feature of programming languages that was however missing in our previous work. Specifically, we introduce two new fixed-point operators in the coalgebraic component calculus. The two follow the previous work on recursion in GoI and are called Girard style and Mackie style: the former obviously exhibits some nice domain-theoretic properties, while the latter allows simpler construction. Their equivalence is established on the categorical (or, traced monoidal) level of abstraction, and is therefore generic with respect to the choice of algebraic effects. Our main result is an adequacy theorem of our mGoI translation, against Plotkin and Power's operational semantics for algebraic effects.}},
  url = {https://doi.org/10.1145/2837614.2837672},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup404,
  title = {Funcons for HGMP: the fundamental constructs of homogeneous generative meta-programming (short paper)}},
  author = {van Binsbergen, L. Thomas}},
  year = {2018}},
  journal = {Proceedings of the 17th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The PLanCompS project proposes a component-based approach to programming-language development in which fundamental constructs (funcons) are reused across language definitions. Homogeneous Generative Meta-Programming (HGMP) enables writing programs that generate code as data, at run-time or compile-time, for manipulation and staged evaluation. Building on existing formalisations of HGMP, this paper introduces funcons for HGMP and demonstrates their usage in component-based semantics.}},
  url = {https://doi.org/10.1145/3278122.3278132},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup405,
  title = {Finding low-utility data structures}},
  author = {Xu, Guoqing and Mitchell, Nick and Arnold, Matthew and Rountev, Atanas and Schonberg, Edith and Sevitsky, Gary}},
  year = {2010}},
  journal = {Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many opportunities for easy, big-win, program optimizations are missed by compilers. This is especially true in highly layered Java applications. Often at the heart of these missed optimization opportunities lie computations that, with great expense, produce data values that have little impact on the program's final output. Constructing a new date formatter to format every date, or populating a large set full of expensively constructed structures only to check its size: these involve costs that are out of line with the benefits gained. This disparity between the formation costs and accrued benefits of data structures is at the heart of much runtime bloat.We introduce a run-time analysis to discover these low-utility data structures. The analysis employs dynamic thin slicing, which naturally associates costs with value flows rather than raw data flows. It constructs a model of the incremental, hop-to-hop, costs and benefits of each data structure. The analysis then identifies suspicious structures based on imbalances of its incremental costs and benefits. To decrease the memory requirements of slicing, we introduce abstract dynamic thin slicing, which performs thin slicing over bounded abstract domains. We have modified the IBM J9 commercial JVM to implement this approach.We demonstrate two client analyses: one that finds objects that are expensive to construct but are not necessary for the forward execution, and second that pinpoints ultimately-dead values. We have successfully applied them to large-scale and long-running Java applications. We show that these analyses are effective at detecting operations that have unbalanced costs and benefits.}},
  url = {https://doi.org/10.1145/1806596.1806617},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup406,
  title = {Garbage collection auto-tuning for Java mapreduce on multi-cores}},
  author = {Singer, Jeremy and Kovoor, George and Brown, Gavin and Luj\'{a}},
  year = {2011}},
  journal = {Proceedings of the International Symposium on Memory Management}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {MapReduce has been widely accepted as a simple programming pattern that can form the basis for efficient, large-scale, distributed data processing. The success of the MapReduce pattern has led to a variety of implementations for different computational scenarios. In this paper we present MRJ, a MapReduce Java framework for multi-core architectures. We evaluate its scalability on a four-core, hyperthreaded Intel Core i7 processor, using a set of standard MapReduce benchmarks. We investigate the significant impact that Java runtime garbage collection has on the performance and scalability of MRJ. We propose the use of memory management auto-tuning techniques based on machine learning. With our auto-tuning approach, we are able to achieve MRJ performance within 10\% of optimal on 75\% of our benchmark tests.}},
  url = {https://doi.org/10.1145/1993478.1993495},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup407,
  title = {Counter-factual typing for debugging type errors}},
  author = {Chen, Sheng and Erwig, Martin}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Changing a program in response to a type error plays an important part in modern software development. However, the generation of good type error messages remains a problem for highly expressive type systems. Existing approaches often suffer from a lack of precision in locating errors and proposing remedies. Specifically, they either fail to locate the source of the type error consistently, or they report too many potential error locations. Moreover, the change suggestions offered are often incorrect. This makes the debugging process tedious and ineffective.We present an approach to the problem of type debugging that is based on generating and filtering a comprehensive set of type-change suggestions. Specifically, we generate all (program-structure-preserving) type changes that can possibly fix the type error. These suggestions will be ranked and presented to the programmer in an iterative fashion. In some cases we also produce suggestions to change the program. In most situations, this strategy delivers the correct change suggestions quickly, and at the same time never misses any rare suggestions. The computation of the potentially huge set of type-change suggestions is efficient since it is based on a variational type inference algorithm that type checks a program with variations only once, efficiently reusing type information for shared parts.We have evaluated our method and compared it with previous approaches. Based on a large set of examples drawn from the literature, we have found that our method outperforms other approaches and provides a viable alternative.}},
  url = {https://doi.org/10.1145/2535838.2535863},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup408,
  title = {From Datalog to flix: a declarative language for fixed points on lattices}},
  author = {Madsen, Magnus and Yee, Ming-Ho and Lhot\'{a}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present Flix, a declarative programming language for specifying and solving least fixed point problems, particularly static program analyses. Flix is inspired by Datalog and extends it with lattices and monotone functions. Using Flix, implementors of static analyses can express a broader range of analyses than is currently possible in pure Datalog, while retaining its familiar rule-based syntax. We define a model-theoretic semantics of Flix as a natural extension of the Datalog semantics. This semantics captures the declarative meaning of Flix programs without imposing any specific evaluation strategy. An efficient strategy is semi-naive evaluation which we adapt for Flix. We have implemented a compiler and runtime for Flix, and used it to express several well-known static analyses, including the IFDS and IDE algorithms. The declarative nature of Flix clearly exposes the similarity between these two algorithms.}},
  url = {https://doi.org/10.1145/2908080.2908096},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup409,
  title = {Adoption protocols for fanout-optimal fault-tolerant termination detection}},
  author = {Lifflander, Jonathan and Miller, Phil and Kale, Laxmikant}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Termination detection is relevant for signaling completion (all processors are idle and no messages are in flight) of many operations in distributed systems, including work stealing algorithms, dynamic data exchange, and dynamically structured computations. In the face of growing supercomputers with increasing likelihood that each job may encounter faults, it is important for high-performance computing applications that rely on termination detection that such an algorithm be able to tolerate the inevitable faults. We provide a trio of new practical fault tolerance schemes for a standard approach to termination detection that are easy to implement, present low overhead in both theory and practice, and have scalable costs when recovering from faults. These schemes tolerate all single-process faults, and are probabilistically tolerant of faults affecting multiple processes. We combine the theoretical failure probabilities we can calculate for each algorithm with historical fault records from real machines to show that these algorithms have excellent overall survivability.}},
  url = {https://doi.org/10.1145/2442516.2442519},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup410,
  title = {Into the depths of C: elaborating the de facto standards}},
  author = {Memarian, Kayvan and Matthiesen, Justus and Lingard, James and Nienhuis, Kyndylan and Chisnall, David and Watson, Robert N. M. and Sewell, Peter}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {C remains central to our computing infrastructure. It is notionally defined by ISO standards, but in reality the properties of C assumed by systems code and those implemented by compilers have diverged, both from the ISO standards and from each other, and none of these are clearly understood. We make two contributions to help improve this error-prone situation. First, we describe an in-depth analysis of the design space for the semantics of pointers and memory in C as it is used in practice. We articulate many specific questions, build a suite of semantic test cases, gather experimental data from multiple implementations, and survey what C experts believe about the de facto standards. We identify questions where there is a consensus (either following ISO or differing) and where there are conflicts. We apply all this to an experimental C implemented above capability hardware. Second, we describe a formal model, Cerberus, for large parts of C. Cerberus is parameterised on its memory model; it is linkable either with a candidate de facto memory object model, under construction, or with an operational C11 concurrency model; it is defined by elaboration to a much simpler Core language for accessibility, and it is executable as a test oracle on small examples. This should provide a solid basis for discussion of what mainstream C is now: what programmers and analysis tools can assume and what compilers aim to implement. Ultimately we hope it will be a step towards clear, consistent, and accepted semantics for the various use-cases of C.}},
  url = {https://doi.org/10.1145/2908080.2908081},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup411,
  title = {DVM: towards a datacenter-scale virtual machine}},
  author = {Ma, Zhiqiang and Sheng, Zhonghua and Gu, Lin and Wen, Liufei and Zhang, Gong}},
  year = {2012}},
  journal = {Proceedings of the 8th ACM SIGPLAN/SIGOPS Conference on Virtual Execution Environments}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {As cloud-based computation becomes increasingly important, providing a general computational interface to support datacenter-scale programming has become an imperative research agenda. Many cloud systems use existing virtual machine monitor (VMM) technologies, such as Xen, VMware, and Windows Hypervisor, to multiplex a physical host into multiple virtual hosts and isolate computation on the shared cluster platform. However, traditional multiplexing VMMs do not scale beyond one single physical host, and it alone cannot provide the programming interface and cluster-wide computation that a datacenter system requires. We design a new instruction set architecture, DISA, to unify myriads of compute nodes to form a big virtual machine called DVM, and present programmers the view of a single computer where thousands of tasks run concurrently in a large, unified, and snapshotted memory space. The DVM provides a simple yet scalable programming model and mitigates the scalability bottleneck of traditional distributed shared memory systems. Along with an efficient execution engine, the capacity of a DVM can scale up to support large clusters. We have implemented and tested DVM on three platforms, and our evaluation shows that DVM has excellent performance in terms of execution time and speedup. On one physical host, the system overhead of DVM is comparable to that of traditional VMMs. On 16 physical hosts, the DVM runs 10 times faster than MapReduce/Hadoop and X10. On 256 EC2 instances, DVM shows linear speedup on a parallelizable workload.}},
  url = {https://doi.org/10.1145/2151024.2151032},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup412,
  title = {Polymorphism, subtyping, and type inference in MLsub}},
  author = {Dolan, Stephen and Mycroft, Alan}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a type system combining subtyping and ML-style parametric polymorphism. Unlike previous work, our system supports type inference and has compact principal types. We demonstrate this system in the minimal language MLsub, which types a strict superset of core ML programs. This is made possible by keeping a strict separation between the types used to describe inputs and those used to describe outputs, and extending the classical unification algorithm to handle subtyping constraints between these input and output types. Principal types are kept compact by type simplification, which exploits deep connections between subtyping and the algebra of regular languages. An implementation is available online.}},
  url = {https://doi.org/10.1145/3009837.3009882},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup413,
  title = {On-the-fly detection of instability problems in floating-point program execution}},
  author = {Bao, Tao and Zhang, Xiangyu}},
  year = {2013}},
  journal = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The machine representation of floating point values has limited precision such that errors may be introduced during execution. These errors may get propagated and magnified by the following operations, leading to instability problems, e.g., control flow path may be undesirably altered and faulty output may be emitted. In this paper, we develop an on-the-fly efficient monitoring technique that can predict if an execution is stable. The technique does not explicitly compute errors as doing so incurs high overhead. Instead, it detects possible places where an error becomes substantially inflated regarding the corresponding value, and then tags the value with one bit to denote that it has an inflated error. It then tracks inflation bit propagation, taking care of operations that may cut off such propagation. It reports instability if any inflation bit reaches a critical execution point, such as a predicate, where the inflated error may induce substantial execution difference, such as different execution paths. Our experiment shows that with appropriate thresholds, the technique can correctly detect that over 99.999996\% of the inputs of all the programs we studied are stable while a traditional technique relying solely on inflation detection mistakenly classifies majority of the inputs as unstable for some of the programs. Compared to the state of the art technique that is based on high precision computation and causes several hundred times slowdown, our technique only causes 7.91 times slowdown on average and can report all the true unstable executions with the appropriate thresholds.}},
  url = {https://doi.org/10.1145/2509136.2509526},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup414,
  title = {A memory-bounded, deterministic and terminating semantics for the synchronous programming language C\'{e}},
  author = {Santos, Rodrigo C. M. and Lima, Guilherme F. and Sant'Anna, Francisco and Ierusalimschy, Roberto and Haeusler, Edward H.}},
  year = {2018}},
  journal = {Proceedings of the 19th ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {C\'{e}},
  url = {https://doi.org/10.1145/3211332.3211334},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup415,
  title = {Petascale computing with accelerators}},
  author = {Kistler, Michael and Gunnels, John and Brokenshire, Daniel and Benton, Brad}},
  year = {2009}},
  journal = {Proceedings of the 14th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A trend is developing in high performance computing in which commodity processors are coupled to various types of computational accelerators. Such systems are commonly called hybrid systems. In this paper, we describe our experience developing an implementation of the Linpack benchmark for a petascale hybrid system, the LANL Roadrunner cluster built by IBM for Los Alamos National Laboratory. This system combines traditional x86-64 host processors with IBM PowerXCell™" 8i1 accelerator processors. The implementation of Linpack we developed was the first to achieve a performance result in excess of 1.0 PFLOPS, and made Roadrunner the #1 system on the Top500 list in June 2008. We describe the design and implementation of hybrid Linpack, including the special optimizations we developed for this hybrid architecture. We then present actual results for single node and multi-node executions. From this work, we conclude that it is possible to achieve high performance for certain applications on hybrid architectures when careful attention is given to efficient use of memory bandwidth, scheduling of data movement between the host and accelerator memories, and proper distribution of work between the host and accelerator processors.}},
  url = {https://doi.org/10.1145/1504176.1504212},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup416,
  title = {Active learning of points-to specifications}},
  author = {Bastani, Osbert and Sharma, Rahul and Aiken, Alex and Liang, Percy}},
  year = {2018}},
  journal = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {When analyzing programs, large libraries pose significant challenges to static points-to analysis. A popular solution is to have a human analyst provide points-to specifications that summarize relevant behaviors of library code, which can substantially improve precision and handle missing code such as native code. We propose Atlas, a tool that automatically infers points-to specifications. Atlas synthesizes unit tests that exercise the library code, and then infers points-to specifications based on observations from these executions. Atlas automatically infers specifications for the Java standard library, and produces better results for a client static information flow analysis on a benchmark of 46 Android apps compared to using existing handwritten specifications.}},
  url = {https://doi.org/10.1145/3192366.3192383},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup417,
  title = {Monadic abstract interpreters}},
  author = {Sergey, Ilya and Devriese, Dominique and Might, Matthew and Midtgaard, Jan and Darais, David and Clarke, Dave and Piessens, Frank}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Recent developments in the systematic construction of abstract interpreters hinted at the possibility of a broad unification of concepts in static analysis. We deliver that unification by showing context-sensitivity, polyvariance, flow-sensitivity, reachability-pruning, heap-cloning and cardinality-bounding to be independent of any particular semantics. Monads become the unifying agent between these concepts and between semantics. For instance, by plugging the same "context-insensitivity monad" into a monadically-parameterized semantics for Java or for the lambda calculus, it yields the expected context-insensitive analysis.To achieve this unification, we develop a systematic method for transforming a concrete semantics into a monadically-parameterized abstract machine. Changing the monad changes the behavior of the machine. By changing the monad, we recover a spectrum of machines---from the original concrete semantics to a monovariant, flow- and context-insensitive static analysis with a singly-threaded heap and weak updates.The monadic parameterization also suggests an abstraction over the ubiquitous monotone fixed-point computation found in static analysis. This abstraction makes it straightforward to instrument an analysis with high-level strategies for improving precision and performance, such as abstract garbage collection and widening.While the paper itself runs the development for continuation-passing style, our generic implementation replays it for direct-style lambda-calculus and Featherweight Java to support generality.}},
  url = {https://doi.org/10.1145/2491956.2491979},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup418,
  title = {Deriving a probability density calculator (functional pearl)}},
  author = {Mohammed Ismail, Wazim and Shan, Chung-chieh}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Given an expression that denotes a probability distribution, often we want a corresponding density function, to use in probabilistic inference. Fortunately, the task of finding a density has been automated. It turns out that we can derive a compositional procedure for finding a density, by equational reasoning about integrals, starting with the mathematical specification of what a density is. Moreover, the density found can be run as an estimation algorithm, as well as simplified as an exact formula to improve the estimate.}},
  url = {https://doi.org/10.1145/2951913.2951922},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup419,
  title = {FunTAL: reasonably mixing a functional language with assembly}},
  author = {Patterson, Daniel and Perconti, Jamie and Dimoulas, Christos and Ahmed, Amal}},
  year = {2017}},
  journal = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present FunTAL, the first multi-language system to formalize safe interoperability between a high-level functional language and low-level assembly code while supporting compositional reasoning about the mix. A central challenge in developing such a multi-language is bridging the gap between assembly, which is staged into jumps to continuations, and high-level code, where subterms return a result. We present a compositional stack-based typed assembly language that supports components, comprised of one or more basic blocks, that may be embedded in high-level contexts. We also present a logical relation for FunTAL that supports reasoning about equivalence of high-level components and their assembly replacements, mixed-language programs with callbacks between languages, and assembly components comprised of different numbers of basic blocks.}},
  url = {https://doi.org/10.1145/3062341.3062347},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup420,
  title = {From control effects to typed continuation passing}},
  author = {Thielecke, Hayo}},
  year = {2003}},
  journal = {Proceedings of the 30th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {First-class continuations are a powerful computational effect, allowing the programmer to express any form of jumping. Types and effect systems can be used to reason about continuations, both in the source language and in the target language of the continuation-passing transform. In this paper, we establish the connection between an effect system for first-class continuations and typed versions of continuation-passing style. A region in the effect system determines a local answer type for continuations, such that the continuation transforms of pure expressions are parametrically polymorphic in their answer types. We use this polymorphism to derive transforms that make use of effect information, in particular, a mixed linear/non-linear continuation-passing transform, in which expressions without control effects are passed their continuations linearly.}},
  url = {https://doi.org/10.1145/604131.604144},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup421,
  title = {Type theory in type theory using quotient inductive types}},
  author = {Altenkirch, Thorsten and Kaposi, Ambrus}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present an internal formalisation of a type heory with dependent types in Type Theory using a special case of higher inductive types from Homotopy Type Theory which we call quotient inductive types (QITs). Our formalisation of type theory avoids referring to preterms or a typability relation but defines directly well typed objects by an inductive definition. We use the elimination principle to define the set-theoretic and logical predicate interpretation. The work has been formalized using the Agda system extended with QITs using postulates.}},
  url = {https://doi.org/10.1145/2837614.2837638},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup422,
  title = {Which simple types have a unique inhabitant?}},
  author = {Scherer, Gabriel and R\'{e}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We study the question of whether a given type has a unique inhabitant modulo program equivalence. In the setting of simply-typed lambda-calculus with sums, equipped with the strong --equivalence, we show that uniqueness is decidable. We present a saturating focused logic that introduces irreducible cuts on positive types ``as soon as possible''. Backward search in this logic gives an effective algorithm that returns either zero, one or two distinct inhabitants for any given type. Preliminary application studies show that such a feature can be useful in strongly-typed programs, inferring the code of highly-polymorphic library functions, or ``glue code'' inside more complex terms.}},
  url = {https://doi.org/10.1145/2784731.2784757},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup423,
  title = {The reachability-bound problem}},
  author = {Gulwani, Sumit and Zuleger, Florian}},
  year = {2010}},
  journal = {Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We define the reachability-bound problem to be the problem of finding a symbolic worst-case bound on the number of times a given control location inside a procedure is visited in terms of the inputs to that procedure. This has applications in bounding resources consumed by a program such as time, memory, network-traffic, power, as well as estimating quantitative properties (as opposed to boolean properties) of data in programs, such as information leakage or uncertainty propagation. Our approach to solving the reachability-bound problem brings together two different techniques for reasoning about loops in an effective manner. One of these techniques is an abstract-interpretation based iterative technique for computing precise disjunctive invariants (to summarize nested loops). The other technique is a non-iterative proof-rules based technique (for loop bound computation) that takes over the role of doing inductive reasoning, while deriving its power from the use of SMT solvers to reason about abstract loop-free fragments.Our solution to the reachability-bound problem allows us to compute precise symbolic complexity bounds for several loops in .Net base-class libraries for which earlier techniques fail. We also illustrate the precision of our algorithm for disjunctive invariant computation (which has a more general applicability beyond the reachability-bound problem) on a set of benchmark examples.}},
  url = {https://doi.org/10.1145/1806596.1806630},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup424,
  title = {A dynamic program analysis to find floating-point accuracy problems}},
  author = {Benz, Florian and Hildebrandt, Andreas and Hack, Sebastian}},
  year = {2012}},
  journal = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Programs using floating-point arithmetic are prone to accuracy problems caused by rounding and catastrophic cancellation. These phenomena provoke bugs that are notoriously hard to track down: the program does not necessarily crash and the results are not necessarily obviously wrong, but often subtly inaccurate. Further use of these values can lead to catastrophic errors.In this paper, we present a dynamic program analysis that supports the programmer in finding accuracy problems. Our analysis uses binary translation to perform every floating-point computation side by side in higher precision. Furthermore, we use a lightweight slicing approach to track the evolution of errors.We evaluate our analysis by demonstrating that it catches wellknown floating-point accuracy problems and by analyzing the Spec CFP2006 floating-point benchmark. In the latter, we show how our tool tracks down a catastrophic cancellation that causes a complete loss of accuracy leading to a meaningless program result. Finally, we apply our program to a complex, real-world bioinformatics application in which our program detected a serious cancellation. Correcting the instability led not only to improved quality of the result, but also to an improvement of the program's run time.In this paper, we present a dynamic program analysis that supports the programmer in finding accuracy problems. Our analysis uses binary translation to perform every floating-point computation side by side in higher precision. Furthermore, we use a lightweight slicing approach to track the evolution of errors. We evaluate our analysis by demonstrating that it catches wellknown floating-point accuracy problems and by analyzing the SpecfiCFP2006 floating-point benchmark. In the latter, we show how our tool tracks down a catastrophic cancellation that causes a complete loss of accuracy leading to a meaningless program result. Finally, we apply our program to a complex, real-world bioinformatics application in which our program detected a serious cancellation. Correcting the instability led not only to improved quality of the result, but also to an improvement of the program's run time.}},
  url = {https://doi.org/10.1145/2254064.2254118},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup425,
  title = {Giga-scale exhaustive points-to analysis for Java in under a minute}},
  author = {Dietrich, Jens and Hollingum, Nicholas and Scholz, Bernhard}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Computing a precise points-to analysis for very large Java programs remains challenging despite the large body of research on points-to analysis. Any approach must solve an underlying dynamic graph reachability problem, for which the best algorithms have near-cubic worst-case runtime complexity, and, hence, previous work does not scale to programs with millions of lines of code. In this work, we present a novel approach for solving the field-sensitive points-to problem for Java with the means of (1) a transitive-closure data-structure, and (2) a pre-computed set of potentially matching load/store pairs to accelerate the fix-point calculation. Experimentation on Java benchmarks validates the superior performance of our approach over the standard context-free language reachability implementations. Our approach computes a points-to index for the OpenJDK with over 1.5 billion tuples in under a minute.}},
  url = {https://doi.org/10.1145/2814270.2814307},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup426,
  title = {Small bisimulations for reasoning about higher-order imperative programs}},
  author = {Koutavas, Vasileios and Wand, Mitchell}},
  year = {2006}},
  journal = {Conference Record of the 33rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We introduce a new notion of bisimulation for showing contextual equivalence of expressions in an untyped lambda-calculus with an explicit store, and in which all expressed values, including higher-order values, are storable. Our notion of bisimulation leads to smaller and more tractable relations than does the method of Sumii and Pierce [31]. In particular, our method allows one to write down a bisimulation relation directly in cases where [31] requires an inductive specification, and where the principle of local invariants [22] is inapplicable. Our method can also express examples with higher-order functions, in contrast with the most widely known previous methods [4, 22, 32] which are limited in their ability to deal with such examples. The bisimulation conditions are derived by manually extracting proof obligations from a hypothetical direct proof of contextual equivalence.}},
  url = {https://doi.org/10.1145/1111037.1111050},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup427,
  title = {Loop transformations: convexity, pruning and optimization}},
  author = {Pouchet, Louis-No\"{e}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {High-level loop transformations are a key instrument in mapping computational kernels to effectively exploit the resources in modern processor architectures. Nevertheless, selecting required compositions of loop transformations to achieve this remains a significantly challenging task; current compilers may be off by orders of magnitude in performance compared to hand-optimized programs. To address this fundamental challenge, we first present a convex characterization of all distinct, semantics-preserving, multidimensional affine transformations. We then bring together algebraic, algorithmic, and performance analysis results to design a tractable optimization algorithm over this highly expressive space. Our framework has been implemented and validated experimentally on a representative set of benchmarks running on state-of-the-art multi-core platforms.}},
  url = {https://doi.org/10.1145/1926385.1926449},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup428,
  title = {Towards more natural functional programming languages}},
  author = {Myers, Brad A.}},
  year = {2002}},
  journal = {Proceedings of the Seventh ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Programming languages are the way for a person to express a mental plan in a way that the computer can understand. Therefore, it is appropriate to consider properties of people when designing new programming languages. In our research, we are investigating how people think about algorithms, and how programming languages can be made easier to learn and more effective for people to use. By taking human-productivity aspects of programming languages seriously, designers can more effectively match programming language features with human capabilities and problem solving methods. Human factors methods can be used to measure the effects, so unsubstantiated claims can be avoided.This talk will present a quick summary of new and old results in what is known about people and programming, from areas that are sometimes called "empirical studies of programmers" and "psychology of programming." Much is known about what people find difficult, and what syntax and language features are especially tricky and bug-prone. Our new research has discovered how people naturally think about algorithms and data structures, which can help with making programming languages more closely match people's problem solving techniques.}},
  url = {https://doi.org/10.1145/581478.581479},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup429,
  title = {M-LISP: its natural semantics and equational logic}},
  author = {Muller, Robert}},
  year = {1991}},
  journal = {Proceedings of the 1991 ACM SIGPLAN Symposium on Partial Evaluation and Semantics-Based Program Manipulation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {},
  url = {https://doi.org/10.1145/115865.115891},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup430,
  title = {Automatically comparing memory consistency models}},
  author = {Wickerson, John and Batty, Mark and Sorensen, Tyler and Constantinides, George A.}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A memory consistency model (MCM) is the part of a programming language or computer architecture specification that defines which values can legally be read from shared memory locations. Because MCMs take into account various optimisations employed by architectures and compilers, they are often complex and counterintuitive, which makes them challenging to design and to understand. We identify four tasks involved in designing and understanding MCMs: generating conformance tests, distinguishing two MCMs, checking compiler optimisations, and checking compiler mappings. We show that all four tasks are instances of a general constraint-satisfaction problem to which the solution is either a program or a pair of programs. Although this problem is intractable for automatic solvers when phrased over programs directly, we show how to solve analogous constraints over program executions, and then construct programs that satisfy the original constraints. Our technique, which is implemented in the Alloy modelling framework, is illustrated on several software- and architecture-level MCMs, both axiomatically and operationally defined. We automatically recreate several known results, often in a simpler form, including: distinctions between variants of the C11 MCM; a failure of the "SC-DRF guarantee" in an early C11 draft; that x86 is "multi-copy atomic" and Power is not; bugs in common C11 compiler optimisations; and bugs in a compiler mapping from OpenCL to AMD-style GPUs. We also use our technique to develop and validate a new MCM for NVIDIA GPUs that supports a natural mapping from OpenCL.}},
  url = {https://doi.org/10.1145/3009837.3009838},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup431,
  title = {Iterative optimization in the polyhedral model: part ii, multidimensional time}},
  author = {Pouchet, Louis-No\"{e}},
  year = {2008}},
  journal = {Proceedings of the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {High-level loop optimizations are necessary to achieve good performance over a wide variety of processors. Their performance impact can be significant because they involve in-depth program transformations that aim to sustain a balanced workload over the computational, storage, and communication resources of the target architecture. Therefore, it is mandatory that the compiler accurately models the target architecture as well as the effects of complex code restructuring.However, because optimizing compilers (1) use simplistic performance models that abstract away many of the complexities of modern architectures, (2) rely on inaccurate dependence analysis, and (3) lack frameworks to express complex interactions of transformation sequences, they typically uncover only a fraction of the peak performance available on many applications. We propose a complete iterative framework to address these issues. We rely on the polyhedral model to construct and traverse a large and expressive search space. This space encompasses only legal, distinct versions resulting from the restructuring of any static control loop nest. We first propose a feedback-driven iterative heuristic tailored to the search space properties of the polyhedral model. Though, it quickly converges to good solutions for small kernels, larger benchmarks containing higher dimensional spaces are more challenging and our heuristic misses opportunities for significant performance improvement. Thus, we introduce the use of a genetic algorithm with specialized operators that leverage the polyhedral representation of program dependences. We provide experimental evidence that the genetic algorithm effectively traverses huge optimization spaces, achieving good performance improvements on large loop nests.}},
  url = {https://doi.org/10.1145/1375581.1375594},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup432,
  title = {Bisimulation for quantum processes}},
  author = {Feng, Yuan and Duan, Runyao and Ying, Mingsheng}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Quantum cryptographic systems have been commercially available, with a striking advantage over classical systems that their security and ability to detect the presence of eavesdropping are provable based on the principles of quantum mechanics. On the other hand, quantum protocol designers may commit much more faults than classical protocol designers since human intuition is much better adapted to the classical world than the quantum world. To offer formal techniques for modeling and verification of quantum protocols, several quantum extensions of process algebra have been proposed. One of the most serious issues in quantum process algebra is to discover a quantum generalization of the notion of bisimulation, which lies in a central position in process algebra, preserved by parallel composition in the presence of quantum entanglement, which has no counterpart in classical computation. Quite a few versions of bisimulation have been defined for quantum processes in the literature, but in the best case they are only proved to be preserved by parallel composition of purely quantum processes where no classical communications are involved.Many quantum cryptographic protocols, however, employ the LOCC (Local Operations and Classical Communications) scheme, where classical communications must be explicitly specified. So, a notion of bisimulation preserved by parallel composition in the circumstance of both classical and quantum communications is crucial for process algebra approach to verification of quantum cryptographic protocols. In this paper we introduce a novel notion of bisimulation for quantum processes and prove that it is congruent with respect to various process algebra combinators including parallel composition even when both classical and quantum communications are present. We also establish some basic algebraic laws for this bisimulation. In particular, we prove uniqueness of the solutions to recursive equations of quantum processes, which provides a powerful proof technique for verifying complex quantum protocols.}},
  url = {https://doi.org/10.1145/1926385.1926446},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup433,
  title = {Automatic formal verification of MPI-based parallel programs}},
  author = {Siegel, Stephen F. and Zirkel, Timothy K.}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The Toolkit for Accurate Scientific Software (TASS) is a suite of tools for the formal verification of MPI-based parallel programs used in computational science. TASS can verify various safety properties as well as compare two programs for functional equivalence. The TASS front end takes an integer n ≥ 1 and a C/MPI program, and constructs an abstract model of the program with n processes. Procedures, structs, (multi-dimensional) arrays, heap-allocated data, pointers, and pointer arithmetic are all representable in a TASS model. The model is then explored using symbolic execution and explicit state space enumeration. A number of techniques are used to reduce the time and memory consumed. A variety of realistic MPI programs have been verified with TASS, including Jacobi iteration and manager-worker type programs, and some subtle defects have been discovered. TASS is written in Java and is available from http://vsl.cis.udel.edu/tass under the Gnu Public License.}},
  url = {https://doi.org/10.1145/1941553.1941603},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup434,
  title = {Security-typed programming within dependently typed programming}},
  author = {Morgenstern, Jamie and Licata, Daniel R.}},
  year = {2010}},
  journal = {Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Several recent security-typed programming languages, such as Aura, PCML5, and Fine, allow programmers to express and enforce access control and information flow policies. In this paper, we show that security-typed programming can be embedded as a library within a general-purpose dependently typed programming language, Agda. Our library, Aglet, accounts for the major features of existing security-typed programming languages, such as decentralized access control, typed proof-carrying authorization, ephemeral and dynamic policies, authentication, spatial distribution, and information flow. The implementation of Aglet consists of the following ingredients: First, we represent the syntax and proofs of an authorization logic, Garg and Pfenning's BL0, using dependent types. Second, we implement a proof search procedure, based on a focused sequent calculus, to ease the burden of constructing proofs. Third, we represent computations using a monad indexed by pre- and post-conditions drawn from the authorization logic, which permits ephemeral policies that change during execution. We describe the implementation of our library and illustrate its use on a number of the benchmark examples considered in the literature.}},
  url = {https://doi.org/10.1145/1863543.1863569},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup435,
  title = {Back to the future: time travel in FRP}},
  author = {Perez, Ivan}},
  year = {2017}},
  journal = {Proceedings of the 10th ACM SIGPLAN International Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Functional Reactive Programming (FRP) allows interactive applications to be modelled in a declarative manner using time-varying values. For practical reasons, however, operational constraints are often imposed, such as having a fixed time domain, time always flowing forward, and limiting the exploration of the past. In this paper we show how these constraints can be overcome, giving local control over the time domain, the direction of time and the sampling step. We study the behaviour of FRP expressions when time flows backwards, and demonstrate how to synchronize subsystems running asynchronously and at different sampling rates. We have verified the practicality of our approach with two non-trivial games in which time control is central to the gameplay.}},
  url = {https://doi.org/10.1145/3122955.3122957},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup436,
  title = {Ynot: dependent types for imperative programs}},
  author = {Nanevski, Aleksandar and Morrisett, Greg and Shinnar, Avraham and Govereau, Paul and Birkedal, Lars}},
  year = {2008}},
  journal = {Proceedings of the 13th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We describe an axiomatic extension to the Coq proof assistant, that supports writing, reasoning about, and extracting higher-order, dependently-typed programs with side-effects. Coq already includes a powerful functional language that supports dependent types, but that language is limited to pure, total functions. The key contribution of our extension, which we call Ynot, is the added support for computations that may have effects such as non-termination, accessing a mutable store, and throwing/catching exceptions.The axioms of Ynot form a small trusted computing base which has been formally justified in our previous work on Hoare Type Theory (HTT). We show how these axioms can be combined with the powerful type and abstraction mechanisms of Coq to build higher-level reasoning mechanisms which in turn can be used to build realistic, verified software components. To substantiate this claim, we describe here a representative series of modules that implement imperative finite maps, including support for a higher-order (effectful) iterator. The implementations range from simple (e.g., association lists) to complex (e.g., hash tables) but share a common interface which abstracts the implementation details and ensures that the modules properly implement the finite map abstraction.}},
  url = {https://doi.org/10.1145/1411204.1411237},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup437,
  title = {A relational model of types-and-effects in higher-order concurrent separation logic}},
  author = {Krogh-Jespersen, Morten and Svendsen, Kasper and Birkedal, Lars}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Recently we have seen a renewed interest in programming languages that tame the complexity of state and concurrency through refined type systems with more fine-grained control over effects. In addition to simplifying reasoning and eliminating whole classes of bugs, statically tracking effects opens the door to advanced compiler optimizations. In this paper we present a relational model of a type-and-effect system for a higher-order, concurrent program- ming language. The model precisely captures the semantic invariants expressed by the effect annotations. We demonstrate that these invariants are strong enough to prove advanced program transformations, including automatic parallelization of expressions with suitably disjoint effects. The model also supports refinement proofs between abstract data types implementations with different internal data representations, including proofs that fine-grained concurrent algorithms refine their coarse-grained counterparts. This is the first model for such an expressive language that supports both effect-based optimizations and data abstraction. The logical relation is defined in Iris, a state-of-the-art higher-order concurrent separation logic. This greatly simplifies proving well-definedness of the logical relation and also provides us with a powerful logic for reasoning in the model.}},
  url = {https://doi.org/10.1145/3009837.3009877},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup438,
  title = {Ghosts of departed proofs (functional pearl)}},
  author = {Noonan, Matt}},
  year = {2018}},
  journal = {Proceedings of the 11th ACM SIGPLAN International Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Library authors often are faced with a design choice: should a function with preconditions be implemented as a partial function, or by returning a failure condition on incorrect use? Neither option is ideal. Partial functions lead to frustrating run-time errors. Failure conditions must be checked at the use-site, placing an unfair tax on the users who have ensured that the function's preconditions were correctly met. In this paper, we introduce an API design concept called ``ghosts of departed proofs'' based on the following observation: sophisticated preconditions can be encoded in Haskell's type system with no run-time overhead, by using proofs that inhabit phantom type parameters attached to newtype wrappers. The user expresses correctness arguments by constructing proofs to inhabit these phantom types. Critically, this technique allows the library user to decide when and how to validate that the API's preconditions are met. The ``ghosts of departed proofs'' approach to API design can achieve many of the benefits of dependent types and refinement types, yet only requires some minor and well-understood extensions to Haskell 2010. We demonstrate the utility of this approach through a series of case studies, showing how to enforce novel invariants for lists, maps, graphs, shared memory regions, and more.}},
  url = {https://doi.org/10.1145/3242744.3242755},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup439,
  title = {Continuations and transducer composition}},
  author = {Shivers, Olin and Might, Matthew}},
  year = {2006}},
  journal = {Proceedings of the 27th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {On-line transducers are an important class of computational agent; we construct and compose together many software systems using them, such as stream processors, layered network protocols, DSP networks and graphics pipelines. We show an interesting use of continuations, that, when taken in a CPS setting, exposes the control flow of these systems. This enables a CPS-based compiler to optimise systems composed of these transducers, using only standard, known analyses and optimisations. Critically, the analysis permits optimisation across the composition of these transducers, allowing efficient construction of systems in a hierarchical way.}},
  url = {https://doi.org/10.1145/1133981.1134016},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup440,
  title = {AsyncClock: Scalable Inference of Asynchronous Event Causality}},
  author = {Hsiao, Chun-Hung and Narayanasamy, Satish and Khan, Essam Muhammad Idris and Pereira, Cristiano L. and Pokam, Gilles A.}},
  year = {2017}},
  journal = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Asynchronous programming model is commonly used in mobile systems and Web 2.0 environments. Asynchronous race detectors use algorithms that are an order of magnitude performance and space inefficient compared to conventional data race detectors. We solve this problem by identifying and addressing two important problems in reasoning about causality between asynchronous events.Unlike conventional signal-wait operations, establishing causal order between two asynchronous events is fundamentally more challenging as there is no common handle they operate on. We propose a new primitive named AsyncClock that addresses this problem by explicitly tracking causally preceding events, and show that AsyncClock can handle a wide variety of asynchronous causality models. We also address the important scalability problem of efficiently identifying heirless events whose metadata can be reclaimed.We built the first single-pass, non-graph-based Android race detector using our algorithm and applied it to find errors in 20 popular applications. Our tool incurs about 6x performance overhead, which is several times more efficient than the state-of-the-art solution. It also scales well with the execution length. We used our tool to find 147 previously unknown harmful races.}},
  url = {https://doi.org/10.1145/3037697.3037712},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup441,
  title = {Compositional recurrence analysis revisited}},
  author = {Kincaid, Zachary and Breck, Jason and Boroujeni, Ashkan Forouhi and Reps, Thomas}},
  year = {2017}},
  journal = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Compositional recurrence analysis (CRA) is a static-analysis method based on a combination of symbolic analysis and abstract interpretation. This paper addresses the problem of creating a context-sensitive interprocedural version of CRA that handles recursive procedures. The problem is non-trivial because there is an "impedance mismatch" between CRA, which relies on analysis techniques based on regular languages (i.e., Tarjan's path-expression method), and the context-free-language underpinnings of context-sensitive analysis. We show how to address this impedance mismatch by augmenting the CRA abstract domain with additional operations. We call the resulting algorithm Interprocedural CRA (ICRA). Our experiments with ICRA show that it has broad overall strength compared with several state-of-the-art software model checkers.}},
  url = {https://doi.org/10.1145/3062341.3062373},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup442,
  title = {Optimistic Hybrid Analysis: Accelerating Dynamic Analysis through Predicated Static Analysis}},
  author = {Devecsery, David and Chen, Peter M. and Flinn, Jason and Narayanasamy, Satish}},
  year = {2018}},
  journal = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Dynamic analysis tools, such as those that detect data-races, verify memory safety, and identify information flow, have become a vital part of testing and debugging complex software systems. While these tools are powerful, their slow speed often limits how effectively they can be deployed in practice. Hybrid analysis speeds up these tools by using static analysis to decrease the work performed during dynamic analysis. In this paper we argue that current hybrid analysis is needlessly hampered by an incorrect assumption that preserving the soundness of dynamic analysis requires an underlying sound static analysis. We observe that, even with unsound static analysis, it is possible to achieve sound dynamic analysis for the executions which fall within the set of states statically considered. This leads us to a new approach, called optimistic hybrid analysis. We first profile a small set of executions and generate a set of likely invariants that hold true during most, but not necessarily all, executions. Next, we apply a much more precise, but unsound, static analysis that assumes these invariants hold true. Finally, we run the resulting dynamic analysis speculatively while verifying whether the assumed invariants hold true during that particular execution; if not, the program is reexecuted with a traditional hybrid analysis. Optimistic hybrid analysis is as precise and sound as traditional dynamic analysis, but is typically much faster because (1) unsound static analysis can speed up dynamic analysis much more than sound static analysis can and (2) verifications rarely fail. We apply optimistic hybrid analysis to race detection and program slicing and achieve 1.8x over a state-of-the-art race detector (FastTrack) optimized with traditional hybrid analysis and 8.3x over a hybrid backward slicer (Giri).}},
  url = {https://doi.org/10.1145/3173162.3177153},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup443,
  title = {Mixed-size concurrency: ARM, POWER, C/C++11, and SC}},
  author = {Flur, Shaked and Sarkar, Susmit and Pulte, Christopher and Nienhuis, Kyndylan and Maranget, Luc and Gray, Kathryn E. and Sezgin, Ali and Batty, Mark and Sewell, Peter}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Previous work on the semantics of relaxed shared-memory concurrency has only considered the case in which each load reads the data of exactly one store. In practice, however, multiprocessors support mixed-size accesses, and these are used by systems software and (to some degree) exposed at the C/C++ language level. A semantic foundation for software, therefore, has to address them. We investigate the mixed-size behaviour of ARMv8 and IBM POWER architectures and implementations: by experiment, by developing semantic models, by testing the correspondence between these, and by discussion with ARM and IBM staff. This turns out to be surprisingly subtle, and on the way we have to revisit the fundamental concepts of coherence and sequential consistency, which change in this setting. In particular, we show that adding a memory barrier between each instruction does not restore sequential consistency. We go on to extend the C/C++11 model to support non-atomic mixed-size memory accesses. This is a necessary step towards semantics for real-world shared-memory concurrent code, beyond litmus tests.}},
  url = {https://doi.org/10.1145/3009837.3009839},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup444,
  title = {MITRE's future generation computer architectures program}},
  author = {Bensley, E. H. and Brando, T. J. and Fohlin, J. C. and Prelle, M. J. and Wollrath, A. M.}},
  year = {1988}},
  journal = {Proceedings of the 1988 ACM SIGPLAN Workshop on Object-Based Concurrent Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {MITRE — through its Future Generation Computer Architectures program — has conducted research in parallel computing since 1983 [2-5.7]. Our research is currently directed toward operating systems for massive distributed-memory MIMDs running general-purpose, object-oriented programs. Scalability and reliability are central concerns in our research. To us, scalability means that a system can be expanded incrementally, and the addition of processors always increases the processing power of the system. Reliability means that application programs continue to run, and run correctly, in spite of isolated hardware failures.For our research, we assume a message-passing system with no shared memory and no broadcast facility. We also assume the network changes with time because processors fail and because processors are added to the running system. The system must be able to recognize failed processors and avoid them. It must also be able to recognize new processors and make them useful members of the working community while the system is running. The only kind of failure we consider is catastrophic processor failure. We assume that conventional error-detecting and correcting techniques are used to ensure that processors that function do so in a fault-free manner and that communication between processors is reliable.Our research has taught us that the constraints imposed by a massive message-passing architecture, with even the narrow fault tolerance goals we have described, demand a view that emphasizes control at the local level, coordination at the global level, and the ability to tolerate inexact information. The foremost lesson we have learned about such systems is that they are nonintuitive: algorithms that work well with few processors may not scale well to systems with many processors — in general, you must do things differently to keep overhead from overwhelming the system. We have observed algorithms perform well in simulated systems of 256 processors and break down in systems of 1024. (We look forward to seeing what happens when we can simulate systems of substantially greater size.) Another lesson we have learned is that the cost of implementing a fault-tolerant algorithm can be very high and depends a great deal on the nature of the algorithm — the degree to which it employs local control and can tolerate inexact information.Our model of computation is object-oriented programming; for us, objects represent the independent computational units that can enable parallelism within a program. We have not yet committed to a particular object-oriented language; instead, we have concentrated on operating system issues we feel are common to supporting many distributed object-oriented programming systems. Our research specifically concerns distributed techniques for resource management, object addressing, garbage collection, and computation management. We constrain ourselves to techniques that do not require centralized control or global information, and can be made tolerant of hardware failures.We expect that objects are dynamically created and discarded as a program executes. We think of objects as representing units of work assigned to processors. Memory management must provide a means of finding processors with sufficient free memory to store new objects. Processor management must provide a means of dynamically balancing the load on processors by distributing objects in a relatively equitable manner.We have developed a resource management strategy that meets these requirements. One feature of the scheme is that objects are distributed among processors in an equitable manner when they are created; another feature is that objects are redistributed among processors dynamically to maintain a relatively equitable distribution.The scheme is based on the use of resource agents — operating system servers distributed throughout the system. Each agent manages memory allocation for the processors in its local communication neighborhood. A processor sends allocation requests to its local agent; that agent assigns each request to whichever processor in its neighborhood it deems most appropriate. If there is none, the agent forwards the request to a superagent — another operating system server that overseas activity in several agents' neighborhoods. If the superagent has another Time Warp [6] to ensure that messages are processed in the correct order by each object. Time Warp was proposed to synchronize the execution of discrete-event simulations on multiprocessors. While our approach is based on Time Warp, it extends the mechanism to facilitate general-purpose programming.In Time Warp, an object processes messages as they arrive, but before processing each message, it saves its state. If a message arrives with a simulation time earlier than a message already processed, the object rolls back to a state at or before the time of the new message, processes that message at the correct simulation time, and reprocesses messages over which it rolled back.There are two kinds of messages that can be sent in Time Warp — event messages and query messages. A query message cannot cause side effects, and always returns a reply message to the sender. An event message can cause side effects, and never returns a reply message. In general-purpose computation, it is common for a method to send a message and use the result that is returned. If that message is an event message, Time Warp forces the programmer to actually write two methods — the event message is sent in the first and the result is used in the second. A second event message must be introduced to signal the availability of the result and trigger the execution of the second method. It is up to the programmer to make sure that the second event message is processed correctly if other messages can be received before it.These restrictions may be natural in the context of simulations of real-world situations; however, they force a programmer to structure general-purpose programs in an unnatural way that is by no means trivial.Time Warp places another restriction on the programmer. A cycle of recursive query messages can be processed at the same simulation time; however, a cycle of recursive event messages cannot. The purpose of this restriction is to avoid Time Warp's equivalent of deadlock — infinite rollback. However, it makes side-effecting recursion difficult to accomplish. It requires the programmer to manage the timing of events so that no intervening messages are processed while the recursion is in progress. This is not an easy task, since it may be hard to predict (until execution time) the depth of a recursion and, hence, the number of messages involved.In our model, a sending object can use the result of a side-effecting message it sent later in the same method, and side-effecting recursion is fully supported. There are two reasons for this. First, our model of execution controls time in a manner that is completely transparent to the programmer. Our computation time dynamically and automatically attains as fine a granularity as is necessary to support replies from side-effecting messages and side-effecting recursion. Second, our model allows method execution to be rolled back so that side-effecting recursion can be handled correctly.We currently have code for a computation manager that embodies our model of execution and runs compiled programs on a multiprocessor simulator. The compiler takes object-oriented programs written in a subset of Common Lisp using Flavors and produces programs in which each object class is combined with an executive class that implements the operations of the model of execution. The compiler generates compiled methods in which each pseudo-instruction is a piece of Lisp code; method execution consists of stepping through these instructions, saving state information as appropriate.Our next goal is to implement a fault-tolerant version of our computation manager on a multiprocessor. Our objective this year was to design a model of execution for distributed, general-purpose, object-oriented computation to support concurrency in a manner transparent to the programmer. The mechanisms we have described involve considerable overhead; a major goal for the future is to develop techniques for reducing overhead sufficiently to make this approach practical.We also plan to add computation management to an existing simulation that includes fault-tolerant resource management as described above. We will also implement one of the object-addressing schemes and one of the garbage-collection schemes we have described. The integrated simulation will be used to observe the functioning of the operating system as a whole and will itself be implemented on a multiprocessor system.}},
  url = {https://doi.org/10.1145/67386.67412},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup445,
  title = {Hindley-milner elaboration in applicative style: functional pearl}},
  author = {Pottier, Fran\c{c}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Type inference - the problem of determining whether a program is well-typed - is well-understood. In contrast, elaboration - the task of constructing an explicitly-typed representation of the program - seems to have received relatively little attention, even though, in a non-local type inference system, it is non-trivial. We show that the constraint-based presentation of Hindley-Milner type inference can be extended to deal with elaboration, while preserving its elegance. This involves introducing a new notion of "constraint with a value", which forms an applicative functor.}},
  url = {https://doi.org/10.1145/2628136.2628145},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup446,
  title = {Contract-based resource verification for higher-order functions with memoization}},
  author = {Madhavan, Ravichandhran and Kulal, Sumith and Kuncak, Viktor}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a new approach for specifying and verifying resource utilization of higher-order functional programs that use lazy evaluation and memoization. In our approach, users can specify the desired resource bound as templates with numerical holes e.g. as steps ≤ ? * size(l) + ? in the contracts of functions. They can also express invariants necessary for establishing the bounds that may depend on the state of memoization. Our approach operates in two phases: first generating an instrumented first-order program that accurately models the higher-order control flow and the effects of memoization on resources using sets, algebraic datatypes and mutual recursion, and then verifying the contracts of the first-order program by producing verification conditions of the form ∃ ∀ using an extended assume/guarantee reasoning. We use our approach to verify precise bounds on resources such as evaluation steps and number of heap-allocated objects on 17 challenging data structures and algorithms. Our benchmarks, comprising of 5K lines of functional Scala code, include lazy mergesort, Okasaki's real-time queue and deque data structures that rely on aliasing of references to first-class functions; lazy data structures based on numerical representations such as the conqueue data structure of Scala's data-parallel library, cyclic streams, as well as dynamic programming algorithms such as knapsack and Viterbi. Our evaluations show that when averaged over all benchmarks the actual runtime resource consumption is 80\% of the value inferred by our tool when estimating the number of evaluation steps, and is 88\% for the number of heap-allocated objects.}},
  url = {https://doi.org/10.1145/3009837.3009874},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup447,
  title = {Data-parallel finite-state machines}},
  author = {Mytkowicz, Todd and Musuvathi, Madanlal and Schulte, Wolfram}},
  year = {2014}},
  journal = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A finite-state machine (FSM) is an important abstraction for solving several problems, including regular-expression matching, tokenizing text, and Huffman decoding. FSM computations typically involve data-dependent iterations with unpredictable memory-access patterns making them difficult to parallelize. This paper describes a parallel algorithm for FSMs that breaks dependences across iterations by efficiently enumerating transitions from all possible states on each input symbol. This allows the algorithm to utilize various sources of data parallelism available on modern hardware, including vector instructions and multiple processors/cores. For instance, on benchmarks from three FSM applications: regular expressions, Huffman decoding, and HTML tokenization, the parallel algorithm achieves up to a 3x speedup over optimized sequential baselines on a single core, and linear speedups up to 21x on 8 cores.}},
  url = {https://doi.org/10.1145/2541940.2541988},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup448,
  title = {Fast and loose reasoning is morally correct}},
  author = {Danielsson, Nils Anders and Hughes, John and Jansson, Patrik and Gibbons, Jeremy}},
  year = {2006}},
  journal = {Conference Record of the 33rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Functional programmers often reason about programs as if they were written in a total language, expecting the results to carry over to non-total (partial) languages. We justify such reasoning.Two languages are defined, one total and one partial, with identical syntax. The semantics of the partial language includes partial and infinite values, and all types are lifted, including the function spaces. A partial equivalence relation (PER) is then defined, the domain of which is the total subset of the partial language. For types not containing function spaces the PER relates equal values, and functions are related if they map related values to related values.It is proved that if two closed terms have the same semantics in the total language, then they have related semantics in the partial language. It is also shown that the PER gives rise to a bicartesian closed category which can be used to reason about values in the domain of the relation.}},
  url = {https://doi.org/10.1145/1111037.1111056},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup449,
  title = {Bamboo: a data-centric, object-oriented approach to many-core software}},
  author = {Zhou, Jin and Demsky, Brian}},
  year = {2010}},
  journal = {Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Traditional data-oriented programming languages such as dataflow languages and stream languages provide a natural abstraction for parallel programming. In these languages, a developer focuses on the flow of data through the computation and these systems free the developer from the complexities of low-level, thread-oriented concurrency primitives. This simplification comes at a cost --- traditional data-oriented approaches restrict the mutation of state and, in practice, the types of data structures a program can effectively use. Bamboo borrows from work in typestate and software transactions to relax the traditional restrictions of data-oriented programming models to support mutation of arbitrary data structures.We have implemented a compiler for Bamboo which generates code for the TILEPro64 many-core processor. We have evaluated this implementation on six benchmarks: Tracking, a feature tracking algorithm from computer vision; KMeans, a K-means clustering algorithm; MonteCarlo, a Monte Carlo simulation; FilterBank, a multi-channel filter bank; Fractal, a Mandelbrot set computation; and Series, a Fourier series computation. We found that our compiler generated implementations that obtained speedups ranging from 26.2x to 61.6x when executed on 62 cores.}},
  url = {https://doi.org/10.1145/1806596.1806640},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup450,
  title = {Lattice-theoretic progress measures and coalgebraic model checking}},
  author = {Hasuo, Ichiro and Shimizu, Shunsuke and C\^{\i}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In the context of formal verification in general and model checking in particular, parity games serve as a mighty vehicle: many problems are encoded as parity games, which are then solved by the seminal algorithm by Jurdzinski. In this paper we identify the essence of this workflow to be the notion of progress measure, and formalize it in general, possibly infinitary, lattice-theoretic terms. Our view on progress measures is that they are to nested/alternating fixed points what invariants are to safety/greatest fixed points, and what ranking functions are to liveness/least fixed points. That is, progress measures are combination of the latter two notions (invariant and ranking function) that have been extensively studied in the context of (program) verification. We then apply our theory of progress measures to a general model-checking framework, where systems are categorically presented as coalgebras. The framework's theoretical robustness is witnessed by a smooth transfer from the branching-time setting to the linear-time one. Although the framework can be used to derive some decision procedures for finite settings, we also expect the proposed framework to form a basis for sound proof methods for some undecidable/infinitary problems.}},
  url = {https://doi.org/10.1145/2837614.2837673},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup451,
  title = {A model and an implementation of a logic programming environment}},
  author = {Komorowski, Henryk Jan and Omori, Shigeo}},
  year = {1985}},
  journal = {Proceedings of the ACM SIGPLAN 85 Symposium on Language Issues in Programming Environments}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {It has been claimed that logic programming offers outstanding possibilities for new concepts in programming environments. But with the exceptions of the pioneering work of Shapiro on algorithmic debugging, Pereira's rational debugging and early work on expert systems from Imperial College, there has been little progress reported in the field of logic programming environments. This summary describes our current work on a generic software engineering shell for logic programming. We use reflection and the amalgamation of meta-level language with the object level to express and support the incremental character of specifying/programming. An important facet of the shell is that we formalize some aspects of programming methodology and provide heuristics for avoiding errors. These heuristics formalize what experienced programmers may already know.The shell bears similarities to an expert system since it has explanation mechanisms and provides programming-knowledge acquisition. Currently, it supports single user Prolog programming and runs in C-Prolog. The shell is generic in that it provides support for activities ranging from artificial intelligence programming to formal specification development.This research has been supported in part by the IBM Young Faculty Development Award and in part by the NSF grant # MCS-84-05079.}},
  url = {https://doi.org/10.1145/800225.806840},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup452,
  title = {A simple proof technique for certain parametricity results}},
  author = {Crary, Karl}},
  year = {1999}},
  journal = {Proceedings of the Fourth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many properties of parametric, polymorphic functions can be determined simply by inspection of their types. Such results are usually proven using Reynolds's parametricity theorem. However, Reynolds's theorem can be difficult to show in some settings, particularly ones involving computational effects. I present an alternative technique for proving some parametricity results. This technique is considerably simpler and easily generalizes to effectful settings. It works by instantiating polymorphic functions with singleton types that fully specify the behavior of the functions. Using this technique, I show that callers' stacks are protected from corruption during function calls in Typed Assembly Language programs.}},
  url = {https://doi.org/10.1145/317636.317787},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup453,
  title = {From principles to programming languages (and back)}},
  author = {Krishnamurthi, Shriram}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {},
  url = {https://doi.org/10.1145/2429069.2429097},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup454,
  title = {Accurate data redistribution cost estimation in software distributed shared memory systems}},
  author = {Morris, Donald G. and Lowenthal, David K.}},
  year = {2001}},
  journal = {Proceedings of the Eighth ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Distributing data is one of the key problems in implementing efficient distributed-memory parallel programs. The problem becomes more difficult in programs where data redistribution between computational phases is considered. The global data distribution problem is to find the optimal distribution in multi-phase parallel programs. Solving this problem requires accurate knowledge of data redistribution cost.We are investigating this problem in the context of a software distributed shared memory (SDSM) system, in which obtaining accurate redistribution cost estimates is difficult. This is because SDSM communication is implicit: It depends on access patterns, page locations, and the SDSM consistency protocol.We have developed integrated compile- and run-time analysis for SDSM systems to determine accurate redistribution cost estimates with low overhead. Our resulting system, SUIF-Adapt, can efficiently and accurately estimate execution time, including redistribution, to within 5\% of the actual time in all of our test cases and is often much closer. These precise costs enable SUIF-Adapt to find efficient global data distributions in multiple-phase programs.}},
  url = {https://doi.org/10.1145/379539.379570},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup455,
  title = {Functional morphology}},
  author = {Forsberg, Markus and Ranta, Aarne}},
  year = {2004}},
  journal = {Proceedings of the Ninth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents a methodology for implementing natural language morphology in the functional language Haskell. The main idea behind is simple: instead of working with untyped regular expressions, which is the state of the art of morphology in computational linguistics, we use finite functions over hereditarily finite algebraic datatypes. The definitions of these datatypes and functions are the language-dependent part of the morphology. The language-independent part consists of an untyped dictionary format which is used for synthesis of word forms, and a decorated trie, which is used for analysis.Functional Morphology builds on ideas introduced by Huet in his computational linguistics toolkit Zen, which he has used to implement the morphology of Sanskrit. The goal has been to make it easy for linguists, who are not trained as functional programmers, to apply the ideas to new languages. As a proof of the productivity of the method, morphologies for Swedish, Italian, Russian, Spanish, and Latin have already been implemented using the library. The Latin morphology is used as a running example in this article.}},
  url = {https://doi.org/10.1145/1016850.1016879},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup456,
  title = {Declarative visitors to ease fine-grained source code mining with full history on billions of AST nodes}},
  author = {Dyer, Robert and Rajan, Hridesh and Nguyen, Tien N.}},
  year = {2013}},
  journal = {Proceedings of the 12th International Conference on Generative Programming: Concepts \&amp; Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Software repositories contain a vast wealth of information about software development. Mining these repositories has proven useful for detecting patterns in software development, testing hypotheses for new software engineering approaches, etc. Specifically, mining source code has yielded significant insights into software development artifacts and processes. Unfortunately, mining source code at a large-scale remains a difficult task. Previous approaches had to either limit the scope of the projects studied, limit the scope of the mining task to be more coarse-grained, or sacrifice studying the history of the code due to both human and computational scalability issues. In this paper we address the substantial challenges of mining source code: a) at a very large scale; b) at a fine-grained level of detail; and c) with full history information.To address these challenges, we present domain-specific language features for source code mining. Our language features are inspired by object-oriented visitors and provide a default depth-first traversal strategy along with two expressions for defining custom traversals. We provide an implementation of these features in the Boa infrastructure for software repository mining and describe a code generation strategy into Java code. To show the usability of our domain-specific language features, we reproduced over 40 source code mining tasks from two large-scale previous studies in just 2 person-weeks. The resulting code for these tasks show between 2.0x--4.8x reduction in code size. Finally we perform a small controlled experiment to gain insights into how easily mining tasks written using our language features can be understood, with no prior training. We show a substantial number of tasks (77\%) were understood by study participants, in about 3 minutes per task.}},
  url = {https://doi.org/10.1145/2517208.2517226},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup457,
  title = {Moonwalk: NRE Optimization in ASIC Clouds}},
  author = {Khazraee, Moein and Zhang, Lu and Vega, Luis and Taylor, Michael Bedford}},
  year = {2017}},
  journal = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Cloud services are becoming increasingly globalized and data-center workloads are expanding exponentially. GPU and FPGA-based clouds have illustrated improvements in power and performance by accelerating compute-intensive workloads. ASIC-based clouds are a promising way to optimize the Total Cost of Ownership (TCO) of a given datacenter computation (e.g. YouTube transcoding) by reducing both energy consumption and marginal computation cost.The feasibility of an ASIC Cloud for a particular application is directly gated by the ability to manage the Non-Recurring Engineering (NRE) costs of designing and fabricating the ASIC, so that it is significantly lower (e.g. 2X) than the TCO of the best available alternative.In this paper, we show that technology node selection is a major tool for managing ASIC Cloud NRE, and allows the designer to trade off an accelerator's excess energy efficiency and cost performance for lower total cost.We explore NRE and cross-technology optimization of ASIC Clouds for four different applications: Bitcoin mining, YouTube-style video transcoding, Litecoin, and Deep Learning. We address these challenges and show large reductions in the NRE, potentially enabling ASIC Clouds to address a wider variety of datacenter workloads. Our results suggest that advanced nodes like 16nm will lead to sub-optimal TCO for many workloads, and that use of older nodes like 65nm can enable a greater diversity of ASIC Clouds.}},
  url = {https://doi.org/10.1145/3037697.3037749},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup458,
  title = {Mobile values, new names, and secure communication}},
  author = {Abadi, Mart\'{\i}},
  year = {2001}},
  journal = {Proceedings of the 28th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We study the interaction of the "new" construct with a rich but common form of (first-order) communication. This interaction is crucial in security protocols, which are the main motivating examples for our work; it also appears in other programming-language contexts. Specifically, we introduce a simple, general extension of the pi calculus with value passing, primitive functions, and equations among terms. We develop semantics and proof techniques for this extended language and apply them in reasoning about some security protocols.}},
  url = {https://doi.org/10.1145/360204.360213},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup459,
  title = {Morph algorithms on GPUs}},
  author = {Nasre, Rupesh and Burtscher, Martin and Pingali, Keshav}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {There is growing interest in using GPUs to accelerate graph algorithms such as breadth-first search, computing page-ranks, and finding shortest paths. However, these algorithms do not modify the graph structure, so their implementation is relatively easy compared to general graph algorithms like mesh generation and refinement, which morph the underlying graph in non-trivial ways by adding and removing nodes and edges. We know relatively little about how to implement morph algorithms efficiently on GPUs.In this paper, we present and study four morph algorithms: (i) a computational geometry algorithm called Delaunay Mesh Refinement (DMR), (ii) an approximate SAT solver called Survey Propagation (SP), (iii) a compiler analysis called Points-To Analysis (PTA), and (iv) Boruvka's Minimum Spanning Tree algorithm (MST). Each of these algorithms modifies the graph data structure in different ways and thus poses interesting challenges.We overcome these challenges using algorithmic and GPU-specific optimizations. We propose efficient techniques to perform concurrent subgraph addition, subgraph deletion, conflict detection and several optimizations to improve the scalability of morph algorithms. For an input mesh with 10 million triangles, our DMR code achieves an 80x speedup over the highly optimized serial Triangle program and a 2.3x speedup over a multicore implementation running with 48 threads. Our SP code is 3x faster than a multicore implementation with 48 threads on an input with 1 million literals. The PTA implementation is able to analyze six SPEC 2000 benchmark programs in just 74 milliseconds, achieving a geometric mean speedup of 9.3x over a 48-thread multicore version. Our MST code is slower than a multicore version with 48 threads for sparse graphs but significantly faster for denser graphs.This work provides several insights into how other morph algorithms can be efficiently implemented on GPUs.}},
  url = {https://doi.org/10.1145/2442516.2442531},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup460,
  title = {Synthesis modulo recursive functions}},
  author = {Kneuss, Etienne and Kuraj, Ivan and Kuncak, Viktor and Suter, Philippe}},
  year = {2013}},
  journal = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We describe techniques for synthesis and verification of recursive functional programs over unbounded domains. Our techniques build on top of an algorithm for satisfiability modulo recursive functions, a framework for deductive synthesis, and complete synthesis procedures for algebraic data types. We present new counterexample-guided algorithms for constructing verified programs. We have implemented these algorithms in an integrated environment for interactive verification and synthesis from relational specifications. Our system was able to synthesize a number of useful recursive functions that manipulate unbounded numbers and data structures.}},
  url = {https://doi.org/10.1145/2509136.2509555},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup461,
  title = {Proofs that count}},
  author = {Farzan, Azadeh and Kincaid, Zachary and Podelski, Andreas}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Counting arguments are among the most basic proof methods in mathematics. Within the field of formal verification, they are useful for reasoning about programs with infinite control, such as programs with an unbounded number of threads, or (concurrent) programs with recursive procedures. While counting arguments are common in informal, hand-written proofs of such programs, there are no fully automated techniques to construct counting arguments. The key questions involved in automating counting arguments are: how to decide what should be counted?, and how to decide when a counting argument is valid? In this paper, we present a technique for automatically constructing and checking counting arguments, which includes novel solutions to these questions.}},
  url = {https://doi.org/10.1145/2535838.2535885},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup462,
  title = {Software data spreading: leveraging distributed caches to improve single thread performance}},
  author = {Kamruzzaman, Md and Swanson, Steven and Tullsen, Dean M.}},
  year = {2010}},
  journal = {Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Single thread performance remains an important consideration even for multicore, multiprocessor systems. As a result, techniques for improving single thread performance using multiple cores have received considerable attention. This work describes a technique, software data spreading, that leverages the cache capacity of extra cores and extra sockets rather than their computational resources. Software data spreading is a software-only technique that uses compiler-directed thread migration to aggregate cache capacity across cores and chips and improve performance. This paper describes an automated scheme that applies data spreading to various types of loops. Experiments with a set of SPEC2000, SPEC2006, NAS, and microbenchmark workloads show that data spreading can provide speedup of over 2, averaging 17\% for the SPEC and NAS applications on two systems. In addition, despite using more cores for the same computation, data spreading actually saves power since it reduces access to DRAM.}},
  url = {https://doi.org/10.1145/1806596.1806648},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup463,
  title = {Decoupling contention management from scheduling}},
  author = {Johnson, F. Ryan and Stoica, Radu and Ailamaki, Anastasia and Mowry, Todd C.}},
  year = {2010}},
  journal = {Proceedings of the Fifteenth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many parallel applications exhibit unpredictable communication between threads, leading to contention for shared objects. The choice of contention management strategy impacts strongly the performance and scalability of these applications: spinning provides maximum performance but wastes significant processor resources, while blocking-based approaches conserve processor resources but introduce high overheads on the critical path of computation. Under situations of high or changing load, the operating system complicates matters further with arbitrary scheduling decisions which often preempt lock holders, leading to long serialization delays until the preempted thread resumes execution.We observe that contention management is orthogonal to the problems of scheduling and load management and propose to decouple them so each may be solved independently and effectively. To this end, we propose a load control mechanism which manages the number of active threads in the system separately from any contention which may exist. By isolating contention management from damaging interactions with the OS scheduler, we combine the efficiency of spinning with the robustness of blocking. The proposed load control mechanism results in stable, high performance for both lightly and heavily loaded systems, requires no special privileges or modifications at the OS level, and can be implemented as a library which benefits existing code.}},
  url = {https://doi.org/10.1145/1736020.1736035},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup464,
  title = {Type families with class, type classes with family}},
  author = {Serrano, Alejandro and Hage, Jurriaan and Bahr, Patrick}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Type classes and type families are key ingredients in Haskell programming. Type classes were introduced to deal with ad-hoc polymorphism, although with the introduction of functional dependencies, their use expanded to type-level programming. Type families also allow encoding type-level functions, but more directly in the form of rewrite rules. In this paper we show that type families are powerful enough to simulate type classes (without overlapping instances), and we provide a formal proof of the soundness and completeness of this simulation. Encoding instance constraints as type families eases the path to proposed extensions to type classes, like closed sets of instances, instance chains, and control over the search procedure. The only feature which type families cannot simulate is elaboration, that is, generating code from the derivation of a rewriting. We look at ways to solve this problem in current Haskell, and propose an extension to allow elaboration during the rewriting phase.}},
  url = {https://doi.org/10.1145/2804302.2804304},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup465,
  title = {PMAF: an algebraic framework for static analysis of probabilistic programs}},
  author = {Wang, Di and Hoffmann, Jan and Reps, Thomas}},
  year = {2018}},
  journal = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Automatically establishing that a probabilistic program satisfies some property ϕ is a challenging problem. While a sampling-based approach—which involves running the program repeatedly—can suggest that ϕ holds, to establish that the program satisfies ϕ, analysis techniques must be used. Despite recent successes, probabilistic static analyses are still more difficult to design and implement than their deterministic counterparts. This paper presents a framework, called PMAF, for designing, implementing, and proving the correctness of static analyses of probabilistic programs with challenging features such as recursion, unstructured control-flow, divergence, nondeterminism, and continuous distributions. PMAF introduces pre-Markov algebras to factor out common parts of different analyses. To perform interprocedural analysis and to create procedure summaries, PMAF extends ideas from non-probabilistic interprocedural dataflow analysis to the probabilistic setting. One novelty is that PMAF is based on a semantics formulated in terms of a control-flow hyper-graph for each procedure, rather than a standard control-flow graph. To evaluate its effectiveness, PMAF has been used to reformulate and implement existing intraprocedural analyses for Bayesian-inference and the Markov decision problem, by creating corresponding interprocedural analyses. Additionally, PMAF has been used to implement a new interprocedural linear expectation-invariant analysis. Experiments with benchmark programs for the three analyses demonstrate that the approach is practical.}},
  url = {https://doi.org/10.1145/3192366.3192408},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup466,
  title = {A program logic for concurrent objects under fair scheduling}},
  author = {Liang, Hongjin and Feng, Xinyu}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Existing work on verifying concurrent objects is mostly concerned with safety only, e.g., partial correctness or linearizability. Although there has been recent work verifying lock-freedom of non-blocking objects, much less efforts are focused on deadlock-freedom and starvation-freedom, progress properties of blocking objects. These properties are more challenging to verify than lock-freedom because they allow the progress of one thread to depend on the progress of another, assuming fair scheduling. We propose LiLi, a new rely-guarantee style program logic for verifying linearizability and progress together for concurrent objects under fair scheduling. The rely-guarantee style logic unifies thread-modular reasoning about both starvation-freedom and deadlock-freedom in one framework. It also establishes progress-aware abstraction for concurrent objects, which can be applied when verifying safety and liveness of client code. We have successfully applied the logic to verify starvation-freedom or deadlock-freedom of representative algorithms such as ticket locks, queue locks, lock-coupling lists, optimistic lists and lazy lists.}},
  url = {https://doi.org/10.1145/2837614.2837635},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup467,
  title = {Interactive proofs in higher-order concurrent separation logic}},
  author = {Krebbers, Robbert and Timany, Amin and Birkedal, Lars}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {When using a proof assistant to reason in an embedded logic -- like separation logic -- one cannot benefit from the proof contexts and basic tactics of the proof assistant. This results in proofs that are at a too low level of abstraction because they are cluttered with bookkeeping code related to manipulating the object logic. In this paper, we introduce a so-called proof mode that extends the Coq proof assistant with (spatial and non-spatial) named proof contexts for the object logic. We show that thanks to these contexts we can implement high-level tactics for introduction and elimination of the connectives of the object logic, and thereby make reasoning in the embedded logic as seamless as reasoning in the meta logic of the proof assistant. We apply our method to Iris: a state of the art higher-order impredicative concurrent separation logic. We show that our method is very general, and is not just limited to program verification. We demonstrate its generality by formalizing correctness proofs of fine-grained concurrent algorithms, derived constructs of the Iris logic, and a unary and binary logical relation for a language with concurrency, higher-order store, polymorphism, and recursive types. This is the first formalization of a binary logical relation for such an expressive language. We also show how to use the logical relation to prove contextual refinement of fine-grained concurrent algorithms.}},
  url = {https://doi.org/10.1145/3009837.3009855},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup468,
  title = {Composition of dynamic analysis aspects}},
  author = {Tanter, \'{E}},
  year = {2010}},
  journal = {Proceedings of the Ninth International Conference on Generative Programming and Component Engineering}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Aspect-oriented programming provides a convenient high-level model to define several kinds of dynamic analyses, in particular thanks to recent advances in exhaustive weaving in core libraries. Casting dynamic analyses as aspects allows the use of a single weaving infrastructure to apply different analyses to the same base program, simultaneously. However, even if dynamic analysis aspects are mutually independent, their mere presence perturbates the observations of others: this is due to the fact that aspectual computation is potentially visible to all aspects. Because current aspect composition approaches do not address this kind of computational interference, combining different analysis aspects yields at best unpredictable results. It is also impossible to flexibly combine various analyses, for instance to analyze an analysis aspect. In this paper we show how the notion of execution levels makes it possible to effectively address these composition issues. In order to realize this approach, we explore the practical and efficient integration of execution levels in a mainstream aspect language, AspectJ. We report on a case study of composing two out-of-the-box analysis aspects in a variety of ways, highlighting the benefits of the approach.}},
  url = {https://doi.org/10.1145/1868294.1868311},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup469,
  title = {Crellvm: verified credible compilation for LLVM}},
  author = {Kang, Jeehoon and Kim, Yoonseung and Song, Youngju and Lee, Juneyoung and Park, Sanghoon and Shin, Mark Dongyeon and Kim, Yonghyun and Cho, Sungkeun and Choi, Joonwon and Hur, Chung-Kil and Yi, Kwangkeun}},
  year = {2018}},
  journal = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Production compilers such as GCC and LLVM are large complex software systems, for which achieving a high level of reliability is hard. Although testing is an effective method for finding bugs, it alone cannot guarantee a high level of reliability. To provide a higher level of reliability, many approaches that examine compilers' internal logics have been proposed. However, none of them have been successfully applied to major optimizations of production compilers. This paper presents Crellvm: a verified credible compilation framework for LLVM, which can be used as a systematic way of providing a high level of reliability for major optimizations in LLVM. Specifically, we augment an LLVM optimizer to generate translation results together with their correctness proofs, which can then be checked by a proof checker formally verified in Coq. As case studies, we applied our approach to two major optimizations of LLVM: register promotion mem2reg and global value numbering gvn, having found four new miscompilation bugs (two in each).}},
  url = {https://doi.org/10.1145/3192366.3192377},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup470,
  title = {Decidability of inferring inductive invariants}},
  author = {Padon, Oded and Immerman, Neil and Shoham, Sharon and Karbyshev, Aleksandr and Sagiv, Mooly}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Induction is a successful approach for verification of hardware and software systems. A common practice is to model a system using logical formulas, and then use a decision procedure to verify that some logical formula is an inductive safety invariant for the system. A key ingredient in this approach is coming up with the inductive invariant, which is known as invariant inference. This is a major difficulty, and it is often left for humans or addressed by sound but incomplete abstract interpretation. This paper is motivated by the problem of inductive invariants in shape analysis and in distributed protocols. This paper approaches the general problem of inferring first-order inductive invariants by restricting the language L of candidate invariants. Notice that the problem of invariant inference in a restricted language L differs from the safety problem, since a system may be safe and still not have any inductive invariant in L that proves safety. Clearly, if L is finite (and if testing an inductive invariant is decidable), then inferring invariants in L is decidable. This paper presents some interesting cases when inferring inductive invariants in L is decidable even when L is an infinite language of universal formulas. Decidability is obtained by restricting L and defining a suitable well-quasi-order on the state space. We also present some undecidability results that show that our restrictions are necessary. We further present a framework for systematically constructing infinite languages while keeping the invariant inference problem decidable. We illustrate our approach by showing the decidability of inferring invariants for programs manipulating linked-lists, and for distributed protocols.}},
  url = {https://doi.org/10.1145/2837614.2837640},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup471,
  title = {Efficient synthesis of probabilistic programs}},
  author = {Nori, Aditya V. and Ozair, Sherjil and Rajamani, Sriram K. and Vijaykeerthy, Deepak}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We show how to automatically synthesize probabilistic programs from real-world datasets. Such a synthesis is feasible due to a combination of two techniques: (1) We borrow the idea of ``sketching'' from synthesis of deterministic programs, and allow the programmer to write a skeleton program with ``holes''. Sketches enable the programmer to communicate domain-specific intuition about the structure of the desired program and prune the search space, and (2) we design an efficient Markov Chain Monte Carlo (MCMC) based synthesis algorithm to instantiate the holes in the sketch with program fragments. Our algorithm efficiently synthesizes a probabilistic program that is most consistent with the data. A core difficulty in synthesizing probabilistic programs is computing the likelihood L(P | D) of a candidate program P generating data D. We propose an approximate method to compute likelihoods using mixtures of Gaussian distributions, thereby avoiding expensive computation of integrals. The use of such approximations enables us to speed up evaluation of the likelihood of candidate programs by a factor of 1000, and makes Markov Chain Monte Carlo based search feasible. We have implemented our algorithm in a tool called PSKETCH, and our results are encouraging PSKETCH is able to automatically synthesize 16 non-trivial real-world probabilistic programs.}},
  url = {https://doi.org/10.1145/2737924.2737982},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup472,
  title = {Monadic augment and generalised short cut fusion}},
  author = {Ghani, Neil and Johann, Patricia and Uustalu, Tarmo and Vene, Varmo}},
  year = {2005}},
  journal = {Proceedings of the Tenth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Monads are commonplace programming devices that are used to uniformly structure computations with effects such as state, exceptions, and I/O. This paper further develops the monadic programming paradigm by investigating the extent to which monadic computations can be optimised by using generalisations of short cut fusion to eliminate monadic structures whose sole purpose is to "glue together" monadic program components.We make several contributions. First, we show that every inductive type has an associated build combinator and an associated short cut fusion rule. Second, we introduce the notion of an inductive monad to describe those monads that give rise to inductive types, and we give examples of such monads which are widely used in functional programming. Third, we generalise the standard augment combinators and cata/augment fusion rules for algebraic data types to types induced by inductive monads. This allows us to give the first cata/augment rules for some common data types, such as rose trees. Fourth, we demonstrate the practical applicability of our generalisations by providing Haskell implementations for all concepts and examples in the paper. Finally, we offer deep theoretical insights by showing that the augment combinators are monadic in nature, and thus that our cata/build and cata/augment rules are arguably the best generally applicable fusion rules obtainable.}},
  url = {https://doi.org/10.1145/1086365.1086403},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup473,
  title = {Equations, Contractions, and Unique Solutions}},
  author = {Sangiorgi, Davide}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {One of the most studied behavioural equivalences is bisimilarity. Its success is much due to the associated bisimulation proof method, which can be further enhanced by means of "up-to bisimulation" techniques such as "up-to context".A different proof method is discussed, based on unique solution of special forms of inequations called contractions, and inspired by Milner's theorem on unique solution of equations. The method is as powerful as the bisimulation proof method and its "up-to context" enhancements. The definition of contraction can be transferred onto other behavioural equivalences, possibly contextual and noncoinductive.This enables a coinductive reasoning style on such equivalences, either by applying the method based on unique solution of contractions, or by injecting appropriate contraction preorders into the bisimulation game. The techniques are illustrated on CCS-like languages; an example dealing with higher-order languages is also shown.}},
  url = {https://doi.org/10.1145/2676726.2676965},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup474,
  title = {The marriage of effects and monads}},
  author = {Wadler, Philip}},
  year = {1998}},
  journal = {Proceedings of the Third ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Gifford and others proposed an effect typing discipline to delimit the scope of computational effects within a program, while Moggi and others proposed monads for much the same purpose. Here we marry effects to monads, uniting two previously separate lines of research. In particular, we show that the type, region, and effect system of Talpin and Jouvelot carries over directly to an analogous system for monads, including a type and effect reconstruction algorithm. The same technique should allow one to transpose any effect systems into a corresponding monad system.}},
  url = {https://doi.org/10.1145/289423.289429},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup475,
  title = {Cardinalities and universal quantifiers for verifying parameterized systems}},
  author = {Gleissenthall, Klaus v. and Bj\o{}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Parallel and distributed systems rely on intricate protocols to manage shared resources and synchronize, i.e., to manage how many processes are in a particular state. Effective verification of such systems requires universally quantification to reason about parameterized state and cardinalities tracking sets of processes, messages, failures to adequately capture protocol logic. In this paper we present Tool, an automatic invariant synthesis method that integrates cardinality-based reasoning and universal quantification. The resulting increase of expressiveness allows Tool to verify, for the first time, a representative collection of intricate parameterized protocols.}},
  url = {https://doi.org/10.1145/2908080.2908129},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup476,
  title = {Gunrock: a high-performance graph processing library on the GPU}},
  author = {Wang, Yangzihao and Davidson, Andrew and Pan, Yuechao and Wu, Yuduo and Riffel, Andy and Owens, John D.}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {For large-scale graph analytics on the GPU, the irregularity of data access/control flow and the complexity of programming GPUs have been two significant challenges for developing a programmable high-performance graph library. "Gunrock," our high-level bulk-synchronous graph-processing system targeting the GPU, takes a new approach to abstracting GPU graph analytics: rather than designing an abstraction around computation, Gunrock instead implements a novel data-centric abstraction centered on operations on a vertex or edge frontier. Gunrock achieves a balance between performance and expressiveness by coupling high-performance GPU computing primitives and optimization strategies with a high-level programming model that allows programmers to quickly develop new graph primitives with small code size and minimal GPU programming knowledge. We evaluate Gunrock on five graph primitives (BFS, BC, SSSP, CC, and PageRank) and show that Gunrock has on average at least an order of magnitude speedup over Boost and PowerGraph, comparable performance to the fastest GPU hardwired primitives, and better performance than any other GPU high-level graph library.}},
  url = {https://doi.org/10.1145/2851141.2851145},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup477,
  title = {Type-safe higher-order channels in ML-like languages}},
  author = {Park, Sungwoo}},
  year = {2007}},
  journal = {Proceedings of the 12th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {As a means of transmitting not only data but also code encapsulated within functions, higher-order channels provide an advanced form of task parallelism in parallel computations. In the presence of mutable references, however, they pose a safety problem because references may be transmitted to remote threads where they are no longer valid.This paper presents an ML-like parallel language with type-safe higher-order channels. By type safety, we mean that no value written to a channel contains references, or equivalently, that no reference escapes via a channel from the thread where it is created. The type system uses a typing judgment that is capable of deciding whether the value to which a term evaluates contains references or not. The use of such a typing judgment also makes it easy to achieve another desirable feature of channels, channel locality, that associates every channel with a unique thread for serving all values addressed to it.Our type system permits mutable references in sequential computations and also ensures that mutable references never interfere with parallel computations. Thus it provides both flexibility in sequential programming and ease of implementing parallel computations.}},
  url = {https://doi.org/10.1145/1291151.1291181},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup478,
  title = {Exact Bayesian inference by symbolic disintegration}},
  author = {Shan, Chung-chieh and Ramsey, Norman}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Bayesian inference, of posterior knowledge from prior knowledge and observed evidence, is typically defined by Bayes's rule, which says the posterior multiplied by the probability of an observation equals a joint probability. But the observation of a continuous quantity usually has probability zero, in which case Bayes's rule says only that the unknown times zero is zero. To infer a posterior distribution from a zero-probability observation, the statistical notion of disintegration tells us to specify the observation as an expression rather than a predicate, but does not tell us how to compute the posterior. We present the first method of computing a disintegration from a probabilistic program and an expression of a quantity to be observed, even when the observation has probability zero. Because the method produces an exact posterior term and preserves a semantics in which monadic terms denote measures, it composes with other inference methods in a modular way-without sacrificing accuracy or performance.}},
  url = {https://doi.org/10.1145/3009837.3009852},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup479,
  title = {AURA: a programming language for authorization and audit}},
  author = {Jia, Limin and Vaughan, Jeffrey A. and Mazurak, Karl and Zhao, Jianzhou and Zarko, Luke and Schorr, Joseph and Zdancewic, Steve}},
  year = {2008}},
  journal = {Proceedings of the 13th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents AURA, a programming language for access control that treats ordinary programming constructs (e.g., integers and recursive functions) and authorization logic constructs (e.g., principals and access control policies) in a uniform way. AURA is based on polymorphic DCC and uses dependent types to permit assertions that refer directly to AURA values while keeping computation out of the assertion level to ensure tractability. The main technical results of this paper include fully mechanically verified proofs of the decidability and soundness for AURA's type system, and a prototype typechecker and interpreter.}},
  url = {https://doi.org/10.1145/1411204.1411212},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup480,
  title = {The semantics of transactions and weak memory in x86, Power, ARM, and C++}},
  author = {Chong, Nathan and Sorensen, Tyler and Wickerson, John}},
  year = {2018}},
  journal = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Weak memory models provide a complex, system-centric semantics for concurrent programs, while transactional memory (TM) provides a simpler, programmer-centric semantics. Both have been studied in detail, but their combined semantics is not well understood. This is problematic because such widely-used architectures and languages as x86, Power, and C++ all support TM, and all have weak memory models. Our work aims to clarify the interplay between weak memory and TM by extending existing axiomatic weak memory models (x86, Power, ARMv8, and C++) with new rules for TM. Our formal models are backed by automated tooling that enables (1) the synthesis of tests for validating our models against existing implementations and (2) the model-checking of TM-related transformations, such as lock elision and compiling C++ transactions to hardware. A key finding is that a proposed TM extension to ARMv8 currently being considered within ARM Research is incompatible with lock elision without sacrificing portability or performance.}},
  url = {https://doi.org/10.1145/3192366.3192373},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup481,
  title = {Safe zero-cost coercions for Haskell}},
  author = {Breitner, Joachim and Eisenberg, Richard A. and Peyton Jones, Simon and Weirich, Stephanie}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Generative type abstractions -- present in Haskell, OCaml, and other languages -- are useful concepts to help prevent programmer errors. They serve to create new types that are distinct at compile time but share a run-time representation with some base type. We present a new mechanism that allows for zero-cost conversions between generative type abstractions and their representations, even when such types are deeply nested. We prove type safety in the presence of these conversions and have implemented our work in GHC.}},
  url = {https://doi.org/10.1145/2628136.2628141},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup482,
  title = {Newtonian program analysis via tensor product}},
  author = {Reps, Thomas and Turetsky, Emma and Prabhu, Prathmesh}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Recently, Esparza et al. generalized Newton's method -- a numerical-analysis algorithm for finding roots of real-valued functions---to a method for finding fixed-points of systems of equations over semirings. Their method provides a new way to solve interprocedural dataflow-analysis problems. As in its real-valued counterpart, each iteration of their method solves a simpler ``linearized'' problem. One of the reasons this advance is exciting is that some numerical analysts have claimed that ```all' effective and fast iterative [numerical] methods are forms (perhaps very disguised) of Newton's method.'' However, there is an important difference between the dataflow-analysis and numerical-analysis contexts: when Newton's method is used on numerical-analysis problems, multiplicative commutativity is relied on to rearrange expressions of the form ``c*X + X*d'' into ``(c+d) * X.'' Such equations correspond to path problems described by regular languages. In contrast, when Newton's method is used for interprocedural dataflow analysis, the ``multiplication'' operation involves function composition, and hence is non-commutative: ``c*X + X*d'' cannot be rearranged into ``(c+d) * X.'' Such equations correspond to path problems described by linear context-free languages (LCFLs). In this paper, we present an improved technique for solving the LCFL sub-problems produced during successive rounds of Newton's method. Our method applies to predicate abstraction, on which most of today's software model checkers rely.}},
  url = {https://doi.org/10.1145/2837614.2837659},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup483,
  title = {Precise concrete type inference for object-oriented languages}},
  author = {Plevyak, John and Chien, Andrew A.}},
  year = {1994}},
  journal = {Proceedings of the Ninth Annual Conference on Object-Oriented Programming Systems, Language, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Concrete type information is invaluable for program optimization. The determination of concrete types in object-oriented languages is a flow sensitive global data flow problem. It is made difficult by dynamic dispatch (virtual function invocation) and first class functions (and selectors)—the very program structures for whose optimization its results are most critical. Previous work has shown that constraint-based type inference systems can be used to safely approximate concrete types [15], but their use can be expensive and their results imprecise.We present an incremental constraint-based type inference which produces precise concrete type information for a much larger class of programs at lower cost. Our algorithm extends the analysis in response to discovered imprecisions, guiding the analysis' effort to where it is most productive. This produces precise information at a cost proportional to the type complexity of the program. Many programs untypable by previous approaches or practically untypable due to computational expense, can be precisely analyzed by our new algorithm. Performance results, precision, and running time, are reported for a number of concurrent object-oriented programs. These results confirm the algorithm's precision and efficiency.}},
  url = {https://doi.org/10.1145/191080.191130},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup484,
  title = {Dependent types and multi-monadic effects in F*}},
  author = {Swamy, Nikhil and Hri\c{t}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a new, completely redesigned, version of F*, a language that works both as a proof assistant as well as a general-purpose, verification-oriented, effectful programming language. In support of these complementary roles, F* is a dependently typed, higher-order, call-by-value language with _primitive_ effects including state, exceptions, divergence and IO. Although primitive, programmers choose the granularity at which to specify effects by equipping each effect with a monadic, predicate transformer semantics. F* uses this to efficiently compute weakest preconditions and discharges the resulting proof obligations using a combination of SMT solving and manual proofs. Isolated from the effects, the core of F* is a language of pure functions used to write specifications and proof terms---its consistency is maintained by a semantic termination check based on a well-founded order. We evaluate our design on more than 55,000 lines of F* we have authored in the last year, focusing on three main case studies. Showcasing its use as a general-purpose programming language, F* is programmed (but not verified) in F*, and bootstraps in both OCaml and F#. Our experience confirms F*'s pay-as-you-go cost model: writing idiomatic ML-like code with no finer specifications imposes no user burden. As a verification-oriented language, our most significant evaluation of F* is in verifying several key modules in an implementation of the TLS-1.2 protocol standard. For the modules we considered, we are able to prove more properties, with fewer annotations using F* than in a prior verified implementation of TLS-1.2. Finally, as a proof assistant, we discuss our use of F* in mechanizing the metatheory of a range of lambda calculi, starting from the simply typed lambda calculus to System F-omega and even micro-F*, a sizeable fragment of F* itself---these proofs make essential use of F*'s flexible combination of SMT automation and constructive proofs, enabling a tactic-free style of programming and proving at a relatively large scale.}},
  url = {https://doi.org/10.1145/2837614.2837655},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup485,
  title = {Featherweight Java: a minimal core calculus for Java and GJ}},
  author = {Igarashi, Atshushi and Pierce, Benjamin and Wadler, Philip}},
  year = {1999}},
  journal = {Proceedings of the 14th ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Several recent studies have introduced lightweight versions of Java: reduced languages in which complex features like threads and reflection are dropped to enable rigorous arguments about key properties such as type safety. We carry this process a step further, omitting almost all features of the full language (including interfaces and even assignment) to obtain a small calculus, Featherweight Java, for which rigorous proofs are not only possible but easy.Featherweight Java bears a similar relation to full Java as the lambda-calculus does to languages such as ML and Haskell. It offers a similar computational “feel,” providing classes, methods, fields, inheritance, and dynamic typecasts, with a semantics closely following Java's. A proof of type safety for Featherweight Java thus illustrates many of the interesting features of a safety proof for the full language, while remaining pleasingly compact. The syntax, type rules, and operational semantics of Featherweight Java fit on one page, making it easier to understand the consequences of extensions and variations.As an illustration of its utility in this regard, we extend Featherweight Java with generic classes in the style of GJ (Bracha, Odersky, Stoutamire, and Wadler) and sketch a proof of type safety. The extended system formalizes for the first time some of the key features of GJ.}},
  url = {https://doi.org/10.1145/320384.320395},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup486,
  title = {Quantified class constraints}},
  author = {Bottu, Gert-Jan and Karachalias, Georgios and Schrijvers, Tom and Oliveira, Bruno C. d. S. and Wadler, Philip}},
  year = {2017}},
  journal = {Proceedings of the 10th ACM SIGPLAN International Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Quantified class constraints have been proposed many years ago to raise the expressive power of type classes from Horn clauses to the universal fragment of Hereditiary Harrop logic. Yet, while it has been much asked for over the years, the feature was never implemented or studied in depth. Instead, several workarounds have been proposed, all of which are ultimately stopgap measures. This paper revisits the idea of quantified class constraints and elaborates it into a practical language design. We show the merit of quantified class constraints in terms of more expressive modeling and in terms of terminating type class resolution. In addition, we provide a declarative specification of the type system as well as a type inference algorithm that elaborates into System F. Moreover, we discuss termination conditions of our system and also provide a prototype implementation.}},
  url = {https://doi.org/10.1145/3122955.3122967},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup487,
  title = {Backtracking, interleaving, and terminating monad transformers: (functional pearl)}},
  author = {Kiselyov, Oleg and Shan, Chung-chieh and Friedman, Daniel P. and Sabry, Amr}},
  year = {2005}},
  journal = {Proceedings of the Tenth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We design and implement a library for adding backtracking computations to any Haskell monad. Inspired by logic programming, our library provides, in addition to the operations required by the MonadPlus interface, constructs for fair disjunctions, fair conjunctions, conditionals, pruning, and an expressive top-level interface. Implementing these additional constructs is easy in models of backtracking based on streams, but not known to be possible in continuation-based models. We show that all these additional constructs can be generically and monadically realized using a single primitive msplit. We present two implementations of the library: one using success and failure continuations; and the other using control operators for manipulating delimited continuations.}},
  url = {https://doi.org/10.1145/1086365.1086390},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup488,
  title = {Identifying Security Critical Properties for the Dynamic Verification of a Processor}},
  author = {Zhang, Rui and Stanley, Natalie and Griggs, Christopher and Chi, Andrew and Sturton, Cynthia}},
  year = {2017}},
  journal = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a methodology for identifying security critical properties for use in the dynamic verification of a processor. Such verification has been shown to be an effective way to prevent exploits of vulnerabilities in the processor, given a meaningful set of security properties. We use known processor errata to establish an initial set of security-critical invariants of the processor. We then use machine learning to infer an additional set of invariants that are not tied to any particular, known vulnerability, yet are critical to security.We build a tool chain implementing the approach and evaluate it for the open-source OR1200 RISC processor. We find that our tool can identify 19 (86.4\%) of the 22 manually crafted security-critical properties from prior work and generates 3 new security properties not covered in prior work.}},
  url = {https://doi.org/10.1145/3037697.3037734},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup489,
  title = {Taming undefined behavior in LLVM}},
  author = {Lee, Juneyoung and Kim, Yoonseung and Song, Youngju and Hur, Chung-Kil and Das, Sanjoy and Majnemer, David and Regehr, John and Lopes, Nuno P.}},
  year = {2017}},
  journal = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A central concern for an optimizing compiler is the design of its intermediate representation (IR) for code. The IR should make it easy to perform transformations, and should also afford efficient and precise static analysis. In this paper we study an aspect of IR design that has received little attention: the role of undefined behavior. The IR for every optimizing compiler we have looked at, including GCC, LLVM, Intel's, and Microsoft's, supports one or more forms of undefined behavior (UB), not only to reflect the semantics of UB-heavy programming languages such as C and C++, but also to model inherently unsafe low-level operations such as memory stores and to avoid over-constraining IR semantics to the point that desirable transformations become illegal. The current semantics of LLVM's IR fails to justify some cases of loop unswitching, global value numbering, and other important "textbook" optimizations, causing long-standing bugs. We present solutions to the problems we have identified in LLVM's IR and show that most optimizations currently in LLVM remain sound, and that some desirable new transformations become permissible. Our solutions do not degrade compile time or performance of generated code.}},
  url = {https://doi.org/10.1145/3062341.3062343},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup490,
  title = {Search, align, and repair: data-driven feedback generation for introductory programming exercises}},
  author = {Wang, Ke and Singh, Rishabh and Su, Zhendong}},
  year = {2018}},
  journal = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper introduces the “Search, Align, and Repair” data-driven program repair framework to automate feedback generation for introductory programming exercises. Distinct from existing techniques, our goal is to develop an efficient, fully automated, and problem-agnostic technique for large or MOOC-scale introductory programming courses. We leverage the large amount of available student submissions in such settings and develop new algorithms for identifying similar programs, aligning correct and incorrect programs, and repairing incorrect programs by finding minimal fixes. We have implemented our technique in the Sarfgen system and evaluated it on thousands of real student attempts from the Microsoft-DEV204.1x edX course and the Microsoft CodeHunt platform. Our results show that Sarfgen can, within two seconds on average, generate concise, useful feedback for 89.7\% of the incorrect student submissions. It has been integrated with the Microsoft-DEV204.1X edX class and deployed for production use.}},
  url = {https://doi.org/10.1145/3192366.3192384},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup491,
  title = {Scaling LAPACK panel operations using parallel cache assignment}},
  author = {Castaldo, Anthony M. and Whaley, R. Clint}},
  year = {2010}},
  journal = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In LAPACK many matrix operations are cast as block algorithms which iteratively process a panel using an unblocked algorithm and then update a remainder matrix using the high performance Level 3 BLAS. The Level~3 BLAS have excellent weak scaling, but panel processing tends to be bus bound, and thus scales with bus speed rather than the number of processors (p). Amdahl's law therefore ensures that as p grows, the panel computation will become the dominant cost of these LAPACK routines. Our contribution is a novel parallel cache assignment approach which we show scales well with p. We apply this general approach to the QR and LU panel factorizations on two commodity 8-core platforms with very different cache structures, and demonstrate superlinear panel factorization speedups on both machines. Other approaches to this problem demand complicated reformulations of the computational approach, new kernels to be tuned, new mathematics, an inflation of the high-order flop count, and do not perform as well. By demonstrating a straight-forward alternative that avoids all of these contortions and scales with p, we address a critical stumbling block for dense linear algebra in the age of massive parallelism.}},
  url = {https://doi.org/10.1145/1693453.1693484},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup492,
  title = {The tree width of auxiliary storage}},
  author = {Madhusudan, P. and Parlato, Gennaro}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We propose a generalization of results on the decidability of emptiness for several restricted classes of sequential and distributed automata with auxiliary storage (stacks, queues) that have recently been proved. Our generalization relies on reducing emptiness of these automata to finite-state graph automata (without storage) restricted to monadic second-order (MSO) definable graphs of bounded tree-width, where the graph structure encodes the mechanism provided by the auxiliary storage. Our results outline a uniform mechanism to derive emptiness algorithms for automata, explaining and simplifying several existing results, as well as proving new decidability results.}},
  url = {https://doi.org/10.1145/1926385.1926419},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup493,
  title = {Liquid Silicon-Monona: A Reconfigurable Memory-Oriented Computing Fabric with Scalable Multi-Context Support}},
  author = {Zha, Yue and Li, Jing}},
  year = {2018}},
  journal = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {With the recent trend of promoting Field-Programmable Gate Arrays (FPGAs) to first-class citizens in accelerating compute-intensive applications in networking, cloud services and artificial intelligence, FPGAs face two major challenges in sustaining competitive advantages in performance and energy efficiency for diverse cloud workloads: 1) limited configuration capability for supporting light-weight computations/on-chip data storage to accelerate emerging search-/data-intensive applications. 2) lack of architectural support to hide reconfiguration overhead for assisting virtualization in a cloud computing environment. In this paper, we propose a reconfigurable memory-oriented computing fabric, namely Liquid Silicon-Monona (L-Si), enabled by emerging nonvolatile memory technology i.e. RRAM, to address these two challenges. Specifically, L-Si addresses the first challenge by virtue of a new architecture comprising a 2D array of physically identical but functionally-configurable building blocks. It, for the first time, extends the configuration capabilities of existing FPGAs from computation to the whole spectrum ranging from computation to data storage. It allows users to better customize hardware by flexibly partitioning hardware resources between computation and memory, greatly benefiting emerging search- and data-intensive applications. To address the second challenge, L-Si provides scalable multi-context architectural support to minimize reconfiguration overhead for assisting virtualization. In addition, we provide compiler support to facilitate the programming of applications written in high-level programming languages (e.g. OpenCL) and frameworks (e.g. TensorFlow, MapReduce) while fully exploiting the unique architectural capability of L-Si. Our evaluation results show L-Si achieves 99.6\% area reduction, 1.43\texttimes{}},
  url = {https://doi.org/10.1145/3173162.3173167},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup494,
  title = {Google Workloads for Consumer Devices: Mitigating Data Movement Bottlenecks}},
  author = {Boroumand, Amirali and Ghose, Saugata and Kim, Youngsok and Ausavarungnirun, Rachata and Shiu, Eric and Thakur, Rahul and Kim, Daehyun and Kuusela, Aki and Knies, Allan and Ranganathan, Parthasarathy and Mutlu, Onur}},
  year = {2018}},
  journal = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We are experiencing an explosive growth in the number of consumer devices, including smartphones, tablets, web-based computers such as Chromebooks, and wearable devices. For this class of devices, energy efficiency is a first-class concern due to the limited battery capacity and thermal power budget. We find that data movement is a major contributor to the total system energy and execution time in consumer devices. The energy and performance costs of moving data between the memory system and the compute units are significantly higher than the costs of computation. As a result, addressing data movement is crucial for consumer devices. In this work, we comprehensively analyze the energy and performance impact of data movement for several widely-used Google consumer workloads: (1) the Chrome web browser; (2) TensorFlow Mobile, Google's machine learning framework; (3) video playback, and (4) video capture, both of which are used in many video services such as YouTube and Google Hangouts. We find that processing-in-memory (PIM) can significantly reduce data movement for all of these workloads, by performing part of the computation close to memory. Each workload contains simple primitives and functions that contribute to a significant amount of the overall data movement. We investigate whether these primitives and functions are feasible to implement using PIM, given the limited area and power constraints of consumer devices. Our analysis shows that offloading these primitives to PIM logic, consisting of either simple cores or specialized accelerators, eliminates a large amount of data movement, and significantly reduces total system energy (by an average of 55.4\% across the workloads) and execution time (by an average of 54.2\%).}},
  url = {https://doi.org/10.1145/3173162.3173177},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup495,
  title = {From parametricity to conservation laws, via Noether's theorem}},
  author = {Atkey, Robert}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Invariance is of paramount importance in programming languages and in physics. In programming languages, John Reynolds' theory of relational parametricity demonstrates that parametric polymorphic programs are invariant under change of data representation, a property that yields "free" theorems about programs just from their types. In physics, Emmy Noether showed that if the action of a physical system is invariant under change of coordinates, then the physical system has a conserved quantity: a quantity that remains constant for all time. Knowledge of conserved quantities can reveal deep properties of physical systems. For example, the conservation of energy is by Noether's theorem a consequence of a system's invariance under time-shifting.In this paper, we link Reynolds' relational parametricity with Noether's theorem for deriving conserved quantities. We propose an extension of System F$omega$ with new kinds, types and term constants for writing programs that describe classical mechanical systems in terms of their Lagrangians. We show, by constructing a relationally parametric model of our extension of F$omega$, that relational parametricity is enough to satisfy the hypotheses of Noether's theorem, and so to derive conserved quantities for free, directly from the polymorphic types of Lagrangians expressed in our system.}},
  url = {https://doi.org/10.1145/2535838.2535867},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup496,
  title = {DVSleak: combining leakage reduction and voltage scaling in feedback EDF scheduling}},
  author = {Zhu, Yifan and Mueller, Frank}},
  year = {2007}},
  journal = {Proceedings of the 2007 ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Recent trends in CMOS fabrication have the demand to conserve power of processors. While dynamic voltage scaling (DVS) is effective in reducing dynamic power, microprocessors produced in ever smaller fabrication processes are increasingly dominated bystatic power. For such processors, voltage/frequency pairs below acritical speed result in higher energy per cycle than entering a processor sleep mode. Yet, computational demand above this critical speed is best met by DVS techniques while still conserving power.We develop a novel combined leakage and DVS scheduling algorithm forreal-time systems, DVS leak, based on earliest-deadline-first scheduling (EDF). Our method trades off DVS with leakage, where the former slows down execution while the latter intelligently defers dispatching of jobs when sleeping is beneficial. We further capitalize on feedback knowledge about actual execution times to anticipate computational demands without sacrificing deadline guarantees. As such, we contribute a novel feedback delay policy for leakage awareness, which addresses structural limitations of prior approaches. Experiments show that this combined DVS/leakage algorithm results in an average of (a) 50\% additional energy savings over a leakage-oblivious DVS algorithm, (b) 20\% more energy savings over a more simplistic combination of DVS and sleep policies and (c) 8.5\% or more over dynamic slack reclamation with procrastination. Particularly task sets with periods shorter than ten milliseconds profit from our approach with 15\% energy savings over best prior schemes. This makes DVS leak the best combined DVS/leakage regulation approach for real-time systems that we know of.}},
  url = {https://doi.org/10.1145/1254766.1254772},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup497,
  title = {Analyzing divergence in bisimulation semantics}},
  author = {Liu, Xinxin and Yu, Tingting and Zhang, Wenhui}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Some bisimulation based abstract equivalence relations may equate divergent systems with non-divergent ones, examples including weak bisimulation equivalence and branching bisimulation equivalence. Thus extra efforts are needed to analyze divergence for the compared systems. In this paper we propose a new method for analyzing divergence in bisimulation semantics, which relies only on simple observations of individual transitions. We show that this method can verify several typical divergence preserving bisimulation equivalences including two well-known ones. As an application case study, we use the proposed method to verify the HSY collision stack to draw the conclusion that the stack implementation is correct in terms of linearizability with lock-free progress condition.}},
  url = {https://doi.org/10.1145/3009837.3009870},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup498,
  title = {Blame and coercion: together again for the first time}},
  author = {Siek, Jeremy and Thiemann, Peter and Wadler, Philip}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {C#, Dart, Pyret, Racket, TypeScript, VB: many recent languages integrate dynamic and static types via gradual typing. We systematically develop three calculi for gradual typing and the relations between them, building on and strengthening previous work. The calculi are: λB, based on the blame calculus of Wadler and Findler (2009); λC, inspired by the coercion calculus of Henglein (1994); λS inspired by the space-efficient calculus of Herman, Tomb, and Flanagan (2006) and the threesome calculus of Siek and Wadler (2010). While λB is little changed from previous work, λC and λS are new. Together, λB, λC, and λS provide a coherent foundation for design, implementation, and optimisation of gradual types. We define translations from λB to λC and from λC to λS. Much previous work lacked proofs of correctness or had weak correctness criteria; here we demonstrate the strongest correctness criterion one could hope for, that each of the translations is fully abstract. Each of the calculi reinforces the design of the others: λC has a particularly simple definition, and the subtle definition of blame safety for λB is justified by the simple definition of blame safety for λC. Our calculus λS is implementation-ready: the first space-efficient calculus that is both straightforward to implement and easy to understand. We give two applications: first, using full abstraction from λC to λS to validate the challenging part of full abstraction between λB and λC; and, second, using full abstraction from λB to λS to easily establish the Fundamental Property of Casts, which required a custom bisimulation and six lemmas in earlier work.}},
  url = {https://doi.org/10.1145/2737924.2737968},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup499,
  title = {Declarative programming for artificial intelligence applications}},
  author = {Lloyd, John W.}},
  year = {2007}},
  journal = {Proceedings of the 12th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this talk, I will consider some possible extensions to existing functional programming languages that would make them more suitable for the important and growing class of artificial intelligence applications. First, I will motivate the need for these language extensions. Then I will give some technical detail about these extensions that provide the logic programming idioms, probabilistic computation, and modal computation. Some examples will be given to illustrate these ideas which have been implemented in the Bach programming language that is an extension of Haskell.}},
  url = {https://doi.org/10.1145/1291151.1291152},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup500,
  title = {HoTTSQL: proving query rewrites with univalent SQL semantics}},
  author = {Chu, Shumo and Weitz, Konstantin and Cheung, Alvin and Suciu, Dan}},
  year = {2017}},
  journal = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Every database system contains a query optimizer that performs query rewrites. Unfortunately, developing query optimizers remains a highly challenging task. Part of the challenges comes from the intricacies and rich features of query languages, which makes reasoning about rewrite rules difficult. In this paper, we propose a machine-checkable denotational semantics for SQL, the de facto language for relational database, for rigorously validating rewrite rules. Unlike previously proposed semantics that are either non-mechanized or only cover a small amount of SQL language features, our semantics covers all major features of SQL, including bags, correlated subqueries, aggregation, and indexes. Our mechanized semantics, called HoTT SQL, is based on K-Relations and homotopy type theory, where we denote relations as mathematical functions from tuples to univalent types. We have implemented HoTTSQL in Coq, which takes only fewer than 300 lines of code and have proved a wide range of SQL rewrite rules, including those from database research literature (e.g., magic set rewrites) and real-world query optimizers (e.g., subquery elimination). Several of these rewrite rules have never been previously proven correct. In addition, while query equivalence is generally undecidable, we have implemented an automated decision procedure using HoTTSQL for conjunctive queries: a well studied decidable fragment of SQL that encompasses many real-world queries.}},
  url = {https://doi.org/10.1145/3062341.3062348},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup501,
  title = {Classless Java}},
  author = {Wang, Yanlin and Zhang, Haoyuan and Oliveira, Bruno C. d. S. and Servetto, Marco}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents an OO style without classes, which we call interface-based object-oriented programming (IB). IB is a natural extension of closely related ideas such as traits. Abstract state operations provide a new way to deal with state, which allows for flexibility not available in class-based languages. In IB state can be type-refined in subtypes. The combination of a purely IB style and type-refinement enables powerful idioms using multiple inheritance and state. To introduce IB to programmers we created Classless Java: an embedding of IB directly into Java. Classless Java uses annotation processing for code generation and relies on new features of Java 8 for interfaces. The code generation techniques used in Classless Java have interesting properties, including guarantees that the generated code is type-safe and good integration with IDEs. Usefulness of IB and Classless Java is shown with examples and case studies.}},
  url = {https://doi.org/10.1145/2993236.2993238},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup502,
  title = {A trustworthy mechanized formalization of R}},
  author = {Bodin, Martin and Diaz, Tom\'{a}},
  year = {2018}},
  journal = {Proceedings of the 14th ACM SIGPLAN International Symposium on Dynamic Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The R programming language is very popular for developing statistical software and data analysis, thanks to rich libraries, concise and expressive syntax, and support for interactive programming. Yet, the semantics of R is fairly complex, contains many subtle corner cases, and is not formally specified. This makes it difficult to reason about R programs. In this work, we develop a big-step operational semantics for R in the form of an interpreter written in the Coq proof assistant. We ensure the trustworthiness of the formalization by introducing a monadic encoding that allows the Coq interpreter, CoqR, to be in direct visual correspondence with the reference R interpreter, GNU R. Additionally, we provide a testing framework that supports systematic comparison of CoqR and GNU R. In its current state, CoqR covers the nucleus of the R language as well as numerous additional features, making it pass a significant number of realistic test cases from the GNU R and FastR projects. To exercise the formal specification, we prove in Coq the preservation of memory invariants in selected parts of the interpreter. This work is an important first step towards a robust environment for formal verification of R programs.}},
  url = {https://doi.org/10.1145/3276945.3276946},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup503,
  title = {Ownership, encapsulation and the disjointness of type and effect}},
  author = {Clarke, Dave and Drossopoulou, Sophia}},
  year = {2002}},
  journal = {Proceedings of the 17th ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Ownership types provide a statically enforceable notion of object-level encapsulation. We extend ownership types with computational effects to support reasoning about object-oriented programs. The ensuing system provides both access control and effects reporting. Based on this type system, we codify two formal systems for reasoning about aliasing and the disjointness of computational effects. The first can be used to prove that evaluation of two expressions will never lead to aliases, while the latter can be used to show the non-interference of two expressions.}},
  url = {https://doi.org/10.1145/582419.582447},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup504,
  title = {A relationally parametric model of dependent type theory}},
  author = {Atkey, Robert and Ghani, Neil and Johann, Patricia}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Reynolds' theory of relational parametricity captures the invariance of polymorphically typed programs under change of data representation. Reynolds' original work exploited the typing discipline of the polymorphically typed lambda-calculus System F, but there is now considerable interest in extending relational parametricity to type systems that are richer and more expressive than that of System F.This paper constructs parametric models of predicative and impredicative dependent type theory. The significance of our models is twofold. Firstly, in the impredicative variant we are able to deduce the existence of initial algebras for all indexed=functors. To our knowledge, ours is the first account of parametricity for dependent types that is able to lift the useful deduction of the existence of initial algebras in parametric models of System F to the dependently typed setting. Secondly, our models offer conceptual clarity by uniformly expressing relational parametricity for dependent types in terms of reflexive graphs, which allows us to unify the interpretations of types and kinds, instead of taking the relational interpretation of types as a primitive notion. Expressing our model in terms of reflexive graphs ensures that it has canonical choices for the interpretations of the standard type constructors of dependent type theory, except for the interpretation of the universe of small types, where we formulate a refined interpretation tailored for relational parametricity. Moreover, our reflexive graph model opens the door to generalisations of relational parametricity, for example to higher-dimensional relational parametricity.}},
  url = {https://doi.org/10.1145/2535838.2535852},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup505,
  title = {Time Dilation and Contraction for Programmable Analog Devices with Jaunt}},
  author = {Achour, Sara and Rinard, Martin}},
  year = {2018}},
  journal = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Programmable analog devices are a powerful new computing substrate that are especially appropriate for performing computationally intensive simulations of neuromorphic and cytomorphic models. Current state of the art techniques for configuring analog devices to simulate dynamical systems do not consider the current and voltage operating ranges of analog device components or the sampling limitations of the digital interface of the device. We present Jaunt, a new solver that scales the values that configure the analog device to ensure the resulting analog computation executes within the operating constraints of the device, preserves the recoverable dynamics of the original simulation, and executes slowly enough to observe these dynamics at the sampled digital outputs. Our results show that, on a set of benchmark biological simulations, 1) unscaled configurations produce incorrect simulations because they violate the operating ranges of the device and 2) Jaunt delivers scaled configurations that respect the operating ranges to produce correct simulations with observable dynamics.}},
  url = {https://doi.org/10.1145/3173162.3173179},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup506,
  title = {Type-and-example-directed program synthesis}},
  author = {Osera, Peter-Michael and Zdancewic, Steve}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents an algorithm for synthesizing recursive functions that process algebraic datatypes. It is founded on proof-theoretic techniques that exploit both type information and input–output examples to prune the search space. The algorithm uses refinement trees, a data structure that succinctly represents constraints on the shape of generated code. We evaluate the algorithm by using a prototype implementation to synthesize more than 40 benchmarks and several non-trivial larger examples. Our results demonstrate that the approach meets or outperforms the state-of-the-art for this domain, in terms of synthesis time or attainable size of the generated programs.}},
  url = {https://doi.org/10.1145/2737924.2738007},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup507,
  title = {Programming up to Congruence}},
  author = {Sj\"{o}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents the design of Zombie, a dependently-typed programming language that uses an adaptation of a congruence closure algorithm for proof and type inference. This algorithm allows the type checker to automatically use equality assumptions from the context when reasoning about equality. Most dependently-typed languages automatically use equalities that follow from beta-reduction during type checking; however, such reasoning is incompatible with congruence closure. In contrast, Zombie does not use automatic beta-reduction because types may contain potentially diverging terms. Therefore Zombie provides a unique opportunity to explore an alternative definition of equivalence in dependently-typed language design.Our work includes the specification of the language via a bidirectional type system, which works "up-to-congruence,'' and an algorithm for elaborating expressions in this language to an explicitly typed core language. We prove that our elaboration algorithm is complete with respect to the source type system, and always produces well typed terms in the core language. This algorithm has been implemented in the Zombie language, which includes general recursion, irrelevant arguments, heterogeneous equality and datatypes.}},
  url = {https://doi.org/10.1145/2676726.2676974},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup508,
  title = {LOIS: syntax and semantics}},
  author = {Kopczy\'{n}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present the semantics of an imperative programming language called LOIS (Looping Over Infinite Sets), which allows iterating through certain infinite sets, in finite time. Our semantics intuitively correspond to execution of infinitely many threads in parallel. This allows to merge the power of abstract mathematical constructions into imperative programming. Infinite sets are internally represented using first order formulas over some underlying logical structure, and SMT solvers are employed to evaluate programs.}},
  url = {https://doi.org/10.1145/3009837.3009876},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup509,
  title = {Soft contract verification}},
  author = {Nguyen, Ph\'{u}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Behavioral software contracts are a widely used mechanism for governing the flow of values between components. However, run-time monitoring and enforcement of contracts imposes significant overhead and delays discovery of faulty components to run-time.To overcome these issues, we present soft contract verification, which aims to statically prove either complete or partial contract correctness of components, written in an untyped, higher-order language with first-class contracts. Our approach uses higher-order symbolic execution, leveraging contracts as a source of symbolic values including unknown behavioral values, and employs an updatable heap of contract invariants to reason about flow-sensitive facts. We prove the symbolic execution soundly approximates the dynamic semantics and that verified programs can't be blamed.The approach is able to analyze first-class contracts, recursive data structures, unknown functions, and control-flow-sensitive refinements of values, which are all idiomatic in dynamic languages. It makes effective use of an off-the-shelf solver to decide problems without heavy encodings. The approach is competitive with a wide range of existing tools - including type systems, flow analyzers, and model checkers - on their own benchmarks.}},
  url = {https://doi.org/10.1145/2628136.2628156},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup510,
  title = {How to Build Static Checking Systems Using Orders of Magnitude Less Code}},
  author = {Brown, Fraser and N\"{o}},
  year = {2016}},
  journal = {Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Modern static bug finding tools are complex. They typically consist of hundreds of thousands of lines of code, and most of them are wedded to one language (or even one compiler). This complexity makes the systems hard to understand, hard to debug, and hard to retarget to new languages, thereby dramatically limiting their scope. This paper reduces checking system complexity by addressing a fundamental assumption, the assumption that checkers must depend on a full-blown language specification and compiler front end. Instead, our program checkers are based on drastically incomplete language grammars ("micro-grammars") that describe only portions of a language relevant to a checker. As a result, our implementation is tiny-roughly 2500 lines of code, about two orders of magnitude smaller than a typical system. We hope that this dramatic increase in simplicity will allow people to use more checkers on more systems in more languages.We implement our approach in μchex, a language-agnostic framework for writing static bug checkers. We use it to build micro-grammar based checkers for six languages (C, the C preprocessor, C++, Java, JavaScript, and Dart) and find over 700 errors in real-world projects.}},
  url = {https://doi.org/10.1145/2872362.2872364},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup511,
  title = {Toward foundations for type-reflective metaprogramming}},
  author = {Garcia, Ronald and Lumsdaine, Andrew}},
  year = {2009}},
  journal = {Proceedings of the Eighth International Conference on Generative Programming and Component Engineering}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {C++ template metaprogramming has been used with great success to build software applications and libraries. In practice, however, template metaprogramming suffers usability, reliability, and capability shortcomings, and it is not well understood in theory. Template metaprogramming has these problems because it relies on emergent properties of disparate language features that were tailored to other purposes. As a step toward solid and sound language support for metaprogramming, this paper establishes firm semantic foundations for select capabilities of template metaprogramming.We analyze C++ and the idioms of template metaprogramming and isolate, in a language-neutral fashion, fundamental capabilities of C++ that enable metaprogramming. Guided by this analysis, we present a design for a core calculus that directly expresses fundamental metaprogramming capabilities, including static computation, code generation, and type reflection. We prove a typesafety property for compile-time evaluation of metaprograms. To formally connect the core calculus to programming practice, we present a more convenient surface language for metaprogramming. Its semantics are captured by type-directed translation to the core calculus. We prove that this translation preserves well-typing.This idealized presentation averts some of the shortcomings of C++ template metaprogramming and provides a framework for further study.}},
  url = {https://doi.org/10.1145/1621607.1621613},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup512,
  title = {From dirt to shovels: fully automatic tool generation from ad hoc data}},
  author = {Fisher, Kathleen and Walker, David and Zhu, Kenny Q. and White, Peter}},
  year = {2008}},
  journal = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {An ad hoc data source is any semistructured data source for which useful data analysis and transformation tools are not readily available. Such data must be queried, transformed and displayed by systems administrators, computational biologists, financial analysts and hosts of others on a regular basis. In this paper, we demonstrate that it is possible to generate a suite of useful data processing tools, including a semi-structured query engine, several format converters, a statistical analyzer and data visualization routines directly from the ad hoc data itself, without any human intervention. The key technical contribution of the work is a multi-phase algorithm that automatically infers the structure of an ad hoc data source and produces a format specification in the PADS data description language. Programmers wishing to implement custom data analysis tools can use such descriptions to generate printing and parsing libraries for the data. Alternatively, our software infrastructure will push these descriptions through the PADS compiler, creating format-dependent modules that, when linked with format-independent algorithms for analysis and transformation, result infully functional tools. We evaluate the performance of our inference algorithm, showing it scales linearlyin the size of the training data - completing in seconds, as opposed to the hours or days it takes to write a description by hand. We also evaluate the correctness of the algorithm, demonstrating that generating accurate descriptions often requires less than 5\% of theavailable data.}},
  url = {https://doi.org/10.1145/1328438.1328488},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup513,
  title = {A promising semantics for relaxed-memory concurrency}},
  author = {Kang, Jeehoon and Hur, Chung-Kil and Lahav, Ori and Vafeiadis, Viktor and Dreyer, Derek}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Despite many years of research, it has proven very difficult to develop a memory model for concurrent programming languages that adequately balances the conflicting desiderata of programmers, compilers, and hardware. In this paper, we propose the first relaxed memory model that (1) accounts for a broad spectrum of features from the C++11 concurrency model, (2) is implementable, in the sense that it provably validates many standard compiler optimizations and reorderings, as well as standard compilation schemes to x86-TSO and Power, (3) justifies simple invariant-based reasoning, thus demonstrating the absence of bad "out-of-thin-air" behaviors, (4) supports "DRF" guarantees, ensuring that programmers who use sufficient synchronization need not understand the full complexities of relaxed-memory semantics, and (5) defines the semantics of racy programs without relying on undefined behaviors, which is a prerequisite for applicability to type-safe languages like Java. The key novel idea behind our model is the notion of *promises*: a thread may promise to execute a write in the future, thus enabling other threads to read from that write out of order. Crucially, to prevent out-of-thin-air behaviors, a promise step requires a thread-local certification that it will be possible to execute the promised write even in the absence of the promise. To establish confidence in our model, we have formalized most of our key results in Coq.}},
  url = {https://doi.org/10.1145/3009837.3009850},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup514,
  title = {Queueing and glueing for optimal partitioning (functional pearl)}},
  author = {Mu, Shin-Cheng and Chiang, Yu-Hsi and Lyu, Yu-Han}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The queueing-glueing algorithm is the nickname we give to an algorithmic pattern that provides amortised linear time solutions to a number of optimal list partition problems that have a peculiar property: at various moments we know that two of three candidate solutions could be optimal. The algorithm works by keeping a queue of lists, glueing them from one end, while chopping from the other end, hence the name. We give a formal derivation of the algorithm, and demonstrate it with several non-trivial examples.}},
  url = {https://doi.org/10.1145/2951913.2951923},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup515,
  title = {Full Abstraction for Signal Flow Graphs}},
  author = {Bonchi, Filippo and Sobocinski, Pawel and Zanasi, Fabio}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Network theory uses the string diagrammatic language of monoidal categories to study graphical structures formally, eschewing specialised translations into intermediate formalisms. Recently, there has been a concerted research focus on developing a network theoretic approach to signal flow graphs, which are classical structures in control theory, signal processing and a cornerstone in the study of feedback. In this approach, signal flow graphs are given a relational denotational semantics in terms of formal power series.Thus far, the operational behaviour of such signal flow graphs has only been discussed at an intuitive level. In this paper we equip them with a structural operational semantics. As is typically the case, the purely operational picture is too concrete -- two graphs that are denotationally equal may exhibit different operational behaviour. We classify the ways in which this can occur and show that any graph can be realised -- rewritten, using the graphical theory, into an executable form where the operational behavior and the denotation coincides.}},
  url = {https://doi.org/10.1145/2676726.2676993},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup516,
  title = {LoPC: modeling contention in parallel algorithms}},
  author = {Frank, Matthew I. and Agarwal, Anant and Vernon, Mary K.}},
  year = {1997}},
  journal = {Proceedings of the Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Parallel algorithm designers need computational models that take first order system costs into account, but are also simple enough to use in practice. This paper introduces the LoPC model, which is inspired by the LogP model but accounts for contention for message processing resources in parallel algorithms on a multiprocessor or network of workstations. LoPC takes the L, o and P parameters directly from the LogP model and uses them to predict the cost of contention, C.This paper defines the LoPC model and derives the general form of the model for parallel applications that communicate via active messages. Model modifications for systems that implement coherent shared memory abstractions are also discussed. We carry out the analysis for two important classes of applications that have irregular communication. In the case of parallel applications with homogeneous all-to-any communication, such as sparse matrix computations, the analysis yields a simple rule of thumb and insight into contention costs. In the case of parallel client-server algorithms, the LoPC analysis provides a simple and accurate calculation of the optimal allocation of nodes between clients and servers. The LoPC estimates for these applications are shown to be accurate when compared against event driven simulation and against a sparse matrix computation on the MIT Alewife multiprocessor.}},
  url = {https://doi.org/10.1145/263764.263803},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup517,
  title = {CONCURRIT: a domain specific language for reproducing concurrency bugs}},
  author = {Elmas, Tayfun and Burnim, Jacob and Necula, George and Sen, Koushik}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present CONCURRIT, a domain-specific language (DSL) for reproducing concurrency bugs. Given some partial information about the nature of a bug in an application, a programmer can write a CONCURRIT script to formally and concisely specify a set of thread schedules to explore in order to find a schedule exhibiting the bug. Further, the programmer can specify how these thread schedules should be searched to find a schedule that reproduces the bug. We implemented CONCURRIT as an embedded DSL in C++, which uses manual or automatic source instrumentation to partially control the scheduling of the software under test. Using CONCURRIT, we were able to write concise tests to reproduce concurrency bugs in a variety of benchmarks, including the Mozilla's SpiderMonkey JavaScript engine, Memcached, Apache's HTTP server, and MySQL.}},
  url = {https://doi.org/10.1145/2491956.2462162},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup518,
  title = {Incremental incrementally compacting garbage collection}},
  author = {Lang, B. and Dupont, F.}},
  year = {1987}},
  journal = {Papers of the Symposium on Interpreters and Interpretive Techniques}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A mixed-strategy garbage collection algorithm is presented, which combines mark-and-sweep and copy collection. The intent is to benefit from the compacting and linearizing properties of copy collection without losing computational use of half the memory. The stop-and-collect version of the algorithm is a simple and cheap technique to fight memory fragmentation. The collection strategy may be dynamically adapted to minimize the cost of collection, according to the amount of memory actually accessed by the computing process. The parallel version of the algorithm is to our knowledge the only parallel compacting collector for varisized cells, that leaves most (more than half) of the memory available for the computing process.}},
  url = {https://doi.org/10.1145/29650.29677},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup519,
  title = {A vision for online verification-validation}},
  author = {Hammer, Matthew A. and Chang, Bor-Yuh Evan and Van Horn, David}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Today's programmers face a false choice between creating software that is extensible and software that is correct. Specifically, dynamic languages permit software that is richly extensible (via dynamic code loading, dynamic object extension, and various forms of reflection), and today's programmers exploit this flexibility to "bring their own language features" to enrich extensible languages (e.g., by using common JavaScript libraries). Meanwhile, such library-based language extensions generally lack enforcement of their abstractions, leading to programming errors that are complex to avoid and predict. To offer verification for this extensible world, we propose online verification-validation (OVV), which consists of language and VM design that enables a "phaseless" approach to program analysis, in contrast to the standard static-dynamic phase distinction. Phaseless analysis freely interposes abstract interpretation with concrete execution, allowing analyses to use dynamic (concrete) information to prove universal (abstract) properties about future execution. In this paper, we present a conceptual overview of OVV through a motivating example program that uses a hypothetical database library. We present a generic semantics for OVV, and an extension to this semantics that offers a simple gradual type system for the database library primitives. The result of instantiating this gradual type system in an OVV setting is a checker that can progressively type successive continuations of the program until a continuation is fully verified. To evaluate the proposed vision of OVV for this example, we implement the VM semantics (in Rust), and show that this design permits progressive typing in this manner.}},
  url = {https://doi.org/10.1145/2993236.2993255},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup520,
  title = {An improved algorithm for slicing machine code}},
  author = {Srinivasan, Venkatesh and Reps, Thomas}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Machine-code slicing is an important primitive for building binary analysis and rewriting tools, such as taint trackers, fault localizers, and partial evaluators. However, it is not easy to create a machine-code slicer that exhibits a high level of precision. Moreover, the problem of creating such a tool is compounded by the fact that a small amount of local imprecision can be amplified via cascade effects. Most instructions in instruction sets such as Intel's IA-32 and ARM are multi-assignments: they have several inputs and several outputs (registers, flags, and memory locations). This aspect of the instruction set introduces a granularity issue during slicing: there are often instructions at which we would like the slice to include only a subset of the instruction's semantics, whereas the slice is forced to include the entire instruction. Consequently, the slice computed by state-of-the-art tools is very imprecise, often including essentially the entire program. This paper presents an algorithm to slice machine code more accurately. To counter the granularity issue, our algorithm performs slicing at the microcode level, instead of the instruction level, and obtains a more precise microcode slice. To reconstitute a machine-code program from a microcode slice, our algorithm uses machine-code synthesis. Our experiments on IA-32 binaries of FreeBSD utilities show that, in comparison to slices computed by a state-of-the-art tool, our algorithm reduces the size of backward slices by 33\%, and forward slices by 70\%.}},
  url = {https://doi.org/10.1145/2983990.2984003},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup521,
  title = {The exp-log normal form of types: decomposing extensional equality and representing terms compactly}},
  author = {Ilik, Danko}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Lambda calculi with algebraic data types lie at the core of functional programming languages and proof assistants, but conceal at least two fundamental theoretical problems already in the presence of the simplest non-trivial data type, the sum type. First, we do not know of an explicit and implemented algorithm for deciding the beta-eta-equality of terms---and this in spite of the first decidability results proven two decades ago. Second, it is not clear how to decide when two types are essentially the same, i.e. isomorphic, in spite of the meta-theoretic results on decidability of the isomorphism. In this paper, we present the exp-log normal form of types---derived from the representation of exponential polynomials via the unary exponential and logarithmic functions---that any type built from arrows, products, and sums, can be isomorphically mapped to. The type normal form can be used as a simple heuristic for deciding type isomorphism, thanks to the fact that it is a systematic application of the high-school identities. We then show that the type normal form allows to reduce the standard beta-eta equational theory of the lambda calculus to a specialized version of itself, while preserving completeness of the equality on terms. We end by describing an alternative representation of normal terms of the lambda calculus with sums, together with a Coq-implemented converter into/from our new term calculus. The difference with the only other previously implemented heuristic for deciding interesting instances of eta-equality by Balat, Di Cosmo, and Fiore, is that we exploits the type information of terms substantially and this often allows us to obtain a canonical representation of terms without performing a sophisticated term analyses.}},
  url = {https://doi.org/10.1145/3009837.3009841},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup522,
  title = {Local reasoning about a copying garbage collector}},
  author = {Birkedal, Lars and Torp-Smith, Noah and Reynolds, John C.}},
  year = {2004}},
  journal = {Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a programming language, model, and logic appropriate for implementing and reasoning about a memory management system. We then state what is meant by correctness of a copying garbage collector, and employ a variant of the novel separation logics [18, 23] to formally specify partial correctness of Cheney's copying garbage collector [8]. Finally, we prove that our implementation of Cheney's algorithm meets its specification, using the logic we have given, and auxiliary variables [19].}},
  url = {https://doi.org/10.1145/964001.964020},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup523,
  title = {Automatically disproving fair termination of higher-order functional programs}},
  author = {Watanabe, Keiichi and Sato, Ryosuke and Tsukada, Takeshi and Kobayashi, Naoki}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We propose an automated method for disproving fair termination of higher-order functional programs, which is complementary to Murase et al.’s recent method for proving fair termination. A program is said to be fair terminating if it has no infinite execution trace that satisfies a given fairness constraint. Fair termination is an important property because program verification problems for arbitrary ω-regular temporal properties can be transformed to those of fair termination. Our method reduces the problem of disproving fair termination to higher-order model checking by using predicate abstraction and CEGAR. Given a program, we convert it to an abstract program that generates an approximation of the (possibly infinite) execution traces of the original program, so that the original program has a fair infinite execution trace if the tree generated by the abstract program satisfies a certain property. The method is a non-trivial extension of Kuwahara et al.’s method for disproving plain termination.}},
  url = {https://doi.org/10.1145/2951913.2951919},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup524,
  title = {Typed self-evaluation via intensional type functions}},
  author = {Brown, Matt and Palsberg, Jens}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many popular languages have a self-interpreter, that is, an interpreter for the language written in itself. So far, work on polymorphically-typed self-interpreters has concentrated on self-recognizers that merely recover a program from its representation. A larger and until now unsolved challenge is to implement a polymorphically-typed self-evaluator that evaluates the represented program and produces a representation of the result. We present Fωμi, the first λ-calculus that supports a polymorphically-typed self-evaluator. Our calculus extends Fω with recursive types and intensional type functions and has decidable type checking. Our key innovation is a novel implementation of type equality proofs that enables us to define a versatile representation of programs. Our results establish a new category of languages that can support polymorphically-typed self-evaluators.}},
  url = {https://doi.org/10.1145/3009837.3009853},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup525,
  title = {Revealing parallel scans and reductions in sequential loops through function reconstruction}},
  author = {Jiang, Peng and Agrawal, Gagan}},
  year = {2018}},
  journal = {Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many sequential loops are actually scans or reductions and can be parallelized across iterations despite the loop-carried dependences. In this work, we consider the parallelization of such scan/reduction loops, and propose a practical runtime approach called sampling-and-reconstruction to extract the hidden scan/reduction patterns in these loops.}},
  url = {https://doi.org/10.1145/3178487.3178523},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup526,
  title = {Hoare-style specifications as correctness conditions for non-linearizable concurrent objects}},
  author = {Sergey, Ilya and Nanevski, Aleksandar and Banerjee, Anindya and Delbianco, Germ\'{a}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Designing efficient concurrent objects often requires abandoning the standard specification technique of linearizability in favor of more relaxed correctness conditions. However, the variety of alternatives makes it difficult to choose which condition to employ, and how to compose them when using objects specified by different conditions. In this work, we propose a uniform alternative in the form of Hoare logic, which can explicitly capture--in the auxiliary state--the interference of environment threads. We demonstrate the expressiveness of our method by verifying a number of concurrent objects and their clients, which have so far been specified only by non-standard conditions of concurrency-aware linearizability, quiescent, and quantitative quiescent consistency. We report on the implementation of the ideas in an existing Coq-based tool, providing the first mechanized proofs for all the examples in the paper.}},
  url = {https://doi.org/10.1145/2983990.2983999},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup527,
  title = {Proof search for propositional abstract separation logics via labelled sequents}},
  author = {H\'{o}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Abstract separation logics are a family of extensions of Hoare logic for reasoning about programs that mutate memory. These logics are "abstract" because they are independent of any particular concrete memory model. Their assertion languages, called propositional abstract separation logics, extend the logic of (Boolean) Bunched Implications (BBI) in various ways.We develop a modular proof theory for various propositional abstract separation logics using cut-free labelled sequent calculi. We first extend the cut-fee labelled sequent calculus for BBI of Hou et al to handle Calcagno et al's original logic of separation algebras by adding sound rules for partial-determinism and cancellativity, while preserving cut-elimination. We prove the completeness of our calculus via a sound intermediate calculus that enables us to construct counter-models from the failure to find a proof. We then capture other propositional abstract separation logics by adding sound rules for indivisible unit and disjointness, while maintaining completeness and cut-elimination. We present a theorem prover based on our labelled calculus for these logics.}},
  url = {https://doi.org/10.1145/2535838.2535864},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup528,
  title = {Dynamic witnesses for static type errors (or, ill-typed programs usually go wrong)}},
  author = {Seidel, Eric L. and Jhala, Ranjit and Weimer, Westley}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Static type errors are a common stumbling block for newcomers to typed functional languages. We present a dynamic approach to explaining type errors by generating counterexample witness inputs that illustrate how an ill-typed program goes wrong. First, given an ill-typed function, we symbolically execute the body to synthesize witness values that make the program go wrong. We prove that our procedure synthesizes general witnesses in that if a witness is found, then for all inhabited input types, there exist values that can make the function go wrong. Second, we show how to extend the above procedure to produce a reduction graph that can be used to interactively visualize and debug witness executions. Third, we evaluate the coverage of our approach on two data sets comprising over 4,500 ill-typed student programs. Our technique is able to generate witnesses for 88\% of the programs, and our reduction graph yields small counterexamples for 81\% of the witnesses. Finally, we evaluate whether our witnesses help students understand and fix type errors, and find that students presented with our witnesses show a greater understanding of type errors than those presented with a standard error message.}},
  url = {https://doi.org/10.1145/2951913.2951915},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup529,
  title = {Generational garbage collection and the radioactive decay model}},
  author = {Clinger, William D. and Hansen, Lars T.}},
  year = {1997}},
  journal = {Proceedings of the ACM SIGPLAN 1997 Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {If a fixed exponentially decreasing probability distribution function is used to model every object's lifetime, then the age of an object gives no information about its future life expectancy. This radioactive decay model implies there can be no rational basis for deciding which live objects should be promoted to another generation. Yet there remains a rational basis for deciding how many objects to promote, when to collect garbage, and which generations to collect.Analysis of the model leads to a new kind of generational garbage collector whose effectiveness does not depend upon heuristics that predict which objects will live longer than others.This result provides insight into the computational advantages of generational garbage collection, with implications for the management of objects whose life expectancies are difficult to predict.}},
  url = {https://doi.org/10.1145/258915.258925},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup530,
  title = {Understanding the GPU Microarchitecture to Achieve Bare-Metal Performance Tuning}},
  author = {Zhang, Xiuxia and Tan, Guangming and Xue, Shuangbai and Li, Jiajia and Zhou, Keren and Chen, Mingyu}},
  year = {2017}},
  journal = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this paper, we present a methodology to understand GPU microarchitectural features and improve performance for compute-intensive kernels. The methodology relies on a reverse engineering approach to crack the GPU ISA encodings in order to build a GPU assembler. An assembly microbenchmark suite correlates microarchitectural features with their performance factors to uncover instruction-level and memory hierarchy preferences. We use SGEMM as a running example to show the ways to achieve bare-metal performance tuning. The performance boost is achieved by tuning FFMA throughput by activating dual-issue, eliminating register bank conflicts, adding non-FFMA instructions with little penalty, and choosing proper width of global/shared load instructions. On NVIDIA Kepler K20m, we develop a faster SGEMM with 3.1Tflop/s performance and 88\% efficiency; the performance is 15\% higher than cuBLAS7.0. Applying these optimizations to convolution, the implementation gains 39\%-62\% performance improvement compared with cuDNN4.0. The toolchain is an attempt to automatically crack different GPU ISA encodings and build an assembler adaptively for the purpose of performance enhancements to applications on GPUs.}},
  url = {https://doi.org/10.1145/3018743.3018755},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup531,
  title = {Reasoning of feature models from derived features}},
  author = {Ryssel, Uwe and Ploennigs, Joern and Kabitzsch, Klaus}},
  year = {2012}},
  journal = {Proceedings of the 11th International Conference on Generative Programming and Component Engineering}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {When using product lines, whose variability models are based on derived features, e.g., Simulink variant objects, the dependencies among the features are only described implicitly. This makes it difficult to verify the mapping of the features to the solution space and to create a comprehensive overview of the feature dependencies like in a feature model. In this paper, an OWL-based approach is presented, which permits the automatic verification of the feature mapping and an automatic feature model synthesis for derived features using OWL reasoning and formal concept analysis.}},
  url = {https://doi.org/10.1145/2371401.2371405},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup532,
  title = {Parser generation by example for legacy pattern languages}},
  author = {Zaytsev, Vadim}},
  year = {2017}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Most modern software languages enjoy relatively free and relaxed concrete syntax, with significant flexibility of formatting of the program/model/sheet text. Yet, in the dark legacy corners of software engineering there are still languages with a strict fixed column-based structure — the compromises of times long gone, attempting to combine some human readability with some ease of machine processing. In this paper, we consider an industrial case study for retirement of a legacy domain-specific language, completed under extreme circumstances: absolute lack of documentation, varying line structure, hierarchical blocks within one file, scalability demands for millions of lines of code, performance demands for manipulating tens of thousands multi-megabyte files, etc. However, the regularity of the language allowed to infer its structure from the available examples, automatically, and produce highly efficient parsers for it.}},
  url = {https://doi.org/10.1145/3136040.3136058},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup533,
  title = {The case for evolvable software}},
  author = {Forrest, Stephanie}},
  year = {2010}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {As programmers, we like to think of software as the product of our intelligent design, carefully crafted to meet well-specified goals. In reality, software evolves inadvertently through the actions of many individual programmers, often leading to unanticipated consequences. Large complex software systems are subject to constraints similar to those faced by evolving biological systems, and we have much to gain by viewing software through the lens of evolutionary biology. The talk will highlight recent research that applies the mechanisms of evolution quite directly to the problem of repairing software bugs.}},
  url = {https://doi.org/10.1145/1869459.1869539},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup534,
  title = {Programmatic and direct manipulation, together at last}},
  author = {Chugh, Ravi and Hempel, Brian and Spradlin, Mitchell and Albers, Jacob}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Direct manipulation interfaces and programmatic systems have distinct and complementary strengths. The former provide intuitive, immediate visual feedback and enable rapid prototyping, whereas the latter enable complex, reusable abstractions. Unfortunately, existing systems typically force users into just one of these two interaction modes. We present a system called Sketch-n-Sketch that integrates programmatic and direct manipulation for the particular domain of Scalable Vector Graphics (SVG). In Sketch-n-Sketch, the user writes a program to generate an output SVG canvas. Then the user may directly manipulate the canvas while the system immediately infers a program update in order to match the changes to the output, a workflow we call live synchronization. To achieve this, we propose (i) a technique called trace-based program synthesis that takes program execution history into account in order to constrain the search space and (ii) heuristics for dealing with ambiguities. Based on our experience with examples spanning 2,000 lines of code and from the results of a preliminary user study, we believe that Sketch-n-Sketch provides a novel workflow that can augment traditional programming systems. Our approach may serve as the basis for live synchronization in other application domains, as well as a starting point for yet more ambitious ways of combining programmatic and direct manipulation.}},
  url = {https://doi.org/10.1145/2908080.2908103},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup535,
  title = {Incremental type-checking for type-reflective metaprograms}},
  author = {Miao, Weiyu and Siek, Jeremy G.}},
  year = {2010}},
  journal = {Proceedings of the Ninth International Conference on Generative Programming and Component Engineering}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Garcia introduces a calculus for type-reflective metaprogramming that provides much of the power and flexibility of C++ templates and solves many of its problems. However, one of the problems that remains is that the residual program is not type checked until after meta computation is complete. Ideally, one would like the type system of the metaprogram to also guarantee that the residual program will type check, as is the case in MetaML. However, in a language with type-reflective metaprogramming, type expressions in the residual program may be the result of meta computation, making the MetaML guarantee next to impossible to achieve.In this paper we offer an approach to detecting errors earlier without sacrificing flexibility: we incrementally type check code fragments as they are created and spliced together during meta computation. The incremental type system is a variant of the gradual type system of Siek and Taha, in which we use type variables to represent type expressions that are not yet normalized and a new dynamic variation on existential types to represent residual code fragments. A type error in a code fragment is treated as a run-time error of the meta computation. We show that the incremental type checker can be implemented efficiently and we prove that if a well-typed metaprogram generates a residual program, then the residual program is also well-typed.}},
  url = {https://doi.org/10.1145/1868294.1868319},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup536,
  title = {Reducing crash recoverability to reachability}},
  author = {Koskinen, Eric and Yang, Junfeng}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Software applications run on a variety of platforms (filesystems, virtual slices, mobile hardware, etc.) that do not provide 100\% uptime. As such, these applications may crash at any unfortunate moment losing volatile data and, when re-launched, they must be able to correctly recover from potentially inconsistent states left on persistent storage. From a verification perspective, crash recovery bugs can be particularly frustrating because, even when it has been formally proved for a program that it satisfies a property, the proof is foiled by these external events that crash and restart the program. In this paper we first provide a hierarchical formal model of what it means for a program to be crash recoverable. Our model captures the recoverability of many real world programs, including those in our evaluation which use sophisticated recovery algorithms such as shadow paging and write-ahead logging. Next, we introduce a novel technique capable of automatically proving that a program correctly recovers from a crash via a reduction to reachability. Our technique takes an input control-flow automaton and transforms it into an encoding that blends the capture of snapshots of pre-crash states into a symbolic search for a proof that recovery terminates and every recovered execution simulates some crash-free execution. Our encoding is designed to enable one to apply existing abstraction techniques in order to do the work that is necessary to prove recoverability. We have implemented our technique in a tool called Eleven82, capable of analyzing C programs to detect recoverability bugs or prove their absence. We have applied our tool to benchmark examples drawn from industrial file systems and databases, including GDBM, LevelDB, LMDB, PostgreSQL, SQLite, VMware and ZooKeeper. Within minutes, our tool is able to discover bugs or prove that these fragments are crash recoverable.}},
  url = {https://doi.org/10.1145/2837614.2837648},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup537,
  title = {Type safety analysis for Dart}},
  author = {Heinze, Thomas S. and M\o{}},
  year = {2016}},
  journal = {Proceedings of the 12th Symposium on Dynamic Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Optional typing is traditionally viewed as a compromise between static and dynamic type checking, where code without type annotations is not checked until runtime. We demonstrate that optional type annotations in Dart programs can be integrated into a flow analysis to provide static type safety guarantees both for annotated and non-annotated parts of the code. We explore two approaches: one that uses type annotations for filtering, and one that uses them as specifications. What makes this particularly challenging for Dart is that its type system is unsound even for fully annotated code. Experimental results show that the technique is remarkably effective, even without context sensitivity: 99.3\% of all property lookup operations are reported type safe in a collection of benchmark programs.}},
  url = {https://doi.org/10.1145/2989225.2989226},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup538,
  title = {Divide and recycle: types and compilation for a hybrid synchronous language}},
  author = {Benveniste, Albert and Bourke, Timothy and Caillaud, Beno\^{\i}},
  year = {2011}},
  journal = {Proceedings of the 2011 SIGPLAN/SIGBED Conference on Languages, Compilers and Tools for Embedded Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Hybrid modelers such as Simulink have become corner stones of embedded systems development. They allow both discrete controllers and their continuous environments to be expressed in a single language. Despite the availability of such tools, there remain a number of issues related to the lack of reproducibility of simulations and to the separation of the continuous part, which has to be exercised by a numerical solver, from the discrete part, which must be guaranteed not to evolve during a step.Starting from a minimal, yet full-featured, Lustre-like synchronous language, this paper presents a conservative extension where data-flow equations can be mixed with ordinary differential equations (ODEs) with possible reset. A type system is proposed to statically distinguish discrete computations from continuous ones and to ensure that signals are used in their proper domains. We propose a semantics based on non-standard analysis which gives a synchronous interpretation to the whole language, clarifies the discrete/continuous interaction and the treatment of zero-crossings, and also allows the correctness of the type system to be established.The extended data-flow language is realized through a source-to-source transformation into a synchronous subset, which can then be compiled using existing tools into routines that are both efficient and bounded in their use of memory. These routines are orchestrated with a single off-the-shelf numerical solver using a simple but precise algorithm which treats causally-related cascades of zero-crossings. We have validated the viability of the approach through experiments with the Sundials library.}},
  url = {https://doi.org/10.1145/1967677.1967687},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup539,
  title = {Jiazzi: new-age components for old-fasioned Java}},
  author = {McDirmid, Sean and Flatt, Matthew and Hsieh, Wilson C.}},
  year = {2001}},
  journal = {Proceedings of the 16th ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present Jiazzi, a system that enables the construction of large-scale binary components in Java. Jiazzi components can be thought of as generalizations of Java packages with added support for external linking and separate compilation. Jiazzi components are practical becuase they are constructed out of standard Java source code. Jiazzi requires neither extensions to the Java language nor special conventions for writing Java source code that will go inside a component. Our components are expressive becuase Jiazzi supports cyclic component linking and mixins, which are used together in an open class pattern that enables the modular addition of new features to existing classes. This paper describes Jiazzi, how it enhances Java with components, its implementation, and how type checking works. An implementation of Jiazzi is available for download.}},
  url = {https://doi.org/10.1145/504282.504298},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup540,
  title = {Verifying higher-order functional programs with pattern-matching algebraic data types}},
  author = {Ong, C.-H. Luke and Ramsay, Steven J.}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Type-based model checking algorithms for higher-order recursion schemes have recently emerged as a promising approach to the verification of functional programs. We introduce pattern-matching recursion schemes (PMRS) as an accurate model of computation for functional programs that manipulate algebraic data-types. PMRS are a natural extension of higher-order recursion schemes that incorporate pattern-matching in the defining rules.This paper is concerned with the following (undecidable) verification problem: given a correctness property φ, a functional program ℘ (qua PMRS) and a regular input set ℑ, does every term that is reachable from ℑ under rewriting by ℘ satisfy φ? To solve the PMRS verification problem, we present a sound semi-algorithm which is based on model-checking and counterexample guided abstraction refinement. Given a no-instance of the verification problem, the method is guaranteed to terminate.From an order-n PMRS and an input set generated by a regular tree grammar, our method constructs an order-n weak PMRS which over-approximates only the first-order pattern-matching behaviour, whilst remaining completely faithful to the higher-order control flow. Using a variation of Kobayashi's type-based approach, we show that the (trivial automaton) model-checking problem for weak PMRS is decidable. When a violation of the property is detected in the abstraction which does not correspond to a violation in the model, the abstraction is automatically refined by `unfolding' the pattern-matching rules in the program to give successively more and more accurate weak PMRS models.}},
  url = {https://doi.org/10.1145/1926385.1926453},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup541,
  title = {Dependent types and program equivalence}},
  author = {Jia, Limin and Zhao, Jianzhou and Sj\"{o}},
  year = {2010}},
  journal = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The definition of type equivalence is one of the most important design issues for any typed language. In dependently typed languages, because terms appear in types, this definition must rely on a definition of term equivalence. In that case, decidability of type checking requires decidability for the term equivalence relation.Almost all dependently-typed languages require this relation to be decidable. Some, such as Coq, Epigram or Agda, do so by employing analyses to force all programs to terminate. Conversely, others, such as DML, ATS, Ωmega, or Haskell, allow nonterminating computation, but do not allow those terms to appear in types. Instead, they identify a terminating index language and use singleton types to connect indices to computation. In both cases, decidable type checking comes at a cost, in terms of complexity and expressiveness.Conversely, the benefits to be gained by decidable type checking are modest. Termination analyses allow dependently typed programs to verify total correctness properties. However, decidable type checking is not a prerequisite for type safety. Furthermore, decidability does not imply tractability. A decidable approximation of program equivalence may not be useful in practice.Therefore, we take a different approach: instead of a fixed notion for term equivalence, we parameterize our type system with an abstract relation that is not necessarily decidable. We then design a novel set of typing rules that require only weak properties of this abstract relation in the proof of the preservation and progress lemmas. This design provides flexibility: we compare valid instantiations of term equivalence which range from beta-equivalence, to contextual equivalence, to some exotic equivalences.}},
  url = {https://doi.org/10.1145/1706299.1706333},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup542,
  title = {Understanding idiomatic traversals backwards and forwards}},
  author = {Bird, Richard and Gibbons, Jeremy and Mehner, Stefan and Voigtl\"{a}},
  year = {2013}},
  journal = {Proceedings of the 2013 ACM SIGPLAN Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present new ways of reasoning about a particular class of effectful Haskell programs, namely those expressed as idiomatic traversals. Starting out with a specific problem about labelling and unlabelling binary trees, we extract a general inversion law, applicable to any monad, relating a traversal over the elements of an arbitrary traversable type to a traversal that goes in the opposite direction. This law can be invoked to show that, in a suitable sense, unlabelling is the inverse of labelling. The inversion law, as well as a number of other properties of idiomatic traversals, is a corollary of a more general theorem characterising traversable functors as finitary containers: an arbitrary traversable object can be decomposed uniquely into shape and contents, and traversal be understood in terms of those. Proof of the theorem involves the properties of traversal in a special idiom related to the free applicative functor.}},
  url = {https://doi.org/10.1145/2503778.2503781},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup543,
  title = {Datafun: a functional Datalog}},
  author = {Arntzenius, Michael and Krishnaswami, Neelakantan R.}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Datalog may be considered either an unusually powerful query language or a carefully limited logic programming language. Datalog is declarative, expressive, and optimizable, and has been applied successfully in a wide variety of problem domains. However, most use-cases require extending Datalog in an application-specific manner. In this paper we define Datafun, an analogue of Datalog supporting higher-order functional programming. The key idea is to track monotonicity with types.}},
  url = {https://doi.org/10.1145/2951913.2951948},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup544,
  title = {Mtac: a monad for typed tactic programming in Coq}},
  author = {Ziliani, Beta and Dreyer, Derek and Krishnaswami, Neelakantan R. and Nanevski, Aleksandar and Vafeiadis, Viktor}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Effective support for custom proof automation is essential for large scale interactive proof development. However, existing languages for automation via *tactics* either (a) provide no way to specify the behavior of tactics within the base logic of the accompanying theorem prover, or (b) rely on advanced type-theoretic machinery that is not easily integrated into established theorem provers.We present Mtac, a lightweight but powerful extension to Coq that supports dependently-typed tactic programming. Mtac tactics have access to all the features of ordinary Coq programming, as well as a new set of typed tactical primitives. We avoid the need to touch the trusted kernel typechecker of Coq by encapsulating uses of these new tactical primitives in a *monad*, and instrumenting Coq so that it executes monadic tactics during type inference.}},
  url = {https://doi.org/10.1145/2500365.2500579},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup545,
  title = {An operational semantics for C/C++11 concurrency}},
  author = {Nienhuis, Kyndylan and Memarian, Kayvan and Sewell, Peter}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The C/C++11 concurrency model balances two goals: it is relaxed enough to be efficiently implementable and (leaving aside the ``thin-air'' problem) it is strong enough to give useful guarantees to programmers. It is mathematically precise and has been used in verification research and compiler testing. However, the model is expressed in an axiomatic style, as predicates on complete candidate executions. This suffices for computing the set of allowed executions of a small litmus test, but it does not directly support the incremental construction of executions of larger programs. It is also at odds with conventional operational semantics, as used implicitly in the rest of the C/C++ standards. Our main contribution is the development of an operational model for C/C++11 concurrency. This covers all the features of the previous formalised axiomatic model, and we have a mechanised proof that the two are equivalent, in Isabelle/HOL. We also integrate this semantics with an operational semantics for sequential C (described elsewhere); the combined semantics can incrementally execute programs in a small fragment of C. Doing this uncovered several new aspects of the C/C++11 model: we show that one cannot build an equivalent operational model that simply follows program order, sequential consistent order, or the synchronises-with order. The first negative result is forced by hardware-observable behaviour, but the latter two are not, and so might be ameliorated by changing C/C++11. More generally, we hope that this work, with its focus on incremental construction of executions, will inform the future design of new concurrency models.}},
  url = {https://doi.org/10.1145/2983990.2983997},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup546,
  title = {Polymorphic type inference for machine code}},
  author = {Noonan, Matt and Loginov, Alexey and Cok, David}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {For many compiled languages, source-level types are erased very early in the compilation process. As a result, further compiler passes may convert type-safe source into type-unsafe machine code. Type-unsafe idioms in the original source and type-unsafe optimizations mean that type information in a stripped binary is essentially nonexistent. The problem of recovering high-level types by performing type inference over stripped machine code is called type reconstruction, and offers a useful capability in support of reverse engineering and decompilation. In this paper, we motivate and develop a novel type system and algorithm for machine-code type inference. The features of this type system were developed by surveying a wide collection of common source- and machine-code idioms, building a catalog of challenging cases for type reconstruction. We found that these idioms place a sophisticated set of requirements on the type system, inducing features such as recursively-constrained polymorphic types. Many of the features we identify are often seen only in expressive and powerful type systems used by high-level functional languages. Using these type-system features as a guideline, we have developed Retypd: a novel static type-inference algorithm for machine code that supports recursive types, polymorphism, and subtyping. Retypd yields more accurate inferred types than existing algorithms, while also enabling new capabilities such as reconstruction of pointer const annotations with 98\% recall. Retypd can operate on weaker program representations than the current state of the art, removing the need for high-quality points-to information that may be impractical to compute.}},
  url = {https://doi.org/10.1145/2908080.2908119},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup547,
  title = {Path invariants}},
  author = {Beyer, Dirk and Henzinger, Thomas A. and Majumdar, Rupak and Rybalchenko, Andrey}},
  year = {2007}},
  journal = {Proceedings of the 28th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The success of software verification depends on the ability to find a suitable abstraction of a program automatically. We propose a method for automated abstraction refinement which overcomes some limitations of current predicate discovery schemes. In current schemes, the cause of a false alarm is identified as an infeasible error path, and the abstraction is refined in order to remove that path. By contrast, we view the cause of a false alarm -the spurious counterexample- as a full-fledged program, namely, a fragment of the original program whose control-flow graph may contain loops and represent unbounded computations. There are two advantages to using such path programs as counterexamples for abstraction refinement. First, we can bring the whole machinery of program analysis to bear on path programs, which are typically small compared to the original program. Specifically, we use constraint-based invariant generation to automatically infer invariants of path programs-so-called path invariants. Second, we use path invariants for abstraction refinement in order to remove not one infeasibility at a time, but at once all (possibly infinitely many) infeasible error computations that are represented by a path program. Unlike previous predicate discovery schemes, our method handles loops without unrolling them; it infers abstractions that involve universal quantification and naturally incorporates disjunctive reasoning.}},
  url = {https://doi.org/10.1145/1250734.1250769},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup548,
  title = {Actors as a special case of concurrent constraint (logic) programming}},
  author = {Kahn, K. and Saraswat, Vijay A.}},
  year = {1990}},
  journal = {Proceedings of the European Conference on Object-Oriented Programming on Object-Oriented Programming Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Saraswat recently introduced the framework of concurrent constraint programming [14]. The essence of the framework is that computations consist of concurrent agents interacting by communicating constraints. Several concurrent constraint programming languages have been defined. They differ in the kinds of constraints that can be used as well as the kinds of operations on constraints which are available. In this paper we introduce a very simple concurrent constraint language we call Lucy, designed to closely mimic the actor model of computation. Agents can communicate only by the posting of constraints upon bags (un-ordered collections possibly with duplicate elements). This very impoverished concurrent constraint language is a syntactic subset of Janus, a concurrent constraint language which closely resembles concurrent logic programming languages such as Guarded Horn Clauses [21], Strand [5], Parlog [2] and Flat Concurrent Prolog [13]. By identifying the subset of Janus which is an actor language, we elucidate the relationship between actors and concurrent logic programming (and its generalization as concurrent constraint programming). Lucy is best not thought of as a unification of logic and constraint programming with actor and object-oriented programming, but as the missing link between these programming language genera.}},
  url = {https://doi.org/10.1145/97945.97955},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup549,
  title = {Bridge the Gap between Neural Networks and Neuromorphic Hardware with a Neural Network Compiler}},
  author = {Ji, Yu and Zhang, Youhui and Chen, Wenguang and Xie, Yuan}},
  year = {2018}},
  journal = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Different from developing neural networks (NNs) for general-purpose processors, the development for NN chips usually faces with some hardware-specific restrictions, such as limited precision of network signals and parameters, constrained computation scale, and limited types of non-linear functions. This paper proposes a general methodology to address the challenges. We decouple the NN applications from the target hardware by introducing a compiler that can transform an existing trained, unrestricted NN into an equivalent network that meets the given hardware's constraints. We propose multiple techniques to make the transformation adaptable to different kinds of NN chips, and reliable for restrict hardware constraints. We have built such a software tool that supports both spiking neural networks (SNNs) and traditional artificial neural networks (ANNs). We have demonstrated its effectiveness with a fabricated neuromorphic chip and a processing-in-memory (PIM) design. Tests show that the inference error caused by this solution is insignificant and the transformation time is much shorter than the retraining time. Also, we have studied the parameter-sensitivity evaluations to explore the tradeoffs between network error and resource utilization for different transformation strategies, which could provide insights for co-design optimization of neuromorphic hardware and software.}},
  url = {https://doi.org/10.1145/3173162.3173205},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup550,
  title = {Synthesizing program input grammars}},
  author = {Bastani, Osbert and Sharma, Rahul and Aiken, Alex and Liang, Percy}},
  year = {2017}},
  journal = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present an algorithm for synthesizing a context-free grammar encoding the language of valid program inputs from a set of input examples and blackbox access to the program. Our algorithm addresses shortcomings of existing grammar inference algorithms, which both severely overgeneralize and are prohibitively slow. Our implementation, GLADE, leverages the grammar synthesized by our algorithm to fuzz test programs with structured inputs. We show that GLADE substantially increases the incremental coverage on valid inputs compared to two baseline fuzzers.}},
  url = {https://doi.org/10.1145/3062341.3062349},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup551,
  title = {Overhauling SC atomics in C11 and OpenCL}},
  author = {Batty, Mark and Donaldson, Alastair F. and Wickerson, John}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Despite the conceptual simplicity of sequential consistency (SC), the semantics of SC atomic operations and fences in the C11 and OpenCL memory models is subtle, leading to convoluted prose descriptions that translate to complex axiomatic formalisations. We conduct an overhaul of SC atomics in C11, reducing the associated axioms in both number and complexity. A consequence of our simplification is that the SC operations in an execution no longer need to be totally ordered. This relaxation enables, for the first time, efficient and exhaustive simulation of litmus tests that use SC atomics. We extend our improved C11 model to obtain the first rigorous memory model formalisation for OpenCL (which extends C11 with support for heterogeneous many-core programming). In the OpenCL setting, we refine the SC axioms still further to give a sensible semantics to SC operations that employ a ‘memory scope’ to restrict their visibility to specific threads. Our overhaul requires slight strengthenings of both the C11 and the OpenCL memory models, causing some behaviours to become disallowed. We argue that these strengthenings are natural, and that all of the formalised C11 and OpenCL compilation schemes of which we are aware (Power and x86 CPUs for C11, AMD GPUs for OpenCL) remain valid in our revised models. Using the HERD memory model simulator, we show that our overhaul leads to an exponential improvement in simulation time for C11 litmus tests compared with the original model, making *exhaustive* simulation competitive, time-wise, with the *non-exhaustive* CDSChecker tool.}},
  url = {https://doi.org/10.1145/2837614.2837637},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup552,
  title = {High-performance client-side web applications through Haskell EDSLs}},
  author = {Ekblad, Anton}},
  year = {2016}},
  journal = {Proceedings of the 9th International Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present Aplite, a domain-specific language embedded in Haskell for implementing performance-critical functions in client-side web applications. In Aplite, we apply partial evaluation, multi-stage programming and techniques adapted from machine code-targeting, high-performance EDSLs to the domain of web applications. We use Aplite to implement, among other benchmarks, procedural animation using Perlin noise, symmetrical encryption and K-means clustering, showing Aplite to be consistently faster than equivalent hand-written JavaScript -- up to an order of magnitude for some benchmarks. We also demonstrate how Aplite's multi-staged nature can be used to automatically tune programs to the environment in which they are running, as well as to inputs representative of the programs' intended workload. High-performance computation in the web browser is an attractive goal for many reasons: interactive simulations and games, cryptographic applications and reducing web companies' electricity bills by outsourcing expensive computations to users' web browsers. Similarly, functional programming in the browser is attractive due to its promises of simpler, shorter, safer programs. In this paper, we propose a way to combine the two.}},
  url = {https://doi.org/10.1145/2976002.2976015},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup553,
  title = {Scaling network verification using symmetry and surgery}},
  author = {Plotkin, Gordon D. and Bj\o{}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {On the surface, large data centers with about 100,000 stations and nearly a million routing rules are complex and hard to verify. However, these networks are highly regular by design; for example they employ fat tree topologies with backup routers interconnected by redundant patterns. To exploit these regularities, we introduce network transformations: given a reachability formula and a network, we transform the network into a simpler to verify network and a corresponding transformed formula, such that the original formula is valid in the network if and only if the transformed formula is valid in the transformed network. Our network transformations exploit network surgery (in which irrelevant or redundant sets of nodes, headers, ports, or rules are ``sliced'' away) and network symmetry (say between backup routers). The validity of these transformations is established using a formal theory of networks. In particular, using Van Benthem-Hennessy-Milner style bisimulation, we show that one can generally associate bisimulations to transformations connecting networks and formulas with their transforms. Our work is a development in an area of current wide interest: applying programming language techniques (in our case bisimulation and modal logic) to problems in switching networks. We provide experimental evidence that our network transformations can speed up by 65x the task of verifying the communication between all pairs of Virtual Machines in a large datacenter network with about 100,000 VMs. An all-pair reachability calculation, which formerly took 5.5 days, can be done in 2 hours, and can be easily parallelized to complete in}},
  url = {https://doi.org/10.1145/2837614.2837657},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup554,
  title = {Breaking through the normalization barrier: a self-interpreter for f-omega}},
  author = {Brown, Matt and Palsberg, Jens}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {According to conventional wisdom, a self-interpreter for a strongly normalizing lambda-calculus is impossible. We call this the normalization barrier. The normalization barrier stems from a theorem in computability theory that says that a total universal function for the total computable functions is impossible. In this paper we break through the normalization barrier and define a self-interpreter for System F_omega, a strongly normalizing lambda-calculus. After a careful analysis of the classical theorem, we show that static type checking in F_omega can exclude the proof's diagonalization gadget, leaving open the possibility for a self-interpreter. Along with the self-interpreter, we program four other operations in F_omega, including a continuation-passing style transformation. Our operations rely on a new approach to program representation that may be useful in theorem provers and compilers.}},
  url = {https://doi.org/10.1145/2837614.2837623},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup555,
  title = {Snugglebug: a powerful approach to weakest preconditions}},
  author = {Chandra, Satish and Fink, Stephen J. and Sridharan, Manu}},
  year = {2009}},
  journal = {Proceedings of the 30th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Symbolic analysis shows promise as a foundation for bug-finding, specification inference, verification, and test generation. This paper addresses demand-driven symbolic analysis for object-oriented programs and frameworks. Many such codes comprise large, partial programs with highly dynamic behaviors--polymorphism, reflection, and so on--posing significant scalability challenges for any static analysis.We present an approach based on interprocedural backwards propagation of weakest preconditions. We present several novel techniques to improve the efficiency of such analysis. First, we present directed call graph construction, where call graph construction and symbolic analysis are interleaved. With this technique, call graph construction is guided by constraints discovered during symbolic analysis, obviating the need for exhaustively exploring a large, conservative call graph. Second, we describe generalization, a technique that greatly increases the reusability of procedure summaries computed during interprocedural analysis. Instead of tabulating how a procedure transforms a symbolic state in its entirety, our technique tabulates how the procedure transforms only the pertinent portion of the symbolic state. Additionally, we show how integrating an inexpensive, custom logic simplifier with weakest precondition computation dramatically improves performance.We have implemented the analysis in a tool called Snugglebug and evaluated it as a bug-report feasibility checker. Our results show that the algorithmic techniques were critical for successfully analyzing large Java applications.}},
  url = {https://doi.org/10.1145/1542476.1542517},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup556,
  title = {Liveness-based garbage collection for lazy languages}},
  author = {Kumar K., Prasanna and Sanyal, Amitabha and Karkare, Amey}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Symposium on Memory Management}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We consider the problem of reducing the memory required to run lazy first-order functional programs. Our approach is to analyze programs for liveness of heap-allocated data. The result of the analysis is used to preserve only live data—a subset of reachable data—during garbage collection. The result is an increase in the garbage reclaimed and a reduction in the peak memory requirement of programs. Whereas this technique has already been shown to yield benefits for eager first-order languages, the lack of a statically determinable execution order and the presence of closures pose new challenges for lazy languages. These require changes both in the liveness analysis itself and in the design of the garbage collector. To show the effectiveness of our method, we implemented a copying collector that uses the results of the liveness analysis to preserve live objects, both evaluated and closures. Our experiments confirm that for programs running with a liveness-based garbage collector, there is a significant decrease in peak memory requirements. In addition, a sizable reduction in the number of collections ensures that in spite of using a more complex garbage collector, the execution times of programs running with liveness and reachability-based collectors remain comparable.}},
  url = {https://doi.org/10.1145/2926697.2926698},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup557,
  title = {The power of parameterization in coinductive proof}},
  author = {Hur, Chung-Kil and Neis, Georg and Dreyer, Derek and Vafeiadis, Viktor}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Coinduction is one of the most basic concepts in computer science. It is therefore surprising that the commonly-known lattice-theoretic accounts of the principles underlying coinductive proofs are lacking in two key respects: they do not support compositional reasoning (i.e. breaking proofs into separate pieces that can be developed in isolation), and they do not support incremental reasoning (i.e. developing proofs interactively by starting from the goal and generalizing the coinduction hypothesis repeatedly as necessary).In this paper, we show how to support coinductive proofs that are both compositional and incremental, using a dead simple construction we call the parameterized greatest fixed point. The basic idea is to parameterize the greatest fixed point of interest over the accumulated knowledge of "the proof so far". While this idea has been proposed before, by Winskel in 1989 and by Moss in 2001, neither of the previous accounts suggests its general applicability to improving the state of the art in interactive coinductive proof.In addition to presenting the lattice-theoretic foundations of parameterized coinduction, demonstrating its utility on representative examples, and studying its composition with "up-to" techniques, we also explore its mechanization in proof assistants like Coq and Isabelle. Unlike traditional approaches to mechanizing coinduction (e.g. Coq's cofix), which employ syntactic "guardedness checking", parameterized coinduction offers a semantic account of guardedness. This leads to faster and more robust proof development, as we demonstrate using our new Coq library, Paco.}},
  url = {https://doi.org/10.1145/2429069.2429093},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup558,
  title = {Krivine nets: a semantic foundation for distributed execution}},
  author = {Fredriksson, Olle and Ghica, Dan R.}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We define a new approach to compilation to distributed architectures based on networks of abstract machines. Using it we can implement a generalised and fully transparent form of Remote Procedure Call that supports calling higher-order functions across node boundaries, without sending actual code. Our starting point is the classic Krivine machine, which implements reduction for untyped call-by-name PCF. We successively add the features that we need for distributed execution and show the correctness of each addition. Then we construct a two-level operational semantics, where the high level is a network of communicating machines, and the low level is given by local machine transitions. Using these networks, we arrive at our final system, the Krivine Net. We show that Krivine Nets give a correct distributed implementation of the Krivine machine, which preserves both termination and non-termination properties. All the technical results have been formalised and proved correct in Agda. We also implement a prototype compiler which we compare with previous distributing compilers based on Girard's Geometry of Interaction and on Game Semantics.}},
  url = {https://doi.org/10.1145/2628136.2628152},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup559,
  title = {On global dynamics of optimal graph reduction}},
  author = {Lawall, Julia L. and Mairson, Harry G.}},
  year = {1997}},
  journal = {Proceedings of the Second ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Optimal graph reduction technology for the λ-calculus, as developed by Lamping, with modifications by Asperti, Gonthier, Abadl, and Lévy, has a well-understood local dynamics based on a standard menagerie of reduction rules, as well as a global context semantics based on Girard's geometry of interaction. However, the global dynamics of graph reduction has not been subject to careful investigation. In particular, graphs lose their structural resemblance to λ-terms after only a few graph reduction steps, and little is known about graph reduction strategies that maintain efficiency or structure. While the context semantics provides global information about the computation, its use as part of a reduction strategy seems computationally infeasible. We propose a tractable graph reduction strategy that preserves computationally relevant global structure, and allows us to efficiently bound the computational resources needed to implement optimal reduction.A simple canonical representation for graphs is introduced that we call fan-normal form. This normal form allows us to reduce graphs based on efficient identification of β-redexes, rather than being guided by lower-level search for interacting nodes. In addition and perhaps more important, the fan-normal form facilitates a complexity analysis of graph reductions, showing that the number of fan interactions is essentially bounded by a polynomial in the number of unique Lévy labels generated during a labelled reduction. This global analysis of the finitary dynamics of optimal reduction is the first demonstration that a reasonable implementation-independent cost model for the λ-calculus is in fact realized by Lamping's abstract algorithm. It remains to be seen whether similar claims can be made about so-called bookkeeping for fan interactions.}},
  url = {https://doi.org/10.1145/258948.258966},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup560,
  title = {Resourceable, retargetable, modular instruction selection using a machine-independent, type-based tiling of low-level intermediate code}},
  author = {Ramsey, Norman and Dias, Jo\~{a}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a novel variation on the standard technique of selecting instructions by tiling an intermediate-code tree. Typical compilers use a different set of tiles for every target machine. By analyzing a formal model of machine-level computation, we have developed a single set of tiles that is machine-independent while retaining the expressive power of machine code. Using this tileset, we reduce the number of tilers required from one per machine to one per architectural family (e.g., register architecture or stack architecture). Because the tiler is the part of the instruction selector that is most difficult to reason about, our technique makes it possible to retarget an instruction selector with significantly less effort than standard techniques. Retargeting effort is further reduced by applying an earlier result which generates the machine-dependent implementation of our tileset automatically from a declarative description of instructions' semantics. Our design has the additional benefit of enabling modular reasoning about three aspects of code generation that are not typically separated: the semantics of the compiler's intermediate representation, the semantics of the target instruction set, and the techniques needed to generate good target code.}},
  url = {https://doi.org/10.1145/1926385.1926451},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup561,
  title = {Analyzing Program Analyses}},
  author = {Giacobazzi, Roberto and Logozzo, Francesco and Ranzato, Francesco}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We want to prove that a static analysis of a given program is complete, namely, no imprecision arises when asking some query on the program behavior in the concrete (ie, for its concrete semantics) or in the abstract (ie, for its abstract interpretation). Completeness proofs are therefore useful to assign confidence to alarms raised by static analyses. We introduce the completeness class of an abstraction as the set of all programs for which the abstraction is complete. Our first result shows that for any nontrivial abstraction, its completeness class is not recursively enumerable. We then introduce a stratified deductive system to prove the completeness of program analyses over an abstract domain A. We prove the soundness of the deductive system. We observe that the only sources of incompleteness are assignments and Boolean tests --- unlikely a common belief in static analysis, joins do not induce incompleteness. The first layer of this proof system is generic, abstraction-agnostic, and it deals with the standard constructs for program composition, that is, sequential composition, branching and guarded iteration. The second layer is instead abstraction-specific: the designer of an abstract domain A provides conditions for completeness in A of assignments and Boolean tests which have to be checked by a suitable static analysis or assumed in the completeness proof as hypotheses. We instantiate the second layer of this proof system first with a generic nonrelational abstraction in order to provide a sound rule for the completeness of assignments. Orthogonally, we instantiate it to the numerical abstract domains of Intervals and Octagons, providing necessary and sufficient conditions for the completeness of their Boolean tests and of assignments for Octagons.}},
  url = {https://doi.org/10.1145/2676726.2676987},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup562,
  title = {Checking race freedom via linear programming}},
  author = {Terauchi, Tachio}},
  year = {2008}},
  journal = {Proceedings of the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a new static analysis for race freedom and race detection. The analysis checks race freedom by reducing the problem to (rational) linear programming. Unlike conventional static analyses for race freedom or race detection, our analysis avoids explicit computation of locksets and lock linearity/must-aliasness. Our analysis can handle a variety of synchronization idioms that more conventional approaches often have difficulties with, such as thread joining, semaphores, and signals. We achieve efficiency by utilizing modern linear programming solvers that can quickly solve large linear programming instances. This paper reports on the formal properties of the analysis and the experience with applying an implementation to real world C programs.}},
  url = {https://doi.org/10.1145/1375581.1375583},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup563,
  title = {TETRIS: Scalable and Efficient Neural Network Acceleration with 3D Memory}},
  author = {Gao, Mingyu and Pu, Jing and Yang, Xuan and Horowitz, Mark and Kozyrakis, Christos}},
  year = {2017}},
  journal = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The high accuracy of deep neural networks (NNs) has led to the development of NN accelerators that improve performance by two orders of magnitude. However, scaling these accelerators for higher performance with increasingly larger NNs exacerbates the cost and energy overheads of their memory systems, including the on-chip SRAM buffers and the off-chip DRAM channels.This paper presents the hardware architecture and software scheduling and partitioning techniques for TETRIS, a scalable NN accelerator using 3D memory. First, we show that the high throughput and low energy characteristics of 3D memory allow us to rebalance the NN accelerator design, using more area for processing elements and less area for SRAM buffers. Second, we move portions of the NN computations close to the DRAM banks to decrease bandwidth pressure and increase performance and energy efficiency. Third, we show that despite the use of small SRAM buffers, the presence of 3D memory simplifies dataflow scheduling for NN computations. We present an analytical scheduling scheme that matches the efficiency of schedules derived through exhaustive search. Finally, we develop a hybrid partitioning scheme that parallelizes the NN computations over multiple accelerators. Overall, we show that TETRIS improves mthe performance by 4.1x and reduces the energy by 1.5x over NN accelerators with conventional, low-power DRAM memory systems.}},
  url = {https://doi.org/10.1145/3037697.3037702},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup564,
  title = {Refining semantics for multi-stage programming}},
  author = {Ge, Rui and Garcia, Ronald}},
  year = {2017}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The multi-stage programming paradigm supports runtime code generation and execution. Though powerful, its potential is impeded by the lack of static analysis support. Van Horn and Might proposed a general-purpose approach to systematically develop static analyses by transforming an environmental abstract machine, which evolves a control string, an environment and a continuation as a program evaluates. To the best of our knowledge, no such semantics exists for a multi-stage language like MetaML. We develop and prove correct an environmental abstract machine semantics for MetaML by gradually refining the reference substitutional structural operational semantics. Highlights of our approach include leveraging explicit substitutions to bridge the gap between substitutional and environmental semantics, and devising meta-environments to model the complexities of variable bindings in multi-stage environmental semantics.}},
  url = {https://doi.org/10.1145/3136040.3136047},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup565,
  title = {Backpack: retrofitting Haskell with interfaces}},
  author = {Kilpatrick, Scott and Dreyer, Derek and Peyton Jones, Simon and Marlow, Simon}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Module systems like that of Haskell permit only a weak form of modularity in which module implementations depend directly on other implementations and must be processed in dependency order. Module systems like that of ML, on the other hand, permit a stronger form of modularity in which explicit interfaces express assumptions about dependencies, and each module can be typechecked and reasoned about independently.In this paper, we present Backpack, a new language for building separately-typecheckable *packages* on top of a weak module system like Haskell's. The design of Backpack is inspired by the MixML module calculus of Rossberg and Dreyer, but differs significantly in detail. Like MixML, Backpack supports explicit interfaces and recursive linking. Unlike MixML, Backpack supports a more flexible applicative semantics of instantiation. Moreover, its design is motivated less by foundational concerns and more by the practical concern of integration into Haskell, which has led us to advocate simplicity---in both the syntax and semantics of Backpack---over raw expressive power. The semantics of Backpack packages is defined by elaboration to sets of Haskell modules and binary interface files, thus showing how Backpack maintains interoperability with Haskell while extending it with separate typechecking. Lastly, although Backpack is geared toward integration into Haskell, its design and semantics are largely agnostic with respect to the details of the underlying core language.}},
  url = {https://doi.org/10.1145/2535838.2535884},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup566,
  title = {A formal C memory model supporting integer-pointer casts}},
  author = {Kang, Jeehoon and Hur, Chung-Kil and Mansky, William and Garbuzov, Dmitri and Zdancewic, Steve and Vafeiadis, Viktor}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The ISO C standard does not specify the semantics of many valid programs that use non-portable idioms such as integer-pointer casts. Recent efforts at formal definitions and verified implementation of the C language inherit this feature. By adopting high-level abstract memory models, they validate common optimizations. On the other hand, this prevents reasoning about much low-level code relying on the behavior of common implementations, where formal verification has many applications. We present the first formal memory model that allows many common optimizations and fully supports operations on the representation of pointers. All arithmetic operations are well-defined for pointers that have been cast to integers. Crucially, our model is also simple to understand and program with. All our results are fully formalized in Coq.}},
  url = {https://doi.org/10.1145/2737924.2738005},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup567,
  title = {Automatic Hierarchical Parallelization of Linear Recurrences}},
  author = {Maleki, Sepideh and Burtscher, Martin}},
  year = {2018}},
  journal = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Linear recurrences encompass many fundamental computations including prefix sums and digital filters. Later result values depend on earlier result values in recurrences, making it a challenge to compute them in parallel. We present a new work- and space-efficient algorithm to compute linear recurrences that is amenable to automatic parallelization and suitable for hierarchical massively-parallel architectures such as GPUs. We implemented our approach in a domain-specific code generator that emits optimized CUDA code. Our evaluation shows that, for standard prefix sums and single-stage IIR filters, the generated code reaches the throughput of memory copy for large inputs, which cannot be surpassed. On higher-order prefix sums, it performs nearly as well as the fastest handwritten code from the literature. On tuple-based prefix sums and digital filters, our automatically parallelized code outperforms the fastest prior implementations.}},
  url = {https://doi.org/10.1145/3173162.3173168},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup568,
  title = {Learning invariants using decision trees and implication counterexamples}},
  author = {Garg, Pranav and Neider, Daniel and Madhusudan, P. and Roth, Dan}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Inductive invariants can be robustly synthesized using a learning model where the teacher is a program verifier who instructs the learner through concrete program configurations, classified as positive, negative, and implications. We propose the first learning algorithms in this model with implication counter-examples that are based on machine learning techniques. In particular, we extend classical decision-tree learning algorithms in machine learning to handle implication samples, building new scalable ways to construct small decision trees using statistical measures. We also develop a decision-tree learning algorithm in this model that is guaranteed to converge to the right concept (invariant) if one exists. We implement the learners and an appropriate teacher, and show that the resulting invariant synthesis is efficient and convergent for a large suite of programs.}},
  url = {https://doi.org/10.1145/2837614.2837664},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup569,
  title = {Contention in Structured Concurrency: Provably Efficient Dynamic Non-Zero Indicators for Nested Parallelism}},
  author = {Acar, Umut A. and Ben-David, Naama and Rainey, Mike}},
  year = {2017}},
  journal = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Over the past two decades, many concurrent data structures have been designed and implemented. Nearly all such work analyzes concurrent data structures empirically, omitting asymptotic bounds on their efficiency, partly because of the complexity of the analysis needed, and partly because of the difficulty of obtaining relevant asymptotic bounds: when the analysis takes into account important practical factors, such as contention, it is difficult or even impossible to prove desirable bounds.In this paper, we show that considering structured concurrency or relaxed concurrency models can enable establishing strong bounds, also for contention. To this end, we first present a dynamic relaxed counter data structure that indicates the non-zero status of the counter. Our data structure extends a recently proposed data structure, called SNZI, allowing our structure to grow dynamically in response to the increasing degree of concurrency in the system.Using the dynamic SNZI data structure, we then present a concurrent data structure for series-parallel directed acyclic graphs (sp-dags), a key data structure widely used in the implementation of modern parallel programming languages. The key component of sp-dags is an in-counter data structure that is an instance of our dynamic SNZI. We analyze the efficiency of our concurrent sp-dags and in-counter data structures under nested-parallel computing paradigm. This paradigm offers a structured model for concurrency. Under this model, we prove that our data structures require amortized (1) shared memory steps, including contention. We present an implementation and an experimental evaluation that suggests that the sp-dags data structure is practical and can perform well in practice.}},
  url = {https://doi.org/10.1145/3018743.3018762},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup570,
  title = {Syntactic control of interference for separation logic}},
  author = {Reddy, Uday S. and Reynolds, John C.}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Separation Logic has witnessed tremendous success in recent years in reasoning about programs that deal with heap storage. Its success owes to the fundamental principle that one should keep separate areas of the heap storage separate in program reasoning. However, the way Separation Logic deals with program variables continues to be based on traditional Hoare Logic without taking any benefit of the separation principle. This has led to unwieldy proof rules suffering from lack of clarity as well as questions surrounding their soundness. In this paper, we extend the separation idea to the treatment of variables in Separation Logic, especially Concurrent Separation Logic, using the system of Syntactic Control of Interference proposed by Reynolds in 1978. We extend the original system with permission algebras, making it more powerful and able to deal with the issues of concurrent programs. The result is a streamined presentation of Concurrent Separation Logic, whose rules are memorable and soundness obvious. We also include a discussion of how the new rules impact the semantics and devise static analysis techniques to infer the required permissions automatically.}},
  url = {https://doi.org/10.1145/2103656.2103695},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup571,
  title = {CUBA: interprocedural Context-UnBounded Analysis of concurrent programs}},
  author = {Liu, Peizun and Wahl, Thomas}},
  year = {2018}},
  journal = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A classical result by Ramalingam about synchronization-sensitive interprocedural program analysis implies that reachability for concurrent threads running recursive procedures is undecidable. A technique proposed by Qadeer and Rehof, to bound the number of context switches allowed between the threads, leads to an incomplete solution that is, however, believed to catch “most bugs” in practice. The question whether the technique can also prove the absence of bugs at least in some cases has remained largely open. In this paper we introduce a broad verification methodology for resource-parameterized programs that observes how changes to the resource parameter affect the behavior of the program. Applied to the context-unbounded analysis problem (CUBA), the methodology results in partial verification techniques for procedural concurrent programs. Our solutions may not terminate, but are able to both refute and prove context-unbounded safety for concurrent recursive threads. We demonstrate the effectiveness of our method using a variety of examples, the safe of which cannot be proved safe by earlier, context-bounded methods.}},
  url = {https://doi.org/10.1145/3192366.3192419},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup572,
  title = {Engineering with logic: HOL specification and symbolic-evaluation testing for TCP implementations}},
  author = {Bishop, Steve and Fairbairn, Matthew and Norrish, Michael and Sewell, Peter and Smith, Michael and Wansbrough, Keith}},
  year = {2006}},
  journal = {Conference Record of the 33rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The TCP/IP protocols and Sockets API underlie much of modern computation, but their semantics have historically been very complex and ill-defined. The real standard is the de facto one of the common implementations, including, for example, the 15,000--20,000 lines of C in the BSD implementation. Dealing rigorously with the behaviour of such bodies of code is challenging.We have recently developed a post-hoc specification of TCP, UDP, and Sockets that is rigorous, detailed, readable, has broad coverage, and is remarkably accurate. In this paper we describe the novel techniques that were required.Working within a general-purpose proof assistant (HOL), we developed language idioms (within higher-order logic) in which to write the specification: operational semantics with nondeterminism, time, system calls, monadic relational programming, etc. We followed an experimental semantics approach, validating the specification against several thousand traces captured from three implementations (FreeBSD, Linux, and WinXP). Many differences between these were identified, and a number of bugs. Validation was done using a special-purpose symbolic model checker programmed above HOL.We suggest that similar logic engineering techniques could be applied to future critical software infrastructure at design time, leading to cleaner designs and (via specification-based testing using a similar checker) more predictable implementations.}},
  url = {https://doi.org/10.1145/1111037.1111043},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup573,
  title = {Kleenex: compiling nondeterministic transducers to deterministic streaming transducers}},
  author = {Grathwohl, Bj\o{}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present and illustrate Kleenex, a language for expressing general nondeterministic finite transducers, and its novel compilation to streaming string transducers with essentially optimal streaming behavior, worst-case linear-time performance and sustained high throughput. Its underlying theory is based on transducer decomposition into oracle and action machines: the oracle machine performs streaming greedy disambiguation of the input; the action machine performs the output actions. In use cases Kleenex achieves consistently high throughput rates around the 1 Gbps range on stock hardware. It performs well, especially in complex use cases, in comparison to both specialized and related tools such as GNUawk, GNUsed, GNUgrep, RE2, Ragel and regular-expression libraries.}},
  url = {https://doi.org/10.1145/2837614.2837647},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup574,
  title = {PolyCheck: dynamic verification of iteration space transformations on affine programs}},
  author = {Bao, Wenlei and Krishnamoorthy, Sriram and Pouchet, Louis-No\"{e}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {High-level compiler transformations, especially loop transformations, are widely recognized as critical optimizations to restructure programs to improve data locality and expose parallelism. Guaranteeing the correctness of program transformations is essential, and to date three main approaches have been developed: proof of equivalence of affine programs, matching the execution traces of programs, and checking bit-by-bit equivalence of program outputs. Each technique suffers from limitations in the kind of transformations supported, space complexity, or the sensitivity to the testing dataset. In this paper, we take a novel approach that addresses all three limitations to provide an automatic bug checker to verify any iteration reordering transformations on affine programs, including non-affine transformations, with space consumption proportional to the original program data and robust to arbitrary datasets of a given size. We achieve this by exploiting the structure of affine program control- and data-flow to generate at compile-time lightweight checker code to be executed within the transformed program. Experimental results assess the correctness and effectiveness of our method and its increased coverage over previous approaches.}},
  url = {https://doi.org/10.1145/2837614.2837656},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup575,
  title = {Pilsner: a compositionally verified compiler for a higher-order imperative language}},
  author = {Neis, Georg and Hur, Chung-Kil and Kaiser, Jan-Oliver and McLaughlin, Craig and Dreyer, Derek and Vafeiadis, Viktor}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Compiler verification is essential for the construction of fully verified software, but most prior work (such as CompCert) has focused on verifying whole-program compilers. To support separate compilation and to enable linking of results from different verified compilers, it is important to develop a compositional notion of compiler correctness that is modular (preserved under linking), transitive (supports multi-pass compilation), and flexible (applicable to compilers that use different intermediate languages or employ non-standard program transformations). In this paper, building on prior work of Hur et al., we develop a novel approach to compositional compiler verification based on parametric inter-language simulations (PILS). PILS are modular: they enable compiler verification in a manner that supports separate compilation. PILS are transitive: we use them to verify Pilsner, a simple (but non-trivial) multi-pass optimizing compiler (programmed in Coq) from an ML-like source language S to an assembly-like target language T, going through a CPS-based intermediate language. Pilsner is the first multi-pass compiler for a higher-order imperative language to be compositionally verified. Lastly, PILS are flexible: we use them to additionally verify (1) Zwickel, a direct non-optimizing compiler for S, and (2) a hand-coded self-modifying T module, proven correct w.r.t. an S-level specification. The output of Zwickel and the self-modifying T module can then be safely linked together with the output of Pilsner. All together, this has been a significant undertaking, involving several person-years of work and over 55,000 lines of Coq.}},
  url = {https://doi.org/10.1145/2784731.2784764},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup576,
  title = {Lightweight verification of separate compilation}},
  author = {Kang, Jeehoon and Kim, Yoonseung and Hur, Chung-Kil and Dreyer, Derek and Vafeiadis, Viktor}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Major compiler verification efforts, such as the CompCert project, have traditionally simplified the verification problem by restricting attention to the correctness of whole-program compilation, leaving open the question of how to verify the correctness of separate compilation. Recently, a number of sophisticated techniques have been proposed for proving more flexible, compositional notions of compiler correctness, but these approaches tend to be quite heavyweight compared to the simple "closed simulations" used in verifying whole-program compilation. Applying such techniques to a compiler like CompCert, as Stewart et al. have done, involves major changes and extensions to its original verification. In this paper, we show that if we aim somewhat lower---to prove correctness of separate compilation, but only for a *single* compiler---we can drastically simplify the proof effort. Toward this end, we develop several lightweight techniques that recast the compositional verification problem in terms of whole-program compilation, thereby enabling us to largely reuse the closed-simulation proofs from existing compiler verifications. We demonstrate the effectiveness of these techniques by applying them to CompCert 2.4, converting its verification of whole-program compilation into a verification of separate compilation in less than two person-months. This conversion only required a small number of changes to the original proofs, and uncovered two compiler bugs along the way. The result is SepCompCert, the first verification of separate compilation for the full CompCert compiler.}},
  url = {https://doi.org/10.1145/2837614.2837642},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup577,
  title = {Causal commutative arrows and their optimization}},
  author = {Liu, Hai and Cheng, Eric and Hudak, Paul}},
  year = {2009}},
  journal = {Proceedings of the 14th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Arrows are a popular form of abstract computation. Being more general than monads, they are more broadly applicable, and in particular are a good abstraction for signal processing and dataflow computations. Most notably, arrows form the basis for a domain specific language called Yampa, which has been used in a variety of concrete applications, including animation, robotics, sound synthesis, control systems, and graphical user interfaces. Our primary interest is in better understanding the class of abstract computations captured by Yampa. Unfortunately, arrows are not concrete enough to do this with precision. To remedy this situation we introduce the concept of commutative arrows that capture a kind of non-interference property of concurrent computations. We also add an init operator, and identify a crucial law that captures the causal nature of arrow effects. We call the resulting computational model causal commutative arrows. To study this class of computations in more detail, we define an extension to the simply typed lambda calculus called causal commutative arrows (CCA), and study its properties. Our key contribution is the identification of a normal form for CCA called causal commutative normal form (CCNF). By defining a normalization procedure we have developed an optimization strategy that yields dramatic improvements in performance over conventional implementations of arrows. We have implemented this technique in Haskell, and conducted benchmarks that validate the effectiveness of our approach. When combined with stream fusion, the overall methodology can result in speed-ups of greater than two orders of magnitude.}},
  url = {https://doi.org/10.1145/1596550.1596559},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup578,
  title = {Synthesis of divide and conquer parallelism for loops}},
  author = {Farzan, Azadeh and Nicolet, Victor}},
  year = {2017}},
  journal = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Divide-and-conquer is a common parallel programming skeleton supported by many cross-platform multithreaded libraries, and most commonly used by programmers for parallelization. The challenges of producing (manually or automatically) a correct divide-and-conquer parallel program from a given sequential code are two-fold: (1) assuming that a good solution exists where individual worker threads execute a code identical to the sequential one, the programmer has to provide the extra code for dividing the tasks and combining the partial results (i.e. joins), and (2) the sequential code may not be suitable for divide-and-conquer parallelization as is, and may need to be modified to become a part of a good solution. We address both challenges in this paper. We present an automated synthesis technique to synthesize correct joins and an algorithm for modifying the sequential code to make it suitable for parallelization when necessary. This paper focuses on class of loops that traverse a read-only collection and compute a scalar function over that collection. We present theoretical results for when the necessary modifications to sequential code are possible, theoretical guarantees for the algorithmic solutions presented here, and experimental evaluation of the approach's success in practice and the quality of the produced parallel programs.}},
  url = {https://doi.org/10.1145/3062341.3062355},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup579,
  title = {Verification of high-level transformations with inductive refinement types}},
  author = {Al-Sibahi, Ahmad Salim and Jensen, Thomas P. and Dimovski, Aleksandar S. and W\k{a}},
  year = {2018}},
  journal = {Proceedings of the 17th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {High-level transformation languages like Rascal include expressive features for manipulating large abstract syntax trees: first-class traversals, expressive pattern matching, backtracking and generalized iterators. We present the design and implementation of an abstract interpretation tool, Rabit, for verifying inductive type and shape properties for transformations written in such languages. We describe how to perform abstract interpretation based on operational semantics, specifically focusing on the challenges arising when analyzing the expressive traversals and pattern matching. Finally, we evaluate Rabit on a series of transformations (normalization, desugaring, refactoring, code generators, type inference, etc.) showing that we can effectively verify stated properties.}},
  url = {https://doi.org/10.1145/3278122.3278125},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup580,
  title = {Monadic second-order logic on finite sequences}},
  author = {D'Antoni, Loris and Veanes, Margus}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We extend the weak monadic second-order logic of one successor on finite strings (M2L-STR) to symbolic alphabets by allowing character predicates to range over decidable quantifier free theories instead of finite alphabets. We call this logic, which is able to describe sequences over complex and potentially infinite domains, symbolic M2L-STR (S-M2L-STR). We then present a decision procedure for S-M2L-STR based on a reduction to symbolic finite automata, a decidable extension of finite automata that allows transitions to carry predicates and can therefore model symbolic alphabets. The reduction constructs a symbolic automaton over an alphabet consisting of pairs of symbols where the first element of the pair is a symbol in the original formula’s alphabet, while the second element is a bit-vector. To handle this modified alphabet we show that the Cartesian product of two decidable Boolean algebras (e.g., the formula’s one and the bit-vector’s one) also forms a decidable Boolean algebras. To make the decision procedure practical, we propose two efficient representations of the Cartesian product of two Boolean algebras, one based on algebraic decision diagrams and one on a variant of Shannon expansions. Finally, we implement our decision procedure and evaluate it on more than 10,000 formulas. Despite the generality, our implementation has comparable performance with the state-of-the-art M2L-STR solvers.}},
  url = {https://doi.org/10.1145/3009837.3009844},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup581,
  title = {Modelling the ARMv8 architecture, operationally: concurrency and ISA}},
  author = {Flur, Shaked and Gray, Kathryn E. and Pulte, Christopher and Sarkar, Susmit and Sezgin, Ali and Maranget, Luc and Deacon, Will and Sewell, Peter}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this paper we develop semantics for key aspects of the ARMv8 multiprocessor architecture: the concurrency model and much of the 64-bit application-level instruction set (ISA). Our goal is to clarify what the range of architecturally allowable behaviour is, and thereby to support future work on formal verification, analysis, and testing of concurrent ARM software and hardware. Establishing such models with high confidence is intrinsically difficult: it involves capturing the vendor's architectural intent, aspects of which (especially for concurrency) have not previously been precisely defined. We therefore first develop a concurrency model with a microarchitectural flavour, abstracting from many hardware implementation concerns but still close to hardware-designer intuition. This means it can be discussed in detail with ARM architects. We then develop a more abstract model, better suited for use as an architectural specification, which we prove sound w.r.t.~the first. The instruction semantics involves further difficulties, handling the mass of detail and the subtle intensional information required to interface to the concurrency model. We have a novel ISA description language, with a lightweight dependent type system, letting us do both with a rather direct representation of the ARM reference manual instruction descriptions. We build a tool from the combined semantics that lets one explore, either interactively or exhaustively, the full range of architecturally allowed behaviour, for litmus tests and (small) ELF executables. We prove correctness of some optimisations needed for tool performance. We validate the models by discussion with ARM staff, and by comparison against ARM hardware behaviour, for ISA single- instruction tests and concurrent litmus tests.}},
  url = {https://doi.org/10.1145/2837614.2837615},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup582,
  title = {A logic for information flow in object-oriented programs}},
  author = {Amtoft, Torben and Bandhakavi, Sruthi and Banerjee, Anindya}},
  year = {2006}},
  journal = {Conference Record of the 33rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper specifies, via a Hoare-like logic, an interprocedural and flow sensitive (but termination insensitive) information flow analysis for object-oriented programs. Pointer aliasing is ubiquitous in such programs, and can potentially leak confidential information. Thus the logic employs independence assertions to describe the noninterference property that formalizes confidentiality, and employs region assertions to describe possible aliasing. Programmer assertions, in the style of JML, are also allowed, thereby permitting a more fine-grained specification of information flow policy.The logic supports local reasoning about state in the style of separation logic. Small specifications are used; they mention only the variables and addresses relevant to a command. Specifications are combined using a frame rule. An algorithm for the computation of postconditions is described: under certain assumptions, there exists a strongest postcondition which the algorithm computes.}},
  url = {https://doi.org/10.1145/1111037.1111046},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup583,
  title = {Safe \& Efficient Gradual Typing for TypeScript}},
  author = {Rastogi, Aseem and Swamy, Nikhil and Fournet, C\'{e}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Current proposals for adding gradual typing to JavaScript, such as Closure, TypeScript and Dart, forgo soundness to deal with issues of scale, code reuse, and popular programming patterns. We show how to address these issues in practice while retaining soundness. We design and implement a new gradual type system, prototyped for expediency as a 'Safe' compilation mode for TypeScript. Our compiler achieves soundness by enforcing stricter static checks and embedding residual runtime checks in compiled code. It emits plain JavaScript that runs on stock virtual machines. Our main theorem is a simulation that ensures that the checks introduced by Safe TypeScript (1) catch any dynamic type error, and (2) do not alter the semantics of type-safe TypeScript code.Safe TypeScript is carefully designed to minimize the performance overhead of runtime checks. At its core, we rely on two new ideas: differential subtyping, a new form of coercive subtyping that computes the minimum amount of runtime type information that must be added to each object; and an erasure modality, which we use to safely and selectively erase type information. This allows us to scale our design to full-fledged TypeScript, including arrays, maps, classes, inheritance, overloading, and generic types.We validate the usability and performance of Safe TypeScript empirically by type-checking and compiling around 120,000 lines of existing TypeScript source code. Although runtime checks can be expensive, the end-to-end overhead is small for code bases that already have type annotations. For instance, we bootstrap the Safe TypeScript compiler (90,000 lines including the base TypeScript compiler): we measure a 15\% runtime overhead for type safety, and also uncover programming errors as type safety violations. We conclude that, at least during development and testing, subjecting JavaScript/TypeScript programs to safe gradual typing adds significant value to source type annotations at a modest cost.}},
  url = {https://doi.org/10.1145/2676726.2676971},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup584,
  title = {Static typing for a faulty lambda calculus}},
  author = {Walker, David and Mackey, Lester and Ligatti, Jay and Reis, George A. and August, David I.}},
  year = {2006}},
  journal = {Proceedings of the Eleventh ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A transient hardware fault occurs when an energetic particle strikes a transistor, causing it to change state. These faults do not cause permanent damage, but may result in incorrect program execution by altering signal transfers or stored values. While the likelihood that such transient faults will cause any significant damage may seem remote, over the last several years transient faults have caused costly failures in high-end machines at America Online, eBay, and the Los Alamos Neutron Science Center, among others [6, 44, 15]. Because susceptibility to transient faults is proportional to the size and density of transistors, the problem of transient faults will become increasingly important in the coming decades.This paper defines the first formal, type-theoretic framework for studying reliable computation in the presence of transient faults. More specifically, it defines λzap, a lambda calculus that exhibits intermittent data faults. In order to detect and recover from these faults, λzap programs replicate intermediate computations and use majority voting, thereby modeling software-based fault tolerance techniques studied extensively, but informally [10, 20, 30, 31, 32, 33, 41].To ensure that programs maintain the proper invariants and use λzap primitives correctly, the paper defines a type system for the language. This type system guarantees that well-typed programs can tolerate any single data fault. To demonstrate that λzap can serve as an idealized typed intermediate language, we define a type-preserving translation from a standard simply-typed lambda calculus into λzap.}},
  url = {https://doi.org/10.1145/1159803.1159809},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup585,
  title = {A study of concurrent real-time garbage collectors}},
  author = {Pizlo, Filip and Petrank, Erez and Steensgaard, Bjarne}},
  year = {2008}},
  journal = {Proceedings of the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Concurrent garbage collection is highly attractive for real-time systems, because offloading the collection effort from the executing threads allows faster response, allowing for extremely short deadlines at the microseconds level. Concurrent collectors also offer much better scalability over incremental collectors. The main problem with concurrent real-time collectors is their complexity. The first concurrent real-time garbage collector that can support fine synchronization, STOPLESS, has recently been presented by Pizlo et al. In this paper, we propose two additional (and different) algorithms for concurrent real-time garbage collection: CLOVER and CHICKEN. Both collectors obtain reduced complexity over the first collector STOPLESS, but need to trade a benefit for it. We study the algorithmic strengths and weaknesses of CLOVER and CHICKEN and compare them to STOPLESS. Finally, we have implemented all three collectors on the Bartok compiler and runtime for C# and we present measurements to compare their efficiency and responsiveness.}},
  url = {https://doi.org/10.1145/1375581.1375587},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup586,
  title = {Lifting Assembly to Intermediate Representation: A Novel Approach Leveraging Compilers}},
  author = {Hasabnis, Niranjan and Sekar, R.}},
  year = {2016}},
  journal = {Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Translating low-level machine instructions into higher-level intermediate language (IL) is one of the central steps in many binary analysis and instrumentation systems. Existing systems build such translators manually. As a result, it takes a great deal of effort to support new architectures. Even for widely deployed architectures, full instruction sets may not be modeled, e.g., mature systems such as Valgrind still lack support for AVX, FMA4 and SSE4.1 for x86 processors. To overcome these difficulties, we propose a novel approach that leverages knowledge about instruction set semantics that is already embedded into modern compilers such as GCC. In particular, we present a learning-based approach for automating the translation of assembly instructions to a compiler's architecture-neutral IL. We present an experimental evaluation that demonstrates the ability of our approach to easily support many architectures (x86, ARM and AVR), including their advanced instruction sets. Our implementation is available as open-source software.}},
  url = {https://doi.org/10.1145/2872362.2872380},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup587,
  title = {Modules, abstraction, and parametric polymorphism}},
  author = {Crary, Karl}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Reynolds's Abstraction theorem forms the mathematical foundation for data abstraction. His setting was the polymorphic lambda calculus. Today, many modern languages, such as the ML family, employ rich module systems designed to give more expressive support for data abstraction than the polymorphic lambda calculus, but analogues of the Abstraction theorem for such module systems have lagged far behind. We give an account of the Abstraction theorem for a modern module calculus supporting generative and applicative functors, higher-order functors, sealing, and translucent signatures. The main issues to be overcome are: (1) the fact that modules combine both types and terms, so they must be treated as both simultaneously, (2) the effect discipline that models the distinction between transparent and opaque modules, and (3) a very rich language of type constructors supporting singleton kinds. We define logical equivalence for modules and show that it coincides with contextual equivalence. This substantiates the folk theorem that modules are good for data abstraction. All our proofs are formalized in Coq.}},
  url = {https://doi.org/10.1145/3009837.3009892},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup588,
  title = {Algorithmic analysis of qualitative and quantitative termination problems for affine probabilistic programs}},
  author = {Chatterjee, Krishnendu and Fu, Hongfei and Novotn\'{y}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this paper, we consider termination of probabilistic programs with real-valued variables. The questions concerned are: 1. qualitative ones that ask (i) whether the program terminates with probability 1 (almost-sure termination) and (ii) whether the expected termination time is finite (finite termination); 2. quantitative ones that ask (i) to approximate the expected termination time (expectation problem) and (ii) to compute a bound B such that the probability to terminate after B steps decreases exponentially (concentration problem). To solve these questions, we utilize the notion of ranking supermartingales which is a powerful approach for proving termination of probabilistic programs. In detail, we focus on algorithmic synthesis of linear ranking-supermartingales over affine probabilistic programs (APP's) with both angelic and demonic non-determinism. An important subclass of APP's is LRAPP which is defined as the class of all APP's over which a linear ranking-supermartingale exists. Our main contributions are as follows. Firstly, we show that the membership problem of LRAPP (i) can be decided in polynomial time for APP's with at most demonic non-determinism, and (ii) is NP-hard and in PSPACE for APP's with angelic non-determinism; moreover, the NP-hardness result holds already for APP's without probability and demonic non-determinism. Secondly, we show that the concentration problem over LRAPP can be solved in the same complexity as for the membership problem of LRAPP. Finally, we show that the expectation problem over LRAPP can be solved in 2EXPTIME and is PSPACE-hard even for APP's without probability and non-determinism (i.e., deterministic programs). Our experimental results demonstrate the effectiveness of our approach to answer the qualitative and quantitative questions over APP's with at most demonic non-determinism.}},
  url = {https://doi.org/10.1145/2837614.2837639},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup589,
  title = {Using early phase termination to eliminate load imbalances at barrier synchronization points}},
  author = {Rinard, Martin C.}},
  year = {2007}},
  journal = {Proceedings of the 22nd Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a new technique, early phase termination, for eliminating idle processors in parallel computations that use barrier synchronization. This technique simply terminates each parallel phaseas soon as there are too few remaining tasks to keep all of the processors busy.Although this technique completely eliminates the idling that would other wise occur at barrier synchronization points, it may also change the computation and therefore the result that the computation produces. We address this issue by providing probabilistic distortion models that characterize how the use of early phase termination distorts the result that the computation produces. Our experimental results show that for our set of benchmark applications, 1) early phase termination can improve the performance of the parallel computation, 2) the distortion is small (or can be made to be small with the use of an appropriate compensation technique) and 3) the distortion models provide accurate and tight distortion bounds. These bounds can enable users to evaluate the effect of early phase termination and confidently accept results from parallel computations that use this technique if they find the distortion bounds to be acceptable.Finally, we identify a general computational pattern that works well with early phase termination and explain why computations that exhibit this pattern can tolerate the early termination of parallel tasks without producing unacceptable results.}},
  url = {https://doi.org/10.1145/1297027.1297055},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup590,
  title = {Intersection type calculi of bounded dimension}},
  author = {Dudenhefner, Andrej and Rehof, Jakob}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A notion of dimension in intersection typed λ-calculi is presented. The dimension of a typed λ-term is given by the minimal norm of an elaboration (a proof theoretic decoration) necessary for typing the term at its type, and, intuitively, measures intersection introduction as a resource. Bounded-dimensional intersection type calculi are shown to enjoy subject reduction, since terms can be elaborated in non-increasing norm under β-reduction. We prove that a multiset interpretation (corresponding to a non-idempotent and non-linear interpretation of intersection) of dimensionality corresponds to the number of simultaneous constraints required during search for inhabitants. As a consequence, the inhabitation problem is decidable in bounded multiset dimension, and it is proven to be EXPSPACE-complete. This result is a substantial generalization of inhabitation for the rank 2-fragment, yielding a calculus with decidable inhabitation which is independent of rank. Our results give rise to a new criterion (dimensional bound) for subclasses of intersection type calculi with a decidable inhabitation problem, which is orthogonal to previously known criteria, and which should have immediate applications in synthesis. Additionally, we give examples of dimensional analysis of fragments of the intersection type system, including conservativity over simple types, rank 2-types, and normal form typings, and we provide some observations towards dimensional analysis of other systems. It is suggested (for future work) that our notion of dimension may have semantic interpretations in terms of of reduction complexity.}},
  url = {https://doi.org/10.1145/3009837.3009862},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup591,
  title = {1ML – core and modules united (F-ing first-class modules)}},
  author = {Rossberg, Andreas}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {ML is two languages in one: there is the core, with types and expressions, and there are modules, with signatures, structures and functors. Modules form a separate, higher-order functional language on top of the core. There are both practical and technical reasons for this stratification; yet, it creates substantial duplication in syntax and semantics, and it reduces expressiveness. For example, selecting a module cannot be made a dynamic decision. Language extensions allowing modules to be packaged up as first-class values have been proposed and implemented in different variations. However, they remedy expressiveness only to some extent, are syntactically cumbersome, and do not alleviate redundancy. We propose a redesign of ML in which modules are truly first-class values, and core and module layer are unified into one language. In this "1ML", functions, functors, and even type constructors are one and the same construct; likewise, no distinction is made between structures, records, or tuples. Or viewed the other way round, everything is just ("a mode of use of") modules. Yet, 1ML does not require dependent types, and its type structure is expressible in terms of plain System Fω, in a minor variation of our F-ing modules approach. We introduce both an explicitly typed version of 1ML, and an extension with Damas/Milner-style implicit quantification. Type inference for this language is not complete, but, we argue, not substantially worse than for Standard ML. An alternative view is that 1ML is a user-friendly surface syntax for System Fω that allows combining term and type abstraction in a more compositional manner than the bare calculus.}},
  url = {https://doi.org/10.1145/2784731.2784738},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup592,
  title = {String solving with word equations and transducers: towards a logic for analysing mutation XSS}},
  author = {Lin, Anthony W. and Barcel\'{o}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We study the fundamental issue of decidability of satisfiability over string logics with concatenations and finite-state transducers as atomic operations. Although restricting to one type of operations yields decidability, little is known about the decidability of their combined theory, which is especially relevant when analysing security vulnerabilities of dynamic web pages in a more realistic browser model. On the one hand, word equations (string logic with concatenations) cannot precisely capture sanitisation functions (e.g. htmlescape) and implicit browser transductions (e.g. innerHTML mutations). On the other hand, transducers suffer from the reverse problem of being able to model sanitisation functions and browser transductions, but not string concatenations. Naively combining word equations and transducers easily leads to an undecidable logic. Our main contribution is to show that the "straight-line fragment" of the logic is decidable (complexity ranges from PSPACE to EXPSPACE). The fragment can express the program logics of straight-line string-manipulating programs with concatenations and transductions as atomic operations, which arise when performing bounded model checking or dynamic symbolic executions. We demonstrate that the logic can naturally express constraints required for analysing mutation XSS in web applications. Finally, the logic remains decidable in the presence of length, letter-counting, regular, indexOf, and disequality constraints.}},
  url = {https://doi.org/10.1145/2837614.2837641},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup593,
  title = {A type-theoretic foundation for programming with higher-order abstract syntax and first-class substitutions}},
  author = {Pientka, Brigitte}},
  year = {2008}},
  journal = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Higher-order abstract syntax (HOAS) is a simple, powerful technique for implementing object languages, since it directly supports common and tricky routines dealing with variables, such as capture-avoiding substitution and renaming. This is achieved by representing binders in the object-language via binders in the meta-language. However, enriching functional programming languages with direct support for HOAS has been a major challenge, because recursion over HOAS encodings requires one to traverse lambda-abstractions and necessitates programming with open objects.We present a novel type-theoretic foundation based on contextual modal types which allows us to recursively analyze open terms via higher-order pattern matching. By design, variables occurring in open terms can never escape their scope. Using several examples, we demonstrate that our framework provides a name-safe foundation to operations typically found in nominal systems. In contrast to nominal systems however, we also support capture-avoiding substitution operations and even provide first-class substitutions to the programmer. The main contribution of this paper is a syntax-directed bi-directional type system where we distinguish between the data language and the computation language together with the progress and preservation proof for our language.}},
  url = {https://doi.org/10.1145/1328438.1328483},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup594,
  title = {Embedding effect systems in Haskell}},
  author = {Orchard, Dominic and Petricek, Tomas}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM SIGPLAN Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Monads are now an everyday tool in functional programming for abstracting and delimiting effects. The link between monads and effect systems is well-known, but in their typical use, monads provide a much more coarse-grained view of effects. Effect systems capture fine-grained information about the effects, but monads provide only a binary view: effectful or pure.Recent theoretical work has unified fine-grained effect systems with monads using a monad-like structure indexed by a monoid of effect annotations (called parametric effect monads). This aligns the power of monads with the power of effect systems.This paper leverages recent advances in Haskell's type system (as provided by GHC) to embed this approach in Haskell, providing user-programmable effect systems. We explore a number of practical examples that make Haskell even better and safer for effectful programming. Along the way, we relate the examples to other concepts, such as Haskell's implicit parameters and coeffects.}},
  url = {https://doi.org/10.1145/2633357.2633368},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup595,
  title = {Harmless advice}},
  author = {Dantas, Daniel S. and Walker, David}},
  year = {2006}},
  journal = {Conference Record of the 33rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper defines an object-oriented language with harmless aspect-oriented advice. A piece of harmless advice is a computation that, like ordinary aspect-oriented advice, executes when control reaches a designated control-flow point. However, unlike ordinary advice, harmless advice is designed to obey a weak non-interference property. Harmless advice may change the termination behavior of computations and use I/O, but it does not otherwise influence the final result of the mainline code. The benefit of harmless advice is that it facilitates local reasoning about program behavior. More specifically, programmers may ignore harmless advice when reasoning about the partial correctness properties of their programs. In addition, programmers may add new pieces of harmless advice to pre-existing programs in typical "after-the-fact" aspect-oriented style without fear they will break important data invariants used by the mainline code.In order to detect and enforce harmlessness, the paper defines a novel type and effect system related to information-flow type systems. The central technical result is that well-typed harmless advice does not interfere with the mainline computation. The paper also presents an implementation of the language and a case study using harmless advice to implement security policies.}},
  url = {https://doi.org/10.1145/1111037.1111071},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup596,
  title = {Recursive monadic bindings}},
  author = {Erk\"{o}},
  year = {2000}},
  journal = {Proceedings of the Fifth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Monads have become a popular tool for dealing with computational effects in Haskell for two significant reasons: equational reasoning is retained even in the presence of effects; and program modularity is enhanced by hiding "plumbing" issues inside the monadic infrastructure. Unfortunately, not all the facilities provided by the underlying language are readily available for monadic computations. In particular, while recursive monadic computations can be defined directly using Haskell's built-in recursion capabilities, there is no natural way to express recursion over the values of monadic actions. Using examples, we illustrate why this is a problem, and we propose an extension to Haskell's donotation to remedy the situation. It turns out that the structure of monadic value-recursion depends on the structure of the underlying monad. We propose an axiomatization of the recursion operation and provide a catalogue of definitions that satisfy our criteria.}},
  url = {https://doi.org/10.1145/351240.351257},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup597,
  title = {FlashRelate: extracting relational data from semi-structured spreadsheets using examples}},
  author = {Barowy, Daniel W. and Gulwani, Sumit and Hart, Ted and Zorn, Benjamin}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {With hundreds of millions of users, spreadsheets are one of the most important end-user applications. Spreadsheets are easy to use and allow users great flexibility in storing data. This flexibility comes at a price: users often treat spreadsheets as a poor man's database, leading to creative solutions for storing high-dimensional data. The trouble arises when users need to answer queries with their data. Data manipulation tools make strong assumptions about data layouts and cannot read these ad-hoc databases. Converting data into the appropriate layout requires programming skills or a major investment in manual reformatting. The effect is that a vast amount of real-world data is "locked-in" to a proliferation of one-off formats. We introduce FlashRelate, a synthesis engine that lets ordinary users extract structured relational data from spreadsheets without programming. Instead, users extract data by supplying examples of output relational tuples. FlashRelate uses these examples to synthesize a program in Flare. Flare is a novel extraction language that extends regular expressions with geometric constructs. An interactive user interface on top of FlashRelate lets end users extract data by point-and-click. We demonstrate that correct Flare programs can be synthesized in seconds from a small set of examples for 43 real-world scenarios. Finally, our case study demonstrates FlashRelate's usefulness addressing the widespread problem of data trapped in corporate and government formats.}},
  url = {https://doi.org/10.1145/2737924.2737952},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup598,
  title = {Gradual synthesis for static parallelization of single-pass array-processing programs}},
  author = {Fedyukovich, Grigory and Ahmad, Maaz Bin Safeer and Bodik, Rastislav}},
  year = {2017}},
  journal = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Parallelizing of software improves its effectiveness and productivity. To guarantee correctness, the parallel and serial versions of the same code must be formally verified to be equivalent. We present a novel approach, called GRASSP, that automatically synthesizes parallel single-pass array-processing programs by treating the given serial versions as specifications. Given arbitrary segmentation of the input array, GRASSP synthesizes a code to determine a new segmentation of the array that allows computing partial results for each segment and merging them. In contrast to other parallelizers, GRASSP gradually considers several parallelization scenarios and certifies the results using constrained Horn solving. For several classes of programs, we show that such parallelization can be performed efficiently. The C++ translations of the GRASSP solutions sped performance by up to 5X relative to serial code on an 8-thread machine and Hadoop translations by up to 10X on a 10-node Amazon EMR cluster.}},
  url = {https://doi.org/10.1145/3062341.3062382},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup599,
  title = {Higher-order symbolic execution via contracts}},
  author = {Tobin-Hochstadt, Sam and Van Horn, David}},
  year = {2012}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a new approach to automated reasoning about higher-order programs by extending symbolic execution to use behavioral contracts as symbolic values, thus enabling symbolic approximation of higher-order behavior.Our approach is based on the idea of an abstract reduction semantics that gives an operational semantics to programs with both concrete and symbolic components. Symbolic components are approximated by their contract and our semantics gives an operational interpretation of contracts-as-values. The result is an executable semantics that soundly predicts program behavior, including contract failures, for all possible instantiations of symbolic components. We show that our approach scales to an expressive language of contracts including arbitrary programs embedded as predicates, dependent function contracts, and recursive contracts. Supporting this rich language of specifications leads to powerful symbolic reasoning using existing program constructs.We then apply our approach to produce a verifier for contract correctness of components, including a sound and computable approximation to our semantics that facilitates fully automated contract verification. Our implementation is capable of verifying contracts expressed in existing programs, and of justifying contract-elimination optimizations.}},
  url = {https://doi.org/10.1145/2384616.2384655},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup600,
  title = {Hazelnut: a bidirectionally typed structure editor calculus}},
  author = {Omar, Cyrus and Voysey, Ian and Hilton, Michael and Aldrich, Jonathan and Hammer, Matthew A.}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Structure editors allow programmers to edit the tree structure of a program directly. This can have cognitive benefits, particularly for novice and end-user programmers. It also simplifies matters for tool designers, because they do not need to contend with malformed program text. This paper introduces Hazelnut, a structure editor based on a small bidirectionally typed lambda calculus extended with holes and a cursor. Hazelnut goes one step beyond syntactic well-formedness: its edit actions operate over statically meaningful incomplete terms. Na\"{\i}},
  url = {https://doi.org/10.1145/3009837.3009900},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup601,
  title = {High-level small-step operational semantics for transactions}},
  author = {Moore, Katherine F. and Grossman, Dan}},
  year = {2008}},
  journal = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Software transactions have received significant attention as a way to simplify shared-memory concurrent programming, but insufficient focus has been given to the precise meaning of software transactions or their interaction with other language features. This work begins to rectify that situation by presenting a family of formal languages that model a wide variety of behaviors for software transactions. These languages abstract away implementation details of transactional memory, providing high-level definitions suitable for programming languages. We use small-step semantics in order to represent explicitly the interleaved execution of threads that is necessary to investigate pertinent issues.We demonstrate the value of our core approach to modeling transactions by investigating two issues in depth. First, we consider parallel nesting, in which parallelism and transactions can nest arbitrarily. Second, we present multiple models for weak isolation, in which nontransactional code can violate the isolation of a transaction. For both, type-and-effect systems let us soundly and statically restrict what computation can occur inside or outside a transaction. We prove some key language-equivalence theorems to confirm that under sufficient static restrictions, in particular that each mutable memory location is used outside transactions or inside transactions (but not both), no program can determine whether the language implementation uses weak isolation or strong isolation.}},
  url = {https://doi.org/10.1145/1328438.1328448},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup602,
  title = {Making standard ML a practical database programming language}},
  author = {Ohori, Atsushi and Ueno, Katsuhiro}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Integrating a database query language into a programming language is becoming increasingly important in recently emerging high-level cloud computing and other applications, where efficient and sophisticated data manipulation is required during computation. This paper reports on seamless integration of SQL into SML# - an extension of Standard ML. In the integrated language, the type system always infers a principal type for any type consistent SQL expression. This makes SQL queries first-class citizens, which can be freely combined with any other language constructs definable in Standard ML. For a program involving SQL queries, the compiler separates SQL queries and delegates their evaluation to a database server, e.g. PostgreSQL or MySQL in the currently implemented version.The type system of our language is largely based on Machiavelli, which demonstrates that ML with record polymorphism can represent type structure of SQL. In order to develop a practical language, however, a number of technical challenges have to be overcome, including static enforcement of server connection consistency, proper treatment of overloaded SQL primitives, query compilation, and runtime connection management. This paper describes the necessary extensions to the type system and compilation, and reports on the details of its implementation.}},
  url = {https://doi.org/10.1145/2034773.2034815},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup603,
  title = {Modular reasoning in the presence of subclassing}},
  author = {Stata, Raymie and Guttag, John V.}},
  year = {1995}},
  journal = {Proceedings of the Tenth Annual Conference on Object-Oriented Programming Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Considerable progress has been made in understanding how to use subtyping in a way that facilitates modular reasoning. However, using subclassing in a way that facilitates modular reasoning is not well understood. Often methods must be overriden as a group because of dependencies on instance variables, and the programmers of subclasses cannot tell which methods are grouped without looking at the code of superclasses. Also, the programmers of subclasses must look at the code of superclasses to tell what assumptions inherited methods make about the behavior of overriden methods.We present a systematic way to use subclassing that facilitates formal and informal modular reasoning. Separate specifications are given to programmers writing code that manipulates instances of a class and to programmers writing subclasses of the class. The specifications given to programmers of subclasses are divided, by division of labor specifications, into multiple parts. Subclasses may inherit or override entire parts, but not sub-parts. Reasoning about the implementation of each part is done independently of other parts.}},
  url = {https://doi.org/10.1145/217838.217861},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup604,
  title = {Maximal specification synthesis}},
  author = {Albarghouthi, Aws and Dillig, Isil and Gurfinkel, Arie}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many problems in program analysis, verification, and synthesis require inferring specifications of unknown procedures. Motivated by a broad range of applications, we formulate the problem of maximal specification inference: Given a postcondition Phi and a program P calling a set of unknown procedures F_1,…,F_n, what are the most permissive specifications of procedures F_i that ensure correctness of P? In other words, we are looking for the smallest number of assumptions we need to make about the behaviours of F_i in order to prove that $P$ satisfies its postcondition. To solve this problem, we present a novel approach that utilizes a counterexample-guided inductive synthesis loop and reduces the maximal specification inference problem to multi-abduction. We formulate the novel notion of multi-abduction as a generalization of classical logical abduction and present an algorithm for solving multi-abduction problems. On the practical side, we evaluate our specification inference technique on a range of benchmarks and demonstrate its ability to synthesize specifications of kernel routines invoked by device drivers.}},
  url = {https://doi.org/10.1145/2837614.2837628},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup605,
  title = {The hardness of data packing}},
  author = {Lavaee, Rahman}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A program can benefit from improved cache block utilization when contemporaneously accessed data elements are placed in the same memory block. This can reduce the program's memory block working set and thereby, reduce the capacity miss rate. We formally define the problem of data packing for arbitrary number of blocks in the cache and packing factor (the number of data objects fitting in a cache block) and study how well the optimal solution can be approximated for two dual problems. On the one hand, we show that the cache hit maximization problem is approximable within a constant factor, for every fixed number of blocks in the cache. On the other hand, we show that unless P=NP, the cache miss minimization problem cannot be efficiently approximated.}},
  url = {https://doi.org/10.1145/2837614.2837669},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup606,
  title = {DAG inlining: a decision procedure for reachability-modulo-theories in hierarchical programs}},
  author = {Lal, Akash and Qadeer, Shaz}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A hierarchical program is one with multiple procedures but no loops or recursion. This paper studies the problem of deciding reachability queries in hierarchical programs where individual statements can be encoded in a decidable logic (say in SMT). This problem is fundamental to verification and most directly applicable to doing bounded reachability in programs, i.e., reachability under a bound on the number of loop iterations and recursive calls. The usual method of deciding reachability in hierarchical programs is to first inline all procedures and then do reachability on the resulting single-procedure program. Such inlining unfolds the call graph of the program to a tree and may lead to an exponential increase in the size of the program. We design and evaluate a method called DAG inlining that unfolds the call graph to a directed acyclic graph (DAG) instead of a tree by sharing the bodies of procedures at certain points during inlining. DAG inlining can produce much more compact representations than tree inlining. Empirically, we show that it leads to significant improvements in the running time of a state-of-the-art verifier.}},
  url = {https://doi.org/10.1145/2737924.2737987},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup607,
  title = {Race detection for event-driven mobile applications}},
  author = {Hsiao, Chun-Hung and Yu, Jie and Narayanasamy, Satish and Kong, Ziyun and Pereira, Cristiano L. and Pokam, Gilles A. and Chen, Peter M. and Flinn, Jason}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Mobile systems commonly support an event-based model of concurrent programming. This model, used in popular platforms such as Android, naturally supports mobile devices that have a rich array of sensors and user input modalities. Unfortunately, most existing tools for detecting concurrency errors of parallel programs focus on a thread-based model of concurrency. If one applies such tools directly to an event-based program, they work poorly because they infer false dependencies between unrelated events handled sequentially by the same thread.In this paper we present a race detection tool named CAFA for event-driven mobile systems. CAFA uses the causality model that we have developed for the Android event-driven system. A novel contribution of our model is that it accounts for the causal order due to the event queues, which are not accounted for in past data race detectors. Detecting races based on low-level races between memory accesses leads to a large number of false positives. CAFA overcomes this problem by checking for races between high-level operations. We discuss our experience in using CAFA for finding and understanding a number of known and unknown harmful races in open-source Android applications.}},
  url = {https://doi.org/10.1145/2594291.2594330},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup608,
  title = {Relaxing safely: verified on-the-fly garbage collection for x86-TSO}},
  author = {Gammie, Peter and Hosking, Antony L. and Engelhardt, Kai}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We report on a machine-checked verification of safety for a state-of-the-art, on-the-fly, concurrent, mark-sweep garbage collector that is designed for multi-core architectures with weak memory consistency. The proof explicitly incorporates the relaxed memory semantics of x86 multiprocessors. To our knowledge, this is the first fully machine-checked proof of safety for such a garbage collector. We couch the proof in a framework that system implementers will find appealing, with the fundamental components of the system specified in a simple and intuitive programming language. The abstract model is detailed enough for its correspondence with an assembly language implementation to be straightforward.}},
  url = {https://doi.org/10.1145/2737924.2738006},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup609,
  title = {Bounded refinement types}},
  author = {Vazou, Niki and Bakst, Alexander and Jhala, Ranjit}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a notion of bounded quantification for refinement types and show how it expands the expressiveness of refinement typing by using it to develop typed combinators for: (1) relational algebra and safe database access, (2) Floyd-Hoare logic within a state transformer monad equipped with combinators for branching and looping, and (3) using the above to implement a refined IO monad that tracks capabilities and resource usage. This leap in expressiveness comes via a translation to ``ghost" functions, which lets us retain the automated and decidable SMT based checking and inference that makes refinement typing effective in practice.}},
  url = {https://doi.org/10.1145/2784731.2784745},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup610,
  title = {The missing link: explaining ELF static linking, semantically}},
  author = {Kell, Stephen and Mulligan, Dominic P. and Sewell, Peter}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Beneath the surface, software usually depends on complex linker behaviour to work as intended. Even linking <pre>hello_world.c</pre> is surprisingly involved, and systems software such as <pre>libc</pre> and operating system kernels rely on a host of linker features. But linking is poorly understood by working programmers and has largely been neglected by language researchers. In this paper we survey the many use-cases that linkers support and the poorly specified linker speak by which they are controlled: metadata in object files, command-line options, and linker-script language. We provide the first validated formalisation of a realistic executable and linkable format (ELF), and capture aspects of the Application Binary Interfaces for four mainstream platforms (AArch64, AMD64, Power64, and IA32). Using these, we develop an executable specification of static linking, covering (among other things) enough to link small C programs (we use the example of bzip2) into a correctly running executable. We provide our specification in Lem and Isabelle/HOL forms. This is the first formal specification of mainstream linking. We have used the Isabelle/HOL version to prove a sample correctness property for one case of AMD64 ABI relocation, demonstrating that the specification supports formal proof, and as a first step towards the much more ambitious goal of verified linking. Our work should enable several novel strands of research, including linker-aware verified compilation and program analysis, and better languages for controlling linking.}},
  url = {https://doi.org/10.1145/2983990.2983996},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup611,
  title = {Processor-Oblivious Record and Replay}},
  author = {Utterback, Robert and Agrawal, Kunal and Lee, I-Ting Angelina and Kulkarni, Milind}},
  year = {2017}},
  journal = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Record-and-replay systems are useful tools for debugging non-deterministic parallel programs by first recording an execution and then replaying that execution to produce the same access pattern. Existing record-and-replay systems generally target thread-based execution models, and record the behaviors and interleavings of individual threads. Dynamic multithreaded languages and libraries, such as the Cilk family, OpenMP, TBB, etc., do not have a notion of threads. Instead, these languages provide a processor-oblivious model of programming, where programs expose task-parallelism using high-level constructs such as spawn/sync without regard to the number of threads/cores available to run the program. Thread-based record-and-replay would violate the processor-oblivious nature of these programs, as they incorporate the number of threads into the recorded information, constraining the replayed execution to the same number of threads.In this paper, we present a processor-oblivious record-and-replay scheme for such languages where record and replay can use different number of processors and both are scheduled using work stealing. We provide theoretical guarantees for our record and replay scheme --- namely that record is optimal for programs with one lock and replay is near-optimal for all cases. In addition, we implemented this scheme in the Cilk Plus runtime system and our evaluation indicates that processor-obliviousness does not cause substantial overheads.}},
  url = {https://doi.org/10.1145/3018743.3018764},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup612,
  title = {All sorts of permutations (functional pearl)}},
  author = {Christiansen, Jan and Danilenko, Nikita and Dylus, Sandra}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The combination of non-determinism and sorting is mostly associated with permutation sort, a sorting algorithm that is not very useful for sorting and has an awful running time. In this paper we look at the combination of non-determinism and sorting in a different light: given a sorting function, we apply it to a non-deterministic predicate to gain a function that enumerates permutations of the input list. We get to the bottom of necessary properties of the sorting algorithms and predicates in play as well as discuss variations of the modelled non-determinism. On top of that, we formulate and prove a theorem stating that no matter which sorting function we use, the corresponding permutation function enumerates all permutations of the input list. We use free theorems, which are derived from the type of a function alone, to prove the statement.}},
  url = {https://doi.org/10.1145/2951913.2951949},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup613,
  title = {Polymorphic functions with set-theoretic types: part 1: syntax, semantics, and evaluation}},
  author = {Castagna, Giuseppe and Nguyen, Kim and Xu, Zhiwu and Im, Hyeonseung and Lenglet, Sergue\"{\i}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This article is the first part of a two articles series about a calculus with higher-order polymorphic functions, recursive types with arrow and product type constructors and set-theoretic type connectives (union, intersection, and negation).In this first part we define and study the explicitly-typed version of the calculus in which type instantiation is driven by explicit instantiation annotations. In particular, we define an explicitly-typed lambda-calculus with intersection types and an efficient evaluation model for it. In the second part, presented in a companion paper, we define a local type inference system that allows the programmer to omit explicit instantiation annotations, and a type reconstruction system that allows the programmer to omit explicit type annotations.The work presented in the two articles provides the theoretical foundations and technical machinery needed to design and implement higher-order polymorphic functional languages for semi-structured data.}},
  url = {https://doi.org/10.1145/2535838.2535840},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup614,
  title = {Pushdown flow analysis of first-class control}},
  author = {Vardoulakis, Dimitrios and Shivers, Olin}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Pushdown models are better than control-flow graphs for higher-order flow analysis. They faithfully model the call/return structure of a program, which results in fewer spurious flows and increased precision. However, pushdown models require that calls and returns in the analyzed program nest properly. As a result, they cannot be used to analyze language constructs that break call/return nesting such as generators, coroutines, call/cc, etc.In this paper, we extend the CFA2 flow analysis to create the first pushdown flow analysis for languages with first-class control. We modify the abstract semantics of CFA2 to allow continuations to escape to, and be restored from, the heap. We then present a summarization algorithm that handles escaping continuations via a new kind of summary edge. We prove that the algorithm is sound with respect to the abstract semantics.}},
  url = {https://doi.org/10.1145/2034773.2034785},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup615,
  title = {Virgil: objects on the head of a pin}},
  author = {Titzer, Ben L.}},
  year = {2006}},
  journal = {Proceedings of the 21st Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Embedded microcontrollers are becoming increasingly prolific, serving as the primary or auxiliary processor in products and research systems from microwaves to sensor networks. Microcontrollers represent perhaps the most severely resource-constrained embedded processors, often with as little as a few bytes of memory and a few kilobytes of code space. Language and compiler technology has so far been unable to bring the benefits of modern object-oriented languages to such processors. In this paper, I will present the design and implementation of Virgil, a lightweight object-oriented language designed with careful consideration for resource-limited domains. Virgil explicitly separates initialization time from runtime, allowing an application to build complex data structures during compilation and then run directly on the bare hardware without a virtual machine or any language runtime. This separation allows the entire program heap to be available at compile time and enables three new data-sensitive optimizations: reachable members analysis, reference compression, and ROM-ization. Experi-mental results demonstrate that Virgil is well suited for writing microcontroller programs, with five demonstrative applications fitting in less than 256 bytes of RAM with fewer than 50 bytes of metadata. Further results show that the optimizations presented in this paper reduced code size between 20\% and 80\% and RAM size by as much as 75\%.}},
  url = {https://doi.org/10.1145/1167473.1167489},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup616,
  title = {Composing monads using coproducts}},
  author = {L\"{u}},
  year = {2002}},
  journal = {Proceedings of the Seventh ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Monads are a useful abstraction of computation, as they model diverse computational effects such as stateful computations, exceptions and I/O in a uniform manner. Their potential to provide both a modular semantics and a modular programming style was soon recognised. However, in general, monads proved difficult to compose and so research focused on special mechanisms for their composition such as distributive monads and monad transformers.We present a new approach to this problem which is general in that nearly all monads compose, mathematically elegant in using the standard categorical tools underpinning monads and computationally expressive in supporting a canonical recursion operator. In a nutshell, we propose that two monads should be composed by taking their coproduct. Although abstractly this is a simple idea, the actual construction of the coproduct of two monads is non-trivial. We outline this construction, show how to implement the coproduct within Haskell and demonstrate its usage with a few examples. We also discuss its relationship with other ways of combining monads, in particular distributive laws for monads and monad transformers.}},
  url = {https://doi.org/10.1145/581478.581492},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup617,
  title = {Detecting redundant CSS rules in HTML5 applications: a tree rewriting approach}},
  author = {Hague, Matthew and Lin, Anthony W. and Ong, C.-H. Luke}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {HTML5 applications normally have a large set of CSS (Cascading Style Sheets) rules for data display. Each CSS rule consists of a node selector and a declaration block (which assigns values to selected nodes' display attributes). As web applications evolve, maintaining CSS files can easily become problematic. Some CSS rules will be replaced by new ones, but these obsolete (hence redundant) CSS rules often remain in the applications. Not only does this “bloat” the applications – increasing the bandwidth requirement – but it also significantly increases web browsers' processing time. Most works on detecting redundant CSS rules in HTML5 applications do not consider the dynamic behaviours of HTML5 (specified in JavaScript); in fact, the only proposed method that takes these into account is dynamic analysis, which cannot soundly prove redundancy of CSS rules. In this paper, we introduce an abstraction of HTML5 applications based on monotonic tree-rewriting and study its "redundancy problem". We establish the precise complexity of the problem and various subproblems of practical importance (ranging from P to EXP). In particular, our algorithm relies on an efficient reduction to an analysis of symbolic pushdown systems (for which highly optimised solvers are available), which yields a fast method for checking redundancy in practice. We implemented our algorithm and demonstrated its efficacy in detecting redundant CSS rules in HTML5 applications.}},
  url = {https://doi.org/10.1145/2814270.2814288},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup618,
  title = {A type-directed abstraction refinement approach to higher-order model checking}},
  author = {Ramsay, Steven J. and Neatherway, Robin P. and Ong, C.-H. Luke}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The trivial-automaton model checking problem for higher-order recursion schemes has become a widely studied object in connection with the automatic verification of higher-order programs. The problem is formidably hard: despite considerable progress in recent years, no decision procedures have been demonstrated to scale robustly beyond recursion schemes that comprise more than a few hundred rewrite rules. We present a new, fixed-parameter polynomial time algorithm, based on a novel, type directed form of abstraction refinement in which behaviours of a scheme are distinguished by the abstraction according to the intersection types that they inhabit (the properties that they satisfy). Unlike other intersection type approaches, our algorithm reasons both about acceptance by the property automaton and acceptance by its dual, simultaneously, in order to minimize the amount of work done by converging on the solution to a problem instance from both sides. We have constructed Preface, a prototype implementation of the algorithm, and assembled an extensive body of evidence to demonstrate empirically that the algorithm readily scales to recursion schemes of several thousand rules, well beyond the capabilities of current state-of-the-art higher-order model checkers.}},
  url = {https://doi.org/10.1145/2535838.2535873},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup619,
  title = {Extensible access control with authorization contracts}},
  author = {Moore, Scott and Dimoulas, Christos and Findler, Robert Bruce and Flatt, Matthew and Chong, Stephen}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Existing programming language access control frameworks do not meet the needs of all software components. We propose an expressive framework for implementing access control monitors for components. The basis of the framework is a novel concept: the authority environment. An authority environment associates rights with an execution context. The building blocks of access control monitors in our framework are authorization contracts: software contracts that manage authority environments. We demonstrate the expressiveness of our framework by implementing a diverse set of existing access control mechanisms and writing custom access control monitors for three realistic case studies.}},
  url = {https://doi.org/10.1145/2983990.2984021},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup620,
  title = {K-Java: A Complete Semantics of Java}},
  author = {Bogdanas, Denis and Ro\c{s}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents K-Java, a complete executable formal semantics of Java 1.4. K-Java was extensively tested with a test suite developed alongside the project, following the Test Driven Development methodology. In order to maintain clarity while handling the great size of Java, the semantics was split into two separate definitions -- a static semantics and a dynamic semantics. The output of the static semantics is a preprocessed Java program, which is passed as input to the dynamic semantics for execution. The preprocessed program is a valid Java program, which uses a subset of the features of Java. The semantics is applied to model-check multi-threaded programs. Both the test suite and the static semantics are generic and ready to be used in other Java-related projects.}},
  url = {https://doi.org/10.1145/2676726.2676982},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup621,
  title = {Ivy: safety verification by interactive generalization}},
  author = {Padon, Oded and McMillan, Kenneth L. and Panda, Aurojit and Sagiv, Mooly and Shoham, Sharon}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Despite several decades of research, the problem of formal verification of infinite-state systems has resisted effective automation. We describe a system --- Ivy --- for interactively verifying safety of infinite-state systems. Ivy's key principle is that whenever verification fails, Ivy graphically displays a concrete counterexample to induction. The user then interactively guides generalization from this counterexample. This process continues until an inductive invariant is found. Ivy searches for universally quantified invariants, and uses a restricted modeling language. This ensures that all verification conditions can be checked algorithmically. All user interactions are performed using graphical models, easing the user's task. We describe our initial experience with verifying several distributed protocols.}},
  url = {https://doi.org/10.1145/2908080.2908118},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup622,
  title = {From Network Interface to Multithreaded Web Applications: A Case Study in Modular Program Verification}},
  author = {Chlipala, Adam}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many verifications of realistic software systems are monolithic, in the sense that they define single global invariants over complete system state. More modular proof techniques promise to support reuse of component proofs and even reduce the effort required to verify one concrete system, just as modularity simplifies standard software development. This paper reports on one case study applying modular proof techniques in the Coq proof assistant. To our knowledge, it is the first modular verification certifying a system that combines infrastructure with an application of interest to end users. We assume a nonblocking API for managing TCP networking streams, and on top of that we work our way up to certifying multithreaded, database-backed Web applications. Key verified components include a cooperative threading library and an implementation of a domain-specific language for XML processing. We have deployed our case-study system on mobile robots, where it interfaces with off-the-shelf components for sensing, actuation, and control.}},
  url = {https://doi.org/10.1145/2676726.2677003},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup623,
  title = {Safer SDN programming through Arbiter}},
  author = {Lopez, Michael and Casey, C. Jasson and Reis, Gabriel Dos and Chojnacki, Colton}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Software Defined Networking (SDN) programs are written with respect to assumptions on software and hardware facilities and protocol definitions. Silent mismatches between the expected feature set and implemented feature set of SDN artifacts can easily lead to hard to debug network configurations, decreased network performance, outages, or worse, security vulnerabilities. We show how the paradigm of axiomatic programming, supported by practical dependent types, provides effective support for SDN executable specifications and verification.​}},
  url = {https://doi.org/10.1145/2814204.2814218},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup624,
  title = {Disjoint intersection types}},
  author = {Oliveira, Bruno C. d. S. and Shi, Zhiyuan and Alpuim, Jo\~{a}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Dunfield showed that a simply typed core calculus with intersection types and a merge operator is able to capture various programming language features. While his calculus is type-safe, it is not coherent: different derivations for the same expression can elaborate to expressions that evaluate to different values. The lack of coherence is an important disadvantage for adoption of his core calculus in implementations of programming languages, as the semantics of the programming language becomes implementation-dependent. This paper presents λ_i: a coherent and type-safe calculus with a form of intersection types and a merge operator. Coherence is achieved by ensuring that intersection types are disjoint and programs are sufficiently annotated to avoid type ambiguity. We propose a definition of disjointness where two types A and B are disjoint only if certain set of types are common supertypes of A and B. We investigate three different variants of λ_i, with three variants of disjointness. In the simplest variant, which does not allow ⊤ types, two types are disjoint if they do not share any common supertypes at all. The other two variants introduce ⊤ types and refine the notion of disjointness to allow two types to be disjoint when the only the set of common supertypes are top-like. The difference between the two variants with ⊤ types is on the definition of top-like types, which has an impact on which types are allowed on intersections. We present a type system that prevents intersection types that are not disjoint, as well as an algorithmic specifications to determine whether two types are disjoint for all three variants.}},
  url = {https://doi.org/10.1145/2951913.2951945},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup625,
  title = {Synthesizing regular expressions from examples for introductory automata assignments}},
  author = {Lee, Mina and So, Sunbeom and Oh, Hakjoo}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a method for synthesizing regular expressions for introductory automata assignments. Given a set of positive and negative examples, the method automatically synthesizes the simplest possible regular expression that accepts all the positive examples while rejecting all the negative examples. The key novelty is the search-based synthesis algorithm that leverages ideas from over- and under-approximations to effectively prune out a large search space. We have implemented our technique in a tool and evaluated it with non-trivial benchmark problems that students often struggle with. The results show that our system can synthesize desired regular expressions in 6.7 seconds on the average, so that it can be interactively used by students to enhance their understanding of regular expressions.}},
  url = {https://doi.org/10.1145/2993236.2993244},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup626,
  title = {Specifying and checking semantic atomicity for multithreaded programs}},
  author = {Burnim, Jacob and Necula, George and Sen, Koushik}},
  year = {2011}},
  journal = {Proceedings of the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In practice, it is quite difficult to write correct multithreaded programs due to the potential for unintended and nondeterministic interference between parallel threads. A fundamental correctness property for such programs is atomicity---a block of code in a program is atomic if, for any parallel execution of the program, there is an execution with the same overall program behavior in which the block is executed serially.We propose semantic atomicity, a generalization of atomicity with respect to a programmer-defined notion of equivalent behavior. We propose an assertion framework in which a programmer can use bridge predicates to specify noninterference properties at the level of abstraction of their application. Further, we propose a novel algorithm for systematically testing atomicity specifications on parallel executions with a bounded number of interruptions---i.e. atomic blocks whose execution is interleaved with that of other threads. We further propose a set of sound heuristics and optional user annotations that increase the efficiency of checking atomicity specifications in the common case where the specifications hold.We have implemented our assertion framework for specifying and checking semantic atomicity for parallel Java programs, and we have written semantic atomicity specifications for a number of benchmarks. We found that using bridge predicates allowed us to specify the natural and intended atomic behavior of a wider range of programs than did previous approaches. Further, in checking our specifications, we found several previously unknown bugs, including in the widely-used java.util.concurrent library.}},
  url = {https://doi.org/10.1145/1950365.1950377},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup627,
  title = {Protein folding meets functional programming (poster)}},
  author = {Krasnogor, Natalio and Mart\'{\i}},
  year = {1997}},
  journal = {Proceedings of the Second ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {},
  url = {https://doi.org/10.1145/258948.258982},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup628,
  title = {Exploring and enforcing security guarantees via program dependence graphs}},
  author = {Johnson, Andrew and Waye, Lucas and Moore, Scott and Chong, Stephen}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present PIDGIN, a program analysis and understanding tool that enables the specification and enforcement of precise application-specific information security guarantees. PIDGIN also allows developers to interactively explore the information flows in their applications to develop policies and investigate counter-examples. PIDGIN combines program dependence graphs (PDGs), which precisely capture the information flows in a whole application, with a custom PDG query language. Queries express properties about the paths in the PDG; because paths in the PDG correspond to information flows in the application, queries can be used to specify global security policies. PIDGIN is scalable. Generating a PDG for a 330k line Java application takes 90 seconds, and checking a policy on that PDG takes under 14 seconds. The query language is expressive, supporting a large class of precise, application-specific security guarantees. Policies are separate from the code and do not interfere with testing or development, and can be used for security regression testing. We describe the design and implementation of PIDGIN and report on using it: (1) to explore information security guarantees in legacy programs; (2) to develop and modify security policies concurrently with application development; and (3) to develop policies based on known vulnerabilities.}},
  url = {https://doi.org/10.1145/2737924.2737957},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup629,
  title = {Flexible type analysis}},
  author = {Crary, Karl and Weirich, Stephanie}},
  year = {1999}},
  journal = {Proceedings of the Fourth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Run-time type dispatch enables a variety of advanced optimization techniques for polymorphic languages, including tag-free garbage collection, unboxed function arguments, and flattened data structures. However, modern type-preserving compilers transform types between stages of compilation, making type dispatch prohibitively complex at low levels of typed compilation. It is crucial therefore for type analysis at these low levels to refer to the types of previous stages. Unfortunately, no current intermediate language supports this facility.To fill this gap, we present the language LX, which provides a rich language of type constructors supporting type analysis (possibly of previous-stage types) as a programming idiom. This language is quite flexible, supporting a variety of other applications such as analysis of quantified types, analysis with incomplete type information, and type classes. We also show that LX is compatible with a type-erasure semantics.}},
  url = {https://doi.org/10.1145/317636.317906},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup630,
  title = {Manifest Contracts for Datatypes}},
  author = {Sekiyama, Taro and Nishida, Yuki and Igarashi, Atsushi}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We study algebraic data types in a manifest contract system, a software contract system where contract information occurs as refinement types. We first compare two simple approaches: refinements on type constructors and refinements on data constructors. For example, lists of positive integers can be described by {l:int list | for_all (lambda y. y > 0) l}},
  url = {https://doi.org/10.1145/2676726.2676996},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup631,
  title = {Bounding data races in space and time}},
  author = {Dolan, Stephen and Sivaramakrishnan, KC and Madhavapeddy, Anil}},
  year = {2018}},
  journal = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We propose a new semantics for shared-memory parallel programs that gives strong guarantees even in the presence of data races. Our local data race freedom property guarantees that all data-race-free portions of programs exhibit sequential semantics. We provide a straightforward operational semantics and an equivalent axiomatic model, and evaluate an implementation for the OCaml programming language. Our evaluation demonstrates that it is possible to balance a comprehensible memory model with a reasonable (no overhead on x86, ~0.6\% on ARM) sequential performance trade-off in a mainstream programming language.}},
  url = {https://doi.org/10.1145/3192366.3192421},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup632,
  title = {KJS: a complete formal semantics of JavaScript}},
  author = {Park, Daejun and Stef\u{a}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents KJS, the most complete and throughly tested formal semantics of JavaScript to date. Being executable, KJS has been tested against the ECMAScript 5.1 conformance test suite, and passes all 2,782 core language tests. Among the existing implementations of JavaScript, only Chrome V8's passes all the tests, and no other semantics passes more than 90\%. In addition to a reference implementation for JavaScript, KJS also yields a simple coverage metric for a test suite: the set of semantic rules it exercises. Our semantics revealed that the ECMAScript 5.1 conformance test suite fails to cover several semantic rules. Guided by the semantics, we wrote tests to exercise those rules. The new tests revealed bugs both in production JavaScript engines (Chrome V8, Safari WebKit, Firefox SpiderMonkey) and in other semantics. KJS is symbolically executable, thus it can be used for formal analysis and verification of JavaScript programs. We verified non-trivial programs and found a known security vulnerability.}},
  url = {https://doi.org/10.1145/2737924.2737991},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup633,
  title = {SMO: an integrated approach to intra-array and inter-array storage optimization}},
  author = {Bhaskaracharya, Somashekaracharya G. and Bondhugula, Uday and Cohen, Albert}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The polyhedral model provides an expressive intermediate representation that is convenient for the analysis and subsequent transformation of affine loop nests. Several heuristics exist for achieving complex program transformations in this model. However, there is also considerable scope to utilize this model to tackle the problem of automatic memory footprint optimization. In this paper, we present a new automatic storage optimization technique which can be used to achieve both intra-array as well as inter-array storage reuse with a pre-determined schedule for the computation. Our approach works by finding statement-wise storage partitioning hyperplanes that partition a unified global array space so that values with overlapping live ranges are not mapped to the same partition. Our heuristic is driven by a fourfold objective function which not only minimizes the dimensionality and storage requirements of arrays required for each high-level statement, but also maximizes inter-statement storage reuse. The storage mappings obtained using our heuristic can be asymptotically better than those obtained by any existing technique. We implement our technique and demonstrate its practical impact by evaluating its effectiveness on several benchmarks chosen from the domains of image processing, stencil computations, and high-performance computing.}},
  url = {https://doi.org/10.1145/2837614.2837636},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup634,
  title = {Optimal dynamic partial order reduction}},
  author = {Abdulla, Parosh and Aronis, Stavros and Jonsson, Bengt and Sagonas, Konstantinos}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Stateless model checking is a powerful technique for program verification, which however suffers from an exponential growth in the number of explored executions. A successful technique for reducing this number, while still maintaining complete coverage, is Dynamic Partial Order Reduction (DPOR). We present a new DPOR algorithm, which is the first to be provably optimal in that it always explores the minimal number of executions. It is based on a novel class of sets, called source sets, which replace the role of persistent sets in previous algorithms. First, we show how to modify an existing DPOR algorithm to work with source sets, resulting in an efficient and simple to implement algorithm. Second, we extend this algorithm with a novel mechanism, called wakeup trees, that allows to achieve optimality. We have implemented both algorithms in a stateless model checking tool for Erlang programs. Experiments show that source sets significantly increase the performance and that wakeup trees incur only a small overhead in both time and space.}},
  url = {https://doi.org/10.1145/2535838.2535845},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup635,
  title = {Safe nondeterminism in a deterministic-by-default parallel language}},
  author = {Bocchino, Robert L. and Heumann, Stephen and Honarmand, Nima and Adve, Sarita V. and Adve, Vikram S. and Welc, Adam and Shpeisman, Tatiana}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A number of deterministic parallel programming models with strong safety guarantees are emerging, but similar support for nondeterministic algorithms, such as branch and bound search, remains an open question. We present a language together with a type and effect system that supports nondeterministic computations with a deterministic-by-default guarantee: nondeterminism must be explicitly requested via special parallel constructs (marked nd), and any deterministic construct that does not execute any nd construct has deterministic input-output behavior. Moreover, deterministic parallel constructs are always equivalent to a sequential composition of their constituent tasks, even if they enclose, or are enclosed by, nd constructs. Finally, in the execution of nd constructs, interference may occur only between pairs of accesses guarded by atomic statements, so there are no data races, either between atomic statements and unguarded accesses (strong isolation) or between pairs of unguarded accesses (stronger than strong isolation alone). We enforce the guarantees at compile time with modular checking using novel extensions to a previously described effect system. Our effect system extensions also enable the compiler to remove unnecessary transactional synchronization. We provide a static semantics, dynamic semantics, and a complete proof of soundness for the language, both with and without the barrier removal feature. An experimental evaluation shows that our language can achieve good scalability for realistic parallel algorithms, and that the barrier removal techniques provide significant performance gains.}},
  url = {https://doi.org/10.1145/1926385.1926447},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup636,
  title = {Exploiting Vector and Multicore Parallelism for Recursive, Data- and Task-Parallel Programs}},
  author = {Ren, Bin and Krishnamoorthy, Sriram and Agrawal, Kunal and Kulkarni, Milind}},
  year = {2017}},
  journal = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Modern hardware contains parallel execution resources that are well-suited for data-parallelism-vector units-and task parallelism-multicores. However, most work on parallel scheduling focuses on one type of hardware or the other. In this work, we present a scheduling framework that allows for a unified treatment of task- and data-parallelism. Our key insight is an abstraction, task blocks, that uniformly handles data-parallel iterations and task-parallel tasks, allowing them to be scheduled on vector units or executed independently as multicores. Our framework allows us to define schedulers that can dynamically select between executing task- blocks on vector units or multicores. We show that these schedulers are asymptotically optimal, and deliver the maximum amount of parallelism available in computation trees. To evaluate our schedulers, we develop program transformations that can convert mixed data- and task-parallel pro- grams into task block-based programs. Using a prototype instantiation of our scheduling framework, we show that, on an 8-core system, we can simultaneously exploit vector and multicore parallelism to achieve 14\texttimes{}},
  url = {https://doi.org/10.1145/3018743.3018763},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup637,
  title = {Synthesis of ranking functions using extremal counterexamples}},
  author = {Gonnord, Laure and Monniaux, David and Radanne, Gabriel}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a complete method for synthesizing lexicographic linear ranking functions (and thus proving termination), supported by inductive invariants, in the case where the transition relation of the program includes disjunctions and existentials (large block encoding of control flow). Previous work would either synthesize a ranking function at every basic block head, not just loop headers, which reduces the scope of programs that may be proved to be terminating, or expand large block transitions including tests into (exponentially many) elementary transitions, prior to computing the ranking function, resulting in a very large global constraint system. In contrast, our algorithm incrementally refines a global linear constraint system according to extremal counterexamples: only constraints that exclude spurious solutions are included. Experiments with our tool Termite show marked performance and scalability improvements compared to other systems.}},
  url = {https://doi.org/10.1145/2737924.2737976},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup638,
  title = {Equality proofs and deferred type errors: a compiler pearl}},
  author = {Vytiniotis, Dimitrios and Peyton Jones, Simon and Magalh\~{a}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The Glasgow Haskell Compiler is an optimizing compiler that expresses and manipulates first-class equality proofs in its intermediate language. We describe a simple, elegant technique that exploits these equality proofs to support deferred type errors. The technique requires us to treat equality proofs as possibly-divergent terms; we show how to do so without losing either soundness or the zero-overhead cost model that the programmer expects.}},
  url = {https://doi.org/10.1145/2364527.2364554},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup639,
  title = {Expressing contract monitors as patterns of communication}},
  author = {Swords, Cameron and Sabry, Amr and Tobin-Hochstadt, Sam}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a new approach to contract semantics which expresses myriad monitoring strategies using a small core of foundational communication primitives. This approach allows multiple existing contract monitoring approaches, ranging from Findler and Felleisen’s original model of higher-order contracts to semi-eager, parallel, or asynchronous monitors, to be expressed in a single language built on well-understood constructs. We prove that this approach accurately simulates the original semantics of higher-order contracts. A straightforward implementation in Racket demonstrates the practicality of our approach which not only enriches existing Racket monitoring strategies, but also support a new style of monitoring in which collections of contracts collaborate to establish a global invariant.}},
  url = {https://doi.org/10.1145/2784731.2784742},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup640,
  title = {Relatively complete counterexamples for higher-order programs}},
  author = {Nguy\~{\^e}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this paper, we study the problem of generating inputs to a higher-order program causing it to error. We first approach the problem in the setting of PCF, a typed, core functional language and contribute the first relatively complete method for constructing counterexamples for PCF programs. The method is relatively complete with respect to a first-order solver over the base types of PCF. In practice, this means an SMT solver can be used for the effective, automated generation of higher-order counterexamples for a large class of programs. We achieve this result by employing a novel form of symbolic execution for higher-order programs. The remarkable aspect of this symbolic execution is that even though symbolic higher-order inputs and values are considered, the path condition remains a first-order formula. Our handling of symbolic function application enables the reconstruction of higher-order counterexamples from this first-order formula. After establishing our main theoretical results, we sketch how to apply the approach to untyped, higher-order, stateful languages with first-class contracts and show how counterexample generation can be used to detect contract violations in this setting. To validate our approach, we implement a tool generating counterexamples for erroneous modules written in Racket.}},
  url = {https://doi.org/10.1145/2737924.2737971},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup641,
  title = {Verified validation of lazy code motion}},
  author = {Tristan, Jean-Baptiste and Leroy, Xavier}},
  year = {2009}},
  journal = {Proceedings of the 30th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Translation validation establishes a posteriori the correctness of a run of a compilation pass or other program transformation. In this paper, we develop an efficient translation validation algorithm for the Lazy Code Motion (LCM) optimization. LCM is an interesting challenge for validation because it is a global optimization that moves code across loops. Consequently, care must be taken not to move computations that may fail before loops that may not terminate. Our validator includes a specific check for anticipability to rule out such incorrect moves. We present a mechanically-checked proof of correctness of the validation algorithm, using the Coq proof assistant. Combining our validator with an unverified implementation of LCM, we obtain a LCM pass that is provably semantics-preserving and was integrated in the CompCert formally verified compiler.}},
  url = {https://doi.org/10.1145/1542476.1542512},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup642,
  title = {Parsing with first-class derivatives}},
  author = {Brachth\"{a}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Brzozowski derivatives, well known in the context of regular expressions, have recently been rediscovered to give a simplified explanation to parsers of context-free languages. We add derivatives as a novel first-class feature to a standard parser combinator language. First-class derivatives enable an inversion of the control flow, allowing to implement modular parsers for languages that previously required separate pre-processing steps or cross-cutting modifications of the parsers. We show that our framework offers new opportunities for reuse and supports a modular definition of interesting use cases of layout-sensitive parsing.}},
  url = {https://doi.org/10.1145/2983990.2984026},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup643,
  title = {Practical SMT-based type error localization}},
  author = {Pavlinovic, Zvonimir and King, Tim and Wies, Thomas}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Compilers for statically typed functional programming languages are notorious for generating confusing type error messages. When the compiler detects a type error, it typically reports the program location where the type checking failed as the source of the error. Since other error sources are not even considered, the actual root cause is often missed. A more adequate approach is to consider all possible error sources and report the most useful one subject to some usefulness criterion. In our previous work, we showed that this approach can be formulated as an optimization problem related to satisfiability modulo theories (SMT). This formulation cleanly separates the heuristic nature of usefulness criteria from the underlying search problem. Unfortunately, algorithms that search for an optimal error source cannot directly use principal types which are crucial for dealing with the exponential-time complexity of the decision problem of polymorphic type checking. In this paper, we present a new algorithm that efficiently finds an optimal error source in a given ill-typed program. Our algorithm uses an improved SMT encoding to cope with the high complexity of polymorphic typing by iteratively expanding the typing constraints from which principal types are derived. The algorithm preserves the clean separation between the heuristics and the actual search. We have implemented our algorithm for OCaml. In our experimental evaluation, we found that the algorithm reduces the running times for optimal type error localization from minutes to seconds and scales better than previous localization algorithms.}},
  url = {https://doi.org/10.1145/2784731.2784765},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup644,
  title = {Chapar: certified causally consistent distributed key-value stores}},
  author = {Lesani, Mohsen and Bell, Christian J. and Chlipala, Adam}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Today’s Internet services are often expected to stay available and render high responsiveness even in the face of site crashes and network partitions. Theoretical results state that causal consistency is one of the strongest consistency guarantees that is possible under these requirements, and many practical systems provide causally consistent key-value stores. In this paper, we present a framework called Chapar for modular verification of causal consistency for replicated key-value store implementations and their client programs. Specifically, we formulate separate correctness conditions for key-value store implementations and for their clients. The interface between the two is a novel operational semantics for causal consistency. We have verified the causal consistency of two key-value store implementations from the literature using a novel proof technique. We have also implemented a simple automatic model checker for the correctness of client programs. The two independently verified results for the implementations and clients can be composed to conclude the correctness of any of the programs when executed with any of the implementations. We have developed and checked our framework in Coq, extracted it to OCaml, and built executable stores.}},
  url = {https://doi.org/10.1145/2837614.2837622},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup645,
  title = {Natural proofs for data structure manipulation in C using separation logic}},
  author = {Pek, Edgar and Qiu, Xiaokang and Madhusudan, P.}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The natural proof technique for heap verification developed by Qiu et al. [32] provides a platform for powerful sound reasoning for specifications written in a dialect of separation logic called Dryad. Natural proofs are proof tactics that enable automated reasoning exploiting recursion, mimicking common patterns found in human proofs. However, these proofs are known to work only for a simple toy language [32].In this work, we develop a framework called VCDryad that extends the Vcc framework [9] to provide an automated deductive framework against separation logic specifications for C programs based on natural proofs. We develop several new techniques to build this framework, including (a) a novel tool architecture that allows encoding natural proofs at a higher level in order to use the existing Vcc framework (including its intricate memory model, the underlying type-checker, and the SMT-based verification infrastructure), and (b) a synthesis of ghost-code annotations that captures natural proof tactics, in essence forcing Vcc to find natural proofs using primarily decidable theories.We evaluate our tool extensively, on more than 150 programs, ranging from code manipulating standard data structures, well-known open source library routines (Glib, OpenBSD), Linux kernel routines, customized OS data structures, etc. We show that all these C programs can be fully automatically verified using natural proofs (given pre/post conditions and loop invariants) without any user-provided proof tactics. VCDryad is perhaps the first deductive verification framework for heap-manipulating programs in a real language that can prove such a wide variety of programs automatically.}},
  url = {https://doi.org/10.1145/2594291.2594325},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup646,
  title = {Just-in-time static type checking for dynamic languages}},
  author = {Ren, Brianna M. and Foster, Jeffrey S.}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Dynamic languages such as Ruby, Python, and JavaScript have many compelling benefits, but the lack of static types means subtle errors can remain latent in code for a long time. While many researchers have developed various systems to bring some of the benefits of static types to dynamic languages, prior approaches have trouble dealing with metaprogramming, which generates code as the program executes. In this paper, we propose Hummingbird, a new system that uses a novel technique, just-in-time static type checking, to type check Ruby code even in the presence of metaprogramming. In Hummingbird, method type signatures are gathered dynamically at run-time, as those methods are created. When a method is called, Hummingbird statically type checks the method body against current type signatures. Thus, Hummingbird provides thorough static checks on a per-method basis, while also allowing arbitrarily complex metaprogramming. For performance, Hummingbird memoizes the static type checking pass, invalidating cached checks only if necessary. We formalize Hummingbird using a core, Ruby-like language and prove it sound. To evaluate Hummingbird, we applied it to six apps, including three that use Ruby on Rails, a powerful framework that relies heavily on metaprogramming. We found that all apps typecheck successfully using Hummingbird, and that Hummingbird's performance overhead is reasonable. We applied Hummingbird to earlier versions of one Rails app and found several type errors that had been introduced and then fixed. Lastly, we demonstrate using Hummingbird in Rails development mode to typecheck an app as live updates are applied to it.}},
  url = {https://doi.org/10.1145/2908080.2908127},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup647,
  title = {Declarative fence insertion}},
  author = {Bender, John and Lesani, Mohsen and Palsberg, Jens}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Previous work has shown how to insert fences that enforce sequential consistency. However, for many concurrent algorithms, sequential consistency is unnecessarily strong and can lead to high execution overhead. The reason is that, often, correctness relies on the execution order of a few specific pairs of instructions. Algorithm designers can declare those execution orders and thereby enable memory-model-independent reasoning about correctness and also ease implementation of algorithms on multiple platforms. The literature has examples of such reasoning, while tool support for enforcing the orders has been lacking until now. In this paper we present a declarative approach to specify and enforce execution orders. Our fence insertion algorithm first identifies the execution orders that a given memory model enforces automatically, and then inserts fences that enforce the rest. Our benchmarks include three off-the-shelf transactional memory algorithms written in C/C++ for which we specify suitable execution orders. For those benchmarks, our experiments with the x86 and ARMv7 memory models show that our tool inserts fences that are competitive with those inserted by the original authors. Our tool is the first to insert fences into transactional memory algorithms and it solves the long-standing problem of how to easily port such algorithms to a novel memory model.}},
  url = {https://doi.org/10.1145/2814270.2814318},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup648,
  title = {Model checking for symbolic-heap separation logic with inductive predicates}},
  author = {Brotherston, James and Gorogiannis, Nikos and Kanovich, Max and Rowe, Reuben}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We investigate the *model checking* problem for symbolic-heap separation logic with user-defined inductive predicates, i.e., the problem of checking that a given stack-heap memory state satisfies a given formula in this language, as arises e.g. in software testing or runtime verification. First, we show that the problem is *decidable*; specifically, we present a bottom-up fixed point algorithm that decides the problem and runs in exponential time in the size of the problem instance. Second, we show that, while model checking for the full language is EXPTIME-complete, the problem becomes NP-complete or PTIME-solvable when we impose natural syntactic restrictions on the schemata defining the inductive predicates. We additionally present NP and PTIME algorithms for these restricted fragments. Finally, we report on the experimental performance of our procedures on a variety of specifications extracted from programs, exercising multiple combinations of syntactic restrictions.}},
  url = {https://doi.org/10.1145/2837614.2837621},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup649,
  title = {Improving STM performance with transactional structs}},
  author = {Yates, Ryan and Scott, Michael L.}},
  year = {2017}},
  journal = {Proceedings of the 10th ACM SIGPLAN International Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Software transactional memory (STM) has made it significantly easier to write correct concurrent programs in Haskell. Its performance, however, is limited by several inefficiencies. While safe concurrent computations are easy to express in Haskell's STM, concurrent data structures suffer unfortunate bloat in the implementation due to an extra level of indirection for mutable references as well as the inability to express unboxed mutable transactional values. We address these deficiencies by introducing TStruct to the GHC run-time system, allowing strict unboxed transactional values as well as mutable references without an extra indirection. Using TStruct we implement several data structures, discuss their design, and provide benchmark results on a large multicore machine. Our benchmarks show that concurrent data structures built with TStruct out-scale and out-perform their TVar-based equivalents.}},
  url = {https://doi.org/10.1145/3122955.3122972},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup650,
  title = {Occurrence typing modulo theories}},
  author = {Kent, Andrew M. and Kempe, David and Tobin-Hochstadt, Sam}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a new type system combining occurrence typing---a technique previously used to type check programs in dynamically-typed languages such as Racket, Clojure, and JavaScript---with dependent refinement types. We demonstrate that the addition of refinement types allows the integration of arbitrary solver-backed reasoning about logical propositions from external theories. By building on occurrence typing, we can add our enriched type system as a natural extension of Typed Racket, reusing its core while increasing its expressiveness. The result is a well-tested type system with a conservative, decidable core in which types may depend on a small but extensible set of program terms. In addition to describing our design, we present the following: a formal model and proof of correctness; a strategy for integrating new theories, with specific examples including linear arithmetic and bitvectors; and an evaluation in the context of the full Typed Racket implementation. Specifically, we take safe vector operations as a case study, examining all vector accesses in a 56,000 line corpus of Typed Racket programs. Our system is able to prove that 50\% of these are safe with no new annotations, and with a few annotations and modifications we capture more than 70\%.}},
  url = {https://doi.org/10.1145/2908080.2908091},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup651,
  title = {Replicated data types: specification, verification, optimality}},
  author = {Burckhardt, Sebastian and Gotsman, Alexey and Yang, Hongseok and Zawirski, Marek}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Geographically distributed systems often rely on replicated eventually consistent data stores to achieve availability and performance. To resolve conflicting updates at different replicas, researchers and practitioners have proposed specialized consistency protocols, called replicated data types, that implement objects such as registers, counters, sets or lists. Reasoning about replicated data types has however not been on par with comparable work on abstract data types and concurrent data types, lacking specifications, correctness proofs, and optimality results.To fill in this gap, we propose a framework for specifying replicated data types using relations over events and verifying their implementations using replication-aware simulations. We apply it to 7 existing implementations of 4 data types with nontrivial conflict-resolution strategies and optimizations (last-writer-wins register, counter, multi-value register and observed-remove set). We also present a novel technique for obtaining lower bounds on the worst-case space overhead of data type implementations and use it to prove optimality of 4 implementations. Finally, we show how to specify consistency of replicated stores with multiple objects axiomatically, in analogy to prior work on weak memory models. Overall, our work provides foundational reasoning tools to support research on replicated eventually consistent stores.}},
  url = {https://doi.org/10.1145/2535838.2535848},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup652,
  title = {Context transformations for pointer analysis}},
  author = {Thiessen, Rei and Lhot\'{a}},
  year = {2017}},
  journal = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Points-to analysis for Java benefits greatly from context sensitivity. CFL-reachability and k-limited context strings are two approaches to obtaining context sensitivity with different advantages: CFL-reachability allows local reasoning about data-value flow and thus is suitable for demand-driven analyses, whereas k-limited analyses allow object sensitivity which is a superior calling context abstraction for object-oriented languages. We combine the advantages of both approaches to obtain a context-sensitive analysis that is as precise as k-limited context strings, but is more efficient to compute. Our key insight is based on a novel abstraction of contexts adapted from CFL-reachability that represents a relation between two calling contexts as a composition of transformations over contexts. We formulate pointer analysis in an algebraic structure of context transformations, which is a set of functions over calling contexts closed under function composition. We show that the context representation of context-string-based analyses is an explicit enumeration of all input and output values of context transformations. CFL-reachability-based pointer analysis is formulated to use call-strings as contexts, but the context transformations concept can be applied to any context abstraction used in k-limited analyses, including object- and type-sensitive analysis. The result is a more efficient algorithm for computing context-sensitive results for a wide variety of context configurations.}},
  url = {https://doi.org/10.1145/3062341.3062359},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup653,
  title = {Partial type equivalences for verified dependent interoperability}},
  author = {Dagand, Pierre-Evariste and Tabareau, Nicolas and Tanter, \'{E}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Full-spectrum dependent types promise to enable the development of correct-by-construction software. However, even certified software needs to interact with simply-typed or untyped programs, be it to perform system calls, or to use legacy libraries. Trading static guarantees for runtime checks, the dependent interoperability framework provides a mechanism by which simply-typed values can safely be coerced to dependent types and, conversely, dependently-typed programs can defensively be exported to a simply-typed application. In this paper, we give a semantic account of dependent interoperability. Our presentation relies on and is guided by a pervading notion of type equivalence, whose importance has been emphasized in recent work on homotopy type theory. Specifically, we develop the notion of partial type equivalences as a key foundation for dependent interoperability. Our framework is developed in Coq; it is thus constructive and verified in the strictest sense of the terms. Using our library, users can specify domain-specific partial equivalences between data structures. Our library then takes care of the (sometimes, heavy) lifting that leads to interoperable programs. It thus becomes possible, as we shall illustrate, to internalize and hand-tune the extraction of dependently-typed programs to interoperable OCaml programs within Coq itself.}},
  url = {https://doi.org/10.1145/2951913.2951933},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup654,
  title = {Synthesis of interface specifications for Java classes}},
  author = {Alur, Rajeev and \v{C}},
  year = {2005}},
  journal = {Proceedings of the 32nd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {While a typical software component has a clearly specified (static) interface in terms of the methods and the input/output types they support, information about the correct sequencing of method calls the client must invoke is usually undocumented. In this paper, we propose a novel solution for automatically extracting such temporal specifications for Java classes. Given a Java class, and a safety property such as "the exception E should not be raised", the corresponding (dynamic) interface is the most general way of invoking the methods in the class so that the safety property is not violated. Our synthesis method first constructs a symbolic representation of the finite state-transition system obtained from the class using predicate abstraction. Constructing the interface then corresponds to solving a partial-information two-player game on this symbolic graph. We present a sound approach to solve this computationally-hard problem approximately using algorithms for learning finite automata and symbolic model checking for branching-time logics. We describe an implementation of the proposed techniques in the tool JIST--- Java Interface Synthesis Tool---and demonstrate that the tool can construct interfaces accurately and efficiently for sample Java2SDK library classes.}},
  url = {https://doi.org/10.1145/1040305.1040314},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup655,
  title = {Static Detection of Event-based Races in Android Apps}},
  author = {Hu, Yongjian and Neamtiu, Iulian}},
  year = {2018}},
  journal = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Event-based races are the main source of concurrency errors in Android apps. Prior approaches for scalable detection of event-based races have been dynamic. Due to their dynamic nature, these approaches suffer from coverage and false negative issues. We introduce a precise and scalable static approach and tool, named SIERRA, for detecting Android event-based races. SIERRA is centered around a new concept of "concurrency action" (that reifies threads, events/messages, system and user actions) and statically-derived order (happens-before relation) between actions. Establishing action order is complicated in Android, and event-based systems in general, because of externally-orchestrated control flow, use of callbacks, asynchronous tasks, and ad-hoc synchronization. We introduce several novel approaches that enable us to infer order relations statically: auto-generated code models which impose order among lifecycle and GUI events; a novel context abstraction for event-driven programs named action-sensitivity and finally, on-demand path sensitivity via backward symbolic execution to further rule out false positives. We have evaluated SIERRA on 194 Android apps. Of these, we chose 20 apps for manual analysis and comparison with a state-of-the-art dynamic race detector. Experimental results show that SIERRA is effective and efficient, typically taking 960 seconds to analyze an app and revealing 43 potential races. Compared with the dynamic race detector, SIERRA discovered an average 29.5 true races with 3.5 false positives, where the dynamic detector only discovered 4 races (hence missing 25.5 races per app) -- this demonstrates the advantage of a precise static approach. We believe that our approach opens the way for precise analysis and static event race detection in other event-driven systems beyond Android.}},
  url = {https://doi.org/10.1145/3173162.3173173},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup656,
  title = {End-to-end verification of information-flow security for C and assembly programs}},
  author = {Costanzo, David and Shao, Zhong and Gu, Ronghui}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Protecting the confidentiality of information manipulated by a computing system is one of the most important challenges facing today's cybersecurity community. A promising step toward conquering this challenge is to formally verify that the end-to-end behavior of the computing system really satisfies various information-flow policies. Unfortunately, because today's system software still consists of both C and assembly programs, the end-to-end verification necessarily requires that we not only prove the security properties of individual components, but also carefully preserve these properties through compilation and cross-language linking. In this paper, we present a novel methodology for formally verifying end-to-end security of a software system that consists of both C and assembly programs. We introduce a general definition of observation function that unifies the concepts of policy specification, state indistinguishability, and whole-execution behaviors. We show how to use different observation functions for different levels of abstraction, and how to link different security proofs across abstraction levels using a special kind of simulation that is guaranteed to preserve state indistinguishability. To demonstrate the effectiveness of our new methodology, we have successfully constructed an end-to-end security proof, fully formalized in the Coq proof assistant, of a nontrivial operating system kernel (running on an extended CompCert x86 assembly machine model). Some parts of the kernel are written in C and some are written in assembly; we verify all of the code, regardless of language.}},
  url = {https://doi.org/10.1145/2908080.2908100},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup657,
  title = {GC assertions: using the garbage collector to check heap properties}},
  author = {Aftandilian, Edward E. and Guyer, Samuel Z.}},
  year = {2009}},
  journal = {Proceedings of the 30th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper introduces GC assertions, a system interface that programmers can use to check for errors, such as data structure invariant violations, and to diagnose performance problems, such as memory leaks. GC assertions are checked by the garbage collector, which is in a unique position to gather information and answer questions about the lifetime and connectivity of objects in the heap. By piggybacking on existing garbage collector computations, our system is able to check heap properties with very low overhead -- around 3\% of total execution time -- low enough for use in a deployed setting.We introduce several kinds of GC assertions and describe how they are implemented in the collector. We also describe our reporting mechanism, which provides a complete path through the heap to the offending objects. We report results on both the performance of our system and the experience of using our assertions to find and repair errors in real-world programs.}},
  url = {https://doi.org/10.1145/1542476.1542503},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup658,
  title = {Sums of uncertainty: refinements go gradual}},
  author = {Jafery, Khurram A. and Dunfield, Jana}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A long-standing shortcoming of statically typed functional languages is that type checking does not rule out pattern-matching failures (run-time match exceptions). Refinement types distinguish different values of datatypes; if a program annotated with refinements passes type checking, pattern-matching failures become impossible. Unfortunately, refinement is a monolithic property of a type, exacerbating the difficulty of adding refinement types to nontrivial programs. Gradual typing has explored how to incrementally move between static typing and dynamic typing. We develop a type system of gradual sums that combines refinement with imprecision. Then, we develop a bidirectional version of the type system, which rules out excessive imprecision, and give a type-directed translation to a target language with explicit casts. We prove that the static sublanguage cannot have match failures, that a well-typed program remains well-typed if its type annotations are made less precise, and that making annotations less precise causes target programs to fail later. Several of these results correspond to criteria for gradual typing given by Siek et al. (2015).}},
  url = {https://doi.org/10.1145/3009837.3009865},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup659,
  title = {Semantic-directed clumping of disjunctive abstract states}},
  author = {Li, Huisong and Berenger, Francois and Chang, Bor-Yuh Evan and Rival, Xavier}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {To infer complex structural invariants, shape analyses rely on expressive families of logical properties. Many such analyses manipulate abstract memory states that consist of separating conjunctions of basic predicates describing atomic blocks or summaries. Moreover, they use finite disjunctions of abstract memory states in order to account for dissimilar shapes. Disjunctions should be kept small for the sake of scalability, though precision often requires to keep additional case splits. In this context, deciding when and how to merge case splits and to replace them with summaries is critical both for the precision and for the efficiency. Existing techniques use sets of syntactic rules, which are tedious to design and prone to failure. In this paper, we design a semantic criterion to clump abstract states based on their silhouette which applies not only to the conservative union of disjuncts, but also to the weakening of separating conjunction of memory predicates into inductive summaries. Our approach allows to define union and widening operators that aim at preserving the case splits that are required for the analysis to succeed. We implement this approach in the MemCAD analyzer, and evaluate it on real-world C codes from existing libraries, including programs dealing with doubly linked lists, red-black trees and AVL-trees.}},
  url = {https://doi.org/10.1145/3009837.3009881},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup660,
  title = {Programming with angelic nondeterminism}},
  author = {Bodik, Rastislav and Chandra, Satish and Galenson, Joel and Kimelman, Doug and Tung, Nicholas and Barman, Shaon and Rodarmor, Casey}},
  year = {2010}},
  journal = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Angelic nondeterminism can play an important role in program development. It simplifies specifications, for example in deriving programs with a refinement calculus; it is the formal basis of regular expressions; and Floyd relied on it to concisely express backtracking algorithms such as N-queens.We show that angelic nondeterminism is also useful during the development of deterministic programs. The semantics of our angelic operator are the same as Floyd's but we use it as a substitute for yet-to-be-written deterministic code; the final program is fully deterministic. The angelic operator divines a value that makes the program meet its specification, if possible. Because the operator is executable, it allows the programmer to test incomplete programs: if a program has no safe execution, it is already incorrect; if a program does have a safe execution, the execution may reveal an implementation strategy to the programmer.We introduce refinement-based angelic programming, describe our embedding of angelic operators into Scala, report on our implementation with bounded model checking, and describe our experience with two case studies. In one of the studies, we use angelic operators to modularize the Deutsch-Schorr-Waite (DSW) algorithm. The modularization is performed with the notion of a parasitic stack, whose incomplete specification was instantiated for DSW with angelic nondeterminism.}},
  url = {https://doi.org/10.1145/1706299.1706339},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup661,
  title = {Superficially substructural types}},
  author = {Krishnaswami, Neelakantan R. and Turon, Aaron and Dreyer, Derek and Garg, Deepak}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many substructural type systems have been proposed for controlling access to shared state in higher-order languages. Central to these systems is the notion of a *resource*, which may be split into disjoint pieces that different parts of a program can manipulate independently without worrying about interfering with one another. Some systems support a *logical* notion of resource (such as permissions), under which two resources may be considered disjoint even if they govern the *same* piece of state. However, in nearly all existing systems, the notions of resource and disjointness are fixed at the outset, baked into the model of the language, and fairly coarse-grained in the kinds of sharing they enable.In this paper, inspired by recent work on "fictional disjointness" in separation logic, we propose a simple and flexible way of enabling any module in a program to create its own custom type of splittable resource (represented as a commutative monoid), thus providing fine-grained control over how the module's private state is shared with its clients. This functionality can be incorporated into an otherwise standard substructural type system by means of a new typing rule we call *the sharing rule*, whose soundness we prove semantically via a novel resource-oriented Kripke logical relation.}},
  url = {https://doi.org/10.1145/2364527.2364536},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup662,
  title = {An equivalence-preserving CPS translation via multi-language semantics}},
  author = {Ahmed, Amal and Blume, Matthias}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Language-based security relies on the assumption that all potential attacks follow the rules of the language in question. When programs are compiled into a different language, this is true only if the translation process preserves observational equivalence.To prove that a translation preserves equivalence, one must show that if two program fragments cannot be distinguished by any source context, then their translations cannot be distinguished by any target context. Informally, target contexts must be no more powerful than source contexts, i.e., for every target context there exists a source context that "behaves the same." This seems to amount to being able to "back-translate" arbitrary target terms. However, that is simply not viable for practical compilers where the target language is lower-level and, thus, contains expressions that have no source equivalent.In this paper, we give a CPS translation from a less expressive source language (STLC) to a more expressive target language (System F) and prove that the translation preserves observational equivalence. The key to our equivalence-preserving compilation is the choice of the right type translation: a source type σ mandates a set of behaviors and we must ensure that its translation σ+ mandates semantically equivalent behaviors at the target level. Based on this type translation, we demonstrate how to prove that for every target term of type σ+, there exists an equivalent source term of type σ- even when sub-terms of the target term are not necessarily "back-translatable" themselves. A key novelty of our proof, resulting in a pleasant proof structure, is that it leverages a multi-language semantics where source and target terms may interoperate.}},
  url = {https://doi.org/10.1145/2034773.2034830},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup663,
  title = {On the linear ranking problem for integer linear-constraint loops}},
  author = {Ben-Amram, Amir M. and Genaim, Samir}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this paper we study the complexity of the Linear Ranking problem: given a loop, described by linear constraints over a finite set of integer variables, is there a linear ranking function for this loop? While existence of such a function implies termination, this problem is not equivalent to termination. When the variables range over the rationals or reals, the Linear Ranking problem is known to be PTIME decidable. However, when they range over the integers, whether for single-path or multipath loops, the complexity of the Linear Ranking problem has not yet been determined. We show that it is coNP-complete. However, we point out some special cases of importance of PTIME complexity. We also present complete algorithms for synthesizing linear ranking functions, both for the general case and the special PTIME cases.}},
  url = {https://doi.org/10.1145/2429069.2429078},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup664,
  title = {Causal commutative arrows revisited}},
  author = {Yallop, Jeremy and Liu, Hai}},
  year = {2016}},
  journal = {Proceedings of the 9th International Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Causal commutative arrows (CCA) extend arrows with additional constructs and laws that make them suitable for modelling domains such as functional reactive programming, differential equations and synchronous dataflow. Earlier work has revealed that a syntactic transformation of CCA computations into normal form can result in significant performance improvements, sometimes increasing the speed of programs by orders of magnitude. In this work we reformulate the normalization as a type class instance and derive optimized observation functions via a specialization to stream transformers to demonstrate that the same dramatic improvements can be achieved without leaving the language.}},
  url = {https://doi.org/10.1145/2976002.2976019},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup665,
  title = {Probabilistic relational verification for cryptographic implementations}},
  author = {Barthe, Gilles and Fournet, C\'{e}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Relational program logics have been used for mechanizing formal proofs of various cryptographic constructions. With an eye towards scaling these successes towards end-to-end security proofs for implementations of distributed systems, we present RF*, a relational extension of F*, a general-purpose higher-order stateful programming language with a verification system based on refinement types. The distinguishing feature of F* is a relational Hoare logic for a higher-order, stateful, probabilistic language. Through careful language design, we adapt the F* typechecker to generate both classic and relational verification conditions, and to automatically discharge their proofs using an SMT solver. Thus, we are able to benefit from the existing features of F*, including its abstraction facilities for modular reasoning about program fragments. We evaluate RF* experimentally by programming a series of cryptographic constructions and protocols, and by verifying their security properties, ranging from information flow to unlinkability, integrity, and privacy. Moreover, we validate the design of RF* by formalizing in Coq a core probabilistic λ calculus and a relational refinement type system and proving the soundness of the latter against a denotational semantics of the probabilistic lambda λ calculus.}},
  url = {https://doi.org/10.1145/2535838.2535847},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup666,
  title = {Precise reasoning for programs using containers}},
  author = {Dillig, Isil and Dillig, Thomas and Aiken, Alex}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Containers are general-purpose data structures that provide functionality for inserting, reading, removing, and iterating over elements. Since many applications written in modern programming languages, such as C++ and Java, use containers as standard building blocks, precise analysis of many programs requires a fairly sophisticated understanding of container contents. In this paper, we present a sound, precise, and fully automatic technique for static reasoning about contents of containers. We show that the proposed technique adds useful precision for verifying real C++ applications and that it scales to applications with over 100,000 lines of code.}},
  url = {https://doi.org/10.1145/1926385.1926407},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup667,
  title = {Static detection of asymptotic performance bugs in collection traversals}},
  author = {Olivo, Oswaldo and Dillig, Isil and Lin, Calvin}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper identifies and formalizes a prevalent class of asymptotic performance bugs called redundant traversal bugs and presents a novel static analysis for automatically detecting them. We evaluate our technique by implementing it in a tool called CLARITY and applying it to widely-used software packages such as the Google Core Collections Library, the Apache Common Collections, and the Apache Ant build tool. Across 1.6M lines of Java code, CLARITY finds 92 instances of redundant traversal bugs, including 72 that have never been previously reported, with just 5 false positives. To evaluate the performance impact of these bugs, we manually repair these programs and find that for an input size of 50,000, all repaired programs are at least 2.45 faster than their original code.}},
  url = {https://doi.org/10.1145/2737924.2737966},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup668,
  title = {Foundational extensible corecursion: a proof assistant perspective}},
  author = {Blanchette, Jasmin Christian and Popescu, Andrei and Traytel, Dmitriy}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents a formalized framework for defining corecursive functions safely in a total setting, based on corecursion up-to and relational parametricity. The end product is a general corecursor that allows corecursive (and even recursive) calls under "friendly" operations, including constructors. Friendly corecursive functions can be registered as such, thereby increasing the corecursor's expressiveness. The metatheory is formalized in the Isabelle proof assistant and forms the core of a prototype tool. The corecursor is derived from first principles, without requiring new axioms or extensions of the logic.}},
  url = {https://doi.org/10.1145/2784731.2784732},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup669,
  title = {Semantics-based program verifiers for all languages}},
  author = {Stef\u{a}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a language-independent verification framework that can be instantiated with an operational semantics to automatically generate a program verifier. The framework treats both the operational semantics and the program correctness specifications as reachability rules between matching logic patterns, and uses the sound and relatively complete reachability logic proof system to prove the specifications using the semantics. We instantiate the framework with the semantics of one academic language, KernelC, as well as with three recent semantics of real-world languages, C, Java, and JavaScript, developed independently of our verification infrastructure. We evaluate our approach empirically and show that the generated program verifiers can check automatically the full functional correctness of challenging heap-manipulating programs implementing operations on list and tree data structures, like AVL trees. This is the first approach that can turn the operational semantics of real-world languages into correct-by-construction automatic verifiers.}},
  url = {https://doi.org/10.1145/2983990.2984027},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup670,
  title = {Mechanized verification of fine-grained concurrent programs}},
  author = {Sergey, Ilya and Nanevski, Aleksandar and Banerjee, Anindya}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Efficient concurrent programs and data structures rarely employ coarse-grained synchronization mechanisms (i.e., locks); instead, they implement custom synchronization patterns via fine-grained primitives, such as compare-and-swap. Due to sophisticated interference scenarios between threads, reasoning about such programs is challenging and error-prone, and can benefit from mechanization. In this paper, we present the first completely formalized framework for mechanized verification of full functional correctness of fine-grained concurrent programs. Our tool is based on the recently proposed program logic FCSL. It is implemented as an embedded DSL in the dependently-typed language of the Coq proof assistant, and is powerful enough to reason about programming features such as higher-order functions and local thread spawning. By incorporating a uniform concurrency model, based on state-transition systems and partial commutative monoids, FCSL makes it possible to build proofs about concurrent libraries in a thread-local, compositional way, thus facilitating scalability and reuse: libraries are verified just once, and their specifications are used ubiquitously in client-side reasoning. We illustrate the proof layout in FCSL by example, outline its infrastructure, and report on our experience of using FCSL to verify a number of concurrent algorithms and data structures.}},
  url = {https://doi.org/10.1145/2737924.2737964},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup671,
  title = {Low-overhead and fully automated statistical debugging with abstraction refinement}},
  author = {Zuo, Zhiqiang and Fang, Lu and Khoo, Siau-Cheng and Xu, Guoqing and Lu, Shan}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Cooperative statistical debugging is an effective approach for diagnosing production-run failures. To quickly identify failure predictors from the huge program predicate space, existing techniques rely on random or heuristics-guided predicate sampling at the user side. However, none of them can satisfy the requirements of low cost, low diagnosis latency, and high diagnosis quality simultaneously, which are all indispensable for statistical debugging to be practical. This paper presents a new technique that tackles the above challenges. We formulate the technique as an instance of abstraction refinement, where efficient abstract-level profiling is first applied to the whole program and its execution brings information that can pinpoint suspicious coarse-grained entities that need to be refined. The refinement profiles a corresponding set of fine-grained entities, and generates feedback that determines what to prune and what to refine next. The process is fully automated, and more importantly, guided by a mathematically rigorous analysis that guarantees that our approach produces the same debugging results as an exhaustive analysis in deterministic settings. We have implemented this technique for both C and Java on both single machine and distributed system. A thorough evaluation demonstrates that our approach yields (1) an order of magnitude reduction in the user-side runtime overhead even compared to a sampling-based approach and (2) two orders of magnitude reduction in the size of data transferred over the network, completely automatically without sacrificing any debugging capability.}},
  url = {https://doi.org/10.1145/2983990.2984005},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup672,
  title = {Universal properties of impure programming languages}},
  author = {Staton, Sam and Levy, Paul Blain}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We investigate impure, call-by-value programming languages. Our first language only has variables and let-binding. Its equational theory is a variant of Lambek's theory of multicategories that omits the commutativity axiom.We demonstrate that type constructions for impure languages --- products, sums and functions --- can be characterized by universal properties in the setting of 'premulticategories', multicategories where the commutativity law may fail. This leads us to new, universal characterizations of two earlier equational theories of impure programming languages: the premonoidal categories of Power and Robinson, and the monad-based models of Moggi. Our analysis thus puts these earlier abstract ideas on a canonical foundation, bringing them to a new, syntactic level.}},
  url = {https://doi.org/10.1145/2429069.2429091},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup673,
  title = {A short counterexample property for safety and liveness verification of fault-tolerant distributed algorithms}},
  author = {Konnov, Igor and Lazi\'{c}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Distributed algorithms have many mission-critical applications ranging from embedded systems and replicated databases to cloud computing. Due to asynchronous communication, process faults, or network failures, these algorithms are difficult to design and verify. Many algorithms achieve fault tolerance by using threshold guards that, for instance, ensure that a process waits until it has received an acknowledgment from a majority of its peers. Consequently, domain-specific languages for fault-tolerant distributed systems offer language support for threshold guards. We introduce an automated method for model checking of safety and liveness of threshold-guarded distributed algorithms in systems where the number of processes and the fraction of faulty processes are parameters. Our method is based on a short counterexample property: if a distributed algorithm violates a temporal specification (in a fragment of LTL), then there is a counterexample whose length is bounded and independent of the parameters. We prove this property by (i) characterizing executions depending on the structure of the temporal formula, and (ii) using commutativity of transitions to accelerate and shorten executions. We extended the ByMC toolset (Byzantine Model Checker) with our technique, and verified liveness and safety of 10 prominent fault-tolerant distributed algorithms, most of which were out of reach for existing techniques.}},
  url = {https://doi.org/10.1145/3009837.3009860},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup674,
  title = {Dependent Information Flow Types}},
  author = {Louren\c{c}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this paper, we develop a novel notion of dependent information flow types. Dependent information flow types fit within the standard framework of dependent type theory, but, unlike usual dependent types, crucially allow the security level of a type, rather than just the structural data type itself, to depend on runtime values. Our dependent function and dependent sum information flow types provide a direct, natural and elegant way to express and enforce fine grained security policies on programs, including programs that manipulate structured data types in which the security level of a structure field may depend on values dynamically stored in other fields, still considered a challenge to security enforcement in software systems such as data-centric web-based applications.We base our development on the very general setting of a minimal lambda-calculus with references and collections. We illustrate its expressiveness, showing how secure operations on relevant scenarios can be modelled and analysed using our dependent information flow type system, which is also shown to be amenable to algorithmic type checking. Our main results include type-safety and non-interference theorems ensuring that well-typed programs do not violate prescribed security policies.}},
  url = {https://doi.org/10.1145/2676726.2676994},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup675,
  title = {Reconciling exhaustive pattern matching with objects}},
  author = {Isradisaikul, Chinawat and Myers, Andrew C.}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Pattern matching, an important feature of functional languages, is in conflict with data abstraction and extensibility, which are central to object-oriented languages. Modal abstraction offers an integration of deep pattern matching and convenient iteration abstractions into an object-oriented setting; however, because of data abstraction, it is challenging for a compiler to statically verify properties such as exhaustiveness. In this work, we extend modal abstraction in the JMatch language to support static, modular reasoning about exhaustiveness and redundancy. New matching specifications allow these properties to be checked using an SMT solver. We also introduce expressive pattern-matching constructs. Our evaluation shows that these new features enable more concise code and that the performance of checking exhaustiveness and redundancy is acceptable.}},
  url = {https://doi.org/10.1145/2491956.2462194},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup676,
  title = {A nonstandard standardization theorem}},
  author = {Accattoli, Beniamino and Bonelli, Eduardo and Kesner, Delia and Lombardi, Carlos}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Standardization is a fundamental notion for connecting programming languages and rewriting calculi. Since both programming languages and calculi rely on substitution for defining their dynamics, explicit substitutions (ES) help further close the gap between theory and practice.This paper focuses on standardization for the linear substitution calculus, a calculus with ES capable of mimicking reduction in lambda-calculus and linear logic proof-nets. For the latter, proof-nets can be formalized by means of a simple equational theory over the linear substitution calculus.Contrary to other extant calculi with ES, our system can be equipped with a residual theory in the sense of L\'{e}},
  url = {https://doi.org/10.1145/2535838.2535886},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup677,
  title = {Refinement through restraint: bringing down the cost of verification}},
  author = {O'Connor, Liam and Chen, Zilin and Rizkallah, Christine and Amani, Sidney and Lim, Japheth and Murray, Toby and Nagashima, Yutaka and Sewell, Thomas and Klein, Gerwin}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a framework aimed at significantly reducing the cost of verifying certain classes of systems software, such as file systems. Our framework allows for equational reasoning about systems code written in our new language, Cogent. Cogent is a restricted, polymorphic, higher-order, and purely functional language with linear types and without the need for a trusted runtime or garbage collector. Linear types allow us to assign two semantics to the language: one imperative, suitable for efficient C code generation; and one functional, suitable for equational reasoning and verification. As Cogent is a restricted language, it is designed to easily interoperate with existing C functions and to connect to existing C verification frameworks. Our framework is based on certifying compilation: For a well-typed Cogent program, our compiler produces C code, a high-level shallow embedding of its semantics in Isabelle/HOL, and a proof that the C code correctly refines this embedding. Thus one can reason about the full semantics of real-world systems code productively and equationally, while retaining the interoperability and leanness of C. The compiler certificate is a series of language-level proofs and per-program translation validation phases, combined into one coherent top-level theorem in Isabelle/HOL.}},
  url = {https://doi.org/10.1145/2951913.2951940},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup678,
  title = {Injective type families for Haskell}},
  author = {Stolarek, Jan and Peyton Jones, Simon and Eisenberg, Richard A.}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Haskell, as implemented by the Glasgow Haskell Compiler (GHC), allows expressive type-level programming. The most popular type-level programming extension is TypeFamilies, which allows users to write functions on types. Yet, using type functions can cripple type inference in certain situations. In particular, lack of injectivity in type functions means that GHC can never infer an instantiation of a type variable appearing only under type functions. In this paper, we describe a small modification to GHC that allows type functions to be annotated as injective. GHC naturally must check validity of the injectivity annotations. The algorithm to do so is surprisingly subtle. We prove soundness for a simplification of our algorithm, and state and prove a completeness property, though the algorithm is not fully complete. As much of our reasoning surrounds functions defined by a simple pattern-matching structure, we believe our results extend beyond just Haskell. We have implemented our solution on a branch of GHC and plan to make it available to regular users with the next stable release of the compiler.}},
  url = {https://doi.org/10.1145/2804302.2804314},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup679,
  title = {LightDP: towards automating differential privacy proofs}},
  author = {Zhang, Danfeng and Kifer, Daniel}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The growing popularity and adoption of differential privacy in academic and industrial settings has resulted in the development of increasingly sophisticated algorithms for releasing information while preserving privacy. Accompanying this phenomenon is the natural rise in the development and publication of incorrect algorithms, thus demonstrating the necessity of formal verification tools. However, existing formal methods for differential privacy face a dilemma: methods based on customized logics can verify sophisticated algorithms but come with a steep learning curve and significant annotation burden on the programmers, while existing programming platforms lack expressive power for some sophisticated algorithms. In this paper, we present LightDP, a simple imperative language that strikes a better balance between expressive power and usability. The core of LightDP is a novel relational type system that separates relational reasoning from privacy budget calculations. With dependent types, the type system is powerful enough to verify sophisticated algorithms where the composition theorem falls short. In addition, the inference engine of LightDP infers most of the proof details, and even searches for the proof with minimal privacy cost when multiple proofs exist. We show that LightDP verifies sophisticated algorithms with little manual effort.}},
  url = {https://doi.org/10.1145/3009837.3009884},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup680,
  title = {Deriving object typestates in the presence of inter-object references}},
  author = {Nanda, Mangala Gowri and Grothoff, Christian and Chandra, Satish}},
  year = {2005}},
  journal = {Proceedings of the 20th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We are interested in static analysis of Java classes with the goal of discovering the preconditions under which a certain program point within a method may be reached, taking into account the effects of previous method calls on an object of that class. The information pertinent to this computation is represented as the object's typestate, which is a finite set of relevant predicates that abstract the object's actual state. The execution of a method depends on an object's current typestate as well as other input parameters; the object may transition to a different typestate during the method's execution.It is common for objects to contain references to other ob-jects. In such cases, an object's behavior may depend on, in addition to its own state, the state of objects it has a refer-ence to. The main contribution of this paper is to discover relevant object typestates, as well as transitions between typestates, in the presence of inter-object references. Our analysis first performs a combined predicate discovery and predicate abstraction to derive "boolean" versions of Java classes given as input. It then uses abstract interpretation to compute the typestate transitions caused by method calls. A novel aspect of this work is that a set of Java classes is analyzed in isolation, without any client program being pro-vided. To do this, the analysis simulates all possible client's actions via a synthetic heap, all of whose interesting config-urations are explored by our analysis.The information we compute can be put to use in several ways. It can be used in checking whether a given client code erroneously uses a set of Java classes in a way that can throw an exception. It can also be used in creating test drivers for Java classes in order to exercise all relevant code paths in the corresponding methods.}},
  url = {https://doi.org/10.1145/1094811.1094818},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup681,
  title = {Gradual certified programming in coq}},
  author = {Tanter, \'{E}},
  year = {2015}},
  journal = {Proceedings of the 11th Symposium on Dynamic Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Expressive static typing disciplines are a powerful way to achieve high-quality software. However, the adoption cost of such techniques should not be under-estimated. Just like gradual typing allows for a smooth transition from dynamically-typed to statically-typed programs, it seems desirable to support a gradual path to certified programming. We explore gradual certified programming in Coq, providing the possibility to postpone the proofs of selected properties, and to check "at runtime" whether the properties actually hold. Casts can be integrated with the implicit coercion mechanism of Coq to support implicit cast insertion \`{a}},
  url = {https://doi.org/10.1145/2816707.2816710},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup682,
  title = {Recursion and dynamic data-structures in bounded space: towards embedded ML programming}},
  author = {Hughes, John and Pareto, Lars}},
  year = {1999}},
  journal = {Proceedings of the Fourth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a functional language with a type system such that well typed programs run within stated space-bounds. The language is a strict, first-order variant of ML with constructs for explicit storage management. The type system is a variant of Tofte and Talpin's region inference system to which the notion of sized types, of Hughes, Pareto and Sabry, has been added.}},
  url = {https://doi.org/10.1145/317636.317785},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup683,
  title = {Safe privatization in transactional memory}},
  author = {Khyzha, Artem and Attiya, Hagit and Gotsman, Alexey and Rinetzky, Noam}},
  year = {2018}},
  journal = {Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Transactional memory (TM) facilitates the development of concurrent applications by letting the programmer designate certain code blocks as atomic. Programmers using a TM often would like to access the same data both inside and outside transactions, e.g., to improve performance or to support legacy code. In this case, programmers would ideally like the TM to guarantee strong atomicity, where transactions can be viewed as executing atomically also with respect to non-transactional accesses. Since guaranteeing strong atomicity for arbitrary programs is prohibitively expensive, researchers have suggested guaranteeing it only for certain data-race free (DRF) programs, particularly those that follow the privatization idiom: from some point on, threads agree that a given object can be accessed non-transactionally. Supporting privatization safely in a TM is nontrivial, because this often requires correctly inserting transactional fences, which wait until all active transactions complete.Unfortunately, there is currently no consensus on a single definition of transactional DRF, in particular, because no existing notion of DRF takes into account transactional fences. In this paper we propose such a notion and prove that, if a TM satisfies a certain condition generalizing opacity and a program using it is DRF assuming strong atomicity, then the program indeed has strongly atomic semantics. We show that our DRF notion allows the programmer to use privatization idioms. We also propose a method for proving our generalization of opacity and apply it to the TL2 TM.}},
  url = {https://doi.org/10.1145/3178487.3178505},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup684,
  title = {Tapping into the fountain of CPUs: on operating system support for programmable devices}},
  author = {Weinsberg, Yaron and Dolev, Danny and Anker, Tal and Ben-Yehuda, Muli and Wyckoff, Pete}},
  year = {2008}},
  journal = {Proceedings of the 13th International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The constant race for faster and more powerful CPUs is drawing to a close. No longer is it feasible to significantly increase the speed of the CPU without paying a crushing penalty in power consumption and production costs. Instead of increasing single thread performance, the industry is turning to multiple CPU threads or cores (such as SMT and CMP) and heterogeneous CPU architectures (such as the Cell Broadband Engine). While this is a step in the right direction, in every modern PC there is a wealth of untapped compute resources. The NIC has a CPU; the disk controller is programmable; some high-end graphics adaptersare already more powerful than host CPUs. Some of these CPUs can perform some functions more efficiently than the host CPUs. Our operating systems and programming abstractions should be expanded to let applications tap into these computational resources and make the best use of them.Therefore, we propose the HYDRA framework, which lets application developers use the combined power of every compute resource in a coherent way. HYDRA is a programming model and a runtime support layer which enables utilization of host processors as well as various programmable peripheral devices' processors. We present the frameworkand its application for a demonstrative use-case, as well as provide a thorough evaluation of its capabilities. Using HYDRA we were able to cut down the development cost of a system that uses multiple heterogenous compute resources significantly.}},
  url = {https://doi.org/10.1145/1346281.1346304},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup685,
  title = {Relaxed separation logic: a program logic for C11 concurrency}},
  author = {Vafeiadis, Viktor and Narayan, Chinmay}},
  year = {2013}},
  journal = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages \& Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We introduce relaxed separation logic (RSL), the first program logic for reasoning about concurrent programs running under the C11 relaxed memory model. From a user's perspective, RSL is an extension of concurrent separation logic (CSL) with proof rules for the various kinds of C11 atomic accesses. As in CSL, individual threads are allowed to access non-atomically only the memory that they own, thus preventing data races. Ownership can, however, be transferred via certain atomic accesses. For SC-atomic accesses, we permit arbitrary ownership transfer; for acquire/release atomic accesses, we allow ownership transfer only in one direction; whereas for relaxed atomic accesses, we rule out ownership transfer completely. We illustrate RSL with a few simple examples and prove its soundness directly over the axiomatic C11 weak memory model.}},
  url = {https://doi.org/10.1145/2509136.2509532},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup686,
  title = {Sound and precise analysis of parallel programs through schedule specialization}},
  author = {Wu, Jingyue and Tang, Yang and Hu, Gang and Cui, Heming and Yang, Junfeng}},
  year = {2012}},
  journal = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Parallel programs are known to be difficult to analyze. A key reason is that they typically have an enormous number of execution interleavings, or schedules. Static analysis over all schedules requires over-approximations, resulting in poor precision; dynamic analysis rarely covers more than a tiny fraction of all schedules. We propose an approach called schedule specialization to analyze a parallel program over only a small set of schedules for precision, and then enforce these schedules at runtime for soundness of the static analysis results. We build a schedule specialization framework for C/C++ multithreaded programs that use Pthreads. Our framework avoids the need to modify every analysis to be schedule-aware by specializing a program into a simpler program based on a schedule, so that the resultant program can be analyzed with stock analyses for improved precision. Moreover, our framework provides a precise schedule-aware def-use analysis on memory locations, enabling us to build three highly precise analyses: an alias analyzer, a data-race detector, and a path slicer. Evaluation on 17 programs, including 2 real-world programs and 15 popular benchmarks, shows that analyses using our framework reduced may-aliases by 61.9\%, false race reports by 69\%, and path slices by 48.7\%; and detected 7 unknown bugs in well-checked programs.}},
  url = {https://doi.org/10.1145/2254064.2254090},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup687,
  title = {Static and user-extensible proof checking}},
  author = {Stampoulis, Antonis and Shao, Zhong}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Despite recent successes, large-scale proof development within proof assistants remains an arcane art that is extremely time-consuming. We argue that this can be attributed to two profound shortcomings in the architecture of modern proof assistants. The first is that proofs need to include a large amount of minute detail; this is due to the rigidity of the proof checking process, which cannot be extended with domain-specific knowledge. In order to avoid these details, we rely on developing and using tactics, specialized procedures that produce proofs. Unfortunately, tactics are both hard to write and hard to use, revealing the second shortcoming of modern proof assistants. This is because there is no static knowledge about their expected use and behavior. As has recently been demonstrated, languages that allow type-safe manipulation of proofs, like Beluga, Delphin and VeriML, can be used to partly mitigate this second issue, by assigning rich types to tactics. Still, the architectural issues remain. In this paper, we build on this existing work, and demonstrate two novel ideas: an extensible conversion rule and support for static proof scripts. Together, these ideas enable us to support both user-extensible proof checking, and sophisticated static checking of tactics, leading to a new point in the design space of future proof assistants. Both ideas are based on the interplay between a light-weight staging construct and the rich type information available.}},
  url = {https://doi.org/10.1145/2103656.2103690},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup688,
  title = {Pattern synonyms}},
  author = {Pickering, Matthew and \'{E}},
  year = {2016}},
  journal = {Proceedings of the 9th International Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Pattern matching has proven to be a convenient, expressive way of inspecting data. Yet this language feature, in its traditional form, is limited: patterns must be data constructors of concrete data types. No computation or abstraction is allowed. The data type in question must be concrete, with no ability to enforce any invariants. Any change in this data type requires all clients to update their code. This paper introduces pattern synonyms, which allow programmers to abstract over patterns, painting over all the shortcomings listed above. Pattern synonyms are assigned types, enabling a compiler to check the validity of a synonym independent of its definition. These types are intricate; detailing how to assign a type to a pattern synonym is a key contribution of this work. We have implemented pattern synonyms in the Glasgow Haskell Compiler, where they have enjoyed immediate popularity, but we believe this feature could easily be exported to other languages that support pattern matching.}},
  url = {https://doi.org/10.1145/2976002.2976013},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup689,
  title = {Efficient online validation with delta execution}},
  author = {Tucek, Joseph and Xiong, Weiwei and Zhou, Yuanyuan}},
  year = {2009}},
  journal = {Proceedings of the 14th International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Software systems are constantly changing. Patches to fix bugs and patches to add features are all too common. Every change risks breaking a previously working system. Hence administrators loathe change, and are willing to delay even critical security patches until after fully validating their correctness. Compared to off-line validation, on-line validation has clear advantages since it tests against real life workloads. Yet unfortunately it imposes restrictive overheads as it requires running the old and new versions side-by-side. Moreover, due to spurious differences (e.g. event timing, random number generation, and thread interleavings), it is difficult to compare the two for validation.To allow more effective on-line patch validation, we propose a new mechanism, called delta execution, that is based on the observation that most patches are small. Delta execution merges the two side-by-side executions for most of the time and splits only when necessary, such as when they access different data or execute different code. This allows us to perform on-line validation not only with lower overhead but also with greatly reduced spurious differences, allowing us to effectively validate changes.We first validate the feasibility of our idea by studying the characteristics of 240 patches from 4 server programs; our examination shows that 77\% of the changes should not be expected to cause large changes and are thereby feasible for Delta execution. We then implemented Delta execution using dynamic instrumentation. Using real world patches from 7 server applications and 3 other programs, we compared our implementation of Delta execution against a traditional side-by-side on-line validation. Delta execution outperformed traditional validation by up to 128\%; further, for 3 of the changes, spurious differences caused the traditional validation to fail completely while Delta execution succeeded. This demonstrates that Delta execution can allow administrators to use on-line validation to confidently ensure the correctness of the changes they apply.}},
  url = {https://doi.org/10.1145/1508244.1508267},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup690,
  title = {Global value numbering using random interpretation}},
  author = {Gulwani, Sumit and Necula, George C.}},
  year = {2004}},
  journal = {Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a polynomial time randomized algorithm for global value numbering. Our algorithm is complete when conditionals are treated as non-deterministic and all operators are treated as uninterpreted functions. We are not aware of any complete polynomial-time deterministic algorithm for the same problem. The algorithm does not require symbolic manipulations and hence is simpler to implement than the deterministic symbolic algorithms. The price for these benefits is that there is a probability that the algorithm can report a false equality. We prove that this probability can be made arbitrarily small by controlling various parameters of the algorithm.Our algorithm is based on the idea of random interpretation, which relies on executing a program on a number of random inputs and discovering relationships from the computed values. The computations are done by giving random linear interpretations to the operators in the program. Both branches of a conditional are executed. At join points, the program states are combined using a random affine combination. We discuss ways in which this algorithm can be made more precise by using more accurate interpretations for the linear arithmetic operators and other language constructs.}},
  url = {https://doi.org/10.1145/964001.964030},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup691,
  title = {Diagnosing type errors with class}},
  author = {Zhang, Danfeng and Myers, Andrew C. and Vytiniotis, Dimitrios and Peyton-Jones, Simon}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Type inference engines often give terrible error messages, and the more sophisticated the type system the worse the problem. We show that even with the highly expressive type system implemented by the Glasgow Haskell Compiler (GHC)--including type classes, GADTs, and type families--it is possible to identify the most likely source of the type error, rather than the first source that the inference engine trips over. To determine which are the likely error sources, we apply a simple Bayesian model to a graph representation of the typing constraints; the satisfiability or unsatisfiability of paths within the graph provides evidence for or against possible explanations. While we build on prior work on error diagnosis for simpler type systems, inference in the richer type system of Haskell requires extending the graph with new nodes. The augmentation of the graph creates challenges both for Bayesian reasoning and for ensuring termination. Using a large corpus of Haskell programs, we show that this error localization technique is practical and significantly improves accuracy over the state of the art.}},
  url = {https://doi.org/10.1145/2737924.2738009},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup692,
  title = {Abstraction preservation and subtyping in distributed languages}},
  author = {Deni\'{e}},
  year = {2006}},
  journal = {Proceedings of the Eleventh ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In most programming languages, type abstraction is guaranteed by syntactic scoping in a single program, but is not preserved by marshalling during distributed communication. A solution is to generate hash types at compile time that consist of a fingerprint of the source code implementing the data type. These hash types can be tupled with a marshalled value and compared efficiently at unmarshall time to guarantee abstraction safety. In this paper, we extend a core calculus of ML-like modules, functions, distributed communication, and hash types, to integrate structural subtyping, user-declared subtyping between abstract types, and bounded existential types. Our semantics makes two contributions: (1) the explicit tracking of the interaction between abstraction boundaries and subtyping; (2) support for user-declared module upgrades with propagation of the resulting subhashing relation throughout the network during communication. We prove type preservation, progress, determinacy, and erasure for our system.}},
  url = {https://doi.org/10.1145/1159803.1159841},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup693,
  title = {An executable formal semantics of C with applications}},
  author = {Ellison, Chucky and Rosu, Grigore}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper describes an executable formal semantics of C. Being executable, the semantics has been thoroughly tested against the GCC torture test suite and successfully passes 99.2\% of 776 test programs. It is the most complete and thoroughly tested formal definition of C to date. The semantics yields an interpreter, debugger, state space search tool, and model checker "for free". The semantics is shown capable of automatically finding program errors, both statically and at runtime. It is also used to enumerate nondeterministic behavior.}},
  url = {https://doi.org/10.1145/2103656.2103719},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup694,
  title = {Rate types for stream programs}},
  author = {Bartenstein, Thomas W. and Liu, Yu David}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \& Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We introduce RATE TYPES, a novel type system to reason about and optimize data-intensive programs. Built around stream languages, RATE TYPES performs static quantitative reasoning about stream rates -- the frequency of data items in a stream being consumed, processed, and produced. Despite the fact that streams are fundamentally dynamic, we find two essential concepts of stream rate control -- throughput ratio and natural rate -- are intimately related to the program structure itself and can be effectively reasoned about by a type system. RATE TYPES is proven to correspond with a time-aware and parallelism-aware operational semantics. The strong correspondence result tolerates arbitrary schedules, and does not require any synchronization between stream filters.We further implement RATE TYPES, demonstrating its effectiveness in predicting stream data rates in real-world stream programs.}},
  url = {https://doi.org/10.1145/2660193.2660225},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup695,
  title = {A bisimulation for dynamic sealing}},
  author = {Sumii, Eijiro and Pierce, Benjamin C.}},
  year = {2004}},
  journal = {Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We define λseal, an untyped call-by-value λ-calculus with primitives for protecting abstract data by sealing, and develop a bisimulation proof method that is sound and complete with respect to contextual equivalence. This provides a formal basis for reasoning about data abstraction in open, dynamic settings where static techniques such as type abstraction and logical relations are not applicable.}},
  url = {https://doi.org/10.1145/964001.964015},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup696,
  title = {Uncertain<T>: a first-order type for uncertain data}},
  author = {Bornholt, James and Mytkowicz, Todd and McKinley, Kathryn S.}},
  year = {2014}},
  journal = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Emerging applications increasingly use estimates such as sensor data (GPS), probabilistic models, machine learning, big data, and human data. Unfortunately, representing this uncertain data with discrete types (floats, integers, and booleans) encourages developers to pretend it is not probabilistic, which causes three types of uncertainty bugs. (1) Using estimates as facts ignores random error in estimates. (2) Computation compounds that error. (3) Boolean questions on probabilistic data induce false positives and negatives. This paper introduces Uncertain<T>, a new programming language abstraction for uncertain data. We implement a Bayesian network semantics for computation and conditionals that improves program correctness. The runtime uses sampling and hypothesis tests to evaluate computation and conditionals lazily and efficiently. We illustrate with sensor and machine learning applications that Uncertain<T> improves expressiveness and accuracy.Whereas previous probabilistic programming languages focus on experts, Uncertain<T> serves a wide range of developers. Experts still identify error distributions. However, both experts and application writers compute with distributions, improve estimates with domain knowledge, and ask questions with conditionals. The Uncertain<T> type system and operators encourage developers to expose and reason about uncertainty explicitly, controlling false positives and false negatives. These benefits make Uncertain<T> a compelling programming model for modern applications facing the challenge of uncertainty.}},
  url = {https://doi.org/10.1145/2541940.2541958},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup697,
  title = {Abstract semantic differencing via speculative correlation}},
  author = {Partush, Nimrod and Yahav, Eran}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \& Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We address the problem of computing semantic differences between a program and a patched version of the program. Our goal is to obtain a precise characterization of the difference between program versions, or establish their equivalence. We focus on infinite-state numerical programs, and use abstract interpretation to compute an over-approximation of program differences.Computing differences and establishing equivalence under abstraction requires abstracting relationships between variables in the two programs. Towards that end, we use a correlating abstract domain to compute a sound approximation of these relationships which captures semantic difference. This approximation can be computed over any interleaving of the two programs. However, the choice of interleaving can significantly affect precision. We present a speculative search algorithm that aims to find an interleaving of the two programs with minimal abstract semantic difference. This method is unique as it allows the analysis to dynamically alternate between several interleavings.We have implemented our approach and applied it to real-world examples including patches from Git, GNU Coreutils, as well as a few handpicked patches from the Linux kernel and the Mozilla Firefox web browser. Our evaluation shows that we compute precise approximations of semantic differences, and report few false differences.}},
  url = {https://doi.org/10.1145/2660193.2660245},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup698,
  title = {Generating reactive programs for graphical user interfaces from multi-way dataflow constraint systems}},
  author = {Foust, Gabriel and J\"{a}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {For a GUI to remain responsive, it must be able to schedule lengthy tasks to be executed asynchronously. In the traditional approach to GUI implementation--writing functions to handle individual user events--asynchronous programming easily leads to defects. Ensuring that all data dependencies are respected is difficult when new events arrive while prior events are still being handled. Reactive programming techniques, gaining popularity in GUI programming, help since they make data dependencies explicit and enforce them automatically as variables' values change. However, data dependencies in GUIs usually change along with its state. Reactive programming must therefore describe a GUI as a collection of many reactive programs, whose interaction the programmer must explicitly coordinate. This paper presents a declarative approach for GUI programming that relieves the programmer from coordinating asynchronous computations. The approach is based on our prior work on "property models", where GUI state is maintained by a dataflow constraint system. A property model responds to user events by atomically constructing new data dependencies and scheduling asynchronous computations to enforce those dependencies. In essence, a property model dynamically generates a reactive program, adding to it as new events occur. The approach gives the following guarantee: the same sequence of events produces the same results, regardless of the timing of those events.}},
  url = {https://doi.org/10.1145/2814204.2814207},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup699,
  title = {Resource-bounded partial evaluation}},
  author = {Debray, Saumya}},
  year = {1997}},
  journal = {Proceedings of the 1997 ACM SIGPLAN Symposium on Partial Evaluation and Semantics-Based Program Manipulation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Most partial evaluators do not take the availability of machine-level resources, such as registers or cache, into consideration when making their specialization decisions. The resulting resource contention can lead to severe performance degradation---causing, in extreme cases, the specialized code to run slower than the unspecialized code. In this paper we consider how resource considerations can be incorporated within a partial evaluator. We develop an abstract formulation of the problem, show that optimal resource-bounded partial evaluation is NP-complete, and discuss simple heuristics that can be used to address the problem in practice.}},
  url = {https://doi.org/10.1145/258993.259017},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup700,
  title = {Relating models of backtracking}},
  author = {Wand, Mitchell and Vaillancourt, Dale}},
  year = {2004}},
  journal = {Proceedings of the Ninth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Past attempts to relate two well-known models of backtracking computation have met with only limited success. We relate these two models using logical relations. We accommodate higher-order values and infinite computations. We also provide an operational semantics, and we prove it adequate for both models.}},
  url = {https://doi.org/10.1145/1016850.1016861},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup701,
  title = {Automatic parallelization of pure method calls via conditional future synthesis}},
  author = {Surendran, Rishi and Sarkar, Vivek}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We introduce a novel approach for using futures to automatically parallelize the execution of pure method calls. Our approach is built on three new techniques to address the challenge of automatic parallelization via future synthesis: candidate future synthesis, parallelism benefit analysis, and threshold expression synthesis. During candidate future synthesis, our system annotates pure method calls as async expressions and synthesizes a parallel program with future objects and their type declarations. Next, the system performs a parallel benefit analysis to determine which async expressions may need to be executed sequentially due to overhead reasons, based on execution profile information collected from multiple test inputs. Finally, threshold expression synthesis uses the output from parallelism benefit analysis to synthesize predicate expressions that can be used to determine at runtime if a specific pure method call should be executed sequentially or in parallel. We have implemented our approach, and the results obtained from an experimental evaluation of the complete system on a range of sequential Java benchmarks are very encouraging. Our evaluation shows that our approach can provide significant parallel speedups of up to 7.4x (geometric mean of 3.69x) relative to the sequential programs when using 8 processor cores, with zero programmer effort beyond providing the sequential program and test cases for parallelism benefit analysis.}},
  url = {https://doi.org/10.1145/2983990.2984035},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup702,
  title = {Correct blame for contracts: no more scapegoating}},
  author = {Dimoulas, Christos and Findler, Robert Bruce and Flanagan, Cormac and Felleisen, Matthias}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Behavioral software contracts supplement interface information with logical assertions. A rigorous enforcement of contracts provides useful feedback to developers if it signals contract violations as soon as they occur and if it assigns blame to violators with preciseexplanations. Correct blame assignment gets programmers started with the debugging process and can significantly decrease the time needed to discover and fix bugs.Sadly the literature on contracts lacks a framework for making statements about the correctness of blame assignment and for validating such statements. This paper fills the gap and uses the framework to demonstrate how one of the proposed semantics for higher-order contracts satisfies this criteria and another semantics occasionally assigns blame to the wrong module.Concretely, the paper applies the framework to the lax enforcement of dependent higher-order contracts and the picky one. A higher-order dependent contract specifies constraints for the domain and range of higher-order functions and also relates arguments and results in auxiliary assertions. The picky semantics ensures that the use of arguments in the auxiliary assertion satisfies the domain contracts and the lax one does not. While the picky semantics discovers more contract violations than the lax one, it occasionally blames the wrong module. Hence the paper also introduces a third semantics, dubbed indy, which fixes the problems of the picky semantics without giving up its advantages.}},
  url = {https://doi.org/10.1145/1926385.1926410},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup703,
  title = {Logical relations for fine-grained concurrency}},
  author = {Turon, Aaron J. and Thamsborg, Jacob and Ahmed, Amal and Birkedal, Lars and Dreyer, Derek}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Fine-grained concurrent data structures (or FCDs) reduce the granularity of critical sections in both time and space, thus making it possible for clients to access different parts of a mutable data structure in parallel. However, the tradeoff is that the implementations of FCDs are very subtle and tricky to reason about directly. Consequently, they are carefully designed to be contextual refinements of their coarse-grained counterparts, meaning that their clients can reason about them as if all access to them were sequentialized.In this paper, we propose a new semantic model, based on Kripke logical relations, that supports direct proofs of contextual refinement in the setting of a type-safe high-level language. The key idea behind our model is to provide a simple way of expressing the "local life stories" of individual pieces of an FCD's hidden state by means of protocols that the threads concurrently accessing that state must follow. By endowing these protocols with a simple yet powerful transition structure, as well as the ability to assert invariants on both heap states and specification code, we are able to support clean and intuitive refinement proofs for the most sophisticated types of FCDs, such as conditional compare-and-set (CCAS).}},
  url = {https://doi.org/10.1145/2429069.2429111},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup704,
  title = {Modular monadic meta-theory}},
  author = {Delaware, Benjamin and Keuchel, Steven and Schrijvers, Tom and Oliveira, Bruno C.d.S.}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents 3MT, a framework for modular mechanized meta-theory of languages with effects. Using 3MT, individual language features and their corresponding definitions -- semantic functions, theorem statements and proofs-- can be built separately and then reused to create different languages with fully mechanized meta-theory. 3MT combines modular datatypes and monads to define denotational semantics with effects on a per-feature basis, without fixing the particular set of effects or language constructs.One well-established problem with type soundness proofs for denotational semantics is that they are notoriously brittle with respect to the addition of new effects. The statement of type soundness for a language depends intimately on the effects it uses, making it particularly challenging to achieve modularity. 3MT solves this long-standing problem by splitting these theorems into two separate and reusable parts: a feature theorem that captures the well-typing of denotations produced by the semantic function of an individual feature with respect to only the effects used, and an effect theorem that adapts well-typings of denotations to a fixed superset of effects. The proof of type soundness for a particular language simply combines these theorems for its features and the combination of their effects. To establish both theorems, 3MT uses two key reasoning techniques: modular induction and algebraic laws about effects. Several effectful language features, including references and errors, illustrate the capabilities of 3MT. A case study reuses these features to build fully mechanized definitions and proofs for 28 languages, including several versions of mini-ML with effects.}},
  url = {https://doi.org/10.1145/2500365.2500587},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup705,
  title = {Decoding Lua: formal semantics for the developer and the semanticist}},
  author = {Soldevila, Mallku and Ziliani, Beta and Silvestre, Bruno and Fridlender, Daniel and Mascarenhas, Fabio}},
  year = {2017}},
  journal = {Proceedings of the 13th ACM SIGPLAN International Symposium on on Dynamic Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We provide formal semantics for a large subset of the Lua programming language, in its version 5.2. We validate our model by mechanizing it and testing it against the test suite of the reference interpreter of Lua, obtaining evidence that our model accurately represents the language. We target both a PL semanticist "not necessarily versed in Lua", and a Lua developer - not necessarily versed in semantic frameworks. To the former, we present the peculiarities of the language, and how we model them in a modular small-step operational semantics, using concepts from Felleisen-Hieb's reduction semantics with evaluation contexts. Moreover, we mechanize and test the model in PLT Redex, the de facto tool for reduction semantics. To the reader unfamiliar with such concepts, we provide a gentle introduction to the model. It is our hope that developers of the different Lua implementations and dialects understand the model and consider it both for testing their work and for experimenting with new language features.}},
  url = {https://doi.org/10.1145/3133841.3133848},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup706,
  title = {AutoMO: automatic inference of memory order parameters for C/C++11}},
  author = {Ou, Peizhao and Demsky, Brian}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many concurrent data structures are initially designed for the sequential consistency (SC) memory model. Developers often implement these data structures on real-world systems with weaker memory models by adding sufficient fences to ensure that their implementation on the weak memory model exhibits the same executions as the SC memory model. Recently, the C11 and C++11 standards have added a weak memory model to the C and C++ languages. Developing and debugging code for weak memory models can be extremely challenging. We present AutoMO, a framework to support porting data structures designed for the SC memory model to the C/C++11 memory model. AutoMO provides support across the porting process: (1) it automatically infers initial settings for the memory order parameters, (2) it detects whether a C/C++11 execution is equivalent to some SC execution, and (3) it simplifies traces to make them easier to understand. We have used AutoMO to successfully infer memory order parameters for a range of data structures and to check whether executions of several concurrent data structure implementations are SC.}},
  url = {https://doi.org/10.1145/2814270.2814286},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup707,
  title = {Wormholes: introducing effects to FRP}},
  author = {Winograd-Cort, Daniel and Hudak, Paul}},
  year = {2012}},
  journal = {Proceedings of the 2012 Haskell Symposium}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Functional reactive programming (FRP) is a useful model for programming real-time and reactive systems in which one defines a signal function to process a stream of input values into a stream of output values. However, performing side effects (e.g. memory mutation or input/output) in this model is tricky and typically unsafe. In previous work, Winograd-Cort et al. [2012] introduced resource types and wormholes to address this problem.This paper better motivates, expands upon, and formalizes the notion of a wormhole to fully unlock its potential. We show, for example, that wormholes can be used to define the concept of causality. This in turn allows us to provide behaviors such as looping, a core component of most languages, without building it directly into the language. We also improve upon our previous design by making wormholes less verbose and easier to use.To formalize the notion of a wormhole, we define an extension to the simply typed lambda calculus, complete with typing rules and operational semantics. In addition, we present a new form of semantic transition that we call a temporal transition to specify how an FRP program behaves over time and to allow us to better reason about causality. As our model is designed for a Haskell implementation, the semantics are lazy. Finally, with the language defined, we prove that our wormholes indeed allow side effects to be performed safely in an FRP framework.}},
  url = {https://doi.org/10.1145/2364506.2364519},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup708,
  title = {Fully-abstract compilation by approximate back-translation}},
  author = {Devriese, Dominique and Patrignani, Marco and Piessens, Frank}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A compiler is fully-abstract if the compilation from source language programs to target language programs reflects and preserves behavioural equivalence. Such compilers have important security benefits, as they limit the power of an attacker interacting with the program in the target language to that of an attacker interacting with the program in the source language. Proving compiler full-abstraction is, however, rather complicated. A common proof technique is based on the back-translation of target-level program contexts to behaviourally-equivalent source-level contexts. However, constructing such a back-translation is problematic when the source language is not strong enough to embed an encoding of the target language. For instance, when compiling from the simply-typed λ-calculus (λτ) to the untyped λ-calculus (λu), the lack of recursive types in λτ prevents such a back-translation. We propose a general and elegant solution for this problem. The key insight is that it suffices to construct an approximate back-translation. The approximation is only accurate up to a certain number of steps and conservative beyond that, in the sense that the context generated by the back-translation may diverge when the original would not, but not vice versa. Based on this insight, we describe a general technique for proving compiler full-abstraction and demonstrate it on a compiler from λτ to λu . The proof uses asymmetric cross-language logical relations and makes innovative use of step-indexing to express the relation between a context and its approximate back-translation. We believe this proof technique can scale to challenging settings and enable simpler, more scalable proofs of compiler full-abstraction.}},
  url = {https://doi.org/10.1145/2837614.2837618},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup709,
  title = {A concurrency semantics for relaxed atomics that permits optimisation and avoids thin-air executions}},
  author = {Pichon-Pharabod, Jean and Sewell, Peter}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Despite much research on concurrent programming languages, especially for Java and C/C++, we still do not have a satisfactory definition of their semantics, one that admits all common optimisations without also admitting undesired behaviour. Especially problematic are the ``thin-air'' examples involving high-performance concurrent accesses, such as C/C++11 relaxed atomics. The C/C++11 model is in a per-candidate-execution style, and previous work has identified a tension between that and the fact that compiler optimisations do not operate over single candidate executions in isolation; rather, they operate over syntactic representations that represent all executions. In this paper we propose a novel approach that circumvents this difficulty. We define a concurrency semantics for a core calculus, including relaxed-atomic and non-atomic accesses, and locks, that admits a wide range of optimisation while still forbidding the classic thin-air examples. It also addresses other problems relating to undefined behaviour. The basic idea is to use an event-structure representation of the current state of each thread, capturing all of its potential executions, and to permit interleaving of execution and transformation steps over that to reflect optimisation (possibly dynamic) of the code. These are combined with a non-multi-copy-atomic storage subsystem, to reflect common hardware behaviour. The semantics is defined in a mechanised and executable form, and designed to be implementable above current relaxed hardware and strong enough to support the programming idioms that C/C++11 does for this fragment. It offers a potential way forward for concurrent programming language semantics, beyond the current C/C++11 and Java models.}},
  url = {https://doi.org/10.1145/2837614.2837616},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup710,
  title = {Scaling abstraction refinement via pruning}},
  author = {Liang, Percy and Naik, Mayur}},
  year = {2011}},
  journal = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many static analyses do not scale as they are made more precise. For example, increasing the amount of context sensitivity in a k-limited pointer analysis causes the number of contexts to grow exponentially with k. Iterative refinement techniques can mitigate this growth by starting with a coarse abstraction and only refining parts of the abstraction that are deemed relevant with respect to a given client.In this paper, we introduce a new technique called pruning that uses client feedback in a different way. The basic idea is to use coarse abstractions to prune away parts of the program analysis deemed irrelevant for proving a client query, and then using finer abstractions on the sliced program analysis. For a k-limited pointer analysis, this approach amounts to adaptively refining and pruning a set of prefix patterns representing the contexts relevant for the client. By pruning, we are able to scale up to much more expensive abstractions than before. We also prove that the pruned analysis is both sound and complete, that is, it yields the same results as an analysis that uses a more expensive abstraction directly without pruning.}},
  url = {https://doi.org/10.1145/1993498.1993567},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup711,
  title = {Compiling a reflective language using MetaOCaml}},
  author = {Asai, Kenichi}},
  year = {2014}},
  journal = {Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A reflective language makes the language semantics open to user programs and allows them to access, extend, and modify it from within the same language framework. Because of its high flexibility and expressiveness, it can be an ideal platform for programming language research as well as practical applications in dynamic environments. However, efficient implementation of a reflective language is extremely difficult. Under the circumstance where the language semantics can change, a partial evaluator is required for compilation. This paper reports on the experience of using MetaOCaml as a compiler for a reflective language. With staging annotations, MetaOCaml achieves the same effect as using a partial evaluator. Unlike the standard partial evaluator, the run mechanism of MetaOCaml enables us to use the specialized (compiled) code in the current runtime environment. On the other hand, the lack of a binding-time analysis in MetaOCaml prohibits us from compiling a user program under modified compiled semantics.}},
  url = {https://doi.org/10.1145/2658761.2658775},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup712,
  title = {Proactive Control of Approximate Programs}},
  author = {Sui, Xin and Lenharth, Andrew and Fussell, Donald S. and Pingali, Keshav}},
  year = {2016}},
  journal = {Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Approximate computing trades off accuracy of results for resources such as energy or computing time. There is a large and rapidly growing literature on approximate computing that has focused mostly on showing the benefits of approximate computing. However, we know relatively little about how to control approximation in a disciplined way. In this paper, we address the problem of controlling approximation for non-streaming programs that have a set of "knobs" that can be dialed up or down to control the level of approximation of different components in the program. We formulate this control problem as a constrained optimization problem, and describe a system called Capri that uses machine learning to learn cost and error models for the program, and uses these models to determine, for a desired level of approximation, knob settings that optimize metrics such as running time or energy usage. Experimental results with complex benchmarks from different problem domains demonstrate the effectiveness of this approach.}},
  url = {https://doi.org/10.1145/2872362.2872402},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup713,
  title = {Tracelet-based code search in executables}},
  author = {David, Yaniv and Yahav, Eran}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We address the problem of code search in executables. Given a function in binary form and a large code base, our goal is to statically find similar functions in the code base. Towards this end, we present a novel technique for computing similarity between functions. Our notion of similarity is based on decomposition of functions into tracelets: continuous, short, partial traces of an execution. To establish tracelet similarity in the face of low-level compiler transformations, we employ a simple rewriting engine. This engine uses constraint solving over alignment constraints and data dependencies to match registers and memory addresses between tracelets, bridging the gap between tracelets that are otherwise similar. We have implemented our approach and applied it to find matches in over a million binary functions. We compare tracelet matching to approaches based on n-grams and graphlets and show that tracelet matching obtains dramatically better precision and recall.}},
  url = {https://doi.org/10.1145/2594291.2594343},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup714,
  title = {Reusing debugging knowledge via trace-based bug search}},
  author = {Gu, Zhongxian and Barr, Earl T. and Schleck, Drew and Su, Zhendong}},
  year = {2012}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Some bugs, among the millions that exist, are similar to each other. One bug-fixing tactic is to search for similar bugs that have been reported and resolved in the past. A fix for a similar bug can help a developer understand a bug, or even directly fix it. Studying bugs with similar symptoms, programmers may determine how to detect or resolve them. To speed debugging, we advocate the systematic capture and reuse of debugging knowledge, much of which is currently wasted. The core challenge here is how to search for similar bugs. To tackle this problem, we exploit semantic bug information in the form of execution traces, which precisely capture bug semantics. This paper introduces novel tool and language support for semantically querying and analyzing bugs. We describe OSCILLOSCOPE, an Eclipse plugin, that uses a bug trace to exhaustively search its database for similar bugs and return their bug reports. OSCILLOSCOPE displays the traces of the bugs it returns against the trace of the target bug, so a developer can visually examine the quality of the matches. OSCILLOSCOPE rests on our bug query language (BQL), a flexible query language over traces. To realize OSCILLOSCOPE, we developed an open infrastructure that consists of a trace collection engine, BQL, a Hadoop-based query engine for BQL, a trace-indexed bug database, as well as a web-based frontend. OSCILLOSCOPE records and uploads bug traces to its infrastructure; it does so automatically when a JUnit test fails. We evaluated OSCILLOSCOPE on bugs collected from popular open-source projects. We show that OSCILLOSCOPE accurately and efficiently finds similar bugs, some of which could have been immediately used to fix open bugs.}},
  url = {https://doi.org/10.1145/2384616.2384684},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup715,
  title = {Classical BI: a logic for reasoning about dualising resources}},
  author = {Brotherston, James and Calcagno, Cristiano}},
  year = {2009}},
  journal = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We show how to extend O'Hearn and Pym's logic of bunched implications, BI, to classical BI (CBI), in which both the additive and the multiplicative connectives behave classically. Specifically, CBI is a non-conservative extension of (propositional) Boolean BI that includes multiplicative versions of falsity, negation and disjunction. We give an algebraic semantics for CBI that leads us naturally to consider resource models of CBI in which every resource has a unique dual. We then give a cut-eliminating proof system for CBI, based on Belnap's display logic, and demonstrate soundness and completeness of this proof system with respect to our semantics.}},
  url = {https://doi.org/10.1145/1480881.1480923},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup716,
  title = {Complete instantiation-based interpolation}},
  author = {Totla, Nishant and Wies, Thomas}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Craig interpolation has been a valuable tool for formal methods with interesting applications in program analysis and verification. Modern SMT solvers implement interpolation procedures for the theories that are most commonly used in these applications. However, many application-specific theories remain unsupported, which limits the class of problems to which interpolation-based techniques apply. In this paper, we present a generic framework to build new interpolation procedures via reduction to existing interpolation procedures. We consider the case where an application-specific theory can be formalized as an extension of a base theory with additional symbols and axioms. Our technique uses finite instantiation of the extension axioms to reduce an interpolation problem in the theory extension to one in the base theory. We identify a model-theoretic criterion that allows us to detect the cases where our technique is complete. We discuss specific theories that are relevant in program verification and that satisfy this criterion. In particular, we obtain complete interpolation procedures for theories of arrays and linked lists. The latter is the first complete interpolation procedure for a theory that supports reasoning about complex shape properties of heap-allocated data structures. We have implemented this procedure in a prototype on top of existing SMT solvers and used it to automatically infer loop invariants of list-manipulating programs.}},
  url = {https://doi.org/10.1145/2429069.2429132},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup717,
  title = {Indentation-sensitive parsing for Parsec}},
  author = {Adams, Michael D. and A\u{g}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM SIGPLAN Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Several popular languages including Haskell and Python use the indentation and layout of code as an essential part of their syntax. In the past, implementations of these languages used ad hoc techniques to implement layout. Recent work has shown that a simple extension to context-free grammars can replace these ad hoc techniques and provide both formal foundations and efficient parsing algorithms for indentation sensitivity.However, that previous work is limited to bottom-up, LR($k$) parsing, and many combinator-based parsing frameworks including Parsec use top-down algorithms that are outside its scope. This paper remedies this by showing how to add indentation sensitivity to parsing frameworks like Parsec. It explores both the formal semantics of and efficient algorithms for indentation sensitivity. It derives a Parsec-based library for indentation-sensitive parsing and presents benchmarks on a real-world language that show its efficiency and practicality.}},
  url = {https://doi.org/10.1145/2633357.2633369},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup718,
  title = {Impact of JVM superoperators on energy consumption in resource-constrained embedded systems}},
  author = {Badea, Carmen and Nicolau, Alexandru and Veidenbaum, Alexander V.}},
  year = {2008}},
  journal = {Proceedings of the 2008 ACM SIGPLAN-SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Energy consumption is one of the most important issues in resource-constrained embedded systems. Many such systems run Java-based applications due to Java's architecture-independent format (bytecode). Standard techniques for executing bytecode programs, e.g. interpretation or just-in-time compilation, have performance or memory issues that make them unsuitable for resource-constrained embedded systems.A superoperator-extended, lightweight Java Virtual Machine (JVM) can be used in resource-constrained embedded systems to improve performance and reduce memory consumption. This paper shows that such a JVM also significantly reduces energy consumption. This is due primarily to a considerable reduction in the number of memory accesses and thus in energy consumption in the instruction and data TLBs and caches and, in most cases, in DRAM energy consumption. Since the fraction of processor energy dissipated in these units is approximately 60\%, the energy savings achieved are significant.The paper evaluates the number of load, store, and computational instructions eliminated by the use of proposed superoperators as compared to a simple interpreter on a set of embedded benchmarks. Using cache and DRAM per access energy we estimate the total processor/DRAM energy saved by using our JVM. Our results show that with 32KB caches the reduction in energy consumption ranges from 40\% to 60\% of the overall processor, plus DRAM energy. Even higher savings may be achieved with smaller caches and increased access to DRAM as DRAM access energy is fairly high.}},
  url = {https://doi.org/10.1145/1375657.1375661},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup719,
  title = {Checking reachability using matching logic}},
  author = {Rosu, Grigore and Stefanescu, Andrei}},
  year = {2012}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents a verification framework that is parametric in a (trusted) operational semantics of some programming language. The underlying proof system is language-independent and consists of eight proof rules. The proof system is proved partially correct and relatively complete (with respect to the programming language configuration model). To show its practicality, the generic framework is instantiated with a fragment of C and evaluated with encouraging results.}},
  url = {https://doi.org/10.1145/2384616.2384656},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup720,
  title = {Parallel skeletons for structured composition}},
  author = {Darlington, John and Guo, Yi-ke and To, Hing Wing and Yang, Jin}},
  year = {1995}},
  journal = {Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this paper, we propose a straightforward solution to the problems of compositional parallel programming by using skeletons as the uniform mechanism for structured composition. In our approach parallel programs are constructed by composing procedures in a conventional base language using a set of high-level, pre-defined, functional, parallel computational forms known as skeletons. The ability to compose skeletons provides us with the essential tools for building further and more complex application-oriented skeletons specifying important aspects of parallel computation. Compared with the process network based composition approach, such as PCN, the skeleton approach abstracts away the fine details of connecting communication ports to the higher level mechanism of making data distributions conform, thus avoiding the complexity of using lower level ports as the means of interaction. Thus, the framework provides a natural integration of the compositional programming approach with the data parallel programming paradigm.}},
  url = {https://doi.org/10.1145/209936.209940},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup721,
  title = {Static and dynamic semantics of NoSQL languages}},
  author = {Benzaken, V\'{e}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a calculus for processing semistructured data that spans differences of application area among several novel query languages, broadly categorized as "NoSQL". This calculus lets users define their own operators, capturing a wider range of data processing capabilities, whilst providing a typing precision so far typical only of primitive hard-coded operators. The type inference algorithm is based on semantic type checking, resulting in type information that is both precise, and flexible enough to handle structured and semistructured data. We illustrate the use of this calculus by encoding a large fragment of Jaql, including operations and iterators over JSON, embedded SQL expressions, and co-grouping, and show how the encoding directly yields a typing discipline for Jaql as it is, namely without the addition of any type definition or type annotation in the code.}},
  url = {https://doi.org/10.1145/2429069.2429083},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup722,
  title = {Faster WCET flow analysis by program slicing}},
  author = {Sandberg, Christer and Ermedahl, Andreas and Gustafsson, Jan and Lisper, Bj\"{o}},
  year = {2006}},
  journal = {Proceedings of the 2006 ACM SIGPLAN/SIGBED Conference on Language, Compilers, and Tool Support for Embedded Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Static Worst-Case Execution Time (WCET) analysis is a technique to derive upper bounds for the execution times of programs. Such bounds are crucial when designing and verifying real-time systems. WCET analysis needs a program flow analysis to derive constraints on the possible execution paths of the analysed program, like iteration bounds for loops and dependences between conditionals.Current WCET analysis tools typically obtain flow information through manual annotations. Better support for automatic flow analysis would eliminate much of the need for this laborious work. However, to automatically derive high-quality flow information is hard, and solution techniques with large time and space complexity are often required.In this paper we describe how to use program slicing to reduce the computational need of flow analysis methods. The slicing identifes statements and variables which are guaranteed not to influence the program flow. When these are removed, the calculation time of our different flow analyses decreases, in some cases considerably.We also show how program slicing can be used to identify the input variables and globals that control the outcome of a particular loop or conditional. This should be valuable aid when performing WCET analysis and systematic testing of large and complex real-time programs.}},
  url = {https://doi.org/10.1145/1134650.1134666},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup723,
  title = {A relational framework for higher-order shape analysis}},
  author = {Kaki, Gowtham and Jagannathan, Suresh}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We propose the integration of a relational specification framework within a dependent type system capable of verifying complex invariants over the shapes of algebraic datatypes. Our approach is based on the observation that structural properties of such datatypes can often be naturally expressed as inductively-defined relations over the recursive structure evident in their definitions. By interpreting constructor applications (abstractly) in a relational domain, we can define expressive relational abstractions for a variety of complex data structures, whose structural and shape invariants can be automatically verified. Our specification language also allows for definitions of parametricrelations for polymorphic data types that enable highly composable specifications and naturally generalizes to higher-order polymorphic functions.We describe an algorithm that translates relational specifications into a decidable fragment of first-order logic that can be efficiently discharged by an SMT solver. We have implemented these ideas in a type checker called CATALYST that is incorporated within the MLton SML compiler. Experimental results and case studies indicate that our verification strategy is both practical and effective.}},
  url = {https://doi.org/10.1145/2628136.2628159},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup724,
  title = {Program synthesis from polymorphic refinement types}},
  author = {Polikarpova, Nadia and Kuraj, Ivan and Solar-Lezama, Armando}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a method for synthesizing recursive functions that provably satisfy a given specification in the form of a polymorphic refinement type. We observe that such specifications are particularly suitable for program synthesis for two reasons. First, they offer a unique combination of expressive power and decidability, which enables automatic verification—and hence synthesis—of nontrivial programs. Second, a type-based specification for a program can often be effectively decomposed into independent specifications for its components, causing the synthesizer to consider fewer component combinations and leading to a combinatorial reduction in the size of the search space. At the core of our synthesis procedure is a newalgorithm for refinement type checking, which supports specification decomposition. We have evaluated our prototype implementation on a large set of synthesis problems and found that it exceeds the state of the art in terms of both scalability and usability. The tool was able to synthesize more complex programs than those reported in prior work (several sorting algorithms and operations on balanced search trees), as well as most of the benchmarks tackled by existing synthesizers, often starting from a more concise and intuitive user input.}},
  url = {https://doi.org/10.1145/2908080.2908093},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup725,
  title = {Component-based synthesis of table consolidation and transformation tasks from examples}},
  author = {Feng, Yu and Martins, Ruben and Van Geffen, Jacob and Dillig, Isil and Chaudhuri, Swarat}},
  year = {2017}},
  journal = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents a novel component-based synthesis algorithm that marries the power of type-directed search with lightweight SMT-based deduction and partial evaluation. Given a set of components together with their over-approximate first-order specifications, our method first generates a program sketch over a subset of the components and checks its feasibility using an SMT solver. Since a program sketch typically represents many concrete programs, the use of SMT-based deduction greatly increases the scalability of the algorithm. Once a feasible program sketch is found, our algorithm completes the sketch in a bottom-up fashion, using partial evaluation to further increase the power of deduction for rejecting partially-filled program sketches. We apply the proposed synthesis methodology for automating a large class of data preparation tasks that commonly arise in data science. We have evaluated our synthesis algorithm on dozens of data wrangling and consolidation tasks obtained from on-line forums, and we show that our approach can automatically solve a large class of problems encountered by R users.}},
  url = {https://doi.org/10.1145/3062341.3062351},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup726,
  title = {Integrated CPU and l2 cache voltage scaling using machine learning}},
  author = {AbouGhazaleh, Nevine and Ferreira, Alexandre and Rusu, Cosmin and Xu, Ruibin and Liberato, Frank and Childers, Bruce and Mosse, Daniel and Melhem, Rami}},
  year = {2007}},
  journal = {Proceedings of the 2007 ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Embedded systems serve an emerging and diverse set of applications. As a result, more computational and storage capabilities are added to accommodate ever more demanding applications. Unfortunately, adding more resources typically comes on the expense of higher energy costs. New chip design with Multiple Clock Domains (MCD) opens the opportunity for fine-grain power management within theprocessor chip. When used with dynamic voltage scaling (DVS), we can control the voltage and power of each domain independently. A significant power and energy improvement has been shown when using MCD design in comparison to managing a single voltage domain for the whole chip, as in traditional chips with global DVS.In this paper, we propose PACSL a Power-Aware Compiler-based approach using Supervised Learning. PACSL automatically derives an integrated CPU-core and on-chip L2 cache DVS policy tailored to a specific system and workload. Our approach uses supervised machine learning to discover a policy, which relies on monitoring a few performance counters. We present our approach detailing the role of a compiler in constructing a custom power management policy. We also discuss some implementation issues associated with our technique. We show that PACSL improves on traditional power management techniques that are used in general MCD chips. Our technique saves 22\% on average (up to 46\%) in energy-delay product over a DVS technique that applies independent DVS decisions in each domain. Compared to no-power management, our technique improves energy-delay product by 26\% on average (up to 64\%).}},
  url = {https://doi.org/10.1145/1254766.1254773},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup727,
  title = {A traversal-based algorithm for higher-order model checking}},
  author = {Neatherway, Robin P. and Ramsay, Steven J. and Ong, Chih-Hao Luke}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Higher-order model checking - the model checking of trees generated by higher-order recursion schemes (HORS) - is a natural generalisation of finite-state and pushdown model checking. Recent work has shown that it can serve as a basis for software model checking for functional languages such as ML and Haskell. In this paper, we introduce higher-order recursion schemes with cases (HORSC), which extend HORS with a definition-by-cases construct (to express program branching based on data) and non-determinism (to express abstractions of behaviours). This paper is a study of the universal HORSC model checking problem for deterministic trivial automata: does the automaton accept every tree in the tree language generated by the given HORSC? We first characterise the model checking problem by an intersection type system extended with a carefully restricted form of union types. We then present an algorithm for deciding the model checking problem, which is based on the notion of traversals induced by the fully abstract game semantics of these schemes, but presented as a goal-directed construction of derivations in the intersection and union type system. We view HORSC model checking as a suitable backend engine for an approach to verifying functional programs. We have implemented the algorithm in a tool called TravMC, and demonstrated its effectiveness on a test suite of programs, including abstract models of functional programs obtained via an abstraction-refinement procedure from pattern-matching recursion schemes.}},
  url = {https://doi.org/10.1145/2364527.2364578},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup728,
  title = {Similarity of binaries through re-optimization}},
  author = {David, Yaniv and Partush, Nimrod and Yahav, Eran}},
  year = {2017}},
  journal = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a scalable approach for establishing similarity between stripped binaries (with no debug information). The main challenge in binary similarity, is to establish similarity even when the code has been compiled using different compilers, with different optimization levels, or targeting different architectures. Overcoming this challenge, while avoiding false positives, is invaluable to the process of reverse engineering and the process of locating vulnerable code. We present a technique that is scalable and precise, as it alleviates the need for heavyweight semantic comparison by performing out-of-context re-optimization of procedure fragments. It works by decomposing binary procedures to comparable fragments and transforming them to a canonical, normalized form using the compiler optimizer, which enables finding equivalent fragments through simple syntactic comparison. We use a statistical framework built by analyzing samples collected "in the wild" to generate a global context that quantifies the significance of each pair of fragments, and uses it to lift pairwise fragment equivalence to whole procedure similarity. We have implemented our technique in a tool called GitZ and performed an extensive evaluation. We show that GitZ is able to perform millions of comparisons efficiently, and find similarity with high accuracy.}},
  url = {https://doi.org/10.1145/3062341.3062387},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup729,
  title = {Statistical similarity of binaries}},
  author = {David, Yaniv and Partush, Nimrod and Yahav, Eran}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We address the problem of finding similar procedures in stripped binaries. We present a new statistical approach for measuring the similarity between two procedures. Our notion of similarity allows us to find similar code even when it has been compiled using different compilers, or has been modified. The main idea is to use similarity by composition: decompose the code into smaller comparable fragments, define semantic similarity between fragments, and use statistical reasoning to lift fragment similarity into similarity between procedures. We have implemented our approach in a tool called Esh, and applied it to find various prominent vulnerabilities across compilers and versions, including Heartbleed, Shellshock and Venom. We show that Esh produces high accuracy results, with few to no false positives -- a crucial factor in the scenario of vulnerability search in stripped binaries.}},
  url = {https://doi.org/10.1145/2908080.2908126},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup730,
  title = {A Scalable, Correct Time-Stamped Stack}},
  author = {Dodds, Mike and Haas, Andreas and Kirsch, Christoph M.}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Concurrent data-structures, such as stacks, queues, and deques, often implicitly enforce a total order over elements in their underlying memory layout. However, much of this order is unnecessary: linearizability only requires that elements are ordered if the insert methods ran in sequence. We propose a new approach which uses timestamping to avoid unnecessary ordering. Pairs of elements can be left unordered if their associated insert operations ran concurrently, and order imposed as necessary at the eventual removal.We realise our approach in a new non-blocking data-structure, the TS (timestamped) stack. Using the same approach, we can define corresponding queue and deque data-structures. In experiments on x86, the TS stack outperforms and outscales all its competitors -- for example, it outperforms the elimination-backoff stack by factor of two. In our approach, more concurrency translates into less ordering, giving less-contended removal and thus higher performance and scalability. Despite this, the TS stack is linearizable with respect to stack semantics.The weak internal ordering in the TS stack presents a challenge when establishing linearizability: standard techniques such as linearization points work well when there exists a total internal order. We present a new stack theorem, mechanised in Isabelle, which characterises the orderings sufficient to establish stack semantics. By applying our stack theorem, we show that the TS stack is indeed linearizable. Our theorem constitutes a new, generic proof technique for concurrent stacks, and it paves the way for future weakly ordered data-structure designs.}},
  url = {https://doi.org/10.1145/2676726.2676963},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup731,
  title = {Productive coprogramming with guarded recursion}},
  author = {Atkey, Robert and McBride, Conor}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Total functional programming offers the beguiling vision that, just by virtue of the compiler accepting a program, we are guaranteed that it will always terminate. In the case of programs that are not intended to terminate, e.g., servers, we are guaranteed that programs will always be productive. Productivity means that, even if a program generates an infinite amount of data, each piece will be generated in finite time. The theoretical underpinning for productive programming with infinite output is provided by the category theoretic notion of final coalgebras. Hence, we speak of coprogramming with non-well-founded codata, as a dual to programming with well-founded data like finite lists and trees.Systems that offer facilities for productive coprogramming, such as the proof assistants Coq and Agda, currently do so through syntactic guardedness checkers. Syntactic guardedness checkers ensure that all self-recursive calls are guarded by a use of a constructor. Such a check ensures productivity. Unfortunately, these syntactic checks are not compositional, and severely complicate coprogramming.Guarded recursion, originally due to Nakano, is tantalising as a basis for a flexible and compositional type-based approach to coprogramming. However, as we show, by itself, guarded recursion is not suitable for coprogramming due to the fact that there is no way to make finite observations on pieces of infinite data. In this paper, we introduce the concept of clock variables that index Nakano's guarded recursion. Clock variables allow us to "close over" the generation of infinite data, and to make finite observations, something that is not possible with guarded recursion alone.}},
  url = {https://doi.org/10.1145/2500365.2500597},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup732,
  title = {Gradual typing embedded securely in JavaScript}},
  author = {Swamy, Nikhil and Fournet, Cedric and Rastogi, Aseem and Bhargavan, Karthikeyan and Chen, Juan and Strub, Pierre-Yves and Bierman, Gavin}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {JavaScript's flexible semantics makes writing correct code hard and writing secure code extremely difficult. To address the former problem, various forms of gradual typing have been proposed, such as Closure and TypeScript. However, supporting all common programming idioms is not easy; for example, TypeScript deliberately gives up type soundness for programming convenience. In this paper, we propose a gradual type system and implementation techniques that provide important safety and security guarantees.We present TS# , a gradual type system and source-to-source compiler for JavaScript. In contrast to prior gradual type systems, TS# features full runtime reflection over three kinds of types: (1) simple types for higher-order functions, recursive datatypes and dictionary-based extensible records; (2) the type any, for dynamically type-safe TS# expressions; and (3) the type un, for untrusted, potentially malicious JavaScript contexts in which TS# is embedded. After type-checking, the compiler instruments the program with various checks to ensure the type safety of TS# despite its interactions with arbitrary JavaScript contexts, which are free to use eval, stack walks, prototype customizations, and other offensive features. The proof of our main theorem employs a form of type-preserving compilation, wherein we prove all the runtime invariants of the translation of TS# to JavaScript by showing that translated programs are well-typed in JS# , a previously proposed dependently typed language for proving functional correctness of JavaScript programs.We describe a prototype compiler, a secure runtime, and sample applications for TS#. Our examples illustrate how web security patterns that developers currently program in JavaScript (with much difficulty and still with dubious results) can instead be programmed naturally in TS#, retaining a flavor of idiomatic JavaScript, while providing strong safety guarantees by virtue of typing.}},
  url = {https://doi.org/10.1145/2535838.2535889},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup733,
  title = {A calculus for probabilistic languages}},
  author = {Park, Sungwoo}},
  year = {2003}},
  journal = {Proceedings of the 2003 ACM SIGPLAN International Workshop on Types in Languages Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {As probabilistic computation plays an increasing role in diverse fields in computer science, researchers have designed new languages to facilitate the development of probabilistic programs. In this paper, we develop a probabilistic calculus by extending the traditional lambda calculus. In our calculus, every expression denotes a probability distribution yet evaluates to a regular value. The most notable feature of our calculus is that it is founded upon sampling functions, which map the unit interval to probability domains. As a consequence, we achieve a unified representation scheme for all types of probability distributions. In order to support an efficient implementation of the calculus, we also develop a refinement type system which is capable of distinguishing expressions denoting regular values from expressions denoting probability distributions. We use a novel formulation of the intuitionistic modal logic S4 with an intersection connective in the refinement type system. We present preliminary evidence that a probabilistic language based upon our calculus is viable in applications involving massive probabilistic computation.}},
  url = {https://doi.org/10.1145/604174.604180},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup734,
  title = {Functional programs that explain their work}},
  author = {Perera, Roly and Acar, Umut A. and Cheney, James and Levy, Paul Blain}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present techniques that enable higher-order functional computations to "explain" their work by answering questions about how parts of their output were calculated. As explanations, we consider the traditional notion of program slices, which we show can be inadequate, and propose a new notion: trace slices. We present techniques for specifying flexible and rich slicing criteria based on partial expressions, parts of which have been replaced by holes.We characterise program slices in an algorithm-independent fashion and show that a least slice for a given criterion exists. We then present an algorithm, called unevaluation, for computing least program slices from computations reified as traces. Observing a limitation of program slices, we develop a notion of trace slice as another form of explanation and present an algorithm for computing them. The unevaluation algorithm can be applied to any subtrace of a trace slice to compute a program slice whose evaluation generates that subtrace. This close correspondence between programs, traces, and their slices can enable the programmer to understand a computation interactively, in terms of the programming language in which the computation is expressed. We present an implementation in the form of a tool, discuss some important practical implementation concerns and present some techniques for addressing them.}},
  url = {https://doi.org/10.1145/2364527.2364579},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup735,
  title = {Verification modulo versions: towards usable verification}},
  author = {Logozzo, Francesco and Lahiri, Shuvendu K. and F\"{a}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We introduce Verification Modulo Versions (VMV), a new static analysis technique for reducing the number of alarms reported by static verifiers while providing sound semantic guarantees. First, VMV extracts semantic environment conditions from a base program P. Environmental conditions can either be sufficient conditions (implying the safety of P) or necessary conditions (implied by the safety of P). Then, VMV instruments a new version of the program, P', with the inferred conditions. We prove that we can use (i) sufficient conditions to identify abstract regressions of P' w.r.t. P; and (ii) necessary conditions to prove the relative correctness of P' w.r.t. P. We show that the extraction of environmental conditions can be performed at a hierarchy of abstraction levels (history, state, or call conditions) with each subsequent level requiring a less sophisticated matching of the syntactic changes between P' and P. Call conditions are particularly useful because they only require the syntactic matching of entry points and callee names across program versions. We have implemented VMV in a widely used static analysis and verification tool. We report our experience on two large code bases and demonstrate a substantial reduction in alarms while additionally providing relative correctness guarantees.}},
  url = {https://doi.org/10.1145/2594291.2594326},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup736,
  title = {Defining the undefinedness of C}},
  author = {Hathhorn, Chris and Ellison, Chucky and Ro\c{s}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a ``negative'' semantics of the C11 language---a semantics that does not just give meaning to correct programs, but also rejects undefined programs. We investigate undefined behavior in C and discuss the techniques and special considerations needed for formally specifying it. We have used these techniques to modify and extend a semantics of C into one that captures undefined behavior. The amount of semantic infrastructure and effort required to achieve this was unexpectedly high, in the end nearly doubling the size of the original semantics. From our semantics, we have automatically extracted an undefinedness checker, which we evaluate against other popular analysis tools, using our own test suite in addition to a third-party test suite. Our checker is capable of detecting examples of all 77 categories of core language undefinedness appearing in the C11 standard, more than any other tool we considered. Based on this evaluation, we argue that our work is the most comprehensive and complete semantic treatment of undefined behavior in C, and thus of the C language itself.}},
  url = {https://doi.org/10.1145/2737924.2737979},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup737,
  title = {Taming the parallel effect zoo: extensible deterministic parallelism with LVish}},
  author = {Kuper, Lindsey and Todd, Aaron and Tobin-Hochstadt, Sam and Newton, Ryan R.}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A fundamental challenge of parallel programming is to ensure that the observable outcome of a program remains deterministic in spite of parallel execution. Language-level enforcement of determinism is possible, but existing deterministic-by-construction parallel programming models tend to lack features that would make them applicable to a broad range of problems. Moreover, they lack extensibility: it is difficult to add or change language features without breaking the determinism guarantee.The recently proposed LVars programming model, and the accompanying LVish Haskell library, took a step toward broadly-applicable guaranteed-deterministic parallel programming. The LVars model allows communication through shared monotonic data structures to which information can only be added, never removed, and for which the order in which information is added is not observable. LVish provides a Par monad for parallel computation that encapsulates determinism-preserving effects while allowing a more flexible form of communication between parallel tasks than previous guaranteed-deterministic models provided.While applying LVar-based programming to real problems using LVish, we have identified and implemented three capabilities that extend its reach: inflationary updates other than least-upper-bound writes; transitive task cancellation; and parallel mutation of non-overlapping memory locations. The unifying abstraction we use to add these capabilities to LVish---without suffering added complexity or cost in the core LVish implementation, or compromising determinism---is a form of monad transformer, extended to handle the Par monad. With our extensions, LVish provides the most broadly applicable guaranteed-deterministic parallel programming interface available to date. We demonstrate the viability of our approach both with traditional parallel benchmarks and with results from a real-world case study: a bioinformatics application that we parallelized using our extended version of LVish.}},
  url = {https://doi.org/10.1145/2594291.2594312},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup738,
  title = {Packrats parse in packs}},
  author = {Bla\v{z}},
  year = {2017}},
  journal = {Proceedings of the 10th ACM SIGPLAN International Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a novel but remarkably simple formulation of formal language grammars in Haskell as functions mapping a record of pro- duction parsers to itself. Thus formulated grammars are first-class objects, composable and reusable. We also provide a simple parser implementation for them, based on an improved packrat algorithm. In order to make the grammar manipulation code reusable, we introduce a set of type classes mirroring the existing type classes from Haskell base library, but whose methods have rank-2 types.}},
  url = {https://doi.org/10.1145/3122955.3122958},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup739,
  title = {"What's in a name?" going beyond allocation site names in heap analysis}},
  author = {Kanvar, Vini and Khedker, Uday P.}},
  year = {2017}},
  journal = {Proceedings of the 2017 ACM SIGPLAN International Symposium on Memory Management}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A points-to analysis computes a sound abstraction of heap memory conventionally using a name-based abstraction that summarizes runtime memory by grouping locations using the names of allocation sites: All concrete heap locations allocated by the same statement are grouped together. The locations in the same group are treated alike i.e., a pointer to any one location of the group is assumed to point to every location in the group leading to an over-approximation of points-to relations. We propose an access-based abstraction that partitions each name-based group of locations into equivalence classes at every program point using an additional criterion of the sets of access paths (chains of pointer indirections) reaching the locations in the memory. The intuition is that the locations that are both allocated and accessed alike should be grouped into the same equivalence class. Since the access paths in the memory could reach different locations at different program points, our groupings change flow sensitively unlike the name-based groupings. This creates a more precise view of the memory. Theoretically, it is strictly more precise than the name-based abstraction except in some trivial cases; practically it is far more precise. Our empirical measurements show the benefits of our tool Access-Based Heap Analyzer (ABHA) on SPEC CPU 2006 and heap manipulating SV-COMP benchmarks. ABHA, which is field-, flow-, and context-sensitive, scales to 20 kLoC and can improve the precision even up to 99\% (in terms of the number of aliases). Additionally, ABHA allows any user-defined summarization of an access path to be plugged in; we have implemented and evaluated four summarization techniques. ABHA can also act as a front-end to TVLA, a parametrized shape analyzer, in order to automate its parametrization by generating predicates that capture the program behaviour more accurately.}},
  url = {https://doi.org/10.1145/3092255.3092267},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup740,
  title = {BigFoot: static check placement for dynamic race detection}},
  author = {Rhodes, Dustin and Flanagan, Cormac and Freund, Stephen N.}},
  year = {2017}},
  journal = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Precise dynamic data race detectors provide strong correctness guarantees but have high overheads because they generally keep analysis state in a separate shadow location for each heap memory location, and they check (and potentially update) the corresponding shadow location on each heap access. The BigFoot dynamic data race detector uses a combination of static and dynamic analysis techniques to coalesce checks and compress shadow locations. With BigFoot, multiple accesses to an object or array often induce a single coalesced check that manipulates a single compressed shadow location, resulting in a performance improvement over FastTrack of 61\%.}},
  url = {https://doi.org/10.1145/3062341.3062350},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup741,
  title = {A co-contextual formulation of type rules and its application to incremental type checking}},
  author = {Erdweg, Sebastian and Bra\v{c}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Type rules associate types to expressions given a typing context. As the type checker traverses the expression tree top-down, it extends the typing context with additional context information that becomes available. This way, the typing context coordinates type checking in otherwise independent subexpressions, which inhibits parallelization and incrementalization of type checking. We propose a co-contextual formulation of type rules that only take an expression as input and produce a type and a set of context requirements. Co-contextual type checkers traverse an expression tree bottom-up and merge context requirements of independently checked subexpressions. We describe a method for systematically constructing a co-contextual formulation of type rules from a regular context-based formulation and we show how co-contextual type rules give rise to incremental type checking. Using our method, we derive incremental type checkers for PCF and for extensions that introduce records, parametric polymorphism, and subtyping. Our performance evaluation shows that co-contextual type checking has performance comparable to standard context-based type checking, and incrementalization can improve performance significantly.}},
  url = {https://doi.org/10.1145/2814270.2814277},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup742,
  title = {Modular development of certified program verifiers with a proof assistant}},
  author = {Chlipala, Adam}},
  year = {2006}},
  journal = {Proceedings of the Eleventh ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {I report on an experience using the Coq proof assistant to develop a program verification tool with a machine-checkable proof of full correctness. The verifier is able to prove memory safety of x86 machine code programs compiled from code that uses algebraic datatypes. The tool's soundness theorem is expressed in terms of the bit-level semantics of x86 programs, so its correctness depends on very few assumptions. I take advantage of Coq's support for programming with dependent types and modules in the structure of my development. The approach is based on developing a library of reusable functors for transforming a verifier at one level of abstraction into a verifier at a lower level. Using this library, it's possible to prototype a verifier based on a new type system with a minimal amount of work, while obtaining a very strong soundness theorem about the final product.}},
  url = {https://doi.org/10.1145/1159803.1159825},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup743,
  title = {On the runtime complexity of type-directed unboxing}},
  author = {Minamide, Yasuhiko and Garrigue, Jacques}},
  year = {1998}},
  journal = {Proceedings of the Third ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Avoiding boxing when representing native objects is essential for the efficient compilation of any programming language For polymorphic languages this task is difficult, but several schemes have been proposed that remove boxing on the basis of type information. Leroy's type-directed unboxing transformation is one of them. One of its nicest properties is that it relies only on visible types, which makes it compatible with separate compilation. However it has been noticed that it is not safe both in terms of time and space complexity ---i.e. transforming a program may raise its complexity. We propose a refinement of this transformation, still relying only on visible types, and prove that it satisfies the safety condition for time complexity. The proof is an extension of the usual logical relation method, in which correctness and safety are proved simultaneously.}},
  url = {https://doi.org/10.1145/289423.289424},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup744,
  title = {Causality of optimized Haskell: what is burning our cycles?}},
  author = {Wortmann, Peter M. and Duke, David}},
  year = {2013}},
  journal = {Proceedings of the 2013 ACM SIGPLAN Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Profiling real-world Haskell programs is hard, as compiler optimizations make it tricky to establish causality between the source code and program behavior. In this paper we attack the root issue by performing a causality analysis of functional programs under optimization. We apply our findings to build a novel profiling infrastructure on top of the Glasgow Haskell Compiler, allowing for performance analysis even of aggressively optimized programs.}},
  url = {https://doi.org/10.1145/2503778.2503788},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup745,
  title = {Singe: leveraging warp specialization for high performance on GPUs}},
  author = {Bauer, Michael and Treichler, Sean and Aiken, Alex}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present Singe, a Domain Specific Language (DSL) compiler for combustion chemistry that leverages warp specialization to produce high performance code for GPUs. Instead of relying on traditional GPU programming models that emphasize data-parallel computations, warp specialization allows compilers like Singe to partition computations into sub-computations which are then assigned to different warps within a thread block. Fine-grain synchronization between warps is performed efficiently in hardware using producer-consumer named barriers. Partitioning computations using warp specialization allows Singe to deal efficiently with the irregularity in both data access patterns and computation. Furthermore, warp-specialized partitioning of computations allows Singe to fit extremely large working sets into on-chip memories. Finally, we describe the architecture and general compilation techniques necessary for constructing a warp-specializing compiler. We show that the warp-specialized code emitted by Singe is up to 3.75X faster than previously optimized data-parallel GPU kernels.}},
  url = {https://doi.org/10.1145/2555243.2555258},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup746,
  title = {Bounded exhaustive test input generation from hybrid invariants}},
  author = {Rosner, Nicol\'{a}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \& Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a novel technique for producing bounded exhaustive test suites from hybrid invariants, i.e., invariants that are expressed imperatively, declaratively, or as a combination of declarative and imperative predicates. Hybrid specifications are processed using known mechanisms for the imperative and declarative parts, but combined in a way that enables us to exploit information from the declarative side, such as tight bounds computed from the declarative specification, to improve the search both on the imperative and declarative sides. Moreover, our technique automatically evaluates different possible ways of processing the imperative side, and the alternative settings (imperative or declarative) for parts of the invariant available both declaratively and imperatively, to decide the most convenient invariant configuration with respect to efficiency in test generation. This is achieved by transcoping, i.e., by assessing the efficiency of the different alternatives on small scopes (where generation times are negligible), and then extrapolating the results to larger scopes.We also show experiments involving collection classes that support the effectiveness of our technique, by demonstrating that (i) bounded exhaustive suites can be computed from hybrid invariants significantly more efficiently than doing so using state-of-the-art purely imperative and purely declarative approaches, and (ii) our technique is able to automatically determine efficient hybrid invariants, in the sense that they lead to an efficient computation of bounded exhaustive suites, using transcoping.}},
  url = {https://doi.org/10.1145/2660193.2660232},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup747,
  title = {Ornaments: exploiting parametricity for safer, more automated code refactorization and code reuse (invited talk)}},
  author = {R\'{e}},
  year = {2017}},
  journal = {Proceedings of the 10th ACM SIGPLAN International Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Inductive datatypes and parametric polymorphism are two key features introduced in the ML family of languages, which have already been widely exploited for structuring programs: Haskell and ML programs are often more elegant and more correct by construction. Still, we sometimes need code to be refactored or adapted to be reused in a slightly different context. While the type system is considerably helpful in these situations, by automatically locating type-inconsistent program points or incomplete pattern matchings, this process could be made safer and more automated by further exploiting parametricity. We propose a posteriori program abstraction as a principle for such code transformations. We apply this principle to ornamentation which is a way to describe changes in datatype definitions reorganizing, adding, or dropping some pieces of data so that functions operating on the bare definition can be partially and sometimes totally lifted into functions operating on the ornamented structure. We view ornamentation as an a posteriori abstraction of the bare code, called a generic lifting, which can then be instantiated into a concrete lifting, meta-reduced, and simplified. Both the source and target code live in core ML while the lifted code lives in a meta-language above ML equipped with a limited form of dependent types needed to capture some invariants of the generic lifting so that the concrete lifting can be simplified back into an ML program. Importantly, the lifted code can be closely related to the bare code, using logical relations thanks to the generic lifting detour. Different, typical use cases of ornaments will be shown and the approach will be mainly illustrated on examples.}},
  url = {https://doi.org/10.1145/3122955.3127333},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup748,
  title = {A general data dependence test for dynamic, pointer-based data structures}},
  author = {Hummel, Joseph and Hendren, Laurie J. and Nicolau, Alexandru}},
  year = {1994}},
  journal = {Proceedings of the ACM SIGPLAN 1994 Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Optimizing compilers require accurate dependence testing to enable numerous, performance-enhancing transformations. However, data dependence testing is a difficult problem, particularly in the presence of pointers. Though existing approaches work well for pointers to named memory locations (i.e. other variables), they are overly conservative in the case of pointers to unnamed memory locations. The latter occurs in the context of dynamic, pointer-based data structures, used in a variety of applications ranging from system software to computational geometry to N-body and circuit simulations.In this paper we present a new technique for performing more accurate data dependence testing in the presence of dynamic, pointer-based data structures. We will demonstrate its effectiveness by breaking false dependences that existing approaches cannot, and provide results which show that removing these dependences enables significant parallelization of a real application.}},
  url = {https://doi.org/10.1145/178243.178262},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup749,
  title = {Latte: a language, compiler, and runtime for elegant and efficient deep neural networks}},
  author = {Truong, Leonard and Barik, Rajkishore and Totoni, Ehsan and Liu, Hai and Markley, Chick and Fox, Armando and Shpeisman, Tatiana}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Deep neural networks (DNNs) have undergone a surge in popularity with consistent advances in the state of the art for tasks including image recognition, natural language processing, and speech recognition. The computationally expensive nature of these networks has led to the proliferation of implementations that sacrifice abstraction for high performance. In this paper, we present Latte, a domain-specific language for DNNs that provides a natural abstraction for specifying new layers without sacrificing performance. Users of Latte express DNNs as ensembles of neurons with connections between them. The Latte compiler synthesizes a program based on the user specification, applies a suite of domain-specific and general optimizations, and emits efficient machine code for heterogeneous architectures. Latte also includes a communication runtime for distributed memory data-parallelism. Using networks described using Latte, we demonstrate 3-6x speedup over Caffe (C++/MKL) on the three state-of-the-art ImageNet models executing on an Intel Xeon E5-2699 v3 x86 CPU.}},
  url = {https://doi.org/10.1145/2908080.2908105},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup750,
  title = {Late data layout: unifying data representation transformations}},
  author = {Ureche, Vlad and Burmako, Eugene and Odersky, Martin}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \& Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Values need to be represented differently when interacting with certain language features. For example, an integer has to take an object-based representation when interacting with erased generics, although, for performance reasons, the stack-based value representation is better. To abstract over these implementation details, some programming languages choose to expose a unified high-level concept (the integer) and let the compiler choose its exact representation and insert coercions where necessary.This pattern appears in multiple language features such as value classes, specialization and multi-stage programming: they all expose a unified concept which they later refine into multiple representations. Yet, the underlying compiler implementations typically entangle the core mechanism with assumptions about the alternative representations and their interaction with other language features.In this paper we present the Late Data Layout mechanism, a simple but versatile type-driven generalization that subsumes and improves the state-of-the-art representation transformations. In doing so, we make two key observations: (1) annotated types conveniently capture the semantics of using multiple representations and (2) local type inference can be used to consistently and optimally introduce coercions.We validated our approach by implementing three language features as Scala compiler extensions: value classes, specialization (using the miniboxing representation) and a simplified multi-stage programming mechanism.}},
  url = {https://doi.org/10.1145/2660193.2660197},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup751,
  title = {Modular, higher-order cardinality analysis in theory and practice}},
  author = {Sergey, Ilya and Vytiniotis, Dimitrios and Peyton Jones, Simon}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Since the mid '80s, compiler writers for functional languages (especially lazy ones) have been writing papers about identifying and exploiting thunks and lambdas that are used only once. However it has proved difficult to achieve both power and simplicity in practice. We describe a new, modular analysis for a higher-order language, which is both simple and effective, and present measurements of its use in a full-scale, state of the art optimising compiler. The analysis finds many single-entry thunks and one-shot lambdas and enables a number of program optimisations.}},
  url = {https://doi.org/10.1145/2535838.2535861},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup752,
  title = {Pushouts in software architecture design}},
  author = {Rich\'{e}},
  year = {2012}},
  journal = {Proceedings of the 11th International Conference on Generative Programming and Component Engineering}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A classical approach to program derivation is to progressively extend a simple specification and then incrementally refine it to an implementation. We claim this approach is hard or impractical when reverse engineering legacy software architectures. We present a case study that shows optimizations and pushouts---in addition to refinements and extensions---are essential for practical stepwise development of complex software architectures.}},
  url = {https://doi.org/10.1145/2371401.2371415},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup753,
  title = {Subtyping delimited continuations}},
  author = {Materzok, Marek and Biernacki, Dariusz}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a type system with subtyping for first-class delimited continuations that generalizes Danvy and Filinski's type system for shift and reset by maintaining explicit information about the types of contexts in the metacontext. We exploit this generalization by considering the control operators known as shift0 and reset0 that can access arbitrary contexts in the metacontext. We use subtyping to control the level of information about the metacontext the expression actually requires and in particular to coerce pure expressions into effectful ones. For this type system we prove strong type soundness and termination of evaluation and we present a provably correct type reconstruction algorithm. We also introduce two CPS translations for shift0 and reset0: one targeting the untyped lambda calculus, and another - type-directed - targeting the simply-typed lambda calculus. The latter translation preserves typability and is selective in that it keeps pure expressions in direct style.}},
  url = {https://doi.org/10.1145/2034773.2034786},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup754,
  title = {Monitors and blame assignment for higher-order session types}},
  author = {Jia, Limin and Gommerstadt, Hannah and Pfenning, Frank}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Session types provide a means to prescribe the communication behavior between concurrent message-passing processes. However, in a distributed setting, some processes may be written in languages that do not support static typing of sessions or may be compromised by a malicious intruder, violating invariants of the session types. In such a setting, dynamically monitoring communication between processes becomes a necessity for identifying undesirable actions. In this paper, we show how to dynamically monitor communication to enforce adherence to session types in a higher-order setting. We present a system of blame assignment in the case when the monitor detects an undesirable action and an alarm is raised. We prove that dynamic monitoring does not change system behavior for welltyped processes, and that one of an indicated set of possible culprits must have been compromised in case of an alarm.}},
  url = {https://doi.org/10.1145/2837614.2837662},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup755,
  title = {A short cut to parallelization theorems}},
  author = {Morihata, Akimasa}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The third list-homomorphism theorem states that if a function is both foldr and foldl, it has a divide-and-conquer parallel implementation as well. In this paper, we develop a theory for obtaining such parallelization theorems. The key is a new proof of the third list-homomorphism theorem based on shortcut deforestation. The proof implies that there exists a divide-and-conquer parallel program of the form of h(x 'merge' y) = h1 x odot h2 y, where h is the subject of parallelization, merge is the operation of integrating independent substructures, h1 and h2 are computations applied to substructures, possibly in parallel, and odot merges the results calculated for substructures, if (i) h can be specified by two certain forms of iterative programs, and (ii) merge can be implemented by a function of a certain polymorphic type. Therefore, when requirement (ii) is fulfilled, h has a divide-and-conquer implementation if h has two certain forms of implementations. We show that our approach is applicable to structure-consuming operations by catamorphisms (folds), structure-generating operations by anamorphisms (unfolds), and their generalizations called hylomorphisms.}},
  url = {https://doi.org/10.1145/2500365.2500580},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup756,
  title = {Stochastic optimization of floating-point programs with tunable precision}},
  author = {Schkufza, Eric and Sharma, Rahul and Aiken, Alex}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The aggressive optimization of floating-point computations is an important problem in high-performance computing. Unfortunately, floating-point instruction sets have complicated semantics that often force compilers to preserve programs as written. We present a method that treats floating-point optimization as a stochastic search problem. We demonstrate the ability to generate reduced precision implementations of Intel's handwritten C numeric library which are up to 6 times faster than the original code, and achieve end-to-end speedups of over 30\% on a direct numeric simulation and a ray tracer by optimizing kernels that can tolerate a loss of precision while still remaining correct. Because these optimizations are mostly not amenable to formal verification using the current state of the art, we present a stochastic search technique for characterizing maximum error. The technique comes with an asymptotic guarantee and provides strong evidence of correctness.}},
  url = {https://doi.org/10.1145/2594291.2594302},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup757,
  title = {A trusted mechanised JavaScript specification}},
  author = {Bodin, Martin and Chargueraud, Arthur and Filaretti, Daniele and Gardner, Philippa and Maffeis, Sergio and Naudziuniene, Daiva and Schmitt, Alan and Smith, Gareth}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {JavaScript is the most widely used web language for client-side applications. Whilst the development of JavaScript was initially just led by implementation, there is now increasing momentum behind the ECMA standardisation process. The time is ripe for a formal, mechanised specification of JavaScript, to clarify ambiguities in the ECMA standards, to serve as a trusted reference for high-level language compilation and JavaScript implementations, and to provide a platform for high-assurance proofs of language properties.We present JSCert, a formalisation of the current ECMA standard in the Coq proof assistant, and JSRef, a reference interpreter for JavaScript extracted from Coq to OCaml. We give a Coq proof that JSRef is correct with respect to JSCert and assess JSRef using test262, the ECMA conformance test suite. Our methodology ensures that JSCert is a comparatively accurate formulation of the English standard, which will only improve as time goes on. We have demonstrated that modern techniques of mechanised specification can handle the complexity of JavaScript.}},
  url = {https://doi.org/10.1145/2535838.2535876},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup758,
  title = {Colored local type inference}},
  author = {Odersky, Martin and Zenger, Christoph and Zenger, Matthias}},
  year = {2001}},
  journal = {Proceedings of the 28th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a type system for a language based on F≤, which allows certain type annotations to be elided in actual programs. Local type inference determines types by a combination of type propagation and local constraint solving, rather than by global constraint solving. We re ne the previously existing local type inference system of Pierce and Turner[PT98] by allowing partial type information to be propagated. This is expressed by coloring types to indicate propagation directions. Propagating partial type information allows us to omit type annotations for the visitor pattern, the analogue of pattern matching in languages without sum types.}},
  url = {https://doi.org/10.1145/360204.360207},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup759,
  title = {Temporal verification of higher-order functional programs}},
  author = {Murase, Akihiro and Terauchi, Tachio and Kobayashi, Naoki and Sato, Ryosuke and Unno, Hiroshi}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present an automated approach to verifying arbitrary omega-regular properties of higher-order functional programs. Previous automated methods proposed for this class of programs could only handle safety properties or termination, and our approach is the first to be able to verify arbitrary omega-regular liveness properties. Our approach is automata-theoretic, and extends our recent work on binary-reachability-based approach to automated termination verification of higher-order functional programs to fair termination published in ESOP 2014. In that work, we have shown that checking disjunctive well-foundedness of (the transitive closure of) the ``calling relation'' is sound and complete for termination. The extension to fair termination is tricky, however, because the straightforward extension that checks disjunctive well-foundedness of the fair calling relation turns out to be unsound, as we shall show in the paper. Roughly, our solution is to check fairness on the transition relation instead of the calling relation, and propagate the information to determine when it is necessary and sufficient to check for disjunctive well-foundedness on the calling relation. We prove that our approach is sound and complete. We have implemented a prototype of our approach, and confirmed that it is able to automatically verify liveness properties of some non-trivial higher-order programs.}},
  url = {https://doi.org/10.1145/2837614.2837667},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup760,
  title = {Refinement types for TypeScript}},
  author = {Vekris, Panagiotis and Cosman, Benjamin and Jhala, Ranjit}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present Refined TypeScript (RSC), a lightweight refinement type system for TypeScript, that enables static verification of higher-order, imperative programs. We develop a formal system for RSC that delineates the interaction between refinement types and mutability, and enables flow-sensitive reasoning by translating input programs to an equivalent intermediate SSA form. By establishing type safety for the intermediate form, we prove safety for the input programs. Next, we extend the core to account for imperative and dynamic features of TypeScript, including overloading, type reflection, ad hoc type hierarchies and object initialization. Finally, we evaluate RSC on a set of real-world benchmarks, including parts of the Octane benchmarks, D3, Transducers, and the TypeScript compiler. We show how RSC successfully establishes a number of value dependent properties, such as the safety of array accesses and downcasts, while incurring a modest overhead in type annotations and code restructuring.}},
  url = {https://doi.org/10.1145/2908080.2908110},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup761,
  title = {Toward general diagnosis of static errors}},
  author = {Zhang, Danfeng and Myers, Andrew C.}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We introduce a general way to locate programmer mistakes that are detected by static analyses such as type checking. The program analysis is expressed in a constraint language in which mistakes result in unsatisfiable constraints. Given an unsatisfiable system of constraints, both satisfiable and unsatisfiable constraints are analyzed, to identify the program expressions most likely to be the cause of unsatisfiability. The likelihood of different error explanations is evaluated under the assumption that the programmer's code is mostly correct, so the simplest explanations are chosen, following Bayesian principles. For analyses that rely on programmer-stated assumptions, the diagnosis also identifies assumptions likely to have been omitted. The new error diagnosis approach has been implemented for two very different program analyses: type inference in OCaml and information flow checking in Jif. The effectiveness of the approach is evaluated using previously collected programs containing errors. The results show that when compared to existing compilers and other tools, the general technique identifies the location of programmer errors significantly more accurately.}},
  url = {https://doi.org/10.1145/2535838.2535870},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup762,
  title = {Message safety in Dart}},
  author = {Ernst, Erik and M\o{}},
  year = {2015}},
  journal = {Proceedings of the 11th Symposium on Dynamic Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Unlike traditional static type checking, the type system in the Dart programming language is unsound by design, even for fully annotated programs. The rationale has been that this allows compile-time detection of likely errors and enables code completion in integrated development environments, without being restrictive on programmers. Despite unsoundness, judicious use of type annotations can ensure useful properties of the runtime behavior of Dart programs. We present a formal model of a core of Dart with a focus on its type system, which allows us to elucidate the causes of unsoundness. Our main contribution is a characterization of message-safe programs and a theorem stating that such programs will never encounter 'message not understood' errors at runtime. Message safety is less restrictive than traditional type soundness, and we argue that it forms a natural intermediate point between dynamically typed and statically typed Dart programs.}},
  url = {https://doi.org/10.1145/2816707.2816711},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup763,
  title = {Proof Spaces for Unbounded Parallelism}},
  author = {Farzan, Azadeh and Kincaid, Zachary and Podelski, Andreas}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this paper, we present a new approach to automatically verify multi-threaded programs which are executed by an unbounded number of threads running in parallel.The starting point for our work is the problem of how we can leverage existing automated verification technology for sequential programs (abstract interpretation, Craig interpolation, constraint solving, etc.) for multi-threaded programs. Suppose that we are given a correctness proof for a trace of a program (or for some other program fragment). We observe that the proof can always be decomposed into a finite set of Hoare triples, and we ask what can be proved from the finite set of Hoare triples using only simple combinatorial inference rules (without access to a theorem prover and without the possibility to infer genuinely new Hoare triples)?We introduce a proof system where one proves the correctness of a multi-threaded program by showing that for each trace of the program, there exists a correctness proof in the space of proofs that are derivable from a finite set of axioms using simple combinatorial inference rules. This proof system is complete with respect to the classical proof method of establishing an inductive invariant (which uses thread quantification and control predicates). Moreover, it is possible to algorithmically check whether a given set of axioms is sufficient to prove the correctness of a multi-threaded program, using ideas from well-structured transition systems.}},
  url = {https://doi.org/10.1145/2676726.2677012},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup764,
  title = {Modelling string folding with G2L grammars (poster)}},
  author = {Krasnogor, Natalio and Mart\'{\i}},
  year = {1997}},
  journal = {Proceedings of the Second ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {},
  url = {https://doi.org/10.1145/258948.258983},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup765,
  title = {Freeze after writing: quasi-deterministic parallel programming with LVars}},
  author = {Kuper, Lindsey and Turon, Aaron and Krishnaswami, Neelakantan R. and Newton, Ryan R.}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Deterministic-by-construction parallel programming models offer the advantages of parallel speedup while avoiding the nondeterministic, hard-to-reproduce bugs that plague fully concurrent code. A principled approach to deterministic-by-construction parallel programming with shared state is offered by LVars: shared memory locations whose semantics are defined in terms of an application-specific lattice. Writes to an LVar take the least upper bound of the old and new values with respect to the lattice, while reads from an LVar can observe only that its contents have crossed a specified threshold in the lattice. Although it guarantees determinism, this interface is quite limited.We extend LVars in two ways. First, we add the ability to "freeze" and then read the contents of an LVar directly. Second, we add the ability to attach event handlers to an LVar, triggering a callback when the LVar's value changes. Together, handlers and freezing enable an expressive and useful style of parallel programming. We prove that in a language where communication takes place through these extended LVars, programs are at worst quasi-deterministic: on every run, they either produce the same answer or raise an error. We demonstrate the viability of our approach by implementing a library for Haskell supporting a variety of LVar-based data structures, together with a case study that illustrates the programming model and yields promising parallel speedup.}},
  url = {https://doi.org/10.1145/2535838.2535842},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup766,
  title = {Challenges and progress toward efficient gradual typing (invited talk)}},
  author = {Siek, Jeremy}},
  year = {2017}},
  journal = {Proceedings of the 13th ACM SIGPLAN International Symposium on on Dynamic Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Mixing static and dynamic type checking in the same language is catching on, with the TypeScript and Flow variants of JavaScript, the MyPy and Reticulated variants of Python, the Strongtalk and Gradualtalk variants of Smalltalk, as well as Typed Racket, Typed Clojure, and Perl 6. The gradual typing approach to such mixing seeks to protect the statically typed code from the dynamically typed code, allowing compilers to leverage type information when optimizing the static code. Unfortunately, ensuring soundness requires runtime checking at the boundaries of typed and untyped code, and the cost of this checking can drown out the performance benefits of optimization. For example, in Typed Racket, some partially typed programs are 1000X slower than the untyped or fully typed version of the same program. But all is not lost! In this talk I present the results of ongoing research to tame the runtime overheads of gradual typing in the context of a prototype compiler, named Grift, that we are developing at Indiana University.}},
  url = {https://doi.org/10.1145/3133841.3148570},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup767,
  title = {Program Boosting: Program Synthesis via Crowd-Sourcing}},
  author = {Cochran, Robert A. and D'Antoni, Loris and Livshits, Benjamin and Molnar, David and Veanes, Margus}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this paper, we investigate an approach to program synthesis that is based on crowd-sourcing. With the help of crowd-sourcing, we aim to capture the "wisdom of the crowds" to find good if not perfect solutions to inherently tricky programming tasks, which elude even expert developers and lack an easy-to-formalize specification.We propose an approach we call program boosting, which involves crowd-sourcing imperfect solutions to a difficult programming problem from developers and then blending these programs together in a way that improves their correctness.We implement this approach in a system called CROWDBOOST and show in our experiments that interesting and highly non-trivial tasks such as writing regular expressions for URLs or email addresses can be effectively crowd-sourced. We demonstrate that carefully blending the crowd-sourced results together consistently produces a boost, yielding results that are better than any of the starting programs. Our experiments on 465 program pairs show consistent boosts in accuracy and demonstrate that program boosting can be performed at a relatively modest monetary cost.}},
  url = {https://doi.org/10.1145/2676726.2676973},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup768,
  title = {Using passive object garbage collection algorithms for garbage collection of active objects}},
  author = {Vardhan, Abhay and Agha, Gul}},
  year = {2002}},
  journal = {Proceedings of the 3rd International Symposium on Memory Management}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {With the increasing use of active object systems, agents and concurrent object oriented languages like Java, the problem of garbage collection (GC) of unused resources has become more complex. Since active objects are autonomous computational agents, unlike passive object systems the criterion for identifying garbage in active objects cannot be based solely on reachability from a root set. This has led to development of specialized algorithms for GC of active objects. We reduce the problem of GC of active objects to that of passive objects by providing a transformation of the active object reference graph to a passive object reference graph so that if a garbage collector for a passive object system is applied to the transformed graph, precisely those objects are collected which correspond to garbage objects in the original active object reference graph. The transformation technique enables us to reuse the algorithms already developed for passive objects systems. We provide a proof of correctness of the transformation and discuss its cost. An advantage of the transformation is that it can prove valuable for mixed systems of active and passive objects by providing a common approach to GC.}},
  url = {https://doi.org/10.1145/512429.512443},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup769,
  title = {Efficient and precise points-to analysis: modeling the heap by merging equivalent automata}},
  author = {Tan, Tian and Li, Yue and Xue, Jingling}},
  year = {2017}},
  journal = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Mainstream points-to analysis techniques for object-oriented languages rely predominantly on the allocation-site abstraction to model heap objects. We present MAHJONG, a novel heap abstraction that is specifically developed to address the needs of an important class of type-dependent clients, such as call graph construction, devirtualization and may-fail casting. By merging equivalent automata representing type-consistent objects that are created by the allocation-site abstraction, MAHJONG enables an allocation-site-based points-to analysis to run significantly faster while achieving nearly the same precision for type-dependent clients. MAHJONG is simple conceptually, efficient, and drops easily on any allocation-site-based points-to analysis. We demonstrate its effectiveness by discussing some insights on why it is a better alternative of the allocation-site abstraction for type-dependent clients and evaluating it extensively on 12 large real-world Java programs with five context-sensitive points-to analyses and three widely used type-dependent clients. MAHJONG is expected to provide significant benefits for many program analyses where call graphs are required.}},
  url = {https://doi.org/10.1145/3062341.3062360},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup770,
  title = {Lem: reusable engineering of real-world semantics}},
  author = {Mulligan, Dominic P. and Owens, Scott and Gray, Kathryn E. and Ridge, Tom and Sewell, Peter}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Recent years have seen remarkable successes in rigorous engineering: using mathematically rigorous semantic models (not just idealised calculi) of real-world processors, programming languages, protocols, and security mechanisms, for testing, proof, analysis, and design. Building these models is challenging, requiring experimentation, dialogue with vendors or standards bodies, and validation; their scale adds engineering issues akin to those of programming to the task of writing clear and usable mathematics. But language and tool support for specification is lacking. Proof assistants can be used but bring their own difficulties, and a model produced in one, perhaps requiring many person-years effort and maintained over an extended period, cannot be used by those familiar with another.We introduce Lem, a language for engineering reusable large-scale semantic models. The Lem design takes inspiration both from functional programming languages and from proof assistants, and Lem definitions are translatable into OCaml for testing, Coq, HOL4, and Isabelle/HOL for proof, and LaTeX and HTML for presentation. This requires a delicate balance of expressiveness, careful library design, and implementation of transformations - akin to compilation, but subject to the constraint of producing usable and human-readable code for each target. Lem's effectiveness is demonstrated by its use in practice.}},
  url = {https://doi.org/10.1145/2628136.2628143},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup771,
  title = {Principal type inference for GADTs}},
  author = {Chen, Sheng and Erwig, Martin}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a new method for GADT type inference that improves the precision of previous approaches. In particular, our approach accepts more type-correct programs than previous approaches when they do not employ type annotations. A side benefit of our approach is that it can detect a wide range of runtime errors that are missed by previous approaches. Our method is based on the idea to represent type refinements in pattern-matching branches by choice types, which facilitate a separation of the typing and reconciliation phases and thus support case expressions. This idea is formalized in a type system, which is both sound and a conservative extension of the classical Hindley-Milner system. We present the results of an empirical evaluation that compares our algorithm with previous approaches.}},
  url = {https://doi.org/10.1145/2837614.2837665},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup772,
  title = {Parametric higher-order abstract syntax for mechanized semantics}},
  author = {Chlipala, Adam}},
  year = {2008}},
  journal = {Proceedings of the 13th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present parametric higher-order abstract syntax (PHOAS), a new approach to formalizing the syntax of programming languages in computer proof assistants based on type theory. Like higher-order abstract syntax (HOAS), PHOAS uses the meta language's binding constructs to represent the object language's binding constructs. Unlike HOAS, PHOAS types are definable in general-purpose type theories that support traditional functional programming, like Coq's Calculus of Inductive Constructions. We walk through how Coq can be used to develop certified, executable program transformations over several statically-typed functional programming languages formalized with PHOAS; that is, each transformation has a machine-checked proof of type preservation and semantic preservation. Our examples include CPS translation and closure conversion for simply-typed lambda calculus, CPS translation for System F, and translation from a language with ML-style pattern matching to a simpler language with no variable-arity binding constructs. By avoiding the syntactic hassle associated with first-order representation techniques, we achieve a very high degree of proof automation.}},
  url = {https://doi.org/10.1145/1411204.1411226},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup773,
  title = {Ogre and Pythia: an invariance proof method for weak consistency models}},
  author = {Alglave, Jade and Cousot, Patrick}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We design an invariance proof method for concurrent programs parameterised by a weak consistency model. The calculational design of the invariance proof method is by abstract interpretation of a truly parallel analytic semantics. This generalises the methods by Lamport and Owicki-Gries for sequential consistency. We use cat as an example of language to write consistency specifications of both concurrent programs and machine architectures.}},
  url = {https://doi.org/10.1145/3009837.3009883},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup774,
  title = {Evaluating value-graph translation validation for LLVM}},
  author = {Tristan, Jean-Baptiste and Govereau, Paul and Morrisett, Greg}},
  year = {2011}},
  journal = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Translation validators are static analyzers that attempt to verify that program transformations preserve semantics. Normalizing translation validators do so by trying to match the value-graphs of an original function and its transformed counterpart. In this paper, we present the design of such a validator for LLVM's intra-procedural optimizations, a design that does not require any instrumentation of the optimizer, nor any rewriting of the source code to compile, and needs to run only once to validate a pipeline of optimizations. We present the results of our preliminary experiments on a set of benchmarks that include GCC, a perl interpreter, SQLite3, and other C programs.}},
  url = {https://doi.org/10.1145/1993498.1993533},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup775,
  title = {A sound and complete axiomatization of delimited continuations}},
  author = {Kameyama, Yukiyoshi and Hasegawa, Masahito}},
  year = {2003}},
  journal = {Proceedings of the Eighth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The shift and reset operators, proposed by Danvy and Filinski, are powerful control primitives for capturing delimited continuations. Delimited continuation is a similar concept as the standard (unlimited) continuation, but it represents part of the rest of the computation, rather than the whole rest of computation. In the literature, the semantics of shift and reset has been given by a CPS-translation only. This paper gives a direct axiomatization of calculus with shift and reset, namely, we introduce a set of equations, and prove that it is sound and complete with respect to the CPS-translation. We also introduce a calculus with control operators which is as expressive as the calculus with shift and reset, has a sound and complete axiomatization, and is conservative over Sabry and Felleisen's theory for first-class continuations.}},
  url = {https://doi.org/10.1145/944705.944722},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup776,
  title = {Parallelizing dynamic programming through rank convergence}},
  author = {Maleki, Saeed and Musuvathi, Madanlal and Mytkowicz, Todd}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper proposes an efficient parallel algorithm for an important class of dynamic programming problems that includes Viterbi, Needleman-Wunsch, Smith-Waterman, and Longest Common Subsequence. In dynamic programming, the subproblems that do not depend on each other, and thus can be computed in parallel, form stages or wavefronts. The algorithm presented in this paper provides additional parallelism allowing multiple stages to be computed in parallel despite dependences among them. The correctness and the performance of the algorithm relies on rank convergence properties of matrix multiplication in the tropical semiring, formed with plus as the multiplicative operation and max as the additive operation.This paper demonstrates the efficiency of the parallel algorithm by showing significant speed ups on a variety of important dynamic programming problems. In particular, the parallel Viterbi decoder is up-to 24x faster (with 64 processors) than a highly optimized commercial baseline.}},
  url = {https://doi.org/10.1145/2555243.2555264},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup777,
  title = {Conditionally correct superoptimization}},
  author = {Sharma, Rahul and Schkufza, Eric and Churchill, Berkeley and Aiken, Alex}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The aggressive optimization of heavily used kernels is an important problem in high-performance computing. However, both general purpose compilers and highly specialized tools such as superoptimizers often do not have sufficient static knowledge of restrictions on program inputs that could be exploited to produce the very best code. For many applications, the best possible code is conditionally correct: the optimized kernel is equal to the code that it replaces only under certain preconditions on the kernel's inputs. The main technical challenge in producing conditionally correct optimizations is in obtaining non-trivial and useful conditions and proving conditional equivalence formally in the presence of loops. We combine abstract interpretation, decision procedures, and testing to yield a verification strategy that can address both of these problems. This approach yields a superoptimizer for x86 that in our experiments produces binaries that are often multiple times faster than those produced by production compilers.}},
  url = {https://doi.org/10.1145/2814270.2814278},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup778,
  title = {Towards automatic resource bound analysis for OCaml}},
  author = {Hoffmann, Jan and Das, Ankush and Weng, Shu-Chun}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This article presents a resource analysis system for OCaml programs. The system automatically derives worst-case resource bounds for higher-order polymorphic programs with user-defined inductive types. The technique is parametric in the resource and can derive bounds for time, memory allocations and energy usage. The derived bounds are multivariate resource polynomials which are functions of different size parameters that depend on the standard OCaml types. Bound inference is fully automatic and reduced to a linear optimization problem that is passed to an off-the-shelf LP solver. Technically, the analysis system is based on a novel multivariate automatic amortized resource analysis (AARA). It builds on existing work on linear AARA for higher-order programs with user-defined inductive types and on multivariate AARA for first-order programs with built-in lists and binary trees. This is the first amortized analysis, that automatically derives polynomial bounds for higher-order functions and polynomial bounds that depend on user-defined inductive types. Moreover, the analysis handles a limited form of side effects and even outperforms the linear bound inference of previous systems. At the same time, it preserves the expressivity and efficiency of existing AARA techniques. The practicality of the analysis system is demonstrated with an implementation and integration with Inria's OCaml compiler. The implementation is used to automatically derive resource bounds for 411 functions and 6018 lines of code derived from OCaml libraries, the CompCert compiler, and implementations of textbook algorithms. In a case study, the system infers bounds on the number of queries that are sent by OCaml programs to DynamoDB, a commercial NoSQL cloud database service.}},
  url = {https://doi.org/10.1145/3009837.3009842},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup779,
  title = {Regenerate: a language generator for extended regular expressions}},
  author = {Radanne, Gabriel and Thiemann, Peter}},
  year = {2018}},
  journal = {Proceedings of the 17th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Regular expressions are part of every programmer’s toolbox. They are used for a wide variety of language-related tasks and there are many algorithms for manipulating them. In particular, matching algorithms that detect whether a word belongs to the language described by a regular expression are well explored, yet new algorithms appear frequently. However, there is no satisfactory methodology for testing such matchers. We propose a testing methodology which is based on generating positive as well as negative examples of words in the language. To this end, we present a new algorithm to generate the language described by a generalized regular expression with intersection and complement operators. The complement operator allows us to generate both positive and negative example words from a given regular expression. We implement our generator in Haskell and OCaml and show that its performance is more than adequate for testing.}},
  url = {https://doi.org/10.1145/3278122.3278133},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup780,
  title = {Staged parser combinators for efficient data processing}},
  author = {Jonnalagedda, Manohar and Coppey, Thierry and Stucki, Sandro and Rompf, Tiark and Odersky, Martin}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \& Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Parsers are ubiquitous in computing, and many applications depend on their performance for decoding data efficiently. Parser combinators are an intuitive tool for writing parsers: tight integration with the host language enables grammar specifications to be interleaved with processing of parse results. Unfortunately, parser combinators are typically slow due to the high overhead of the host language abstraction mechanisms that enable composition.We present a technique for eliminating such overhead. We use staging, a form of runtime code generation, to dissociate input parsing from parser composition, and eliminate intermediate data structures and computations associated with parser composition at staging time. A key challenge is to maintain support for input dependent grammars, which have no clear stage distinction.Our approach applies to top-down recursive-descent parsers as well as bottom-up non-deterministic parsers with key applications in dynamic programming on sequences, where we auto-generate code for parallel hardware. We achieve performance comparable to specialized, hand-written parsers.}},
  url = {https://doi.org/10.1145/2660193.2660241},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup781,
  title = {A rely-guarantee-based simulation for verifying concurrent program transformations}},
  author = {Liang, Hongjin and Feng, Xinyu and Fu, Ming}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Verifying program transformations usually requires proving that the resulting program (the target) refines or is equivalent to the original one (the source). However, the refinement relation between individual sequential threads cannot be preserved in general with the presence of parallel compositions, due to instruction reordering and the different granularities of atomic operations at the source and the target. On the other hand, the refinement relation defined based on fully abstract semantics of concurrent programs assumes arbitrary parallel environments, which is too strong and cannot be satisfied by many well-known transformations. In this paper, we propose a Rely-Guarantee-based Simulation (RGSim) to verify concurrent program transformations. The relation is parametrized with constraints of the environments that the source and the target programs may compose with. It considers the interference between threads and their environments, thus is less permissive than relations over sequential programs. It is compositional w.r.t. parallel compositions as long as the constraints are satisfied. Also, RGSim does not require semantics preservation under all environments, and can incorporate the assumptions about environments made by specific program transformations in the form of rely/guarantee conditions. We use RGSim to reason about optimizations and prove atomicity of concurrent objects. We also propose a general garbage collector verification framework based on RGSim, and verify the Boehm et al. concurrent mark-sweep GC.}},
  url = {https://doi.org/10.1145/2103656.2103711},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup782,
  title = {Thresher: precise refutations for heap reachability}},
  author = {Blackshear, Sam and Chang, Bor-Yuh Evan and Sridharan, Manu}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a precise, path-sensitive static analysis for reasoning about heap reachability, that is, whether an object can be reached from another variable or object via pointer dereferences. Precise reachability information is useful for a number of clients, including static detection of a class of Android memory leaks. For this client, we found the heap reachability information computed by a state-of-the-art points-to analysis was too imprecise, leading to numerous false-positive leak reports. Our analysis combines a symbolic execution capable of path-sensitivity and strong updates with abstract heap information computed by an initial flow-insensitive points-to analysis. This novel mixed representation allows us to achieve both precision and scalability by leveraging the pre-computed points-to facts to guide execution and prune infeasible paths. We have evaluated our techniques in the Thresher tool, which we used to find several developer-confirmed leaks in Android applications.}},
  url = {https://doi.org/10.1145/2491956.2462186},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup783,
  title = {Dynamic deadlock verification for general barrier synchronisation}},
  author = {Cogumbreiro, Tiago and Hu, Raymond and Martins, Francisco and Yoshida, Nobuko}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present Armus, a dynamic verification tool for deadlock detection and avoidance specialised in barrier synchronisation. Barriers are used to coordinate the execution of groups of tasks, and serve as a building block of parallel computing. Our tool verifies more barrier synchronisation patterns than current state-of-the-art. To improve the scalability of verification, we introduce a novel event-based representation of concurrency constraints, and a graph-based technique for deadlock analysis. The implementation is distributed and fault-tolerant, and can verify X10 and Java programs. To formalise the notion of barrier deadlock, we introduce a core language expressive enough to represent the three most widespread barrier synchronisation patterns: group, split-phase, and dynamic membership. We propose a graph analysis technique that selects from two alternative graph representations: the Wait-For Graph, that favours programs with more tasks than barriers; and the State Graph, optimised for programs with more barriers than tasks. We prove that finding a deadlock in either representation is equivalent, and that the verification algorithm is sound and complete with respect to the notion of deadlock in our core language. Armus is evaluated with three benchmark suites in local and distributed scenarios. The benchmarks show that graph analysis with automatic graph-representation selection can record a 7-fold execution increase versus the traditional fixed graph representation. The performance measurements for distributed deadlock detection between 64 processes show negligible overheads.}},
  url = {https://doi.org/10.1145/2688500.2688519},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup784,
  title = {Example-directed synthesis: a type-theoretic interpretation}},
  author = {Frankle, Jonathan and Osera, Peter-Michael and Walker, David and Zdancewic, Steve}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Input-output examples have emerged as a practical and user-friendly specification mechanism for program synthesis in many environments. While example-driven tools have demonstrated tangible impact that has inspired adoption in industry, their underlying semantics are less well-understood: what are "examples" and how do they relate to other kinds of specifications? This paper demonstrates that examples can, in general, be interpreted as refinement types. Seen in this light, program synthesis is the task of finding an inhabitant of such a type. This insight provides an immediate semantic interpretation for examples. Moreover, it enables us to exploit decades of research in type theory as well as its correspondence with intuitionistic logic rather than designing ad hoc theoretical frameworks for synthesis from scratch. We put this observation into practice by formalizing synthesis as proof search in a sequent calculus with intersection and union refinements that we prove to be sound with respect to a conventional type system. In addition, we show how to handle negative examples, which arise from user feedback or counterexample-guided loops. This theory serves as the basis for a prototype implementation that extends our core language to support ML-style algebraic data types and structurally inductive functions. Users can also specify synthesis goals using polymorphic refinements and import monomorphic libraries. The prototype serves as a vehicle for empirically evaluating a number of different strategies for resolving the nondeterminism of the sequent calculus---bottom-up theorem-proving, term enumeration with refinement type checking, and combinations of both---the results of which classify, explain, and validate the design choices of existing synthesis systems. It also provides a platform for measuring the practical value of a specification language that combines "examples" with the more general expressiveness of refinements.}},
  url = {https://doi.org/10.1145/2837614.2837629},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup785,
  title = {Rely-guarantee references for refinement types over aliased mutable data}},
  author = {Gordon, Colin S. and Ernst, Michael D. and Grossman, Dan}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Reasoning about side effects and aliasing is the heart of verifying imperative programs. Unrestricted side effects through one reference can invalidate assumptions about an alias. We present a new type system approach to reasoning about safe assumptions in the presence of aliasing and side effects, unifying ideas from reference immutability type systems and rely-guarantee program logics. Our approach, rely-guarantee references, treats multiple references to shared objects similarly to multiple threads in rely-guarantee program logics. We propose statically associating rely and guarantee conditions with individual references to shared objects. Multiple aliases to a given object may coexist only if the guarantee condition of each alias implies the rely condition for all other aliases. We demonstrate that existing reference immutability type systems are special cases of rely-guarantee references.In addition to allowing precise control over state modification, rely-guarantee references allow types to depend on mutable data while still permitting flexible aliasing. Dependent types whose denotation is stable over the actions of the rely and guarantee conditions for a reference and its data will not be invalidated by any action through any alias. We demonstrate this with refinement (subset) types that may depend on mutable data. As a special case, we derive the first reference immutability type system with dependent types over immutable data.We show soundness for our approach and describe experience using rely-guarantee references in a dependently-typed monadic DSL in Coq.}},
  url = {https://doi.org/10.1145/2491956.2462160},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup786,
  title = {Effective interactive proofs for higher-order imperative programs}},
  author = {Chlipala, Adam and Malecha, Gregory and Morrisett, Greg and Shinnar, Avraham and Wisnesky, Ryan}},
  year = {2009}},
  journal = {Proceedings of the 14th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a new approach for constructing and verifying higher-order, imperative programs using the Coq proof assistant. We build on the past work on the Ynot system, which is based on Hoare Type Theory. That original system was a proof of concept, where every program verification was accomplished via laborious manual proofs, with much code devoted to uninteresting low-level details. In this paper, we present a re-implementation of Ynot which makes it possible to implement fully-verified, higher-order imperative programs with reasonable proof burden. At the same time, our new system is implemented entirely in Coq source files, showcasing the versatility of that proof assistant as a platform for research on language design and verification. Both versions of the system have been evaluated with case studies in the verification of imperative data structures, such as hash tables with higher-order iterators. The verification burden in our new system is reduced by at least an order of magnitude compared to the old system, by replacing manual proof with automation. The core of the automation is a simplification procedure for implications in higher-order separation logic, with hooks that allow programmers to add domain-specific simplification rules.We argue for the effectiveness of our infrastructure by verifying a number of data structures and a packrat parser, and we compare to similar efforts within other projects. Compared to competing approaches to data structure verification, our system includes much less code that must be trusted; namely, about a hundred lines of Coq code defining a program logic. All of our theorems and decision procedures have or build machine-checkable correctness proofs from first principles, removing opportunities for tool bugs to create faulty verifications.}},
  url = {https://doi.org/10.1145/1596550.1596565},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup787,
  title = {Data-Parallel String-Manipulating Programs}},
  author = {Veanes, Margus and Mytkowicz, Todd and Molnar, David and Livshits, Benjamin}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {String-manipulating programs are an important class of programs with applications in malware detection, graphics, input sanitization for Web security, and large-scale HTML processing. This paper extends prior work on BEK, an expressive domain-specific language for writing string-manipulating programs, with algorithmic insights that make BEK both analyzable and data-parallel. By analyzable we mean that unlike most general purpose programming languages, many algebraic properties of a BEK program are decidable (i.e., one can check whether two programs commute or compute the inverse of a program). By data-parallel we mean that a BEK program can compute on arbitrary subsections of its input in parallel, thus exploiting parallel hardware. This latter requirement is particularly important for programs which operate on large data: without data parallelism, a programmer cannot hide the latency of reading data from various storage media (i.e., reading a terabyte of data from a modern hard drive takes about 3 hours). With a data-parallel approach, the system can split data across multiple disks and thus hide the latency of reading the data.A BEK program is expressive: a programmer can use conditionals, switch statements, and registers--or local variables--in order to implement common string-manipulating programs. Unfortunately, this expressivity induces data dependencies, which are an obstacle to parallelism. The key contribution of this paper is an algorithm which automatically removes these data dependencies by mapping a B EK program into a intermediate format consisting of symbolic transducers, which extend classical transducers with symbolic predicates and symbolic assignments. We present a novel algorithm that we call exploration which performs symbolic loop unrolling of these transducers to obtain simplified versions of the original program. We show how these simplified versions can then be lifted to a stateless form, and from there compiled to data-parallel hardware.To evaluate the efficacy of our approach, we demonstrate up to 8x speedups for a number of real-world, BEK programs, (e.g., HTML encoder and decoder) on data-parallel hardware. To the best of our knowledge, these are the first data parallel implementation of these programs. To validate that our approach is correct, we use an automatic testing technique to compare our generated code to the original implementations and find no semantic deviations.}},
  url = {https://doi.org/10.1145/2676726.2677014},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup788,
  title = {Combining static analysis with probabilistic models to enable market-scale Android inter-component analysis}},
  author = {Octeau, Damien and Jha, Somesh and Dering, Matthew and McDaniel, Patrick and Bartel, Alexandre and Li, Li and Klein, Jacques and Le Traon, Yves}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Static analysis has been successfully used in many areas, from verifying mission-critical software to malware detection. Unfortunately, static analysis often produces false positives, which require significant manual effort to resolve. In this paper, we show how to overlay a probabilistic model, trained using domain knowledge, on top of static analysis results, in order to triage static analysis results. We apply this idea to analyzing mobile applications. Android application components can communicate with each other, both within single applications and between different applications. Unfortunately, techniques to statically infer Inter-Component Communication (ICC) yield many potential inter-component and inter-application links, most of which are false positives. At large scales, scrutinizing all potential links is simply not feasible. We therefore overlay a probabilistic model of ICC on top of static analysis results. Since computing the inter-component links is a prerequisite to inter-component analysis, we introduce a formalism for inferring ICC links based on set constraints. We design an efficient algorithm for performing link resolution. We compute all potential links in a corpus of 11,267 applications in 30 minutes and triage them using our probabilistic approach. We find that over 95.1\% of all 636 million potential links are associated with probability values below 0.01 and are thus likely unfeasible links. Thus, it is possible to consider only a small subset of all links without significant loss of information. This work is the first significant step in making static inter-application analysis more tractable, even at large scales.}},
  url = {https://doi.org/10.1145/2837614.2837661},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup789,
  title = {Communicating quantum processes}},
  author = {Gay, Simon J. and Nagarajan, Rajagopal}},
  year = {2005}},
  journal = {Proceedings of the 32nd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We define a language CQP (Communicating Quantum Processes) for modelling systems which combine quantum and classical communication and computation. CQP combines the communication primitives of the pi-calculus with primitives for measurement and transformation of quantum state; in particular, quantum bits (qubits) can be transmitted from process to process along communication channels. CQP has a static type system which classifies channels, distinguishes between quantum and classical data, and controls the use of quantum state. We formally define the syntax, operational semantics and type system of CQP, prove that the semantics preserves typing, and prove that typing guarantees that each qubit is owned by a unique process within a system. We illustrate CQP by defining models of several quantum communication systems, and outline our plans for using CQP as the foundation for formal analysis and verification of combined quantum and classical systems.}},
  url = {https://doi.org/10.1145/1040305.1040318},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup790,
  title = {Migrating relational data to an ODBMS: strategics and lessons from a molecular biology experience}},
  author = {Oler, Jon and Lindstrom, Gary and Critchlow, Terence}},
  year = {1997}},
  journal = {Proceedings of the 12th ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The growing maturity of ODBMS technology is causing many enterprises to consider migrating relational databases to ODBMS's. While data remapping is relatively straightforward in most cases, greater challenges lie in economically and non-invasively adapting legacy application software. We report on a genetics laboratory database migration experiment, which was facilitated by both organization of the relational data in object-like form and a C++ framework designed to insulate application code from relational artifacts. Although this experiment was largely successful, we discovered to our surprise that the framework failed to encapsulate three subtle aspects of the relational implementation, thereby "contaminating" application code. We analyze the underlying issues, and offer cautionary guidance to future migrators.}},
  url = {https://doi.org/10.1145/263698.263741},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup791,
  title = {A play on regular expressions: functional pearl}},
  author = {Fischer, Sebastian and Huch, Frank and Wilke, Thomas}},
  year = {2010}},
  journal = {Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Cody, Hazel, and Theo, two experienced Haskell programmers and an expert in automata theory, develop an elegant Haskell program for matching regular expressions: (i) the program is purely functional; (ii) it is overloaded over arbitrary semirings, which not only allows to solve the ordinary matching problem but also supports other applications like computing leftmost longest matchings or the number of matchings, all with a single algorithm; (iii) it is more powerful than other matchers, as it can be used for parsing every context-free language by taking advantage of laziness.The developed program is based on an old technique to turn regular expressions into finite automata which makes it efficient both in terms of worst-case time and space bounds and actual performance: despite its simplicity, the Haskell implementation can compete with a recently published professional C++ program for the same problem.}},
  url = {https://doi.org/10.1145/1863543.1863594},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup792,
  title = {Verification of producer-consumer synchronization in GPU programs}},
  author = {Sharma, Rahul and Bauer, Michael and Aiken, Alex}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Previous efforts to formally verify code written for GPUs have focused solely on kernels written within the traditional data-parallel GPU programming model. No previous work has considered the higher performance, but more complex, warp-specialized kernels based on producer-consumer named barriers available on current hardware. In this work we present the first formal operational semantics for named barriers and define what it means for a warp-specialized kernel to be correct. We give algorithms for verifying the correctness of warp-specialized kernels and prove that they are both sound and complete for the most common class of warp-specialized programs. We also present WEFT, a verification tool for checking warp-specialized code. Using WEFT, we discover several non-trivial bugs in production warp-specialized kernels.}},
  url = {https://doi.org/10.1145/2737924.2737962},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup793,
  title = {Hyperstream processing systems: nonstandard modeling of continuous-time signals}},
  author = {Suenaga, Kohei and Sekine, Hiroyoshi and Hasuo, Ichiro}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We exploit the apparent similarity between (discrete-time) stream processing and (continuous-time) signal processing and transfer a deductive verification framework from the former to the latter. Our development is based on rigorous semantics that relies on nonstandard analysis (NSA).Specifically, we start with a discrete framework consisting of a Lustre-like stream processing language, its Kahn-style fixed point semantics, and a program logic (in the form of a type system) for partial correctness guarantees. This stream framework is transferred as it is to one for hyperstreams---streams of streams, that typically arise from sampling (continuous-time) signals with progressively smaller intervals---via the logical infrastructure of NSA. Under a certain continuity assumption we identify hyperstreams with signals; our final outcome thus obtained is a deductive verification framework of signals. In it one verifies properties of signals using the (conventionally discrete) proof principles, like fixed point induction.}},
  url = {https://doi.org/10.1145/2429069.2429120},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup794,
  title = {An operational and axiomatic semantics for non-determinism and sequence points in C}},
  author = {Krebbers, Robbert}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The C11 standard of the C programming language does not specify the execution order of expressions. Besides, to make more effective optimizations possible (eg. delaying of side-effects and interleaving), it gives compilers in certain cases the freedom to use even more behaviors than just those of all execution orders.Widely used C compilers actually exploit this freedom given by the C standard for optimizations, so it should be taken seriously in formal verification. This paper presents an operational and axiomatic semantics (based on separation logic) for non-determinism and sequence points in C. We prove soundness of our axiomatic semantics with respect to our operational semantics. This proof has been fully formalized using the Coq proof assistant.}},
  url = {https://doi.org/10.1145/2535838.2535878},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup795,
  title = {Type-base flow analysis: from polymorphic subtyping to CFL-reachability}},
  author = {Rehof, Jakob and F\"{a}},
  year = {2001}},
  journal = {Proceedings of the 28th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a novel approach to scalable implementation of type-based flow analysis with polymorphic subtyping. Using a new presentation of polymorphic subytping with instantiation constraints, we are able to apply context-free language (CFL) reachability techniques to type-based flow analysis. We develop a CFL-based algorithm for computing flow-information in time O(n³), where n is the size of the typed program. The algorithm substantially improves upon the best previously known algorithm for flow analysis based on polymorphic subtyping with complexity O(n8). Our technique also yields the first demand-driven algorithm for polymorphic subtype-based flow-computation. It works directly on higher-order programs with structured data of finite type (unbounded data structures are incorporated via finite approximations), supports context-sensitive, global flow summariztion and includes polymorphic recursion.}},
  url = {https://doi.org/10.1145/360204.360208},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup796,
  title = {Wellfounded recursion with copatterns: a unified approach to termination and productivity}},
  author = {Abel, Andreas M. and Pientka, Brigitte}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this paper, we study strong normalization of a core language based on System F-omega which supports programming with finite and infinite structures. Building on our prior work, finite data such as finite lists and trees are defined via constructors and manipulated via pattern matching, while infinite data such as streams and infinite trees is defined by observations and synthesized via copattern matching. In this work, we take a type-based approach to strong normalization by tracking size information about finite and infinite data in the type. This guarantees compositionality. More importantly, the duality of pattern and copatterns provide a unifying semantic concept which allows us for the first time to elegantly and uniformly support both well-founded induction and coinduction by mere rewriting. The strong normalization proof is structured around Girard's reducibility candidates. As such our system allows for non-determinism and does not rely on coverage. Since System F-omega is general enough that it can be the target of compilation for the Calculus of Constructions, this work is a significant step towards representing observation-centric infinite data in proof assistants such as Coq and Agda.}},
  url = {https://doi.org/10.1145/2500365.2500591},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup797,
  title = {Computing repair alternatives for malformed programs using constraint attribute grammars}},
  author = {Steimann, Friedrich and Hagemann, J\"{o}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Attribute grammars decorate the nodes of a program's parse tree with attributes whose values are defined by equations encoding the (static) semantics of a programming language. We show how replacing the equations of an attribute grammar with equivalent constraints that can be solved by a constraint solver allows us to compute repairs of a malformed program solely from a specification that was originally designed for checking its well-formedness. We present two repair modes --- shallow and deep fixing --- whose computed repair alternatives are guaranteed to repair every error on which they are invoked. While shallow fixing may introduce new errors, deep fixing never does; to make it tractable, we implement it using neighborhood search. We demonstrate the feasibility of our approach by implementing it on top of ExtendJ, an attribute grammar based Java compiler, and by applying it to an example from the Java EE context, detecting and fixing well-formedness errors (both real and injected) in a body of 14 open-source subject programs.}},
  url = {https://doi.org/10.1145/2983990.2984007},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup798,
  title = {Dynamic partial order reduction for relaxed memory models}},
  author = {Zhang, Naling and Kusano, Markus and Wang, Chao}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Under a relaxed memory model such as TSO or PSO, a concurrent program running on a shared-memory multiprocessor may observe two types of nondeterminism: the nondeterminism in thread scheduling and the nondeterminism in store buffering. Although there is a large body of work on mitigating the scheduling nondeterminism during runtime verification, methods for soundly mitigating the store buffering nondeterminism are lacking. We propose a new dynamic partial order reduction (POR) algorithm for verifying concurrent programs under TSO and PSO. Our method relies on modeling both types of nondeterminism in a unified framework, which allows us to extend existing POR techniques to TSO and PSO without overhauling the verification algorithm. In addition to sound POR, we also propose a buffer-bounding method for more aggressively reducing the state space. We have implemented our new methods in a stateless model checking tool and demonstrated their effectiveness on a set of multithreaded C benchmarks.}},
  url = {https://doi.org/10.1145/2737924.2737956},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup799,
  title = {Bias-variance tradeoffs in program analysis}},
  author = {Sharma, Rahul and Nori, Aditya V. and Aiken, Alex}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {It is often the case that increasing the precision of a program analysis leads to worse results. It is our thesis that this phenomenon is the result of fundamental limits on the ability to use precise abstract domains as the basis for inferring strong invariants of programs. We show that bias-variance tradeoffs, an idea from learning theory, can be used to explain why more precise abstractions do not necessarily lead to better results and also provides practical techniques for coping with such limitations. Learning theory captures precision using a combinatorial quantity called the VC dimension. We compute the VC dimension for different abstractions and report on its usefulness as a precision metric for program analyses. We evaluate cross validation, a technique for addressing bias-variance tradeoffs, on an industrial strength program verification tool called YOGI. The tool produced using cross validation has significantly better running time, finds new defects, and has fewer time-outs than the current production version. Finally, we make some recommendations for tackling bias-variance tradeoffs in program analysis.}},
  url = {https://doi.org/10.1145/2535838.2535853},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup800,
  title = {On verifying causal consistency}},
  author = {Bouajjani, Ahmed and Enea, Constantin and Guerraoui, Rachid and Hamza, Jad}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Causal consistency is one of the most adopted consistency criteria for distributed implementations of data structures. It ensures that operations are executed at all sites according to their causal precedence. We address the issue of verifying automatically whether the executions of an implementation of a data structure are causally consistent. We consider two problems: (1) checking whether one single execution is causally consistent, which is relevant for developing testing and bug finding algorithms, and (2) verifying whether all the executions of an implementation are causally consistent. We show that the first problem is NP-complete. This holds even for the read-write memory abstraction, which is a building block of many modern distributed systems. Indeed, such systems often store data in key-value stores, which are instances of the read-write memory abstraction. Moreover, we prove that, surprisingly, the second problem is undecidable, and again this holds even for the read-write memory abstraction. However, we show that for the read-write memory abstraction, these negative results can be circumvented if the implementations are data independent, i.e., their behaviors do not depend on the data values that are written or read at each moment, which is a realistic assumption. We prove that for data independent implementations, the problem of checking the correctness of a single execution w.r.t. the read-write memory abstraction is polynomial time. Furthermore, we show that for such implementations the set of non-causally consistent executions can be represented by means of a finite number of register automata. Using these machines as observers (in parallel with the implementation) allows to reduce polynomially the problem of checking causal consistency to a state reachability problem. This reduction holds regardless of the class of programs used for the implementation, of the number of read-write variables, and of the used data domain. It allows leveraging existing techniques for assertion/reachability checking to causal consistency verification. Moreover, for a significant class of implementations, we derive from this reduction the decidability of verifying causal consistency w.r.t. the read-write memory abstraction.}},
  url = {https://doi.org/10.1145/3009837.3009888},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup801,
  title = {A meta-EDSL for distributed web applications}},
  author = {Ekblad, Anton}},
  year = {2017}},
  journal = {Proceedings of the 10th ACM SIGPLAN International Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a domain-specific language for constructing and configuring web applications distributed across any number of networked, heterogeneous systems. Our language is embedded in Haskell, provides a common framework for integrating components written in third-party EDSLs, and enables type-safe, access-controlled communication between nodes, as well as effortless sharing and movement of functionality between application components. We give an implementation of our language and demonstrate its applicability by using it to implement several important components of distributed web applications, including RDBMS integration, load balancing, and fine-grained sandboxing of untrusted third party code. The rising popularity of cloud computing and heterogeneous computer architectures is putting a strain on conventional programming models, which commonly assume that one application executes on one machine, or at best on one out of several identical machines. With our language, we take the first step towards a programming model better suited for a computationally multicultural future.}},
  url = {https://doi.org/10.1145/3122955.3122969},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup802,
  title = {Verifying read-copy-update in a logic for weak memory}},
  author = {Tassarotti, Joseph and Dreyer, Derek and Vafeiadis, Viktor}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Read-Copy-Update (RCU) is a technique for letting multiple readers safely access a data structure while a writer concurrently modifies it. It is used heavily in the Linux kernel in situations where fast reads are important and writes are infrequent. Optimized implementations rely only on the weaker memory orderings provided by modern hardware, avoiding the need for expensive synchronization instructions (such as memory barriers) as much as possible. Using GPS, a recently developed program logic for the C/C++11 memory model, we verify an implementation of RCU for a singly-linked list assuming "release-acquire" semantics. Although release-acquire synchronization is stronger than what is required by real RCU implementations, it is nonetheless significantly weaker than the assumption of sequential consistency made in prior work on RCU verification. Ours is the first formal proof of correctness for an implementation of RCU under a weak memory model.}},
  url = {https://doi.org/10.1145/2737924.2737992},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup803,
  title = {Effects as sessions, sessions as effects}},
  author = {Orchard, Dominic and Yoshida, Nobuko}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Effect and session type systems are two expressive behavioural type systems. The former is usually developed in the context of the lambda-calculus and its variants, the latter for the pi-calculus. In this paper we explore their relative expressive power. Firstly, we give an embedding from PCF, augmented with a parameterised effect system, into a session-typed pi-calculus (session calculus), showing that session types are powerful enough to express effects. Secondly, we give a reverse embedding, from the session calculus back into PCF, by instantiating PCF with concurrency primitives and its effect system with a session-like effect algebra; effect systems are powerful enough to express sessions. The embedding of session types into an effect system is leveraged to give a new implementation of session types in Haskell, via an effect system encoding. The correctness of this implementation follows from the second embedding result. We also discuss various extensions to our embeddings.}},
  url = {https://doi.org/10.1145/2837614.2837634},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup804,
  title = {Symbolic abstract data type inference}},
  author = {Emmi, Michael and Enea, Constantin}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Formal specification is a vital ingredient to scalable verification of software systems. In the case of efficient implementations of concurrent objects like atomic registers, queues, and locks, symbolic formal representations of their abstract data types (ADTs) enable efficient modular reasoning, decoupling clients from implementations. Writing adequate formal specifications, however, is a complex task requiring rare expertise. In practice, programmers write reference implementations as informal specifications. In this work we demonstrate that effective symbolic ADT representations can be automatically generated from the executions of reference implementations. Our approach exploits two key features of naturally-occurring ADTs: violations can be decomposed into a small set of representative patterns, and these patterns manifest in executions with few operations. By identifying certain algebraic properties of naturally-occurring ADTs, and exhaustively sampling executions up to a small number of operations, we generate concise symbolic ADT representations which are complete in practice, enabling the application of efficient symbolic verification algorithms without the burden of manual specification. Furthermore, the concise ADT violation patterns we generate are human-readable, and can serve as useful, formal documentation.}},
  url = {https://doi.org/10.1145/2837614.2837645},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup805,
  title = {Towards a practical secure concurrent language}},
  author = {Muller, Stefan and Chong, Stephen}},
  year = {2012}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We demonstrate that a practical concurrent language can be extended in a natural way with information security mechanisms that provably enforce strong information security guarantees. We extend the X10 concurrent programming language with coarse-grained information-flow control. Central to X10 concurrency abstractions is the notion of a place: a container for data and computation. We associate a security level with each place, and restrict each place to store only data appropriate for that security level. When places interact only with other places at the same security level, then our security mechanisms impose no restrictions. When places of differing security levels interact, our information security analysis prevents potentially dangerous information flows, including information flow through covert scheduling channels. The X10 concurrency mechanisms simplify reasoning about information flow in concurrent programs. We present a static analysis that enforces a noninterference-based extensional information security condition in a calculus that captures the key aspects of X10's place abstraction and async-finish parallelism. We extend this security analysis to support many of X10's language features, and have implemented a prototype compiler for the resulting language.}},
  url = {https://doi.org/10.1145/2384616.2384621},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup806,
  title = {Asynchronous programming, analysis and testing with state machines}},
  author = {Deligiannis, Pantazis and Donaldson, Alastair F. and Ketema, Jeroen and Lal, Akash and Thomson, Paul}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Programming efficient asynchronous systems is challenging because it can often be hard to express the design declaratively, or to defend against data races and interleaving-dependent assertion violations. Previous work has only addressed these challenges in isolation, by either designing a new declarative language, a new data race detection tool or a new testing technique. We present P#, a language for high-reliability asynchronous programming co-designed with a static data race analysis and systematic concurrency testing infrastructure. We describe our experience using P# to write several distributed protocols and port an industrial-scale system internal to Microsoft, showing that the combined techniques, by leveraging the design of P#, are effective in finding bugs.}},
  url = {https://doi.org/10.1145/2737924.2737996},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup807,
  title = {The ins and outs of gradual type inference}},
  author = {Rastogi, Aseem and Chaudhuri, Avik and Hosmer, Basil}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Gradual typing lets programmers evolve their dynamically typed programs by gradually adding explicit type annotations, which confer benefits like improved performance and fewer run-time failures.However, we argue that such evolution often requires a giant leap, and that type inference can offer a crucial missing step. If omitted type annotations are interpreted as unknown types, rather than the dynamic type, then static types can often be inferred, thereby removing unnecessary assumptions of the dynamic type. The remaining assumptions of the dynamic type may then be removed by either reasoning outside the static type system, or restructuring the code.We present a type inference algorithm that can improve the performance of existing gradually typed programs without introducing any new run-time failures. To account for dynamic typing, types that flow in to an unknown type are treated in a fundamentally different manner than types that flow out. Furthermore, in the interests of backward-compatibility, an escape analysis is conducted to decide which types are safe to infer. We have implemented our algorithm for ActionScript, and evaluated it on the SunSpider and V8 benchmark suites. We demonstrate that our algorithm can improve the performance of unannotated programs as well as recover most of the type annotations in annotated programs.}},
  url = {https://doi.org/10.1145/2103656.2103714},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup808,
  title = {Fully abstract compilation via universal embedding}},
  author = {New, Max S. and Bowman, William J. and Ahmed, Amal}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A fully abstract compiler guarantees that two source components are observationally equivalent in the source language if and only if their translations are observationally equivalent in the target. Full abstraction implies the translation is secure: target-language attackers can make no more observations of a compiled component than a source-language attacker interacting with the original source component. Proving full abstraction for realistic compilers is challenging because realistic target languages contain features (such as control effects) unavailable in the source, while proofs of full abstraction require showing that every target context to which a compiled component may be linked can be back-translated to a behaviorally equivalent source context. We prove the first full abstraction result for a translation whose target language contains exceptions, but the source does not. Our translation---specifically, closure conversion of simply typed λ-calculus with recursive types---uses types at the target level to ensure that a compiled component is never linked with attackers that have more distinguishing power than source-level attackers. We present a new back-translation technique based on a shallow embedding of the target language into the source language at a dynamic type. Then boundaries are inserted that mediate terms between the untyped embedding and the strongly-typed source. This technique allows back-translating non-terminating programs, target features that are untypeable in the source, and well-bracketed effects.}},
  url = {https://doi.org/10.1145/2951913.2951941},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup809,
  title = {P: safe asynchronous event-driven programming}},
  author = {Desai, Ankush and Gupta, Vivek and Jackson, Ethan and Qadeer, Shaz and Rajamani, Sriram and Zufferey, Damien}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We describe the design and implementation of P, a domain-specific language to write asynchronous event driven code. P allows the programmer to specify the system as a collection of interacting state machines, which communicate with each other using events. P unifies modeling and programming into one activity for the programmer. Not only can a P program be compiled into executable code, but it can also be tested using model checking techniques. P allows the programmer to specify the environment, used to "close" the system during testing, as nondeterministic ghost machines. Ghost machines are erased during compilation to executable code; a type system ensures that the erasure is semantics preserving.The P language is designed so that a P program can be checked for responsiveness---the ability to handle every event in a timely manner. By default, a machine needs to handle every event that arrives in every state. But handling every event in every state is impractical. The language provides a notion of deferred events where the programmer can annotate when she wants to delay processing an event. The default safety checker looks for presence of unhandled events. The language also provides default liveness checks that an event cannot be potentially deferred forever.P was used to implement and verify the core of the USB device driver stack that ships with Microsoft Windows 8. The resulting driver is more reliable and performs better than its prior incarnation (which did not use P); we have more confidence in the robustness of its design due to the language abstractions and verification provided by P.}},
  url = {https://doi.org/10.1145/2491956.2462184},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup810,
  title = {Well-structured futures and cache locality}},
  author = {Herlihy, Maurice and Liu, Zhiyu}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In fork-join parallelism, a sequential program is split into a directed acyclic graph of tasks linked by directed dependency edges, and the tasks are executed, possibly in parallel, in an order consistent with their dependencies. A popular and effective way to extend fork-join parallelism is to allow threads to create futures. A thread creates a future to hold the results of a computation, which may or may not be executed in parallel. That result is returned when some thread touches that future, blocking if necessary until the result is ready. Recent research has shown that while futures can, of course, enhance parallelism in a structured way, they can have a deleterious effect on cache locality. In the worst case, futures can incur Ω(P T∞ + t T∞) deviations, which implies Ω(C P T∞ + C t T∞) additional cache misses, where C is the number of cache lines, P is the number of processors, t is the number of touches, and T∞ is the computation span. Since cache locality has a large impact on software performance on modern multicores, this result is troubling.In this paper, however, we show that if futures are used in a simple, disciplined way, then the situation is much better: if each future is touched only once, either by the thread that created it, or by a later descendant of the thread that created it, then parallel executions with work stealing can incur at most O(C P T2∞) additional cache misses, a substantial improvement. This structured use of futures is characteristic of many (but not all) parallel applications.}},
  url = {https://doi.org/10.1145/2555243.2555257},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup811,
  title = {Game semantics for interface middleweight Java}},
  author = {Murawski, Andrzej S. and Tzevelekos, Nikos}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We consider an object calculus in which open terms interact with the environment through interfaces. The calculus is intended to capture the essence of contextual interactions of Middleweight Java code. Using game semantics, we provide fully abstract models for the induced notions of contextual approximation and equivalence. These are the first denotational models of this kind.}},
  url = {https://doi.org/10.1145/2535838.2535880},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup812,
  title = {Typer inference builds a short cut to deforestation}},
  author = {Chitil, Olaf}},
  year = {1999}},
  journal = {Proceedings of the Fourth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Deforestation optimises a functional program by transforming it into another one that does not create certain intermediate data structures. Short cut deforestation is a deforestation method which is based on a single, local transformation rule. In return, short cut deforestation expects both producer and consumer of the intermediate structure in a certain form. Warm fusion wan proposed to automatically transform functions into this form. Unfortunately, it is costly and hard to implement. Starting from the fact that short cut deforestation is based on a parametricity theorem of the second-order typed λ-calculus, we show how the required form of a list producer can be derived by the use of type inference. Typability for the second-order typed λ-calculus is undecidable. However, we present a linear-time algorithm that solves a partial type inference problem and that, together with controlled inlining and polymorphic type instantiation, suffices for deforestation. The resulting new short cut deforestation algorithm is efficient and removes more intermediate lists than the original.}},
  url = {https://doi.org/10.1145/317636.317907},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup813,
  title = {Concurrent meld}},
  author = {Kaiser, G. E.}},
  year = {1988}},
  journal = {Proceedings of the 1988 ACM SIGPLAN Workshop on Object-Based Concurrent Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Our original goal was to design a programming language for writing software engineering environments. The most important requirements were reusability and the ability to integrate separately developed tools [1]. Our scope was later expanded to general applications, and then to parallel and distributed systems. Our current focus is on 'growing' distributed software environments and tools, that is, building a core environment or tool assuming a long-term evolution path.MELD is a multiparadigm language that combines object-oriented, macro dataflow, transaction processing and module interconnection styles of programming [2]. The most unusual aspect is the dataflow at the source level among the inputs and outputs of statements. Classes may define constraints in addition to instance variables and methods, which are triggered by changes to instance variables and interleaved along with the statements of a method. MELD's constraints are unidirectional, and similar in purpose to active values.Figure 1 gives a trivial example. When a message O is received by an instance of class C, the two statements in method O and any constraints affected by either statement are executed in dataflow order. This means that instance variable b is computed by O as a function of argument c, and only then the constraint recomputes a as a function of the new value of b. Simultaneously with either of these computations, or before or after or in between, d is assigned to the result of a function of argument e; there is no data dependency between this statement and either the other statement in the method or the constraint.MELD's multiple paradigms lead to three granularities of concurrency: Statements from the macro dataflow for fine to medium grain concurrency within a method or among methods. This permits a smaller granularity than dataflow among the inputs and outputs of methods. Atomic methods, which provide a low-level form of concurrency control, force a larger granularity.Objects for medium to large grain parallelism, with synchronous and asynchronous message passing among remote or local objects. Many other concurrent object-oriented languages provide synchronous or asynchronous message passing, but not both.Atomic transactions for high-level concurrency control among users (including tools where interleaved operation is inappropriate). Most other object-based systems provide only one form of concurrency control.A method may be invoked synchronously or asynchronously. In the synchronous case, the caller waits for return with respect to its own thread of control. In the asynchronous case, the caller continues and the invocation creates a new thread of control. Programs may involve an arbitrary number of threads created dynamically during program execution. Several threads may operate within the same address space and one thread may, in effect, operate across multiple address spaces. In either case, the invoked method runs concurrently with any other methods currently active on the same object. These other methods may be reading and writing the same instance variables. The default synchronization among such methods is done by dataflow, as within a single method.There is a serious problem with this approach. Figure 2 operates as indicated if M and N happen to begin execution at exactly the same time, due to simultaneous arrival of messages M and N from other objects. d is computed from the value of c given as argument to N, b is computed from the new value of d, and a from the new value of b. However, if message N arrives a bit later than M, then b is computed from the old value of d, and then the new value of d is computed from the argument c and a is computed from the new value of b. Both statements in N could be executed concurrently since there is no dataflow between them. On the other hand, if message M arrives a bit later than N, then b may be computed from either the old or new value of d and a may be computed from either the old or new value of b. This is obviously rather bewildering for the programmer, since it is necessary that the resulting computation be deemed 'correct' in all of these cases.We support this is to permit maximum concurrency for those applications (and programmers!) that can handle the non-determinism. Our intuition is that non-determinism will not be a problem in many cases. The programmer has in mind a parallel algorithm where he thinks in terms of the dataflow necessary to produce the correct solution. He typically uses a conventional language, such as Fortran or C, to implement his algorithm. The parallelizing compiler must then uncover the parallelism again using dataflow techniques. We avoid this hide-and-seek by allowing the programmer to make the dataflow explicit.There are many programs, however, written without cognizance of the dataflow. We also support these programmers with sequential blocks and atomic blocks. Figure 3 shows method O of class C written as a sequential block rather than a parallel block (i.e., a parallel block is enclosed in curly braces and a sequential block in square brackets). In this example, all of method O executes in the order in which the statements are written. Any constraints whose inputs are changed during the execution of O are triggered as before.Sequential blocks remove concurrency within methods, making them easier to write without the need for the single-assignment mindset, but do not affect concurrency among methods. Figure 4 indicates the ordering if M and N happen to start at the same time. b is computed from the old value of d, a from the new value of b and then the new value of d from the argument c. But if there is a race condition, a may see the new value of b or b may see the new value of d, but not both.In Figure 5, method O executes atomically with respect to the receiver object. Atomic blocks are indicated with parentheses. Both b and d are updated, and only after O terminates is the constraint triggered. The two statements in O may themselves be executed in either sequential or dataflow order, since it is necessary to include them within an inner sequential or parallel block, not shown, if the atomic block is used. In this case it does not matter.Methods M and N are serialized in Figure 6, so they appear atomic to each other. We currently support pessimistic concurrency control by locking the object at the computational grain size of individual methods, or a block within a method. We are integrating real transactions that cut across methods and objects/ We use distributed optimistic concurrency control with multiple versions, since we expect a majority of read-only transactions (in our primary application domain of distributed software development tools). Atomic blocks using validation rather than locking will be surrounded by angle brackets rather than parentheses, and may be either sequential or parallel; begin_transaction, abort_transaction and commit_transaction statements are provided for transactions that begin in one method and may end in another according to circumstances determined at run-time.}},
  url = {https://doi.org/10.1145/67386.67419},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup814,
  title = {Fast narrowing-driven partial evaluation for inductively sequential programs}},
  author = {Ramos, J. Guadalupe and Silva, Josep and Vidal, Germ\'{a}},
  year = {2005}},
  journal = {Proceedings of the Tenth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Narrowing-driven partial evaluation is a powerful technique for the specialization of (first-order) functional and functional logic programs. However, although it gives good results on small programs, it does not scale up well to realistic problems (e.g., interpreter specialization). In this work, we introduce a faster partial evaluation scheme by ensuring the termination of the process offline. For this purpose, we first characterize a class of programs which are quasi-terminating, i.e., the computations performed with needed narrowing—the symbolic computation mechanism of narrowing-driven partial evaluation—only contain finitely many different terms (and, thus, partial evaluation terminates). Since this class is quite restrictive, we also introduce an annotation algorithm for a broader class of programs so that they behave like quasi-terminating programs w.r.t. an extension of needed narrowing. Preliminary experiments are encouraging and demonstrate the usefulness of our approach.}},
  url = {https://doi.org/10.1145/1086365.1086394},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup815,
  title = {To be precise: regression aware debugging}},
  author = {Bavishi, Rohan and Pandey, Awanish and Roy, Subhajit}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Bounded model checking based debugging solutions search for mutations of program expressions that produce the expected output for a currently failing test. However, the current localization tools are not regression aware: they do not use information from the passing tests in their localization formula. On the other hand, the current repair tools attempt to guarantee regression freedom: when provided with a set of passing tests, they guarantee that none of these tests can break due to the suggested repair patch, thereby constructing a large repair formula.In this paper, we propose regression awareness as a means to improve the quality of localization and to scale repair. To enable regression awareness, we summarize the proof of correctness of each passing test by computing Craig Interpolants over a symbolic encoding of the passing execution, and use these summaries as additional soft constraints while synthesizing altered executions corresponding to failing tests. Intuitively, these additional constraints act as roadblocks, thereby discouraging executions that may damage the proof of a passing test. We use a partial MAXSAT solver to relax the proofs in a systematic way, and use a ranking function that penalizes mutations that damage the existing proofs.We have implemented our algorithms into a tool, TINTIN, that enables regression aware localization and repair. For localizations, our strategy is effective in extracting a superior ranking of suspicious locations: on a set of 52 different versions across 12 different programs spanning three benchmark suites, TINTIN achieves a saving of developer effort by almost 45\% (in terms of the locations that must be examined by a developer to reach the ground-truth repair) in the worst case and 27\% in the average case over existing techniques. For automated repairs, on our set of benchmarks, TINTIN achieves a 2.3X speedup over existing techniques without sacrificing much on the ranking of the repair patches: the ground-truth repair appears as the topmost suggestion in more than 70\% of our benchmarks.}},
  url = {https://doi.org/10.1145/2983990.2984014},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup816,
  title = {Complexity verification using guided theorem enumeration}},
  author = {Srikanth, Akhilesh and Sahin, Burak and Harris, William R.}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Determining if a given program satisfies a given bound on the amount of resources that it may use is a fundamental problem with critical practical applications. Conventional automatic verifiers for safety properties cannot be applied to address this problem directly because such verifiers target properties expressed in decidable theories; however, many practical bounds are expressed in nonlinear theories, which are undecidable. In this work, we introduce an automatic verification algorithm, CAMPY, that determines if a given program P satisfies a given resource bound B, which may be expressed using polynomial, exponential, and logarithmic terms. The key technical contribution behind our verifier is an interpolating theorem prover for non-linear theories that lazily learns a sufficiently accurate approximation of non-linear theories by selectively grounding theorems of the nonlinear theory that are relevant to proving that P satisfies B. To evaluate CAMPY, we implemented it to target Java Virtual Machine bytecode. We applied CAMPY to verify that over 20 solutions submitted for programming problems hosted on popular online coding platforms satisfy or do not satisfy expected complexity bounds.}},
  url = {https://doi.org/10.1145/3009837.3009864},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup817,
  title = {Verifying eventual consistency of optimistic replication systems}},
  author = {Bouajjani, Ahmed and Enea, Constantin and Hamza, Jad}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We address the verification problem of eventual consistency of optimistic replication systems. Such systems are typically used to implement distributed data structures over large scale networks. We introduce a formal definition of eventual consistency that applies to a wide class of existing implementations, including the ones using speculative executions. Then, we reduce the problem of checking eventual consistency to reachability and model checking problems. This reduction enables the use of existing verification tools for message-passing programs in the context of verifying optimistic replication systems. Furthermore, we derive from these reductions decision procedures for checking eventual consistency of systems implemented as finite-state programs communicating through unbounded unordered channels.}},
  url = {https://doi.org/10.1145/2535838.2535877},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup818,
  title = {Parallel functional arrays}},
  author = {Kumar, Ananya and Blelloch, Guy E. and Harper, Robert}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The goal of this paper is to develop a form of functional arrays (sequences) that are as efficient as imperative arrays, can be used in parallel, and have well defined cost-semantics. The key idea is to consider sequences with functional value semantics but non-functional cost semantics. Because the value semantics is functional, "updating" a sequence returns a new sequence. We allow operations on "older" sequences (called interior sequences) to be more expensive than operations on the "most recent" sequences (called leaf sequences). We embed sequences in a language supporting fork-join parallelism. Due to the parallelism, operations can be interleaved non-deterministically, and, in conjunction with the different cost for interior and leaf sequences, this can lead to non-deterministic costs for a program. Consequently the costs of programs can be difficult to analyze. The main result is the derivation of a deterministic cost dynamics which makes analyzing the costs easier. The theorems are not specific to sequences and can be applied to other data types with different costs for operating on interior and leaf versions. We present a wait-free concurrent implementation of sequences that requires constant work for accessing and updating leaf sequences, and logarithmic work for accessing and linear work for updating interior sequences. We sketch a proof of correctness for the sequence implementation. The key advantages of the present approach compared to current approaches is that our implementation requires no changes to existing programming languages, supports nested parallelism, and has well defined cost semantics. At the same time, it allows for functional implementations of algorithms such as depth-first search with the same asymptotic complexity as imperative implementations.}},
  url = {https://doi.org/10.1145/3009837.3009869},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup819,
  title = {Fault-tolerant typed assembly language}},
  author = {Perry, Frances and Mackey, Lester and Reis, George A. and Ligatti, Jay and August, David I. and Walker, David}},
  year = {2007}},
  journal = {Proceedings of the 28th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A transient hardware fault occurs when an energetic particle strikes a transistor, causing it to change state. Although transient faults do not permanently damage the hardware, they may corrupt computations by altering stored values and signal transfers. In this paper, we propose a new scheme for provably safe and reliable computing in the presence of transient hardware faults. In our scheme, software computations are replicated to provide redundancy while special instructions compare the independently computed results to detect errors before writing critical data. In stark contrast to any previous efforts in this area, we have analyzed our fault tolerance scheme from a formal, theoretical perspective. To be specific, first, we provide an operational semantics for our assembly language, which includes a precise formal definition of our fault model. Second, we develop an assembly-level type system designed to detect reliability problems in compiled code. Third, we provide a formal specification for program fault tolerance under the given fault model and prove that all well-typed programs are indeed fault tolerant. In addition to the formal analysis, we evaluate our detection scheme and show that it only takes 34\% longer to execute than the unreliable version.}},
  url = {https://doi.org/10.1145/1250734.1250741},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup820,
  title = {Type-theory in color}},
  author = {Bernardy, Jean-Philippe and Guilhem, Moulin}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Dependent type-theory aims to become the standard way to formalize mathematics at the same time as displacing traditional platforms for high-assurance programming. However, current implementations of type theory are still lacking, in the sense that some obvious truths require explicit proofs, making type-theory awkward to use for many applications, both in formalization and programming. In particular, notions of erasure are poorly supported.In this paper we propose an extension of type-theory with colored terms, color erasure and interpretation of colored types as predicates. The result is a more powerful type-theory: some definitions and proofs may be omitted as they become trivial, it becomes easier to program with precise types, and some parametricity results can be internalized.}},
  url = {https://doi.org/10.1145/2500365.2500577},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup821,
  title = {Repairing sequential consistency in C/C++11}},
  author = {Lahav, Ori and Vafeiadis, Viktor and Kang, Jeehoon and Hur, Chung-Kil and Dreyer, Derek}},
  year = {2017}},
  journal = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The C/C++11 memory model defines the semantics of concurrent memory accesses in C/C++, and in particular supports racy "atomic" accesses at a range of different consistency levels, from very weak consistency ("relaxed") to strong, sequential consistency ("SC"). Unfortunately, as we observe in this paper, the semantics of SC atomic accesses in C/C++11, as well as in all proposed strengthenings of the semantics, is flawed, in that (contrary to previously published results) both suggested compilation schemes to the Power architecture are unsound. We propose a model, called RC11 (for Repaired C11), with a better semantics for SC accesses that restores the soundness of the compilation schemes to Power, maintains the DRF-SC guarantee, and provides stronger, more useful, guarantees to SC fences. In addition, we formally prove, for the first time, the correctness of the proposed stronger compilation schemes to Power that preserve load-to-store ordering and avoid "out-of-thin-air" reads.}},
  url = {https://doi.org/10.1145/3062341.3062352},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup822,
  title = {XQuery and static typing: tackling the problem of backward axes}},
  author = {Genev\`{e}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {XQuery is a functional language dedicated to XML data querying and manipulation. As opposed to other W3C-standardized languages for XML (e.g. XSLT), it has been intended to feature strong static typing. Currently, however, some expressions of the language cannot be statically typed with any precision. We argue that this is due to a discrepancy between the semantics of the language and its type algebra: namely, the values of the language are (possibly inner) tree nodes, which may have siblings and ancestors in the data. The types on the other hand are regular tree types, as usual in the XML world: they describe sets of trees. The type associated to a node then corresponds to the subtree whose root is that node and contains no information about the rest of the data. This makes navigation expressions using `backward axes,' which return e.g. the siblings of a node, impossible to type. We discuss how to handle this discrepancy by improving the type system. We describe a logic-based language of extended types able to represent inner tree nodes and show how it can dramatically increase the precision of typing for navigation expressions. We describe how inclusion between these extended types and the classical regular tree types can be decided, allowing a hybrid system combining both type languages. The result is a net increase in precision of typing.}},
  url = {https://doi.org/10.1145/2784731.2784746},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup823,
  title = {A type theory for probability density functions}},
  author = {Bhat, Sooraj and Agarwal, Ashish and Vuduc, Richard and Gray, Alexander}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {There has been great interest in creating probabilistic programming languages to simplify the coding of statistical tasks; however, there still does not exist a formal language that simultaneously provides (1) continuous probability distributions, (2) the ability to naturally express custom probabilistic models, and (3) probability density functions (PDFs). This collection of features is necessary for mechanizing fundamental statistical techniques. We formalize the first probabilistic language that exhibits these features, and it serves as a foundational framework for extending the ideas to more general languages. Particularly novel are our type system for absolutely continuous (AC) distributions (those which permit PDFs) and our PDF calculation procedure, which calculates PDF s for a large class of AC distributions. Our formalization paves the way toward the rigorous encoding of powerful statistical reformulations.}},
  url = {https://doi.org/10.1145/2103656.2103721},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup824,
  title = {Big types in little runtime: open-world soundness and collaborative blame for gradual type systems}},
  author = {Vitousek, Michael M. and Swords, Cameron and Siek, Jeremy G.}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Gradual typing combines static and dynamic typing in the same language, offering programmers the error detection and strong guarantees of static types and the rapid prototyping and flexible programming idioms of dynamic types. Many gradually typed languages are implemented by translation into an untyped target language (e.g., Typed Clojure, TypeScript, Gradualtalk, and Reticulated Python). For such languages, it is desirable to support arbitrary interaction between translated code and legacy code in the untyped language while maintaining the type soundness of the translated code. In this paper we formalize this goal in the form of the open-world soundness criterion. We discuss why it is challenging to achieve open-world soundness using the traditional proxy-based approach for higher-order casts. However, the transient design satisfies open-world soundness. Indeed, we present a formal semantics for the transient design and prove that our semantics satisfies open-world soundness. In this paper we also solve a challenging problem for the transient design: how to provide blame tracking without proxies. We define a semantics for blame and prove the Blame Theorem. We also prove that the Gradual Guarantee holds for this system, ensuring that programs can be evolved freely between static and dynamic typing. Finally, we demonstrate that the runtime overhead of the transient approach is low in the context of Reticulated Python, an implementation of gradual typing for Python.}},
  url = {https://doi.org/10.1145/3009837.3009849},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup825,
  title = {Hypercollecting semantics and its application to static analysis of information flow}},
  author = {Assaf, Mounir and Naumann, David A. and Signoles, Julien and Totel, \'{E}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We show how static analysis for secure information flow can be expressed and proved correct entirely within the framework of abstract interpretation. The key idea is to define a Galois connection that directly approximates the hyperproperty of interest. To enable use of such Galois connections, we introduce a fixpoint characterisation of hypercollecting semantics, i.e. a "set of sets" transformer. This makes it possible to systematically derive static analyses for hyperproperties entirely within the calculational framework of abstract interpretation. We evaluate this technique by deriving example static analyses. For qualitative information flow, we derive a dependence analysis similar to the logic of Amtoft and Banerjee (SAS '04) and the type system of Hunt and Sands (POPL '06). For quantitative information flow, we derive a novel cardinality analysis that bounds the leakage conveyed by a program instead of simply deciding whether it exists. This encompasses problems that are hypersafety but not k-safety. We put the framework to use and introduce variations that achieve precision rivalling the most recent and precise static analyses for information flow.}},
  url = {https://doi.org/10.1145/3009837.3009889},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup826,
  title = {Path-based inductive synthesis for program inversion}},
  author = {Srivastava, Saurabh and Gulwani, Sumit and Chaudhuri, Swarat and Foster, Jeffrey S.}},
  year = {2011}},
  journal = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this paper, we investigate the problem of semi-automated inversion of imperative programs, which has the potential to make it much easier and less error prone to write programs that naturally pair as inverses, such as insert/delete operations, compressors/decompressors, and so on. Viewing inversion as a subproblem of program synthesis, we propose a novel synthesis technique called Path-based inductive synthesis (PINS) and apply it to inversion. PINS starts from a program P and a template T for its inverse. PINS then iteratively refines the space of template instantiations by exploring paths in the composition of P and T with symbolic execution. PINS uses an SMT solver to intelligently guide the refinement process, based on the paths explored so far. The key idea motivating this approach is the small path-bound hypothesis: that the behavior of a program can be summarized with a small, carefully chosen set of its program paths.We evaluated PINS by using it to invert 14 programs such as compressors (e.g., Lempel-Ziv-Welch), encoders (e.g., UUEncode), and arithmetic operations (e.g., vector rotation). Most of these examples are difficult or impossible to invert using prior techniques, but PINS was able to invert all of them. We also found that a semi-automated technique we developed to mine a template from the program to be inverted worked well. In our experiments, PINS takes between one second to thirty minutes to synthesize inverses. We believe this proof-of-concept implementation demonstrates the viability of the PINS approach to program synthesis.}},
  url = {https://doi.org/10.1145/1993498.1993557},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup827,
  title = {Targeted Automatic Integer Overflow Discovery Using Goal-Directed Conditional Branch Enforcement}},
  author = {Sidiroglou-Douskos, Stelios and Lahtinen, Eric and Rittenhouse, Nathan and Piselli, Paolo and Long, Fan and Kim, Deokhwan and Rinard, Martin}},
  year = {2015}},
  journal = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a new technique and system, DIODE, for auto- matically generating inputs that trigger overflows at memory allocation sites. DIODE is designed to identify relevant sanity checks that inputs must satisfy to trigger overflows at target memory allocation sites, then generate inputs that satisfy these sanity checks to successfully trigger the overflow. DIODE works with off-the-shelf, production x86 binaries. Our results show that, for our benchmark set of applications, and for every target memory allocation site exercised by our seed inputs (which the applications process correctly with no overflows), either 1) DIODE is able to generate an input that triggers an overflow at that site or 2) there is no input that would trigger an overflow for the observed target expression at that site.}},
  url = {https://doi.org/10.1145/2694344.2694389},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup828,
  title = {Galois transformers and modular abstract interpreters: reusable metatheory for program analysis}},
  author = {Darais, David and Might, Matthew and Van Horn, David}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The design and implementation of static analyzers has become increasingly systematic. Yet for a given language or analysis feature, it often requires tedious and error prone work to implement an analyzer and prove it sound. In short, static analysis features and their proofs of soundness do not compose well, causing a dearth of reuse in both implementation and metatheory. We solve the problem of systematically constructing static analyzers by introducing Galois transformers: monad transformers that transport Galois connection properties. In concert with a monadic interpreter, we define a library of monad transformers that implement building blocks for classic analysis parameters like context, path, and heap (in)sensitivity. Moreover, these can be composed together independent of the language being analyzed. Significantly, a Galois transformer can be proved sound once and for all, making it a reusable analysis component. As new analysis features and abstractions are developed and mixed in, soundness proofs need not be reconstructed, as the composition of a monad transformer stack is sound by virtue of its constituents. Galois transformers provide a viable foundation for reusable and composable metatheory for program analysis. Finally, these Galois transformers shift the level of abstraction in analysis design and implementation to a level where non-specialists have the ability to synthesize sound analyzers over a number of parameters.}},
  url = {https://doi.org/10.1145/2814270.2814308},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup829,
  title = {Toward a verified relational database management system}},
  author = {Malecha, Gregory and Morrisett, Greg and Shinnar, Avraham and Wisnesky, Ryan}},
  year = {2010}},
  journal = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We report on our experience implementing a lightweight, fully verified relational database management system (RDBMS). The functional specification of RDBMS behavior, RDBMS implementation, and proof that the implementation meets the specification are all written and verified in Coq. Our contributions include: (1) a complete specification of the relational algebra in Coq; (2) an efficient realization of that model (B+ trees) implemented with the Ynot extension to Coq; and (3) a set of simple query optimizations proven to respect both semantics and run-time cost. In addition to describing the design and implementation of these artifacts, we highlight the challenges we encountered formalizing them, including the choice of representation for finite relations of typed tuples and the challenges of reasoning about data structures with complex sharing. Our experience shows that though many challenges remain, building fully-verified systems software in Coq is within reach.}},
  url = {https://doi.org/10.1145/1706299.1706329},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup830,
  title = {Reasoning about Java classes: preliminary report}},
  author = {Jacobs, Bart and van den Berg, Joachim and Huisman, Marieke and van Berkum, Martijn and Hensel, U. and Tews, H.}},
  year = {1998}},
  journal = {Proceedings of the 13th ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present the first results of a project called LOOP, on formal methods for the object-oriented language Java. It aims at verification of program properties, with support of modern tools. We use our own front-end tool (which is still partly under construction) for translating Java classes into higher order logic, and a back-end theorem prover (namely PVS, developed at SRI) for reasoning. In several examples we demonstrate how non-trivial properties of Java programs and classes can be proven following this two-step approach.}},
  url = {https://doi.org/10.1145/286936.286973},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup831,
  title = {The constrained-monad problem}},
  author = {Sculthorpe, Neil and Bracker, Jan and Giorgidze, George and Gill, Andy}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In Haskell, there are many data types that would form monads were it not for the presence of type-class constraints on the operations on that data type. This is a frustrating problem in practice, because there is a considerable amount of support and infrastructure for monads that these data types cannot use. Using several examples, we show that a monadic computation can be restructured into a normal form such that the standard monad class can be used. The technique is not specific to monads, and we show how it can also be applied to other structures, such as applicative functors. One significant use case for this technique is domain-specific languages, where it is often desirable to compile a deep embedding of a computation to some other language, which requires restricting the types that can appear in that computation.}},
  url = {https://doi.org/10.1145/2500365.2500602},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup832,
  title = {Surgical precision JIT compilers}},
  author = {Rompf, Tiark and Sujeeth, Arvind K. and Brown, Kevin J. and Lee, HyoukJoong and Chafi, Hassan and Olukotun, Kunle}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Just-in-time (JIT) compilation of running programs provides more optimization opportunities than offline compilation. Modern JIT compilers, such as those in virtual machines like Oracle's HotSpot for Java or Google's V8 for JavaScript, rely on dynamic profiling as their key mechanism to guide optimizations. While these JIT compilers offer good average performance, their behavior is a black box and the achieved performance is highly unpredictable.In this paper, we propose to turn JIT compilation into a precision tool by adding two essential and generic metaprogramming facilities: First, allow programs to invoke JIT compilation explicitly. This enables controlled specialization of arbitrary code at run-time, in the style of partial evaluation. It also enables the JIT compiler to report warnings and errors to the program when it is unable to compile a code path in the demanded way. Second, allow the JIT compiler to call back into the program to perform compile-time computation. This lets the program itself define the translation strategy for certain constructs on the fly and gives rise to a powerful JIT macro facility that enables "smart" libraries to supply domain-specific compiler optimizations or safety checks.We present Lancet, a JIT compiler framework for Java bytecode that enables such a tight, two-way integration with the running program. Lancet itself was derived from a high-level Java bytecode interpreter: staging the interpreter using LMS (Lightweight Modular Staging) produced a simple bytecode compiler. Adding abstract interpretation turned the simple compiler into an optimizing compiler. This fact provides compelling evidence for the scalability of the staged-interpreter approach to compiler construction.In the case of Lancet, JIT macros also provide a natural interface to existing LMS-based toolchains such as the Delite parallelism and DSL framework, which can now serve as accelerator macros for arbitrary JVM bytecode.}},
  url = {https://doi.org/10.1145/2594291.2594316},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup833,
  title = {Compositional solution space quantification for probabilistic software analysis}},
  author = {Borges, Mateus and Filieri, Antonio and d'Amorim, Marcelo and P\u{a}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Probabilistic software analysis aims at quantifying how likely a target event is to occur during program execution. Current approaches rely on symbolic execution to identify the conditions to reach the target event and try to quantify the fraction of the input domain satisfying these conditions. Precise quantification is usually limited to linear constraints, while only approximate solutions can be provided in general through statistical approaches. However, statistical approaches may fail to converge to an acceptable accuracy within a reasonable time.We present a compositional statistical approach for the efficient quantification of solution spaces for arbitrarily complex constraints over bounded floating-point domains. The approach leverages interval constraint propagation to improve the accuracy of the estimation by focusing the sampling on the regions of the input domain containing the sought solutions. Preliminary experiments show significant improvement on previous approaches both in results accuracy and analysis time.}},
  url = {https://doi.org/10.1145/2594291.2594329},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup834,
  title = {Parametricity and dependent types}},
  author = {Bernardy, Jean-Philippe and Jansson, Patrik and Paterson, Ross}},
  year = {2010}},
  journal = {Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Reynolds' abstraction theorem shows how a typing judgement in System F can be translated into a relational statement (in second order predicate logic) about inhabitants of the type. We (in second order predicate logic) about inhabitants of the type. We obtain a similar result for a single lambda calculus (a pure type system), in which terms, types and their relations are expressed. Working within a single system dispenses with the need for an interpretation layer, allowing for an unusually simple presentation. While the unification puts some constraints on the type system (which we spell out), the result applies to many interesting cases, including dependently-typed ones.}},
  url = {https://doi.org/10.1145/1863543.1863592},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup835,
  title = {Query-guided maximum satisfiability}},
  author = {Zhang, Xin and Mangal, Ravi and Nori, Aditya V. and Naik, Mayur}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We propose a new optimization problem "Q-MaxSAT", an extension of the well-known Maximum Satisfiability or MaxSAT problem. In contrast to MaxSAT, which aims to find an assignment to all variables in the formula, Q-MaxSAT computes an assignment to a desired subset of variables (or queries) in the formula. Indeed, many problems in diverse domains such as program reasoning, information retrieval, and mathematical optimization can be naturally encoded as Q-MaxSAT instances. We describe an iterative algorithm for solving Q-MaxSAT. In each iteration, the algorithm solves a subproblem that is relevant to the queries, and applies a novel technique to check whether the partial assignment found is a solution to the Q-MaxSAT problem. If the check fails, the algorithm grows the subproblem with a new set of clauses identified as relevant to the queries. Our empirical evaluation shows that our Q-MaxSAT solver Pilot achieves significant improvements in runtime and memory consumption over conventional MaxSAT solvers on several Q-MaxSAT instances generated from real-world problems in program analysis and information retrieval.}},
  url = {https://doi.org/10.1145/2837614.2837658},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup836,
  title = {Indexed codata types}},
  author = {Thibodeau, David and Cave, Andrew and Pientka, Brigitte}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Indexed data types allow us to specify and verify many interesting invariants about finite data in a general purpose programming language. In this paper we investigate the dual idea: indexed codata types, which allow us to describe data-dependencies about infinite data structures. Unlike finite data which is defined by constructors, we define infinite data by observations. Dual to pattern matching on indexed data which may refine the type indices, we define copattern matching on indexed codata where type indices guard observations we can make. Our key technical contributions are three-fold: first, we extend Levy's call-by-push value language with support for indexed (co)data and deep (co)pattern matching; second, we provide a clean foundation for dependent (co)pattern matching using equality constraints; third, we describe a small-step semantics using a continuation-based abstract machine, define coverage for indexed (co)patterns, and prove type safety. This is an important step towards building a foundation where (co)data type definitions and dependent types can coexist.}},
  url = {https://doi.org/10.1145/2951913.2951929},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup837,
  title = {Inductive data flow graphs}},
  author = {Farzan, Azadeh and Kincaid, Zachary and Podelski, Andreas}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The correctness of a sequential program can be shown by the annotation of its control flow graph with inductive assertions. We propose inductive data flow graphs, data flow graphs with incorporated inductive assertions, as the basis of an approach to verifying concurrent programs. An inductive data flow graph accounts for a set of dependencies between program actions in interleaved thread executions, and therefore stands as a representation for the set of concurrent program traces which give rise to these dependencies. The approach first constructs an inductive data flow graph and then checks whether all program traces are represented. The size of the inductive data flow graph is polynomial in the number of data dependencies (in a sense that can be made formal); it does not grow exponentially in the number of threads unless the data dependencies do. The approach shifts the burden of the exponential explosion towards the check whether all program traces are represented, i.e., to a combinatorial problem (over finite graphs).}},
  url = {https://doi.org/10.1145/2429069.2429086},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup838,
  title = {Why dependent types matter}},
  author = {McKinna, James}},
  year = {2006}},
  journal = {Conference Record of the 33rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Language designers have in recent years proposed a wealth of richer type systems for programming which seek to extend the range of statically enforced guarantees on data and code. Most such proposals have been evolutionary extensions of ML or Haskell, offering programmers a balanced compromise between expressive strength and existing well-understood technology. Typically they revolve around type- or kind-indexed types such as GADTs, supported by limited equality reasoning at the type-checking level, thus separating the dynamic behaviour of programs from the (simpler) static behaviour of indexing information occurring in their types.I want to argue in this talk for a more radical departure from such practice by examining full spectrum type dependency, lifting such restrictions on the data upon which types may depend. Conor McBride and I designed the language EPIGRAM for experiments in programming with inductive families of data (of which GADTs are a special case). Using it for illustration, I will explore some of the possibilities and challenges afforded by full spectrum type dependency at the static and dynamic level: types directly support modelling complex invariants in terms of other data (rather than their types), with a Curry-Howard flavour of data-as-evidence; such complexity is on a 'pay-as-you-go' basis, while keeping type annotations and other syntactic overheads to a minimum;data decomposition steps, e.g. case analysis, furnish more informative interactions between types and values during typechecking; such steps may moreover be abstractly specified by their types, and thus user definable; this supports a style of programming embracing 'learning by testing', views, and Burstall's 'hand simulation plus a little induction';the absence of a rigid phase distinction need not lead to type-passing or excessive run-time overhead; effectful computation, in particular partiality, can be incorporated via variations on existing ideas such as monads.This talk is based on joint work with Conor McBride, Edwin Brady and Thorsten Altenkirch.}},
  url = {https://doi.org/10.1145/1111037.1111038},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup839,
  title = {Slicing probabilistic programs}},
  author = {Hur, Chung-Kil and Nori, Aditya V. and Rajamani, Sriram K. and Samuel, Selva}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Probabilistic programs use familiar notation of programming languages to specify probabilistic models. Suppose we are interested in estimating the distribution of the return expression r of a probabilistic program P. We are interested in slicing the probabilistic program P and obtaining a simpler program Sli(P) which retains only those parts of P that are relevant to estimating r, and elides those parts of P that are not relevant to estimating r. We desire that the Sli transformation be both correct and efficient. By correct, we mean that P and Sli(P) have identical estimates on r. By efficient, we mean that estimation over Sli(P) be as fast as possible.We show that the usual notion of program slicing, which traverses control and data dependencies backward from the return expression r, is unsatisfactory for probabilistic programs, since it produces incorrect slices on some programs and sub-optimal ones on others. Our key insight is that in addition to the usual notions of control dependence and data dependence that are used to slice non-probabilistic programs, a new kind of dependence called observe dependence arises naturally due to observe statements in probabilistic programs.We propose a new definition of Sli(P) which is both correct and efficient for probabilistic programs, by including observe dependence in addition to control and data dependences for computing slices. We prove correctness mathematically, and we demonstrate efficiency empirically. We show that by applying the Sli transformation as a pre-pass, we can improve the efficiency of probabilistic inference, not only in our own inference tool R2, but also in other systems for performing inference such as Church and Infer.NET.}},
  url = {https://doi.org/10.1145/2594291.2594303},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup840,
  title = {Fencing off go: liveness and safety for channel-based programming}},
  author = {Lange, Julien and Ng, Nicholas and Toninho, Bernardo and Yoshida, Nobuko}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Go is a production-level statically typed programming language whose design features explicit message-passing primitives and lightweight threads, enabling (and encouraging) programmers to develop concurrent systems where components interact through communication more so than by lock-based shared memory concurrency. Go can only detect global deadlocks at runtime, but provides no compile-time protection against all too common communication mismatches or partial deadlocks. This work develops a static verification framework for bounded liveness and safety in Go programs, able to detect communication errors and partial deadlocks in a general class of realistic concurrent programs, including those with dynamic channel creation and infinite recursion. Our approach infers from a Go program a faithful representation of its communication patterns as a behavioural type. By checking a syntactic restriction on channel usage, dubbed fencing, we ensure that programs are made up of finitely many different communication patterns that may be repeated infinitely many times. This restriction allows us to implement bounded verification procedures (akin to bounded model checking) to check for liveness and safety in types which in turn approximates liveness and safety in Go programs. We have implemented a type inference and liveness and safety checks in a tool-chain and tested it against publicly available Go programs.Updated on 27th Feb 2017. See Comments.}},
  url = {https://doi.org/10.1145/3009837.3009847},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup841,
  title = {Bidirectionalization for free! (Pearl)}},
  author = {Voigtl\"{a}},
  year = {2009}},
  journal = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A bidirectional transformation consists of a function get that takes a source (document or value) to a view and a function put that takes an updated view and the original source back to an updated source, governed by certain consistency conditions relating the two functions. Both the database and programming language communities have studied techniques that essentially allow a user to specify only one of get and put and have the other inferred automatically. All approaches so far to this bidirectionalization task have been syntactic in nature, either proposing a domain-specific language with limited expressiveness but built-in (and composable) backward components, or restricting get to a simple syntactic form from which some algorithm can synthesize an appropriate definition for put. Here we present a semantic approach instead. The idea is to take a general-purpose language, Haskell, and write a higher-order function that takes (polymorphic) get-functions as arguments and returns appropriate put-functions. All this on the level of semantic values, without being willing, or even able, to inspect the definition of get, and thus liberated from syntactic restraints. Our solution is inspired by relational parametricity and uses free theorems for proving the consistency conditions. It works beautifully.}},
  url = {https://doi.org/10.1145/1480881.1480904},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup842,
  title = {Specification Inference Using Context-Free Language Reachability}},
  author = {Bastani, Osbert and Anand, Saswat and Aiken, Alex}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a framework for computing context-free language reachability properties when parts of the program are missing. Our framework infers candidate specifications for missing program pieces that are needed for verifying a property of interest, and presents these specifications to a human auditor for validation. We have implemented this framework for a taint analysis of Android apps that relies on specifications for Android library methods. In an extensive experimental study on 179 apps, our tool performs verification with only a small number of queries to a human auditor.}},
  url = {https://doi.org/10.1145/2676726.2676977},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup843,
  title = {On optimizing machine learning workloads via kernel fusion}},
  author = {Ashari, Arash and Tatikonda, Shirish and Boehm, Matthias and Reinwald, Berthold and Campbell, Keith and Keenleyside, John and Sadayappan, P.}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Exploitation of parallel architectures has become critical to scalable machine learning (ML). Since a wide range of ML algorithms employ linear algebraic operators, GPUs with BLAS libraries are a natural choice for such an exploitation. Two approaches are commonly pursued: (i) developing specific GPU accelerated implementations of complete ML algorithms; and (ii) developing GPU kernels for primitive linear algebraic operators like matrix-vector multiplication, which are then used in developing ML algorithms. This paper extends the latter approach by developing fused kernels for a combination of primitive operators that are commonly found in popular ML algorithms. We identify the generic pattern of computation (alpha * X^T (v * (X * y)) + beta * z) and its various instantiations. We develop a fused kernel to optimize this computation on GPUs -- with specialized techniques to handle both sparse and dense matrices. This approach not only reduces the cost of data loads due to improved temporal locality but also enables other optimizations like coarsening and hierarchical aggregation of partial results. We also present an analytical model that considers input data characteristics and available GPU resources to estimate near-optimal settings for kernel launch parameters. The proposed approach provides speedups ranging from 2 to 67 for different instances of the generic pattern compared to launching multiple operator-level kernels using GPU accelerated libraries. We conclude by demonstrating the effectiveness of the approach in improving end-to-end performance on an entire ML algorithm.}},
  url = {https://doi.org/10.1145/2688500.2688521},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup844,
  title = {Race detection for Android applications}},
  author = {Maiya, Pallavi and Kanade, Aditya and Majumdar, Rupak}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Programming environments for smartphones expose a concurrency model that combines multi-threading and asynchronous event-based dispatch. While this enables the development of efficient and feature-rich applications, unforeseen thread interleavings coupled with non-deterministic reorderings of asynchronous tasks can lead to subtle concurrency errors in the applications.In this paper, we formalize the concurrency semantics of the Android programming model. We further define the happens-before relation for Android applications, and develop a dynamic race detection technique based on this relation. Our relation generalizes the so far independently studied happens-before relations for multi-threaded programs and single-threaded event-driven programs. Additionally, our race detection technique uses a model of the Android runtime environment to reduce false positives.We have implemented a tool called DroidRacer. It generates execution traces by systematically testing Android applications and detects data races by computing the happens-before relation on the traces. We analyzed 15 Android applications including popular applications such as Facebook, Twitter and K-9 Mail. Our results indicate that data races are prevalent in Android applications, and that DroidRacer is an effective tool to identify data races.}},
  url = {https://doi.org/10.1145/2594291.2594311},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup845,
  title = {Objects in concurrent logic programming languages}},
  author = {Kahn, Kenneth and Tribble, Eric Dean and Miller, Mark S. and Bobrow, Daniel G.}},
  year = {1986}},
  journal = {Conference Proceedings on Object-Oriented Programming Systems, Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Concurrent Prolog supports object-oriented programming with a clean semantics and additional programming constructs such as incomplete messages, unification, direct broadcasting, and concurrency synchronization [Shapiro 1983a]. While it provides excellent computational support, we claim it does not provide good notation for expressing the abstractions of object-oriented programming. We describe a preprocessor that remedies this problem. The resulting language, Vulcan, is then used as a behicle for exploring new variants of object-oriented programming which become possible in this framework.}},
  url = {https://doi.org/10.1145/28697.28721},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup846,
  title = {Thread modularity at many levels: a pearl in compositional verification}},
  author = {Hoenicke, Jochen and Majumdar, Rupak and Podelski, Andreas}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A thread-modular proof for the correctness of a concurrent program is based on an inductive and interference-free annotation of each thread. It is well-known that the corresponding proof system is not complete (unless one adds auxiliary variables). We describe a hierarchy of proof systems where each level k corresponds to a generalized notion of thread modularity (level 1 corresponds to the original notion). Each level is strictly more expressive than the previous. Further, each level precisely captures programs that can be proved using uniform Ashcroft invariants with k universal quantifiers. We demonstrate the usefulness of the hierarchy by giving a compositional proof of the Mach shootdown algorithm for TLB consistency. We show a proof at level 2 that shows the algorithm is correct for an arbitrary number of CPUs. However, there is no proof for the algorithm at level 1 which does not involve auxiliary state.}},
  url = {https://doi.org/10.1145/3009837.3009893},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup847,
  title = {To-many or to-one? all-in-one! efficient purely functional multi-maps with type-heterogeneous hash-tries}},
  author = {Steindorfer, Michael J. and Vinju, Jurgen J.}},
  year = {2018}},
  journal = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {An immutable multi-map is a many-to-many map data structure with expected fast insert and lookup operations. This data structure is used for applications processing graphs or many-to-many relations as applied in compilers, runtimes of programming languages, or in static analysis of object-oriented systems. Collection data structures are assumed to carefully balance execution time of operations with memory consumption characteristics and need to scale gracefully from a few elements to multiple gigabytes at least. When processing larger in-memory data sets the overhead of the data structure encoding itself becomes a memory usage bottleneck, dominating the overall performance. In this paper we propose AXIOM, a novel hash-trie data structure that allows for a highly efficient and type-safe multi-map encoding by distinguishing inlined values of singleton sets from nested sets of multi-mappings. AXIOM strictly generalizes over previous hash-trie data structures by supporting the processing of fine-grained type-heterogeneous content on the implementation level (while API and language support for type-heterogeneity are not scope of this paper). We detail the design and optimizations of AXIOM and further compare it against state-of-the-art immutable maps and multi-maps in Java, Scala and Clojure. We isolate key differences using microbenchmarks and validate the resulting conclusions on a case study in static analysis. AXIOM reduces the key-value storage overhead by 1.87x; with specializing and inlining across collection boundaries it improves by 5.1x.}},
  url = {https://doi.org/10.1145/3192366.3192420},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup848,
  title = {Generating sound and effective memory debuggers}},
  author = {Wang, Yan and Neamtiu, Iulian and Gupta, Rajiv}},
  year = {2013}},
  journal = {Proceedings of the 2013 International Symposium on Memory Management}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a new approach for constructing debuggers based on declarative specification of bug conditions and root causes, and automatic generation of debugger code. We illustrate our approach on several classes of bugs, memory or otherwise. For each bug class, bug conditions and their root cause are specified declaratively, in First-order logic, using 1 to 4 predicates. We employ a low-level operational semantics and abstract traces to permit concise bug specification and prove soundness. To facilitate locating bugs, we introduce a new concept of value propagation chains that reduce programmer burden by narrowing the fault to a handful of executed instructions (1 to 16 in our experiments). We employ automatic translation to generate the debugger implementation, which runs on top of the Pin infrastructure. Experiments with using our system on 7 versions of 4 real-world programs show that our approach is expressive, effective at finding bugs and their causes, and efficient. We believe that, using our approach, other kinds of declaratively-specified, provably-correct, auto-generated debuggers can be constructed with little effort.}},
  url = {https://doi.org/10.1145/2491894.2464159},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup849,
  title = {Monitoring and Debugging the Quality of Results in Approximate Programs}},
  author = {Ringenburg, Michael and Sampson, Adrian and Ackerman, Isaac and Ceze, Luis and Grossman, Dan}},
  year = {2015}},
  journal = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Energy efficiency is a key concern in the design of modern computer systems. One promising approach to energy-efficient computation, approximate computing, trades off output accuracy for significant gains in energy efficiency. However, debugging the actual cause of output quality problems in approximate programs is challenging. This paper presents dynamic techniques to debug and monitor the quality of approximate computations. We propose both offline debugging tools that instrument code to determine the key sources of output degradation and online approaches that monitor the quality of deployed applications.We present two offline debugging techniques and three online monitoring mechanisms. The first offline tool identifies correlations between output quality and the execution of individual approximate operations. The second tracks approximate operations that flow into a particular value. Our online monitoring mechanisms are complementary approaches designed for detecting quality problems in deployed applications, while still maintaining the energy savings from approximation.We present implementations of our techniques and describe their usage with seven applications. Our online monitors control output quality while still maintaining significant energy efficiency gains, and our offline tools provide new insights into the effects of approximation on output quality.}},
  url = {https://doi.org/10.1145/2694344.2694365},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup850,
  title = {The ramifications of sharing in data structures}},
  author = {Hobor, Aquinas and Villard, Jules}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Programs manipulating mutable data structures with intrinsic sharing present a challenge for modular verification. Deep aliasing inside data structures dramatically complicates reasoning in isolation over parts of these objects because changes to one part of the structure (say, the left child of a dag node) can affect other parts (the right child or some of its descendants) that may point into it. The result is that finding intuitive and compositional proofs of correctness is usually a struggle. We propose a compositional proof system that enables local reasoning in the presence of sharing.While the AI "frame problem" elegantly captures the reasoning required to verify programs without sharing, we contend that natural reasoning about programs with sharing instead requires an answer to a different and more challenging AI problem, the "ramification problem": reasoning about the indirect consequences of actions. Accordingly, we present a RAMIFY proof rule that attacks the ramification problem head-on and show how to reason with it. Our framework is valid in any separation logic and permits sound compositional and local reasoning in the context of both specified and unspecified sharing. We verify the correctness of a number of examples, including programs that manipulate dags, graphs, and overlaid data structures in nontrivial ways.}},
  url = {https://doi.org/10.1145/2429069.2429131},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup851,
  title = {Customizable gradual polymorphic effects for Scala}},
  author = {Toro, Mat\'{\i}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Despite their obvious advantages in terms of static reasoning, the adoption of effect systems is still rather limited in practice. Recent advances such as generic effect systems, lightweight effect polymorphism, and gradual effect checking, all represent promising steps towards making effect systems suitable for widespread use. However, no existing system combines these approaches: the theory of gradual polymorphic effects has not been developed, and there are no implementations of gradual effect checking. In addition, a limiting factor in the adoption of effect systems is their unsuitability for localized and customized effect disciplines. This paper addresses these issues by presenting the first implementation of gradual effect checking, for Scala, which supports both effect polymorphism and a domain-specific language called Effscript to declaratively define and customize effect disciplines. We report on the theory, implementation, and practical application of the system.}},
  url = {https://doi.org/10.1145/2814270.2814315},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup852,
  title = {DReX: A Declarative Language for Efficiently Evaluating Regular String Transformations}},
  author = {Alur, Rajeev and D'Antoni, Loris and Raghothaman, Mukund}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present DReX, a declarative language that can express all regular string-to string transformations, and can still be efficiently evaluated. The class of regular string transformations has a robust theoretical foundation including multiple characterizations, closure properties, and decidable analysis questions, and admits a number of string operations such as insertion, deletion, substring swap, and reversal. Recent research has led to a characterization of regular string transformations using a primitive set of function combinators analogous to the definition of regular languages using regular expressions. While these combinators form the basis for the language DReX proposed in this paper, our main technical focus is on the complexity of evaluating the output of a DReX program on a given input string. It turns out that the natural evaluation algorithm involves dynamic programming, leading to complexity that is cubic in the length of the input string. Our main contribution is identifying a consistency restriction on the use of combinators in DReX programs, and a single-pass evaluation algorithm for consistent programs with time complexity that is linear in the length of the input string and polynomial in the size of the program. We show that the consistency restriction does not limit the expressiveness, and whether a DReX program is consistent can be checked efficiently. We report on a prototype implementation, and evaluate it using a representative set of text processing tasks.}},
  url = {https://doi.org/10.1145/2676726.2676981},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup853,
  title = {Tracing piece by piece: affordable debugging for lazy functional languages}},
  author = {Nilsson, Henrik}},
  year = {1999}},
  journal = {Proceedings of the Fourth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The advantage of lazy functional languages is that programs may be written declaratively without specifying the exact evaluation order. The ensuing order of evaluation can however be quite involved which makes it difficult to debug such programs using traditional, operational techniques. A solution is to trace the computation in a way which focuses on the declarative aspects and hides irrelevant operational details. The main problem with this approach is the immense cost in time and space of tracing large computations. Dealing with these performance issues is thus the key to practical, general purpose debuggers for lazy functional languages. In this paper we show that computing partial traces on demand by re-executing the traced program is a viable way to overcome these difficulties. This allows any program to be traced using only a fixed amount of extra storage. Since it takes a lot of time to build a complete trace, most of which is wasted since only a fraction of a typical trace is investigated during debugging, partial tracing and repeated re-execution is also attractive from a time perspective. Performance figures are presented to substantiate our claims.}},
  url = {https://doi.org/10.1145/317636.317782},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup854,
  title = {Automatic induction proofs of data-structures in imperative programs}},
  author = {Chu, Duc-Hiep and Jaffar, Joxan and Trinh, Minh-Thai}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We consider the problem of automated reasoning about dynamically manipulated data structures. Essential properties are encoded as predicates whose definitions are formalized via user-defined recursive rules. Traditionally, proving relationships between such properties is limited to the unfold-and-match (U+M) paradigm which employs systematic transformation steps of folding/unfolding the rules. A proof, using U+M, succeeds when we find a sequence of transformations that produces a final formula which is obviously provable by simply matching terms. Our contribution here is the addition of the fundamental principle of induction to this automated process. We first show that some proof obligations that are dynamically generated in the process can be used as induction hypotheses in the future, and then we show how to use these hypotheses in an induction step which generates a new proof obligation aside from those obtained by using the fold/unfold operations. While the adding of induction is an obvious need in general, no automated method has managed to include this in a systematic and general way. The main reason for this is the problem of avoiding circular reasoning. We overcome this with a novel checking condition. In summary, our contribution is a proof method which – beyond U+M – performs automatic formula re-writing by treating previously encountered obligations in each proof path as possible induction hypotheses. In the practical evaluation part of this paper, we show how the commonly used technique of using unproven lemmas can be avoided, using realistic benchmarks. This not only removes the current burden of coming up with the appropriate lemmas, but also significantly boosts up the verification process, since lemma applications, coupled with unfolding, often induce a large search space. In the end, our method can automatically reason about a new class of formulas arising from practical program verification.}},
  url = {https://doi.org/10.1145/2737924.2737984},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup855,
  title = {Eliminating distinctions of class: using prototypes to model virtual classes}},
  author = {Hutchins, DeLesley}},
  year = {2006}},
  journal = {Proceedings of the 21st Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In mainstream OO languages, inheritance can be used to add new methods, or to override existing methods. Virtual classes and feature oriented programming are techniques which extend the mechanism of inheritance so that it is possible to refine nested classes as well. These techniques are attractive for programming in the large, because inheritance becomes a tool for manipulating whole class hierarchies rather than individual classes. Nevertheless, it has proved difficult to design static type systems for virtual classes, because virtual classes introduce dependent types. The compile-time type of an expression may depend on the run-time values of objects in that expression.We present a formal object calculus which implements virtual classes in a type-safe manner. Our type system uses a novel technique based on prototypes, which blur the distinction between compile-time and run-time. At run-time, prototypes act as objects, and they can be used in ordinary computations. At compile-time, they act as types. Prototypes are similar in power to dependent types, and subtyping is shown to be a form of partial evaluation. We prove that prototypes are type-safe but undecidable, and briefly outline a decidable semi-algorithm for dealing with them.}},
  url = {https://doi.org/10.1145/1167473.1167475},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup856,
  title = {Genesis: synthesizing forwarding tables in multi-tenant networks}},
  author = {Subramanian, Kausik and D'Antoni, Loris and Akella, Aditya}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Operators in multi-tenant cloud datacenters require support for diverse and complex end-to-end policies, such as, reachability, middlebox traversals, isolation, traffic engineering, and network resource management. We present Genesis, a datacenter network management system which allows policies to be specified in a declarative manner without explicitly programming the network data plane. Genesis tackles the problem of enforcing policies by synthesizing switch forwarding tables. It uses the formal foundations of constraint solving in combination with fast off-the-shelf SMT solvers. To improve synthesis performance, Genesis incorporates a novel search strategy that uses regular expressions to specify properties that leverage the structure of datacenter networks, and a divide-and-conquer synthesis procedure which exploits the structure of policy relationships. We have prototyped Genesis, and conducted experiments with a variety of workloads on real-world topologies to demonstrate its performance.}},
  url = {https://doi.org/10.1145/3009837.3009845},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup857,
  title = {An abstract memory functor for verified C static analyzers}},
  author = {Blazy, Sandrine and Laporte, Vincent and Pichardie, David}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Abstract interpretation provides advanced techniques to infer numerical invariants on programs. There is an abundant literature about numerical abstract domains that operate on scalar variables. This work deals with lifting these techniques to a realistic C memory model. We present an abstract memory functor that takes as argument any standard numerical abstract domain, and builds a memory abstract domain that finely tracks properties about memory contents, taking into account union types, pointer arithmetic and type casts. This functor is implemented and verified inside the Coq proof assistant with respect to the CompCert compiler memory model. Using the Coq extraction mechanism, it is fully executable and used by the Verasco C static analyzer.}},
  url = {https://doi.org/10.1145/2951913.2951937},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup858,
  title = {Shallow embedding of DSLs via online partial evaluation}},
  author = {Lei\ss{}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper investigates shallow embedding of DSLs by means of online partial evaluation. To this end, we present a novel online partial evaluator for continuation-passing style languages. We argue that it has, in contrast to prior work, a predictable termination policy that works well in practice. We present our approach formally using a continuation-passing variant of PCF and prove its termination properties. We evaluate our technique experimentally in the field of visual and high-performance computing and show that our evaluator produces highly specialized and efficient code for CPUs as well as GPUs that matches the performance of hand-tuned expert code.}},
  url = {https://doi.org/10.1145/2814204.2814208},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup859,
  title = {Semantic subtyping for imperative object-oriented languages}},
  author = {Ancona, Davide and Corradi, Andrea}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Semantic subtyping is an approach for defining sound and complete procedures to decide subtyping for expressive types, including union and intersection types; although it has been exploited especially in functional languages for XML based programming, recently it has been partially investigated in the context of object-oriented languages, and a sound and complete subtyping algorithm has been proposed for record types, but restricted to immutable fields, with union and recursive types interpreted coinductively to support cyclic objects. In this work we address the problem of studying semantic subtyping for imperative object-oriented languages, where fields can be mutable; in particular, we add read/write field annotations to record types, and, besides union, we consider intersection types as well, while maintaining coinductive interpretation of recursive types. In this way, we get a richer notion of type with a flexible subtyping relation, able to express a variety of type invariants useful for enforcing static guarantees for mutable objects. The addition of these features radically changes the defi- nition of subtyping, and, hence, the corresponding decision procedure, and surprisingly invalidates some subtyping laws that hold in the functional setting. We propose an intuitive model where mutable record val- ues contain type information to specify the values that can be correctly stored in fields. Such a model, and the correspond- ing subtyping rules, require particular care to avoid circularity between coinductive judgments and their negations which, by duality, have to be interpreted inductively. A sound and complete subtyping algorithm is provided, together with a prototype implementation.}},
  url = {https://doi.org/10.1145/2983990.2983992},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup860,
  title = {Stateless model checking with data-race preemption points}},
  author = {Blum, Ben and Gibson, Garth}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Stateless model checking is a powerful technique for testing concurrent programs, but suffers from exponential state space explosion when the test input parameters are too large. Several reduction techniques can mitigate this explosion, but even after pruning equivalent interleavings, the state space size is often intractable. Most prior tools are limited to preempting only on synchronization APIs, which reduces the space further, but can miss unsynchronized thread communication bugs. Data race detection, another concurrency testing approach, focuses on suspicious memory access pairs during a single test execution. It avoids concerns of state space size, but may report races that do not lead to observable failures, which jeopardizes a user’s willingness to use the analysis. We present Quicksand, a new stateless model checking framework which manages the exploration of many state spaces using different preemption points. It uses state space estimation to prioritize jobs most likely to complete in a fixed CPU budget, and it incorporates data-race analysis to add new preemption points on the fly. Preempting threads during a data race’s instructions can automatically classify the race as buggy or benign, and uncovers new bugs not reachable by prior model checkers. It also enables full verification of all possible schedules when every data race is verified as benign within the CPU budget. In our evaluation, Quicksand found 1.25x as many bugs and verified 4.3x as many tests compared to prior model checking approaches.}},
  url = {https://doi.org/10.1145/2983990.2984036},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup861,
  title = {A new verified compiler backend for CakeML}},
  author = {Tan, Yong Kiam and Myreen, Magnus O. and Kumar, Ramana and Fox, Anthony and Owens, Scott and Norrish, Michael}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We have developed and mechanically verified a new compiler backend for CakeML. Our new compiler features a sequence of intermediate languages that allows it to incrementally compile away high-level features and enables verification at the right levels of semantic detail. In this way, it resembles mainstream (unverified) compilers for strict functional languages. The compiler supports efficient curried multi-argument functions, configurable data representations, exceptions that unwind the call stack, register allocation, and more. The compiler targets several architectures: x86-64, ARMv6, ARMv8, MIPS-64, and RISC-V. In this paper, we present the overall structure of the compiler, including its 12 intermediate languages, and explain how everything fits together. We focus particularly on the interaction between the verification of the register allocator and the garbage collector, and memory representations. The entire development has been carried out within the HOL4 theorem prover.}},
  url = {https://doi.org/10.1145/2951913.2951924},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup862,
  title = {jStar: towards practical verification for java}},
  author = {Distefano, Dino and Parkinson J, Matthew J.}},
  year = {2008}},
  journal = {Proceedings of the 23rd ACM SIGPLAN Conference on Object-Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this paper we introduce a novel methodology for verifying a large set of Java programs which builds on recent theoretical developments in program verification: it combines the idea of abstract predicate families and the idea of symbolic execution and abstraction using separation logic. The proposed technology has been implemented in a new automatic verification system, called jStar, which combines theorem proving and abstract interpretation techniques. We demonstrate the effectiveness of our methodology by using jStar to verify example programs implementing four popular design patterns (subject/observer, visitor, factory, and pooling). Although these patterns are extensively used by object-oriented developers in real-world applications, so far they have been highly challenging for existing object-oriented verification techniques.}},
  url = {https://doi.org/10.1145/1449764.1449782},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup863,
  title = {On the relationship between higher-order recursion schemes and higher-order fixpoint logic}},
  author = {Kobayashi, Naoki and Lozes, \'{E}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We study the relationship between two kinds of higher-order extensions of model checking: HORS model checking, where models are extended to higher-order recursion schemes, and HFL model checking, where the logic is extedned to higher-order modal fixpoint logic. Those extensions have been independently studied until recently, and the former has been applied to higher-order program verification. We show that there exist (arguably) natural reductions between the two problems. To prove the correctness of the translation from HORS to HFL model checking, we establish a type-based characterization of HFL model checking, which should be of independent interest. The results reveal a close relationship between the two problems, enabling cross-fertilization of the two research threads.}},
  url = {https://doi.org/10.1145/3009837.3009854},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup864,
  title = {The anatomy of a loop: a story of scope and control}},
  author = {Shivers, Olin}},
  year = {2005}},
  journal = {Proceedings of the Tenth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Writing loops with tail-recursive function calls is the equivalent of writing them with goto's. Given that loop packages for Lisp-family languages have been around for over 20 years, it is striking that none have had much success in the Scheme world. I suggest the reason is that Scheme forces us to be precise about the scoping of the various variables introduced by our loop forms, something previous attempts to design ambitious loop forms have not managed to do.I present the design of a loop package for Scheme with a well-defined and natural scoping rule, based on a notion of control dominance that generalizes the standard lexical-scope rule of the λ-calculus. The new construct is powerful, clear, modular and extensible.The loop language is defined in terms of an underlying language for expressing control-flow graphs. This language itself has interesting properties as an intermediate representation.}},
  url = {https://doi.org/10.1145/1086365.1086368},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup865,
  title = {Modular and automated type-soundness verification for language extensions}},
  author = {Lorenzen, Florian and Erdweg, Sebastian}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Language extensions introduce high-level programming constructs that protect programmers from low-level details and repetitive tasks. For such an abstraction barrier to be sustainable, it is important that no errors are reported in terms of generated code. A typical strategy is to check the original user code prior to translation into a low-level encoding, applying the assumption that the translation does not introduce new errors. Unfortunately, such assumption is untenable in general, but in particular in the context of extensible programming languages, such as Racket or SugarJ, that allow regular programmers to define language extensions.In this paper, we present a formalism for building and automatically verifying the type-soundness of syntactic language extensions. To build a type-sound language extension with our formalism, a developer declares an extended syntax, type rules for the extended syntax, and translation rules into the (possibly further extended) base language. Our formalism then validates that the user-defined type rules are sufficient to guarantee that the code generated by the translation rules cannot contain any type errors. This effectively ensures that an initial type check prior to translation precludes type errors in generated code. We have implemented a core system in PLT Redex and we have developed a syntactically extensible variant of System Fw that we extend with let notation, monadic do blocks, and algebraic data types. Our formalism verifies the soundness of each extension automatically.}},
  url = {https://doi.org/10.1145/2500365.2500596},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup866,
  title = {Defunctionalized interpreters for programming languages}},
  author = {Danvy, Olivier}},
  year = {2008}},
  journal = {Proceedings of the 13th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This document illustrates how functional implementations of formal semantics (structural operational semantics, reduction semantics, small-step and big-step abstract machines, natural semantics, and denotational semantics) can be transformed into each other. These transformations were foreshadowed by Reynolds in "Definitional Interpreters for Higher-Order Programming Languages" for functional implementations of denotational semantics, natural semantics, and big-step abstract machines using closure conversion, CPS transformation, and defunctionalization. Over the last few years, the author and his students have further observed that functional implementations of small-step and of big-step abstract machines are related using fusion by fixed-point promotion and that functional implementations of reduction semantics and of small-step abstract machines are related using refocusing and transition compression. It furthermore appears that functional implementations of structural operational semantics and of reduction semantics are related as well, also using CPS transformation and defunctionalization. This further relation provides an element of answer to Felleisen's conjecture that any structural operational semantics can be expressed as a reduction semantics: for deterministic languages, a reduction semantics is a structural operational semantics in continuation style, where the reduction context is a defunctionalized continuation. As the defunctionalized counterpart of the continuation of a one-step reduction function, a reduction context represents the rest of the reduction, just as an evaluation context represents the rest of the evaluation since it is the defunctionalized counterpart of the continuation of an evaluation function.}},
  url = {https://doi.org/10.1145/1411204.1411206},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup867,
  title = {Linearity and PCF: a semantic insight!}},
  author = {Gaboardi, Marco and Paolini, Luca and Piccolo, Mauro}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Linearity is a multi-faceted and ubiquitous notion in the analysis and the development of programming language concepts. We study linearity in a denotational perspective by picking out programs that correspond to linear functions between coherence spaces.We introduce a language, named SlPCF*, that increases the higher-order expressivity of a linear core of PCF by means of new operators related to exception handling and parallel evaluation. SlPCF* allows us to program all the finite elements of the model and, consequently, it entails a full abstraction result that makes the reasoning on the equivalence between programs simpler.Denotational linearity provides also crucial information for the operational evaluation of programs. We formalize two evaluation machineries for the language. The first one is an abstract and concise operational semantics designed with the aim of explaining the new operators, and is based on an infinite-branching search of the evaluation space. The second one is more concrete and it prunes such a space, by exploiting the linear assumptions. This can also be regarded as a base for an implementation.}},
  url = {https://doi.org/10.1145/2034773.2034822},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup868,
  title = {Taming the wildcards: combining definition- and use-site variance}},
  author = {Altidor, John and Huang, Shan Shan and Smaragdakis, Yannis}},
  year = {2011}},
  journal = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Variance allows the safe integration of parametric and subtype polymorphism. Two flavors of variance, definition-site versus use-site variance, have been studied and have had their merits hotly debated. Definition-site variance (as in Scala and C#) offers simple type-instantiation rules, but causes fractured definitions of naturally invariant classes; Use-site variance (as in Java) offers simplicity in class definitions, yet complex type-instantiation rules that elude most programmers.We present a unifying framework for reasoning about variance. Our framework is quite simple and entirely denotational, that is, it evokes directly the definition of variance with a small core calculus that does not depend on specific type systems. This general framework can have multiple applications to combine the best of both worlds: for instance, it can be used to add use-site variance annotations to the Scala type system. We show one such application in detail: we extend the Java type system with a mechanism that modularly infers the definition-site variance of type parameters, while allowing use-site variance annotations on any type-instantiation.Applying our technique to six Java generic libraries (including the Java core library) shows that 20-58 (depending on the library) of generic definitions are inferred to have single-variance; 8-63\% of method signatures can be relaxed through this inference, and up to 91\% of existing wildcard annotations are unnecessary and can be elided.}},
  url = {https://doi.org/10.1145/1993498.1993569},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup869,
  title = {A kripke logical relation between ML and assembly}},
  author = {Hur, Chung-Kil and Dreyer, Derek}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {There has recently been great progress in proving the correctness of compilers for increasingly realistic languages with increasingly realistic runtime systems. Most work on this problem has focused on proving the correctness of a particular compiler, leaving open the question of how to verify the correctness of assembly code that is hand-optimized or linked together from the output of multiple compilers. This has led Benton and other researchers to propose more abstract, compositional notions of when a low-level program correctly realizes a high-level one. However, the state of the art in so-called "compositional compiler correctness" has only considered relatively simple high-level and low-level languages.In this paper, we propose a novel, extensional, compiler-independent notion of equivalence between high-level programs in an expressive, impure ML-like λ-calculus and low-level programs in an (only slightly) idealized assembly language. We define this equivalence by means of a biorthogonal, step-indexed, Kripke logical relation, which enables us to reason quite flexibly about assembly code that uses local state in a different manner than the high-level code it implements (e.g. self-modifying code). In contrast to prior work, we factor our relation in a symmetric, language-generic fashion, which helps to simplify and clarify the formal presentation, and we also show how to account for the presence of a garbage collector. Our approach relies on recent developments in Kripke logical relations for ML-like languages, in particular the idea of possible worlds as state transition systems.}},
  url = {https://doi.org/10.1145/1926385.1926402},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup870,
  title = {Tarazu: optimizing MapReduce on heterogeneous clusters}},
  author = {Ahmad, Faraz and Chakradhar, Srimat T. and Raghunathan, Anand and Vijaykumar, T. N.}},
  year = {2012}},
  journal = {Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Data center-scale clusters are evolving towards heterogeneous hardware for power, cost, differentiated price-performance, and other reasons. MapReduce is a well-known programming model to process large amount of data on data center-scale clusters. Most MapReduce implementations have been designed and optimized for homogeneous clusters. Unfortunately, these implementations perform poorly on heterogeneous clusters (e.g., on a 90-node cluster that contains 10 Xeon-based servers and 80 Atom-based servers, Hadoop performs worse than on 10-node Xeon-only or 80-node Atom-only homogeneous sub-clusters for many of our benchmarks). This poor performance remains despite previously proposed optimizations related to management of straggler tasks. In this paper, we address MapReduce's poor performance on heterogeneous clusters. Our first contribution is that the poor performance is due to two key factors: (1) the non-intuitive effect that MapReduce's built-in load balancing results in excessive and bursty network communication during the Map phase, and (2) the intuitive effect that the heterogeneity amplifies load imbalance in the Reduce computation. Our second contribution is Tarazu, a suite of optimizations to improve MapReduce performance on heterogeneous clusters. Tarazu consists of (1) Communication-Aware Load Balancing of Map computation (CALB) across the nodes, (2) Communication-Aware Scheduling of Map computation (CAS) to avoid bursty network traffic and (3) Predictive Load Balancing of Reduce computation (PLB) across the nodes. Using the above 90-node cluster, we show that Tarazu significantly improves performance over a baseline of Hadoop with straightforward tuning for hardware heterogeneity.}},
  url = {https://doi.org/10.1145/2150976.2150984},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup871,
  title = {Call graphs for languages with parametric polymorphism}},
  author = {Petrashko, Dmitry and Ureche, Vlad and Lhot\'{a}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The performance of contemporary object oriented languages depends on optimizations such as devirtualization, inlining, and specialization, and these in turn depend on precise call graph analysis. Existing call graph analyses do not take advantage of the information provided by the rich type systems of contemporary languages, in particular generic type arguments. Many existing approaches analyze Java bytecode, in which generic types have been erased. This paper shows that this discarded information is actually very useful as the context in a context-sensitive analysis, where it significantly improves precision and keeps the running time small. Specifically, we propose and evaluate call graph construction algorithms in which the contexts of a method are (i) the type arguments passed to its type parameters, and (ii) the static types of the arguments passed to its term parameters. The use of static types from the caller as context is effective because it allows more precise dispatch of call sites inside the callee. Our evaluation indicates that the average number of contexts required per method is small. We implement the analysis in the Dotty compiler for Scala, and evaluate it on programs that use the type-parametric Scala collections library and on the Dotty compiler itself. The context-sensitive analysis runs 1.4x faster than a context-insensitive one and discovers 20\% more monomorphic call sites at the same time. When applied to method specialization, the imprecision in a context-insensitive call graph would require the average method to be cloned 22 times, whereas the context-sensitive call graph indicates a much more practical 1.00 to 1.50 clones per method. We applied the proposed analysis to automatically specialize generic methods. The resulting automatic transformation achieves the same performance as state-of-the-art techniques requiring manual annotations, while reducing the size of the generated bytecode by up to 5x.}},
  url = {https://doi.org/10.1145/2983990.2983991},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup872,
  title = {Cache and I/O efficent functional algorithms}},
  author = {Blelloch, Guy E. and Harper, Robert}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The widely studied I/O and ideal-cache models were developed to account for the large difference in costs to access memory at different levels of the memory hierarchy. Both models are based on a two level memory hierarchy with a fixed size primary memory(cache) of size M, an unbounded secondary memory organized in blocks of size B. The cost measure is based purely on the number of block transfers between the primary and secondary memory. All other operations are free. Many algorithms have been analyzed in these models and indeed these models predict the relative performance of algorithms much more accurately than the standard RAM model. The models, however, require specifying algorithms at a very low level requiring the user to carefully lay out their data in arrays in memory and manage their own memory allocation.In this paper we present a cost model for analyzing the memory efficiency of algorithms expressed in a simple functional language. We show how some algorithms written in standard forms using just lists and trees (no arrays) and requiring no explicit memory layout or memory management are efficient in the model. We then describe an implementation of the language and show provable bounds for mapping the cost in our model to the cost in the ideal-cache model. These bound imply that purely functional programs based on lists and trees with no special attention to any details of memory layout can be as asymptotically as efficient as the carefully designed imperative I/O efficient algorithms. For example we describe an O(n_B logM/Bn_B)cost sorting algorithm, which is optimal in the ideal cache and I/O models.}},
  url = {https://doi.org/10.1145/2429069.2429077},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup873,
  title = {A tested semantics for getters, setters, and eval in JavaScript}},
  author = {Politz, Joe Gibbs and Carroll, Matthew J. and Lerner, Benjamin S. and Pombrio, Justin and Krishnamurthi, Shriram}},
  year = {2012}},
  journal = {Proceedings of the 8th Symposium on Dynamic Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present S5, a semantics for the strict mode of the ECMAScript 5.1 (JavaScript) programming language. S5 shrinks the large source language into a manageable core through an implemented transformation. The resulting specification has been tested against real-world conformance suites for the language. This paper focuses on two aspects of S5: accessors (getters and setters) and eval. Since these features are complex and subtle in JavaScript, they warrant special study. Variations on both features are found in several other programming languages, so their study is likely to have broad applicability.}},
  url = {https://doi.org/10.1145/2384577.2384579},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup874,
  title = {Pattern matching without K}},
  author = {Cockx, Jesper and Devriese, Dominique and Piessens, Frank}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Dependent pattern matching is an intuitive way to write programs and proofs in dependently typed languages. It is reminiscent of both pattern matching in functional languages and case analysis in on-paper mathematics. However, in general it is incompatible with new type theories such as homotopy type theory (HoTT). As a consequence, proofs in such theories are typically harder to write and to understand. The source of this incompatibility is the reliance of dependent pattern matching on the so-called K axiom - also known as the uniqueness of identity proofs - which is inadmissible in HoTT. The Agda language supports an experimental criterion to detect definitions by pattern matching that make use of the K axiom, but so far it lacked a formal correctness proof.In this paper, we propose a new criterion for dependent pattern matching without K, and prove it correct by a translation to eliminators in the style of Goguen et al. (2006). Our criterion both allows more good definitions than existing proposals, and solves a previously undetected problem in the criterion offered by Agda. It has been implemented in Agda and is the first to be supported by a formal proof. Thus it brings the benefits of dependent pattern matching to contexts where we cannot assume K, such as HoTT. It also points the way to new forms of dependent pattern matching, for example on higher inductive types.}},
  url = {https://doi.org/10.1145/2628136.2628139},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup875,
  title = {Type-based useless variable elimination}},
  author = {Kobayashi, Naoki}},
  year = {1999}},
  journal = {Proceedings of the 2000 ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Based Program Manipulation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Useless variable elimination [25] is a transformation that eliminates variables whose values contribute nothing to the final outcome of a computation. We present a type-based method for useless variable elimination and prove its correctness. The algorithm is a surprisingly simple extension of the usual type reconstruction algorithm. Our method seems more attractive than other methods for useless variable elimination in several respects. First, it is simple, so that the proof of the correctness is clear and the method can be easily extended to deal with a polymorphic language. Second, it is efficient: it runs in time almost linear in the size of an input expression for a simply-typed λ-calculus, while Wand and Siveroni's 0CFA-based method may require a cubic time. Moreover, our transformation is optimal in a certain sense among those that preserve well-typedness, both for the simply-typed language and for an ML-style polymorphically-typed language. On the other hand, Wand and Siveroni's method is not optimal for the polymophically-typed language.}},
  url = {https://doi.org/10.1145/328690.328702},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup876,
  title = {Exploiting vector instructions with generalized stream fusion}},
  author = {Mainland, Geoffrey and Leshchinskiy, Roman and Peyton Jones, Simon}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Stream fusion is a powerful technique for automatically transforming high-level sequence-processing functions into efficient implementations. It has been used to great effect in Haskell libraries for manipulating byte arrays, Unicode text, and unboxed vectors. However, some operations, like vector append, still do not perform well within the standard stream fusion framework. Others, like SIMD computation using the SSE and AVX instructions available on modern x86 chips, do not seem to fit in the framework at all. In this paper we introduce generalized stream fusion, which solves these issues. The key insight is to bundle together multiple stream representations, each tuned for a particular class of stream consumer. We also describe a stream representation suited for efficient computation with SSE instructions. Our ideas are implemented in modified versions of the GHC compiler and vector library. Benchmarks show that high-level Haskell code written using our compiler and libraries can produce code that is faster than both compiler- and hand-vectorized C.}},
  url = {https://doi.org/10.1145/2500365.2500601},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup877,
  title = {Certified concurrent abstraction layers}},
  author = {Gu, Ronghui and Shao, Zhong and Kim, Jieung and Wu, Xiongnan (Newman) and Koenig, J\'{e}},
  year = {2018}},
  journal = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Concurrent abstraction layers are ubiquitous in modern computer systems because of the pervasiveness of multithreaded programming and multicore hardware. Abstraction layers are used to hide the implementation details (e.g., fine-grained synchronization) and reduce the complex dependencies among components at different levels of abstraction. Despite their obvious importance, concurrent abstraction layers have not been treated formally. This severely limits the applicability of layer-based techniques and makes it difficult to scale verification across multiple concurrent layers. In this paper, we present CCAL---a fully mechanized programming toolkit developed under the CertiKOS project---for specifying, composing, compiling, and linking certified concurrent abstraction layers. CCAL consists of three technical novelties: a new game-theoretical, strategy-based compositional semantic model for concurrency (and its associated program verifiers), a set of formal linking theorems for composing multithreaded and multicore concurrent layers, and a new CompCertX compiler that supports certified thread-safe compilation and linking. The CCAL toolkit is implemented in Coq and supports layered concurrent programming in both C and assembly. It has been successfully applied to build a fully certified concurrent OS kernel with fine-grained locking.}},
  url = {https://doi.org/10.1145/3192366.3192381},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup878,
  title = {Specialization through dynamic staging}},
  author = {Danilewski, Piotr and K\"{o}},
  year = {2014}},
  journal = {Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Partial evaluation allows for specialization of program fragments. This can be realized by staging, where one fragment is executed earlier than its surrounding code. However, taking advantage of these capabilities is often a cumbersome endeavor. In this paper, we present a new metaprogramming concept using staging parameters that are first-class citizen entities and define the order of execution of the program. Staging parameters can be used to define MetaML-like quotations, but can also allow stages to be created and resolved dynamically. The programmer can write generic, polyvariant code which can be reused in the context of different stages. We demonstrate how our approach can be used to define and apply domain-specific optimizations. Our implementation of the proposed metaprogramming concept generates code which is on a par with templated C++ code in terms of execution time.}},
  url = {https://doi.org/10.1145/2658761.2658774},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup879,
  title = {From clarity to efficiency for distributed algorithms}},
  author = {Liu, Yanhong A. and Stoller, Scott D. and Lin, Bo and Gorbovitski, Michael}},
  year = {2012}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper describes a very high-level language for clear description of distributed algorithms and optimizations necessary for generating efficient implementations. The language supports high-level control flows where complex synchronization conditions can be expressed using high-level queries, especially logic quantifications, over message history sequences. Unfortunately, the programs would be extremely inefficient, including consuming unbounded memory, if executed straightforwardly.We present new optimizations that automatically transform complex synchronization conditions into incremental updates of necessary auxiliary values as messages are sent and received. The core of the optimizations is the first general method for efficient implementation of logic quantifications. We have developed an operational semantics of the language, implemented a prototype of the compiler and the optimizations, and successfully used the language and implementation on a variety of important distributed algorithms.}},
  url = {https://doi.org/10.1145/2384616.2384645},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup880,
  title = {Plan B: a buffered memory model for Java}},
  author = {Demange, Delphine and Laporte, Vincent and Zhao, Lei and Jagannathan, Suresh and Pichardie, David and Vitek, Jan}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Recent advances in verification have made it possible to envision trusted implementations of real-world languages. Java with its type-safety and fully specified semantics would appear to be an ideal candidate; yet, the complexity of the translation steps used in production virtual machines have made it a challenging target for verifying compiler technology. One of Java's key innovations, its memory model, poses significant obstacles to such an endeavor. The Java Memory Model is an ambitious attempt at specifying the behavior of multithreaded programs in a portable, hardware agnostic, way. While experts have an intuitive grasp of the properties that the model should enjoy, the specification is complex and not well-suited for integration within a verifying compiler infrastructure. Moreover, the specification is given in an axiomatic style that is distant from the intuitive reordering-based reasonings traditionally used to justify or rule out behaviors, and ill suited to the kind of operational reasoning one would expect to employ in a compiler. This paper takes a step back, and introduces a Buffered Memory Model (BMM) for Java. We choose a pragmatic point in the design space sacrificing generality in favor of a model that is fully characterized in terms of the reorderings it allows, amenable to formal reasoning, and which can be efficiently applied to a specific hardware family, namely x86 multiprocessors. Although the BMM restricts the reorderings compilers are allowed to perform, it serves as the key enabling device to achieving a verification pathway from bytecode to machine instructions. Despite its restrictions, we show that it is backwards compatible with the Java Memory Model and that it does not cripple performance on TSO architectures.}},
  url = {https://doi.org/10.1145/2429069.2429110},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup881,
  title = {Quantitative Interprocedural Analysis}},
  author = {Chatterjee, Krishnendu and Pavlogiannis, Andreas and Velner, Yaron}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We consider the quantitative analysis problem for interprocedural control-flow graphs (ICFGs). The input consists of an ICFG, a positive weight function that assigns every transition a positive integer-valued number, and a labelling of the transitions (events) as good, bad, and neutral events. The weight function assigns to each transition a numerical value that represents a measure of how good or bad an event is. The quantitative analysis problem asks whether there is a run of the ICFG where the ratio of the sum of the numerical weights of good events versus the sum of weights of bad events in the long-run is at least a given threshold (or equivalently, to compute the maximal ratio among all valid paths in the ICFG). The quantitative analysis problem for ICFGs can be solved in polynomial time, and we present an efficient and practical algorithm for the problem.We show that several problems relevant for static program analysis, such as estimating the worst-case execution time of a program or the average energy consumption of a mobile application, can be modeled in our framework. We have implemented our algorithm as a tool in the Java Soot framework. We demonstrate the effectiveness of our approach with two case studies. First, we show that our framework provides a sound approach (no false positives) for the analysis of inefficiently-used containers. Second, we show that our approach can also be used for static profiling of programs which reasons about methods that are frequently invoked. Our experimental results show that our tool scales to relatively large benchmarks, and discovers relevant and useful information that can be used to optimize performance of the programs.}},
  url = {https://doi.org/10.1145/2676726.2676968},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup882,
  title = {swSpTRSV: a fast sparse triangular solve with sparse level tile layout on sunway architectures}},
  author = {Wang, Xinliang and Liu, Weifeng and Xue, Wei and Wu, Li}},
  year = {2018}},
  journal = {Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Sparse triangular solve (SpTRSV) is one of the most important kernels in many real-world applications. Currently, much research on parallel SpTRSV focuses on level-set construction for reducing the number of inter-level synchronizations. However, the out-of-control data reuse and high cost for global memory or shared cache access in inter-level synchronization have been largely neglected in existing work.In this paper, we propose a novel data layout called Sparse Level Tile to make all data reuse under control, and design a Producer-Consumer pairing method to make any inter-level synchronization only happen in very fast register communication. We implement our data layout and algorithms on an SW26010 many-core processor, which is the main building-block of the current world fastest supercomputer Sunway Taihulight. The experimental results of testing all 2057 square matrices from the Florida Matrix Collection show that our method achieves an average speedup of 6.9 and the best speedup of 38.5 over parallel level-set method. Our method also outperforms the latest methods on a KNC many-core processor in 1856 matrices and the latest methods on a K80 GPU in 1672 matrices, respectively.}},
  url = {https://doi.org/10.1145/3178487.3178513},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup883,
  title = {A practical framework for type inference error explanation}},
  author = {Loncaric, Calvin and Chandra, Satish and Schlesinger, Cole and Sridharan, Manu}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many languages have support for automatic type inference. But when inference fails, the reported error messages can be unhelpful, highlighting a code location far from the source of the problem. Several lines of work have emerged proposing error reports derived from correcting sets: a set of program points that, when fixed, produce a well-typed program. Unfortunately, these approaches are tightly tied to specific languages; targeting a new language requires encoding a type inference algorithm for the language in a custom constraint system specific to the error reporting tool. We show how to produce correcting set-based error reports by leveraging existing type inference implementations, easing the burden of adoption and, as type inference algorithms tend to be efficient in practice, producing error reports of comparable quality to similar error reporting tools orders of magnitude faster. Many type inference algorithms are already formulated as dual phases of type constraint generation and solving; rather than (re)implementing type inference in an error explanation tool, we isolate the solving phase and treat it as an oracle for solving typing constraints. Given any set of typing constraints, error explanation proceeds by iteratively removing conflicting constraints from the initial constraint set until discovering a subset on which the solver succeeds; the constraints removed form a correcting set. Our approach is agnostic to the semantics of any particular language or type system, instead leveraging the existing type inference engine to give meaning to constraints.}},
  url = {https://doi.org/10.1145/2983990.2983994},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup884,
  title = {Lightweight monadic programming in ML}},
  author = {Swamy, Nikhil and Guts, Nataliya and Leijen, Daan and Hicks, Michael}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many useful programming constructions can be expressed as monads. Examples include probabilistic modeling, functional reactive programming, parsing, and information flow tracking, not to mention effectful functionality like state and I/O. In this paper, we present a type-based rewriting algorithm to make programming with arbitrary monads as easy as using ML's built-in support for state and I/O. Developers write programs using monadic values of type m τ as if they were of type τ, and our algorithm inserts the necessary binds, units, and monad-to-monad morphisms so that the program type checks. Our algorithm, based on Jones' qualified types, produces principal types. But principal types are sometimes problematic: the program's semantics could depend on the choice of instantiation when more than one instantiation is valid. In such situations we are able to simplify the types to remove any ambiguity but without adversely affecting typability; thus we can accept strictly more programs. Moreover, we have proved that this simplification is efficient (linear in the number of constraints) and coherent: while our algorithm induces a particular rewriting, all related rewritings will have the same semantics. We have implemented our approach for a core functional language and applied it successfully to simple examples from the domains listed above, which are used as illustrations throughout the paper.}},
  url = {https://doi.org/10.1145/2034773.2034778},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup885,
  title = {Translation validation of loop and arithmetic transformations in the presence of recurrences}},
  author = {Banerjee, Kunal and Mandal, Chittaranjan and Sarkar, Dipankar}},
  year = {2016}},
  journal = {Proceedings of the 17th ACM SIGPLAN/SIGBED Conference on Languages, Compilers, Tools, and Theory for Embedded Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Compiler optimization of array-intensive programs involves extensive application of loop transformations and arithmetic transformations. Hence, translation validation of array-intensive programs requires manipulation of intervals of integers (representing domains of array indices) and relations over such intervals to account for loop transformations and simplification of arithmetic expressions to handle arithmetic transformations. A major obstacle for verification of such programs is posed by the presence of recurrences, whereby an element of an array gets defined in a statement S inside a loop in terms of some other element(s) of the same array which have been previously defined through the same statement S. Recurrences lead to cycles in the data-dependence graph of a program which make dependence analyses and simplifications (through closed-form representations) of the data transformations difficult. Another technique which works better for recurrences does not handle arithmetic transformations. In this work, array data-dependence graphs (ADDGs) are used to represent both the original and the optimized versions of the program and a validation scheme is proposed where the cycles due to recurrences in the ADDGs are suitably abstracted as acyclic subgraphs. Thus, this work provides a unified equivalence checking framework to handle loop and arithmetic transformations along with most of the recurrences -- this combination of features had not been achieved by a single verification technique earlier.}},
  url = {https://doi.org/10.1145/2907950.2907954},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup886,
  title = {Complexity of pattern-based verification for multithreaded programs}},
  author = {Esparza, Javier and Ganty, Pierre}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Pattern-based verification checks the correctness of the program executions that follow a given pattern, a regular expression over the alphabet of program transitions of the form w1* ... wn*. For multithreaded programs, the alphabet of the pattern is given by the synchronization operations between threads. We study the complexity of pattern-based verification for abstracted multithreaded programs in which, as usual in program analysis, conditions have been replaced by nondeterminism (the technique works also for boolean programs). While unrestricted verification is undecidable for abstracted multithreaded programs with recursive procedures and PSPACE-complete for abstracted multithreaded while-programs, we show that pattern-based verification is NP-complete for both classes. We then conduct a multiparameter analysis in which we study the complexity in the number of threads, the number of procedures per thread, the size of the procedures, and the size of the pattern. We first show that no algorithm for pattern-based verification can be polynomial in the number of threads, procedures per thread, or the size of the pattern (unless P=NP). Then, using recent results about Parikh images of regular languages and semilinear sets, we present an algorithm exponential in the number of threads, procedures per thread, and size of the pattern, but polynomial in the size of the procedures.}},
  url = {https://doi.org/10.1145/1926385.1926443},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup887,
  title = {Typestate-based semantic code search over partial programs}},
  author = {Mishne, Alon and Shoham, Sharon and Yahav, Eran}},
  year = {2012}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a novel code search approach for answering queries focused on API-usage with code showing how the API should be used. To construct a search index, we develop new techniques for statically mining and consolidating temporal API specifications from code snippets. In contrast to existing semantic-based techniques, our approach handles partial programs in the form of code snippets. Handling snippets allows us to consume code from various sources such as parts of open source projects, educational resources (e.g. tutorials), and expert code sites. To handle code snippets, our approach (i) extracts a possibly partial temporal specification from each snippet using a relatively precise static analysis tracking a generalized notion of typestate, and (ii) consolidates the partial temporal specifications, combining consistent partial information to yield consolidated temporal specifications, each of which captures a full(er) usage scenario.To answer a search query, we define a notion of relaxed inclusion matching a query against temporal specifications and their corresponding code snippets.We have implemented our approach in a tool called PRIME and applied it to search for API usage of several challenging APIs. PRIME was able to analyze and consolidate thousands of snippets per tested API, and our results indicate that the combination of a relatively precise analysis and consolidation allowed PRIME to answer challenging queries effectively.}},
  url = {https://doi.org/10.1145/2384616.2384689},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup888,
  title = {Deterministic parallel random-number generation for dynamic-multithreading platforms}},
  author = {Leiserson, Charles E. and Schardl, Tao B. and Sukha, Jim}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Existing concurrency platforms for dynamic multithreading do not provide repeatable parallel random-number generators. This paper proposes that a mechanism called pedigrees be built into the runtime system to enable efficient deterministic parallel random-number generation. Experiments with the open-source MIT Cilk runtime system show that the overhead for maintaining pedigrees is negligible. Specifically, on a suite of 10 benchmarks, the relative overhead of Cilk with pedigrees to the original Cilk has a geometric mean of less than 1\%.We persuaded Intel to modify its commercial C/C++ compiler, which provides the Cilk Plus concurrency platform, to include pedigrees, and we built a library implementation of a deterministic parallel random-number generator called DotMix that compresses the pedigree and then "RC6-mixes" the result. The statistical quality of DotMix is comparable to that of the popular Mersenne twister, but somewhat slower than a nondeterministic parallel version of this efficient and high-quality serial random-number generator. The cost of calling DotMix depends on the "spawn depth" of the invocation. For a naive Fibonacci calculation with n=40 that calls DotMix in every node of the computation, this "price of determinism" is a factor of 2.65 in running time, but for more realistic applications with less intense use of random numbers -- such as a maximal-independent-set algorithm, a practical samplesort program, and a Monte Carlo discrete-hedging application from QuantLib -- the observed "price" was less than 5\%. Moreover, even if overheads were several times greater, applications using DotMix should be amply fast for debugging purposes, which is a major reason for desiring repeatability.}},
  url = {https://doi.org/10.1145/2145816.2145841},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup889,
  title = {Symbolic execution for memory consumption analysis}},
  author = {Chu, Duc-Hiep and Jaffar, Joxan and Maghareh, Rasool}},
  year = {2016}},
  journal = {Proceedings of the 17th ACM SIGPLAN/SIGBED Conference on Languages, Compilers, Tools, and Theory for Embedded Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {With the advances in both hardware and software of embedded systems in the past few years, dynamic memory allocation can now be safely used in embedded software. As a result, the need to develop methods to avoid heap overflow errors in safety-critical embedded systems has increased. Resource analysis of imperative programs with non-regular loop patterns and signed integers, to support both memory allocation and deallocation, has long been an open problem. Existing methods can generate symbolic bounds that are parametric w.r.t. the program inputs; such bounds, however, are imprecise in the presence of non-regular loop patterns. In this paper, we present a worst-case memory consumption analysis, based upon the framework of symbolic execution. Our assumption is that loops (and recursions) of to-be-analyzed programs are indeed bounded. We then can exhaustively unroll loops and the memory consumption of each iteration can be precisely computed and summarized for aggregation. Because of path-sensitivity, our algorithm generates more precise bounds. Importantly, we demonstrate that by introducing a new concept of reuse, symbolic execution scales to a set of realistic benchmark programs.}},
  url = {https://doi.org/10.1145/2907950.2907955},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup890,
  title = {Operational semantics using the partiality monad}},
  author = {Danielsson, Nils Anders}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The operational semantics of a partial, functional language is often given as a relation rather than as a function. The latter approach is arguably more natural: if the language is functional, why not take advantage of this when defining the semantics? One can immediately see that a functional semantics is deterministic and, in a constructive setting, computable.This paper shows how one can use the coinductive partiality monad to define big-step or small-step operational semantics for lambda-calculi and virtual machines as total, computable functions (total definitional interpreters). To demonstrate that the resulting semantics are useful type soundness and compiler correctness results are also proved. The results have been implemented and checked using Agda, a dependently typed programming language and proof assistant.}},
  url = {https://doi.org/10.1145/2364527.2364546},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup891,
  title = {Meta-programming for cross-domain tensor optimizations}},
  author = {Susungi, Adilla and Rink, Norman A. and Cohen, Albert and Castrillon, Jeronimo and Tadonki, Claude}},
  year = {2018}},
  journal = {Proceedings of the 17th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many modern application domains crucially rely on tensor operations. The optimization of programs that operate on tensors poses difficulties that are not adequately addressed by existing languages and tools. Frameworks such as TensorFlow offer good abstractions for tensor operations, but target a specific domain, i.e. machine learning, and their optimization strategies cannot easily be adjusted to other domains. General-purpose optimization tools such as Pluto and existing meta-languages offer more flexibility in applying optimizations but lack abstractions for tensors. This work closes the gap between domain-specific tensor languages and general-purpose optimization tools by proposing the Tensor optimizations Meta-Language (TeML). TeML offers high-level abstractions for both tensor operations and loop transformations, and enables flexible composition of transformations into effective optimization paths. This compositionality is built into TeML's design, as our formal language specification will reveal. We also show that TeML can express tensor computations as comfortably as TensorFlow and that it can reproduce Pluto's optimization paths. Thus, optimized programs generated by TeML execute at least as fast as the corresponding Pluto programs. In addition, TeML enables optimization paths that often allow outperforming Pluto.}},
  url = {https://doi.org/10.1145/3278122.3278131},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup892,
  title = {Verified peephole optimizations for CompCert}},
  author = {Mullen, Eric and Zuniga, Daryl and Tatlock, Zachary and Grossman, Dan}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Transformations over assembly code are common in many compilers. These transformations are also some of the most bug-dense compiler components. Such bugs could be elim- inated by formally verifying the compiler, but state-of-the- art formally verified compilers like CompCert do not sup- port assembly-level program transformations. This paper presents Peek, a framework for expressing, verifying, and running meaning-preserving assembly-level program trans- formations in CompCert. Peek contributes four new com- ponents: a lower level semantics for CompCert x86 syntax, a liveness analysis, a library for expressing and verifying peephole optimizations, and a verified peephole optimiza- tion pass built into CompCert. Each of these is accompanied by a correctness proof in Coq against realistic assumptions about the calling convention and the system memory alloca- tor. Verifying peephole optimizations in Peek requires prov- ing only a set of local properties, which we have proved are sufficient to ensure global transformation correctness. We have proven these local properties for 28 peephole transfor- mations from the literature. We discuss the development of our new assembly semantics, liveness analysis, representa- tion of program transformations, and execution engine; de- scribe the verification challenges of each component; and detail techniques we applied to mitigate the proof burden.}},
  url = {https://doi.org/10.1145/2908080.2908109},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup893,
  title = {Using web corpus statistics for program analysis}},
  author = {Hsiao, Chun-Hung and Cafarella, Michael and Narayanasamy, Satish}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \& Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Several program analysis tools - such as plagiarism detection and bug finding - rely on knowing a piece of code's relative semantic importance. For example, a plagiarism detector should not bother reporting two programs that have an identical simple loop counter test, but should report programs that share more distinctive code. Traditional program analysis techniques (e.g., finding data and control dependencies) are useful, but do not say how surprising or common a line of code is. Natural language processing researchers have encountered a similar problem and addressed it using an n-gram model of text frequency, derived from statistics computed over text corpora.We propose and compute an n-gram model for programming languages, computed over a corpus of 2.8 million JavaScript programs we downloaded from the Web. In contrast to previous techniques, we describe a code n-gram as a subgraph of the program dependence graph that contains all nodes and edges reachable in n steps from the statement. We can count n-grams in a program and count the frequency of n-grams in the corpus, enabling us to compute tf-idf-style measures that capture the differing importance of different lines of code. We demonstrate the power of this approach by implementing a plagiarism detector with accuracy that beats previous techniques, and a bug-finding tool that discovered over a dozen previously unknown bugs in a collection of real deployed programs.}},
  url = {https://doi.org/10.1145/2660193.2660226},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup894,
  title = {LaCasa: lightweight affinity and object capabilities in Scala}},
  author = {Haller, Philipp and Loiko, Alex}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Aliasing is a known source of challenges in the context of imperative object-oriented languages, which have led to important advances in type systems for aliasing control. However, their large-scale adoption has turned out to be a surprisingly difficult challenge. While new language designs show promise, they do not address the need of aliasing control in existing languages. This paper presents a new approach to isolation and uniqueness in an existing, widely-used language, Scala. The approach is unique in the way it addresses some of the most important obstacles to the adoption of type system extensions for aliasing control. First, adaptation of existing code requires only a minimal set of annotations. Only a single bit of information is required per class. Surprisingly, the paper shows that this information can be provided by the object-capability discipline, widely-used in program security. We formalize our approach as a type system and prove key soundness theorems. The type system is implemented for the full Scala language, providing, for the first time, a sound integration with Scala's local type inference. Finally, we empirically evaluate the conformity of existing Scala open-source code on a corpus of over 75,000 LOC.}},
  url = {https://doi.org/10.1145/2983990.2984042},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup895,
  title = {A systematic derivation of the STG machine verified in Coq}},
  author = {Pirog, Maciej and Biernacki, Dariusz}},
  year = {2010}},
  journal = {Proceedings of the Third ACM Haskell Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Shared Term Graph (STG) is a lazy functional language used as an intermediate language in the Glasgow Haskell Compiler (GHC). In this article, we present a natural operational semantics for STG and we mechanically derive a lazy abstract machine from this semantics, which turns out to coincide with Peyton-Jones and Salkild's Spineless Tagless G-machine (STG machine) used in GHC. Unlike other constructions of STG-like machines present in the literature, ours is based on a systematic and scalable derivation method (inspired by Danvy et al.'s functional correspondence between evaluators and abstract machines) and it leads to an abstract machine that differs from the original STG machine only in inessential details. In particular, it handles non-trivial update scenarios and partial applications identically as the STG machine.The entire derivation has been formalized in the Coq proof assistant. Thus, in effect, we provide a machine checkable proof of the correctness of the STG machine with respect to the natural semantics.}},
  url = {https://doi.org/10.1145/1863523.1863528},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup896,
  title = {Lightweight semiformal time complexity analysis for purely functional data structures}},
  author = {Danielsson, Nils Anders}},
  year = {2008}},
  journal = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Okasaki and others have demonstrated how purely functional data structures that are efficient even in the presence of persistence can be constructed. To achieve good time bounds essential use is often made of laziness. The associated complexity analysis is frequently subtle, requiring careful attention to detail, and hence formalising it is valuable. This paper describes a simple library which can be used to make the analysis of a class of purely functional data structures and algorithms almost fully formal. The basic idea is to use the type system to annotate every function with the time required to compute its result. An annotated monad is used to combine time complexity annotations. The library has been used to analyse some existing data structures, for instance the deque operations of Hinze and Paterson's finger trees.}},
  url = {https://doi.org/10.1145/1328438.1328457},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup897,
  title = {Performance problems you can fix: a dynamic analysis of memoization opportunities}},
  author = {Della Toffola, Luca and Pradel, Michael and Gross, Thomas R.}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Performance bugs are a prevalent problem and recent research proposes various techniques to identify such bugs. This paper addresses a kind of performance problem that often is easy to address but difficult to identify: redundant computations that may be avoided by reusing already computed results for particular inputs, a technique called memoization. To help developers find and use memoization opportunities, we present MemoizeIt, a dynamic analysis that identifies methods that repeatedly perform the same computation. The key idea is to compare inputs and outputs of method calls in a scalable yet precise way. To avoid the overhead of comparing objects at all method invocations in detail, MemoizeIt first compares objects without following any references and iteratively increases the depth of exploration while shrinking the set of considered methods. After each iteration, the approach ignores methods that cannot benefit from memoization, allowing it to analyze calls to the remaining methods in more detail. For every memoization opportunity that MemoizeIt detects, it provides hints on how to implement memoization, making it easy for the developer to fix the performance issue. Applying MemoizeIt to eleven real-world Java programs reveals nine profitable memoization opportunities, most of which are missed by traditional CPU time profilers, conservative compiler optimizations, and other existing approaches for finding performance bugs. Adding memoization as proposed by MemoizeIt leads to statistically significant speedups by factors between 1.04x and 12.93x.}},
  url = {https://doi.org/10.1145/2814270.2814290},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup898,
  title = {Synthesis of biological models from mutation experiments}},
  author = {Koksal, Ali Sinan and Pu, Yewen and Srivastava, Saurabh and Bodik, Rastislav and Fisher, Jasmin and Piterman, Nir}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Executable biology presents new challenges to formal methods. This paper addresses two problems that cell biologists face when developing formally analyzable models.First, we show how to automatically synthesize a concurrent in-silico model for cell development given in-vivo experiments of how particular mutations influence the experiment outcome. The problem of synthesis under mutations is unique because mutations may produce non-deterministic outcomes (presumably by introducing races between competing signaling pathways in the cells) and the synthesized model must be able to replay all these outcomes in order to faithfully describe the modeled cellular processes. In contrast, a "regular" concurrent program is correct if it picks any outcome allowed by the non-deterministic specification. We developed synthesis algorithms and synthesized a model of cell fate determination of the earthworm C. elegans. A version of this model previously took systems biologists months to develop.Second, we address the problem of under-constrained specifications that arise due to incomplete sets of mutation experiments. Under-constrained specifications give rise to distinct models, each explaining the same phenomenon differently. Addressing the ambiguity of specifications corresponds to analyzing the space of plausible models. We develop algorithms for detecting ambiguity in specifications, i.e., whether there exist alternative models that would produce different fates on some unperformed experiment, and for removing redundancy from specifications, i.e., computing minimal non-ambiguous specifications.Additionally, we develop a modeling language and embed it into Scala. We describe how this language design and embedding allows us to build an efficient synthesizer. For our C. elegans case study, we infer two observationally equivalent models expressing different biological hypotheses through different protein interactions. One of these hypotheses was previously unknown to biologists.}},
  url = {https://doi.org/10.1145/2429069.2429125},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup899,
  title = {A separation logic for refining concurrent objects}},
  author = {Turon, Aaron Joseph and Wand, Mitchell}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Fine-grained concurrent data structures are crucial for gaining performance from multiprocessing, but their design is a subtle art. Recent literature has made large strides in verifying these data structures, using either atomicity refinement or separation logic with rely-guarantee reasoning. In this paper we show how the ownership discipline of separation logic can be used to enable atomicity refinement, and we develop a new rely-guarantee method that is localized to the definition of a data structure. We present the first semantics of separation logic that is sensitive to atomicity, and show how to control this sensitivity through ownership. The result is a logic that enables compositional reasoning about atomicity and interference, even for programs that use fine-grained synchronization and dynamic memory allocation.}},
  url = {https://doi.org/10.1145/1926385.1926415},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup900,
  title = {Data-driven precondition inference with learned features}},
  author = {Padhi, Saswat and Sharma, Rahul and Millstein, Todd}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We extend the data-driven approach to inferring preconditions for code from a set of test executions. Prior work requires a fixed set of features, atomic predicates that define the search space of possible preconditions, to be specified in advance. In contrast, we introduce a technique for on-demand feature learning, which automatically expands the search space of candidate preconditions in a targeted manner as necessary. We have instantiated our approach in a tool called PIE. In addition to making precondition inference more expressive, we show how to apply our feature-learning technique to the setting of data-driven loop invariant inference. We evaluate our approach by using PIE to infer rich preconditions for black-box OCaml library functions and using our loop-invariant inference algorithm as part of an automatic program verifier for C++ programs.}},
  url = {https://doi.org/10.1145/2908080.2908099},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup901,
  title = {Asynchronous functional reactive programming for GUIs}},
  author = {Czaplicki, Evan and Chong, Stephen}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Graphical user interfaces (GUIs) mediate many of our interactions with computers. Functional Reactive Programming (FRP) is a promising approach to GUI design, providing high-level, declarative, compositional abstractions to describe user interactions and time-dependent computations. We present Elm, a practical FRP language focused on easy creation of responsive GUIs. Elm has two major features: simple declarative support for Asynchronous FRP; and purely functional graphical layout.Asynchronous FRP allows the programmer to specify when the global ordering of event processing can be violated, and thus enables efficient concurrent execution of FRP programs; long-running computation can be executed asynchronously and not adversely affect the responsiveness of the user interface.Layout in Elm is achieved using a purely functional declarative framework that makes it simple to create and combine text, images, and video into rich multimedia displays.Together, Elm's two major features simplify the complicated task of creating responsive and usable GUIs.}},
  url = {https://doi.org/10.1145/2491956.2462161},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup902,
  title = {Automated application-level checkpointing of MPI programs}},
  author = {Bronevetsky, Greg and Marques, Daniel and Pingali, Keshav and Stodghill, Paul}},
  year = {2003}},
  journal = {Proceedings of the Ninth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The running times of many computational science applications, such as protein-folding using ab initio methods, are much longer than the mean-time-to-failure of high-performance computing platforms. To run to completion, therefore, these applications must tolerate hardware failures.In this paper, we focus on the stopping failure model in which a faulty process hangs and stops responding to the rest of the system. We argue that tolerating such faults is best done by an approach called application-level coordinated non-blocking checkpointing, and that existing fault-tolerance protocols in the literature are not suitable for implementing this approach.We then present a suitable protocol, which is implemented by a co-ordination layer that sits between the application program and the MPI library. We show how this protocol can be used with a precompiler that instruments C/MPI programs to save application and MPI library state. An advantage of our approach is that it is independent of the MPI implementation. We present experimental results that argue that the overhead of using our system can be small.}},
  url = {https://doi.org/10.1145/781498.781513},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup903,
  title = {Channel dependent types for higher-order mobile processes}},
  author = {Yoshida, Nobuko}},
  year = {2004}},
  journal = {Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper introduces a new expressive theory of types for the higher-order π-calculus and demonstrates its applicability via two security analyses for higher-order code mobility. The new theory significantly improves our previous one presented in [55] by the use of channel dependent/existential types. New dependent types control dynamic change of process accessibility via channel passing, while existential types guarantee safe scope-extrusion in higher-order process passing. This solves an open issue in [55], leading to significant enlargement of original typability. The resulting typing system is coherently integrated with the linear/affine typing disciplines as well as state, concurrency and distribution [53, 5, 56, 22], allowing precise analysis of software behaviour with higher-order mobility. As illustration of the usage of the typed calculus, two basic security concerns for mobile computation, secrecy for data confidentiality and r\^{o}},
  url = {https://doi.org/10.1145/964001.964014},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup904,
  title = {Secure distributed programming with value-dependent types}},
  author = {Swamy, Nikhil and Chen, Juan and Fournet, C\'{e}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Distributed applications are difficult to program reliably and securely. Dependently typed functional languages promise to prevent broad classes of errors and vulnerabilities, and to enable program verification to proceed side-by-side with development. However, as recursion, effects, and rich libraries are added, using types to reason about programs, specifications, and proofs becomes challenging.We present F*, a full-fledged design and implementation of a new dependently typed language for secure distributed programming. Unlike prior languages, F* provides arbitrary recursion while maintaining a logically consistent core; it enables modular reasoning about state and other effects using affine types; and it supports proofs of refinement properties using a mixture of cryptographic evidence and logical proof terms. The key mechanism is a new kind system that tracks several sub-languages within F* and controls their interaction. F* subsumes two previous languages, F7 and Fine. We prove type soundness (with proofs mechanized in Coq) and logical consistency for F*.We have implemented a compiler that translates F* to .NET bytecode, based on a prototype for Fine. F* provides access to libraries for concurrency, networking, cryptography, and interoperability with C#, F#, and the other .NET languages. The compiler produces verifiable binaries with 60\% code size overhead for proofs and types, as much as a 45x improvement over the Fine compiler, while still enabling efficient bytecode verification.To date, we have programmed and verified more than 20,000 lines of F* including (1) new schemes for multi-party sessions; (2) a zero-knowledge privacy-preserving payment protocol; (3) a provenance-aware curated database; (4) a suite of 17 web-browser extensions verified for authorization properties; and (5) a cloud-hosted multi-tier web application with a verified reference monitor.}},
  url = {https://doi.org/10.1145/2034773.2034811},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup905,
  title = {Code completion with statistical language models}},
  author = {Raychev, Veselin and Vechev, Martin and Yahav, Eran}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We address the problem of synthesizing code completions for programs using APIs. Given a program with holes, we synthesize completions for holes with the most likely sequences of method calls.Our main idea is to reduce the problem of code completion to a natural-language processing problem of predicting probabilities of sentences. We design a simple and scalable static analysis that extracts sequences of method calls from a large codebase, and index these into a statistical language model. We then employ the language model to find the highest ranked sentences, and use them to synthesize a code completion. Our approach is able to synthesize sequences of calls across multiple objects together with their arguments.Experiments show that our approach is fast and effective. Virtually all computed completions typecheck, and the desired completion appears in the top 3 results in 90\% of the cases.}},
  url = {https://doi.org/10.1145/2594291.2594321},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup906,
  title = {TPC: Target-Driven Parallelism Combining Prediction and Correction to Reduce Tail Latency in Interactive Services}},
  author = {Jeon, Myeongjae and He, Yuxiong and Kim, Hwanju and Elnikety, Sameh and Rixner, Scott and Cox, Alan L.}},
  year = {2016}},
  journal = {Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In interactive services such as web search, recommendations, games and finance, reducing the tail latency is crucial to provide fast response to every user. Using web search as a driving example, we systematically characterize interactive workload to identify the opportunities and challenges for reducing tail latency. We find that the workload consists of mainly short requests that do not benefit from parallelism, and a few long requests which significantly impact the tail but exhibit high parallelism speedup. This motivates estimating request execution time, using a predictor, to identify long requests and to parallelize them. Prediction, however, is not perfect; a long request mispredicted as short is likely to contribute to the server tail latency, setting a ceiling on the achievable tail latency. We propose TPC, an approach that combines prediction information judiciously with dynamic correction for inaccurate prediction. Dynamic correction increases parallelism to accelerate a long request that is mispredicted as short. TPC carefully selects the appropriate target latencies based on system load and parallelism efficiency to reduce tail latency.We implement TPC and several prior approaches to compare them experimentally on a single search server and on a cluster of 40 search servers. The experimental results show that TPC reduces the 99th- and 99.9th-percentile latency by up to 40\% compared with the best prior work. Moreover, we evaluate TPC on a finance server, demonstrating its effectiveness on reducing tail latency of interactive services beyond web search.}},
  url = {https://doi.org/10.1145/2872362.2872370},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup907,
  title = {Delay-bounded scheduling}},
  author = {Emmi, Michael and Qadeer, Shaz and Rakamari\'{c}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We provide a new characterization of scheduling nondeterminism by allowing deterministic schedulers to delay their next-scheduled task. In limiting the delays an otherwise-deterministic scheduler is allowed, we discover concurrency bugs efficiently---by exploring few schedules---and robustly---i.e., independent of the number of tasks, context switches, or buffered events. Our characterization elegantly applies to any systematic exploration (e.g., testing, model checking) of concurrent programs with dynamic task-creation. Additionally, we show that certain delaying schedulers admit efficient reductions from concurrent to sequential program analysis.}},
  url = {https://doi.org/10.1145/1926385.1926432},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup908,
  title = {Abstract effects and proof-relevant logical relations}},
  author = {Benton, Nick and Hofmann, Martin and Nigam, Vivek}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We give a denotational semantics for a region-based effect system that supports type abstraction in the sense that only externally visible effects need to be tracked: non-observable internal modifications, such as the reorganisation of a search tree or lazy initialisation, can count as 'pure' or 'read only'. This 'fictional purity' allows clients of a module to validate soundly more effect-based program equivalences than would be possible with previous semantics. Our semantics uses a novel variant of logical relations that maps types not merely to partial equivalence relations on values, as is commonly done, but rather to a proof-relevant generalisation thereof, namely setoids. The objects of a setoid establish that values inhabit semantic types, whilst its morphisms are understood as proofs of semantic equivalence. The transition to proof-relevance solves twoawkward problems caused by na\"{\i}},
  url = {https://doi.org/10.1145/2535838.2535869},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup909,
  title = {Foundations of path-dependent types}},
  author = {Amin, Nada and Rompf, Tiark and Odersky, Martin}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \& Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A scalable programming language is one in which the same concepts can describe small as well as large parts. Towards this goal, Scala unifies concepts from object and module systems. An essential ingredient of this unification is the concept of objects with type members, which can be referenced through path-dependent types. Unfortunately, path-dependent types are not well-understood, and have been a roadblock in grounding the Scala type system on firm theory.We study several calculi for path-dependent types. We present DOT which captures the essence - DOT stands for Dependent Object Types. We explore the design space bottom-up, teasing apart inherent from accidental complexities, while fully mechanizing our models at each step. Even in this simple setting, many interesting patterns arise from the interaction of structural and nominal features.Whereas our simple calculus enjoys many desirable and intuitive properties, we demonstrate that the theory gets much more complicated once we add another Scala feature, type refinement, or extend the subtyping relation to a lattice. We discuss possible remedies and trade-offs in modeling type systems for Scala-like languages.}},
  url = {https://doi.org/10.1145/2660193.2660216},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup910,
  title = {Online-and-offline partial evaluation (extended abstract): a mixed approach}},
  author = {Sumii, Eijiro and Kobayashi, Naoki}},
  year = {1999}},
  journal = {Proceedings of the 2000 ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Based Program Manipulation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents a hybrid method of partial evaluation (PE), which combines the power of online PE and the efficiency of offline PE, for a typed strict functional language. We begin with a naive online partial evaluator, and make it efficient without sacrificing its power. To this end, we (1) use state (instead of continuation) for let-insertion, (2) take a so-called cogen approach, and (3) decrease unnecessary computations—such as unnecessary let-insertions and unused values/expressions—with a type-based use analysis, which subsumes various monovariant binding-time analyses. Our method yields the same residual programs as the naive online partial evaluator, modulo inlining of redundant let-bindings. We implemented and compared our method and existing methods, both online and offline. Experiments show that our method is at least twice as fast as any other method (e.g., more than 7 times as fast as Thiemann's cogen approach to offline PE in the specialization of the power function, thanks to the reduction of unnecessary let-insertions) when they yield equivalent residual programs.}},
  url = {https://doi.org/10.1145/328690.328694},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup911,
  title = {Eunomia: Scaling Concurrent Search Trees under Contention Using HTM}},
  author = {Wang, Xin and Zhang, Weihua and Wang, Zhaoguo and Wei, Ziyun and Chen, Haibo and Zhao, Wenyun}},
  year = {2017}},
  journal = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {While hardware transactional memory (HTM) has recently been adopted to construct efficient concurrent search tree structures, such designs fail to deliver scalable performance under contention. In this paper, we first conduct a detailed analysis on an HTM-based concurrent B+Tree, which uncovers several reasons for excessive HTM aborts induced by both false and true conflicts under contention. Based on the analysis, we advocate Eunomia, a design pattern for search trees which contains several principles to reduce HTM aborts, including splitting HTM regions with version-based concurrency control to reduce HTM working sets, partitioned data layout to reduce false conflicts, proactively detecting and avoiding true conflicts, and adaptive concurrency control. To validate their effectiveness, we apply such designs to construct a scalable concurrent B+Tree using HTM. Evaluation using key-value store benchmarks on a 20-core HTM-capable multi-core machine shows that Eunomia leads to 5X-11X speedup under high contention, while incurring small overhead under low contention.}},
  url = {https://doi.org/10.1145/3018743.3018752},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup912,
  title = {Proving the unique fixed-point principle correct: an adventure with category theory}},
  author = {Hinze, Ralf and James, Daniel W.H.}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Say you want to prove something about an infinite data-structure, such as a stream or an infinite tree, but you would rather not subject yourself to coinduction. The unique fixed-point principle is an easy-to-use, calculational alternative. The proof technique rests on the fact that certain recursion equations have unique solutions; if two elements of a coinductive type satisfy the same equation of this kind, then they are equal. In this paper we precisely characterize the conditions that guarantee a unique solution. Significantly, we do so not with a syntactic criterion, but with a semantic one that stems from the categorical notion of naturality. Our development is based on distributive laws and bialgebras, and draws heavily on Turi and Plotkin's pioneering work on mathematical operational semantics. Along the way, we break down the design space in two dimensions, leading to a total of nine points. Each gives rise to varying degrees of expressiveness, and we will discuss three in depth. Furthermore, our development is generic in the syntax of equations and in the behaviour they encode - we are not caged in the world of streams.}},
  url = {https://doi.org/10.1145/2034773.2034821},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup913,
  title = {Environmental bisimulations for probabilistic higher-order languages}},
  author = {Sangiorgi, Davide and Vignudelli, Valeria}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Environmental bisimulations for probabilistic higher-order languages are studied. In contrast with applicative bisimulations, environmental bisimulations are known to be more robust and do not require sophisticated techniques such as Howe’s in the proofs of congruence. As representative calculi, call-by-name and call-by-value λ- calculus, and a (call-by-value) λ-calculus extended with references (i.e., a store) are considered. In each case full abstraction results are derived for probabilistic environmental similarity and bisimilarity with respect to contextual preorder and contextual equivalence, respectively. Some possible enhancements of the (bi)simulations, as ‘up-to techniques’, are also presented. Probabilities force a number of modifications to the definition of environmental bisimulations in non-probabilistic languages. Some of these modifications are specific to probabilities, others may be seen as general refinements of environmental bisimulations, applicable also to non-probabilistic languages. Several examples are presented, to illustrate the modifications and the differences.}},
  url = {https://doi.org/10.1145/2837614.2837651},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup914,
  title = {A framework for enhancing data reuse via associative reordering}},
  author = {Stock, Kevin and Kong, Martin and Grosser, Tobias and Pouchet, Louis-No\"{e}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The freedom to reorder computations involving associative operators has been widely recognized and exploited in designing parallel algorithms and to a more limited extent in optimizing compilers.In this paper, we develop a novel framework utilizing the associativity and commutativity of operations in regular loop computations to enhance register reuse. Stencils represent a particular class of important computations where the optimization framework can be applied to enhance performance. We show how stencil operations can be implemented to better exploit register reuse and reduce load/stores. We develop a multi-dimensional retiming formalism to characterize the space of valid implementations in conjunction with other program transformations. Experimental results demonstrate the effectiveness of the framework on a collection of high-order stencils.}},
  url = {https://doi.org/10.1145/2594291.2594342},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup915,
  title = {Library abstraction for C/C++ concurrency}},
  author = {Batty, Mark and Dodds, Mike and Gotsman, Alexey}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {When constructing complex concurrent systems, abstraction is vital: programmers should be able to reason about concurrent libraries in terms of abstract specifications that hide the implementation details. Relaxed memory models present substantial challenges in this respect, as libraries need not provide sequentially consistent abstractions: to avoid unnecessary synchronisation, they may allow clients to observe relaxed memory effects, and library specifications must capture these.In this paper, we propose a criterion for sound library abstraction in the new C11 and C++11 memory model, generalising the standard sequentially consistent notion of linearizability. We prove that our criterion soundly captures all client-library interactions, both through call and return values, and through the subtle synchronisation effects arising from the memory model. To illustrate our approach, we verify implementations against specifications for the lock-free Treiber stack and a producer-consumer queue. Ours is the first approach to compositional reasoning for concurrent C11/C++11 programs.}},
  url = {https://doi.org/10.1145/2429069.2429099},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup916,
  title = {Just-in-time inheritance: a dynamic and implicit multiple inheritance mechanism}},
  author = {De Wael, Mattias and Swalens, Janwillem and De Meuter, Wolfgang}},
  year = {2016}},
  journal = {Proceedings of the 12th Symposium on Dynamic Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Multiple inheritance is often criticised for the ambiguity that arises when multiple parents want to pass on a feature with the same name to their offspring. A survey of programming languages reveals that no programming language has an inherently implicit and dynamic approach to resolve this ambiguity. This paper identifies just-in-time inheritance as the first implicit and dynamic inheritance mechanism. The key idea of just-in-time inheritance is that one of the parents is favoured over the others, which resolves the ambiguity, and that the favoured parent can change at runtime. However, just-in-time inheritance is not the silver bullet to solve all ambiguity problems heir to multiple inheritance, because it is not applicable in all scenarios. We conclude that the applicability of just-in-time inheritance is to be found in systems where multiple inheritance is used to model an ``is-a OR is-a''-relation, rather than the more traditional ``is-a AND is-a''-relation.}},
  url = {https://doi.org/10.1145/2989225.2989229},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup917,
  title = {A proof system for separation logic with magic wand}},
  author = {Lee, Wonyeol and Park, Sungwoo}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Separation logic is an extension of Hoare logic which is acknowledged as an enabling technology for large-scale program verification. It features two new logical connectives, separating conjunction and separating implication, but most of the applications of separation logic have exploited only separating conjunction without considering separating implication. Nevertheless the power of separating implication has been well recognized and there is a growing interest in its use for program verification. This paper develops a proof system for full separation logic which supports not only separating conjunction but also separating implication. The proof system is developed in the style of sequent calculus and satisfies the admissibility of cut. The key challenge in the development is to devise a set of inference rules for manipulating heap structures that ensure the completeness of the proof system with respect to separation logic. We show that our proof of completeness directly translates to a proof search strategy.}},
  url = {https://doi.org/10.1145/2535838.2535871},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup918,
  title = {Regular expression pattern matching for XML}},
  author = {Hosoya, Haruo and Pierce, Benjamin}},
  year = {2001}},
  journal = {Proceedings of the 28th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We propose regular expression pattern matching as a core feature for programming languages for manipulating XML (and similar tree-structured data formats). We extend conventional pattern-matching facilities with regular expression operators such as repetition (*), alternation (I), etc., that can match arbitrarily long sequences of subtrees, allowing a compact pattern to extract data from the middle of a complex sequence. We show how to check standard notions of exhaustiveness and redundancy for these patterns.Regular expression patterns are intended to be used in languages whose type systems are also based on the regular expression types. To avoid excessive type annotations, we develop a type inference scheme that propagates type constraints to pattern variables from the surrounding context. The type inference algorithm translates types and patterns into regular tree automata and then works in terms of standard closure operations (union, intersection, and difference) on tree automata. The main technical challenge is dealing with the interaction of repetition and alternation patterns with the first-match policy, which gives rise to subtleties concerning both the termination and the precision of the analysis. We address these issues by introducing a data structure representing closure operations lazily.}},
  url = {https://doi.org/10.1145/360204.360209},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup919,
  title = {Tabular: a schema-driven probabilistic programming language}},
  author = {Gordon, Andrew D. and Graepel, Thore and Rolland, Nicolas and Russo, Claudio and Borgstrom, Johannes and Guiver, John}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We propose a new kind of probabilistic programming language for machine learning. We write programs simply by annotating existing relational schemas with probabilistic model expressions. We describe a detailed design of our language, Tabular, complete with formal semantics and type system. A rich series of examples illustrates the expressiveness of Tabular. We report an implementation, and show evidence of the succinctness of our notation relative to current best practice. Finally, we describe and verify a transformation of Tabular schemas so as to predict missing values in a concrete database. The ability to query for missing values provides a uniform interface to a wide variety of tasks, including classification, clustering, recommendation, and ranking.}},
  url = {https://doi.org/10.1145/2535838.2535850},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup920,
  title = {A static analyzer for large safety-critical software}},
  author = {Blanchet, Bruno and Cousot, Patrick and Cousot, Radhia and Feret, J\'{e}},
  year = {2003}},
  journal = {Proceedings of the ACM SIGPLAN 2003 Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We show that abstract interpretation-based static program analysis can be made efficient and precise enough to formally verify a class of properties for a family of large programs with few or no false alarms. This is achieved by refinement of a general purpose static analyzer and later adaptation to particular programs of the family by the end-user through parametrization. This is applied to the proof of soundness of data manipulation operations at the machine level for periodic synchronous safety critical embedded software.The main novelties are the design principle of static analyzers by refinement and adaptation through parametrization (Sect. 3 and 7), the symbolic manipulation of expressions to improve the precision of abstract transfer functions (Sect. 6.3), the octagon (Sect. 6.2.2), ellipsoid (Sect. 6.2.3), and decision tree (Sect. 6.2.4) abstract domains, all with sound handling of rounding errors in oating point computations, widening strategies (with thresholds: Sect. 7.1.2, delayed: Sect. 7.1.3) and the automatic determination of the parameters (parametrized packing: Sect. 7.2).}},
  url = {https://doi.org/10.1145/781131.781153},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup921,
  title = {A lambda-calculus foundation for universal probabilistic programming}},
  author = {Borgstr\"{o}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We develop the operational semantics of an untyped probabilistic λ-calculus with continuous distributions, and both hard and soft constraints,as a foundation for universal probabilistic programming languages such as Church, Anglican, and Venture. Our first contribution is to adapt the classic operational semantics of λ-calculus to a continuous setting via creating a measure space on terms and defining step-indexed approximations. We prove equivalence of big-step and small-step formulations of this distribution-based semantics. To move closer to inference techniques, we also define the sampling-based semantics of a term as a function from a trace of random samples to a value. We show that the distribution induced by integration over the space of traces equals the distribution-based semantics. Our second contribution is to formalize the implementation technique of trace Markov chain Monte Carlo (MCMC) for our calculus and to show its correctness. A key step is defining sufficient conditions for the distribution induced by trace MCMC to converge to the distribution-based semantics. To the best of our knowledge, this is the first rigorous correctness proof for trace MCMC for a higher-order functional language, or for a language with soft constraints.}},
  url = {https://doi.org/10.1145/2951913.2951942},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup922,
  title = {A library for light-weight information-flow security in haskell}},
  author = {Russo, Alejandro and Claessen, Koen and Hughes, John}},
  year = {2008}},
  journal = {Proceedings of the First ACM SIGPLAN Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Protecting confidentiality of data has become increasingly important for computing systems. Information-flow techniques have been developed over the years to achieve that purpose, leading to special-purpose languages that guarantee information-flow security in programs. However, rather than producing a new language from scratch, information-flow security can also be provided as a library. This has been done previously in Haskell using the arrow framework. In this paper, we show that arrows are not necessary to design such libraries and that a less general notion, namely monads, is sufficient to achieve the same goals. We present a monadic library to provide information-flow security for Haskell programs. The library introduces mechanisms to protect confidentiality of data for pure computations, that we then easily, and modularly, extend to include dealing with side-effects. We also present combinators to dynamically enforce different declassification policies when release of information is required in a controlled manner. It is possible to enforce policies related to what, by whom, and when information is released or a combination of them. The well-known concept of monads together with the light-weight characteristic of our approach makes the library suitable to build applications where confidentiality of data is an issue.}},
  url = {https://doi.org/10.1145/1411286.1411289},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup923,
  title = {Mathematizing C++ concurrency}},
  author = {Batty, Mark and Owens, Scott and Sarkar, Susmit and Sewell, Peter and Weber, Tjark}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Shared-memory concurrency in C and C++ is pervasive in systems programming, but has long been poorly defined. This motivated an ongoing shared effort by the standards committees to specify concurrent behaviour in the next versions of both languages. They aim to provide strong guarantees for race-free programs, together with new (but subtle) relaxed-memory atomic primitives for high-performance concurrent code. However, the current draft standards, while the result of careful deliberation, are not yet clear and rigorous definitions, and harbour substantial problems in their details.In this paper we establish a mathematical (yet readable) semantics for C++ concurrency. We aim to capture the intent of the current (`Final Committee') Draft as closely as possible, but discuss changes that fix many of its problems. We prove that a proposed x86 implementation of the concurrency primitives is correct with respect to the x86-TSO model, and describe our Cppmem tool for exploring the semantics of examples, using code generated from our Isabelle/HOL definitions.Having already motivated changes to the draft standard, this work will aid discussion of any further changes, provide a correctness condition for compilers, and give a much-needed basis for analysis and verification of concurrent C and C++ programs.}},
  url = {https://doi.org/10.1145/1926385.1926394},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup924,
  title = {Cartesian hoare logic for verifying k-safety properties}},
  author = {Sousa, Marcelo and Dillig, Isil}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Unlike safety properties which require the absence of a “bad” program trace, k-safety properties stipulate the absence of a “bad” interaction between k traces. Examples of k-safety properties include transitivity, associativity, anti-symmetry, and monotonicity. This paper presents a sound and relatively complete calculus, called Cartesian Hoare Logic (CHL), for verifying k-safety properties. We also present an automated verification algorithm based on CHL and implement it in a tool called DESCARTES. We use DESCARTES to analyze user-defined relational operators in Java and demonstrate that DESCARTES is effective at verifying (or finding violations of) multiple k-safety properties.}},
  url = {https://doi.org/10.1145/2908080.2908092},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup925,
  title = {Bisimulation congruences in safe ambients}},
  author = {Merro, Massimo and Hennessy, Matthew}},
  year = {2002}},
  journal = {Proceedings of the 29th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We study a variant of Levi and Sangiorgi's Safe Ambients (SA) enriched with passwords (SAP). In SAP by managing passwords, for example generating new ones and distributing them selectively, an ambient may now program who may migrate into its computation space, and when. Moreover in SAP an ambient may provide different services depending on the passwords exhibited by its incoming clients.We give an lts based operational semantics for SAP and a labelled bisimulation based equivalence which is proved to coincide with barbed congruence.Our notion of bisimulation is used to prove a set of algebraic laws which are subsequently exploited to prove more significant examples.}},
  url = {https://doi.org/10.1145/503272.503280},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup926,
  title = {Mixing type checking and symbolic execution}},
  author = {Khoo, Yit Phang and Chang, Bor-Yuh Evan and Foster, Jeffrey S.}},
  year = {2010}},
  journal = {Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Static analysis designers must carefully balance precision and efficiency. In our experience, many static analysis tools are built around an elegant, core algorithm, but that algorithm is then extensively tweaked to add just enough precision for the coding idioms seen in practice, without sacrificing too much efficiency. There are several downsides to adding precision in this way: the tool's implementation becomes much more complicated; it can be hard for an end-user to interpret the tool's results; and as software systems vary tremendously in their coding styles, it may require significant algorithmic engineering to enhance a tool to perform well in a particular software domain.In this paper, we present Mix, a novel system that mixes type checking and symbolic execution. The key aspect of our approach is that these analyses are applied independently on disjoint parts of the program, in an off-the-shelf manner. At the boundaries between nested type checked and symbolically executed code regions, we use special mix rules to communicate information between the off-the-shelf systems. The resulting mixture is a provably sound analysis that is more precise than type checking alone and more efficient than exclusive symbolic execution. In addition, we also describe a prototype implementation, Mixy, for C. Mixy checks for potential null dereferences by mixing a null/non-null type qualifier inference system with a symbolic executor.}},
  url = {https://doi.org/10.1145/1806596.1806645},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup927,
  title = {Effect handlers in scope}},
  author = {Wu, Nicolas and Schrijvers, Tom and Hinze, Ralf}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM SIGPLAN Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Algebraic effect handlers are a powerful means for describing effectful computations. They provide a lightweight and orthogonal technique to define and compose the syntax and semantics of different effects. The semantics is captured by handlers, which are functions that transform syntax trees.Unfortunately, the approach does not support syntax for scoping constructs, which arise in a number of scenarios. While handlers can be used to provide a limited form of scope, we demonstrate that this approach constrains the possible interactions of effects and rules out some desired semantics.This paper presents two different ways to capture scoped constructs in syntax, and shows how to achieve different semantics by reordering handlers. The first approach expresses scopes using the existing algebraic handlers framework, but has some limitations. The problem is fully solved in the second approach where we introduce higher-order syntax.}},
  url = {https://doi.org/10.1145/2633357.2633358},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup928,
  title = {Introspective pushdown analysis of higher-order programs}},
  author = {Earl, Christopher and Sergey, Ilya and Might, Matthew and Van Horn, David}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In the static analysis of functional programs, pushdown flow analysis and abstract garbage collection skirt just inside the boundaries of soundness and decidability. Alone, each method reduces analysis times and boosts precision by orders of magnitude. This work illuminates and conquers the theoretical challenges that stand in the way of combining the power of these techniques. The challenge in marrying these techniques is not subtle: computing the reachable control states of a pushdown system relies on limiting access during transition to the top of the stack; abstract garbage collection, on the other hand, needs full access to the entire stack to compute a root set, just as concrete collection does. Introspective pushdown systems resolve this conflict. Introspective pushdown systems provide enough access to the stack to allow abstract garbage collection, but they remain restricted enough to compute control-state reachability, thereby enabling the sound and precise product of pushdown analysis and abstract garbage collection. Experiments reveal synergistic interplay between the techniques, and the fusion demonstrates "better-than-both-worlds" precision.}},
  url = {https://doi.org/10.1145/2364527.2364576},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup929,
  title = {Actor profiling in virtual execution environments}},
  author = {Ros\`{a}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Nowadays, many virtual execution environments benefit from concurrency offered by the actor model. Unfortunately, while actors are used in many applications, existing profiling tools are not much effective in analyzing the performance of applications using actors. In this paper, we present a new instrumentation-based technique to profile actors in virtual execution environments. Our technique adopts platform-independent profiling metrics that minimize the perturbations induced by the instrumentation logic and allow comparing profiling results across different platforms. In particular, our technique measures the initialization cost, the amount of executed computations, and the messages sent and received by each actor. We implement our technique within a profiling tool for Akka actors on the Java platform. Evaluation results show that our profiling technique helps performance analysis of actor utilization and communication between actors in large-scale computing frameworks.}},
  url = {https://doi.org/10.1145/2993236.2993241},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup930,
  title = {Fast partial evaluation of pattern matching in strings}},
  author = {Ager, Mads Sig and Danvy, Olivier and Rohde, Henning Korsholm}},
  year = {2003}},
  journal = {Proceedings of the 2003 ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Based Program Manipulation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We show how to obtain all of Knuth, Morris, and Pratt's linear-time string matcher by partial evaluation of a quadratic-time string matcher with respect to a pattern string. Although it has been known for 15 years how to obtain this linear matcher by partial evaluation of a quadratic one, how to obtain it in linear time has remained an open problem.Obtaining a linear matcher by partial evaluation of a quadratic one is achieved by performing its backtracking at specialization time and memoizing its results. We show (1) how to rewrite the source matcher such that its static intermediate computations can be shared at specialization time and (2) how to extend the memoization capabilities of a partial evaluator to static functions. Such an extended partial evaluator, if its memoization is implemented efficiently, specializes the rewritten source matcher in linear time.}},
  url = {https://doi.org/10.1145/777388.777390},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup931,
  title = {Separation logic + superposition calculus = heap theorem prover}},
  author = {Navarro P\'{e}},
  year = {2011}},
  journal = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Program analysis and verification tools crucially depend on the ability to symbolically describe and reason about sets of program behaviors. Separation logic provides a promising foundation for dealing with heap manipulating programs, while the development of practical automated deduction/satisfiability checking tools for separation logic is a challenging problem. In this paper, we present an efficient, sound and complete automated theorem prover for checking validity of entailments between separation logic formulas with list segment predicates. Our theorem prover integrates separation logic inference rules that deal with list segments and a superposition calculus to deal with equality/aliasing between memory locations. The integration follows a modular combination approach that allows one to directly incorporate existing advanced techniques for first-order reasoning with equality, as well as account for additional theories, e.g., linear arithmetic, using extensions of superposition. An experimental evaluation of our entailment prover indicates speedups of several orders of magnitude with respect to the available state-of-the-art tools.}},
  url = {https://doi.org/10.1145/1993498.1993563},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup932,
  title = {Dependent types for JavaScript}},
  author = {Chugh, Ravi and Herman, David and Jhala, Ranjit}},
  year = {2012}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present Dependent JavaScript (DJS), a statically typed dialect of the imperative, object-oriented, dynamic language. DJS supports the particularly challenging features such as run-time type-tests, higher-order functions, extensible objects, prototype inheritance, and arrays through a combination of nested refinement types, strong updates to the heap, and heap unrolling to precisely track prototype hierarchies. With our implementation of DJS, we demonstrate that the type system is expressive enough to reason about a variety of tricky idioms found in small examples drawn from several sources, including the popular book JavaScript: The Good Parts and the SunSpider benchmark suite.}},
  url = {https://doi.org/10.1145/2384616.2384659},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup933,
  title = {AMNESIAC: Amnesic Automatic Computer}},
  author = {Akturk, Ismail and Karpuzcu, Ulya R.}},
  year = {2017}},
  journal = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Due to imbalances in technology scaling, the energy consumption of data storage and communication by far exceeds the energy consumption of actual data production, i.e., computation. As a consequence, recomputing data can become more energy efficient than storing and retrieving precomputed data. At the same time, recomputation can relax the pressure on the memory hierarchy and the communication bandwidth. This study hence assesses the energy efficiency prospects of trading computation for communication. We introduce an illustrative proof-of-concept design, identify practical limitations, and provide design guidelines.}},
  url = {https://doi.org/10.1145/3037697.3037741},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup934,
  title = {Type classes as objects and implicits}},
  author = {Oliveira, Bruno C.d.S. and Moors, Adriaan and Odersky, Martin}},
  year = {2010}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Type classes were originally developed in Haskell as a disciplined alternative to ad-hoc polymorphism. Type classes have been shown to provide a type-safe solution to important challenges in software engineering and programming languages such as, for example, retroactive extension of programs. They are also recognized as a good mechanism for concept-based generic programming and, more recently, have evolved into a mechanism for type-level computation.This paper presents a lightweight approach to type classes in object-oriented (OO) languages with generics using the CONCEPT pattern and implicits (a type-directed implicit parameter passing mechanism). This paper also shows how Scala's type system conspires with implicits to enable, and even surpass, many common extensions of the Haskell type class system, making Scala ideally suited for generic programming in the large.}},
  url = {https://doi.org/10.1145/1869459.1869489},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup935,
  title = {Modular and verified automatic program repair}},
  author = {Logozzo, Francesco and Ball, Thomas}},
  year = {2012}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We study the problem of suggesting code repairs at design time, based on the warnings issued by modular program verifiers. We introduce the concept of a verified repair, a change to a program's source that removes bad execution traces while increasing the number of good traces, where the bad/good traces form a partition of all the traces of a program. Repairs are property-specific. We demonstrate our framework in the context of warnings produced by the modular cccheck (a.k.a. Clousot) abstract interpreter, and generate repairs for missing contracts, incorrect locals and objects initialization, wrong conditionals, buffer overruns, arithmetic overflow and incorrect floating point comparisons. We report our experience with automatically generating repairs for the .NET framework libraries, generating verified repairs for over 80\% of the warnings generated by cccheck.}},
  url = {https://doi.org/10.1145/2384616.2384626},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup936,
  title = {A typed store-passing translation for general references}},
  author = {Pottier, Fran\c{c}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a store-passing translation of System F with general references into an extension of System Fω with certain well-behaved recursive kinds. This seems to be the first type-preserving store-passing translation for general references. It can be viewed as a purely syntactic account of a possible worlds model.}},
  url = {https://doi.org/10.1145/1926385.1926403},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup937,
  title = {Rewriting for sound and complete union, intersection and negation types}},
  author = {Pearce, David J.}},
  year = {2017}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Implementing the type system of a programming language is a critical task that is often done in an ad-hoc fashion. Whilst this makes it hard to ensure the system is sound, it also makes it difficult to extend as the language evolves. We are interested in describing type systems using declarative rewrite rules from which an implementation can be automatically generated. Whilst not all type systems are easily expressed in this manner, those involving unions, intersections and negations are well-suited for this. In this paper, we consider a relatively complex type system involving unions, intersections and negations developed previously. This system was not developed with rewriting in mind, though clear parallels are immediately apparent from the original presentation. For example, the system presented required types be first converted into a variation on Disjunctive Normal Form. We identify that the original system can, for the most part, be reworked to enable a natural expression using declarative rewrite rules. We present an implementation of our rewrite rules in the Whiley Rewrite Language (WyRL), and report performance results compared with a hand-coded solution.}},
  url = {https://doi.org/10.1145/3136040.3136042},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup938,
  title = {A verified compiler for an impure functional language}},
  author = {Chlipala, Adam}},
  year = {2010}},
  journal = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a verified compiler to an idealized assembly language from a small, untyped functional language with mutable references and exceptions. The compiler is programmed in the Coq proof assistant and has a proof of total correctness with respect to big-step operational semantics for the source and target languages. Compilation is staged and includes standard phases like translation to continuation-passing style and closure conversion, as well as a common subexpression elimination optimization. In this work, our focus has been on discovering and using techniques that make our proofs easy to engineer and maintain. While most programming language work with proof assistants uses very manual proof styles, all of our proofs are implemented as adaptive programs in Coq's tactic language, making it possible to reuse proofs unchanged as new language features are added.In this paper, we focus especially on phases of compilation that rearrange the structure of syntax with nested variable binders. That aspect has been a key challenge area in past compiler verification projects, with much more effort expended in the statement and proof of binder-related lemmas than is found in standard pencil-and-paper proofs. We show how to exploit the representation technique of parametric higher-order abstract syntax to avoid the need to prove any of the usual lemmas about binder manipulation, often leading to proofs that are actually shorter than their pencil-and-paper analogues. Our strategy is based on a new approach to encoding operational semantics which delegates all concerns about substitution to the meta language, without using features incompatible with general-purpose type theories like Coq's logic.}},
  url = {https://doi.org/10.1145/1706299.1706312},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup939,
  title = {Interval-based memory reclamation}},
  author = {Wen, Haosen and Izraelevitz, Joseph and Cai, Wentao and Beadle, H. Alan and Scott, Michael L.}},
  year = {2018}},
  journal = {Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this paper we present interval-based reclamation (IBR), a new approach to safe reclamation of disconnected memory blocks in nonblocking concurrent data structures. Safe reclamation is a difficult problem: a thread, before freeing a block, must ensure that no other threads are accessing that block; the required synchronization tends to be expensive. In contrast with epoch-based reclamation, in which threads reserve all blocks created after a certain time, or pointer-based reclamation (e.g., hazard pointers), in which threads reserve individual blocks, IBR allows a thread to reserve all blocks known to have existed in a bounded interval of time. By comparing a thread's reserved interval with the lifetime of a detached but not yet reclaimed block, the system can determine if the block is safe to free. Like hazard pointers, IBR avoids the possibility that a single stalled thread may reserve an unbounded number of blocks; unlike hazard pointers, it avoids a memory fence on most pointer-following operations. It also avoids the need to explicitly "unreserve" a no-longer-needed pointer.We describe three specific IBR schemes (one with several variants) that trade off performance, applicability, and space requirements. IBR requires no special hardware or OS support. In experiments with data structure microbenchmarks, it also compares favorably (in both time and space) to other state-of-the-art approaches, making it an attractive alternative for libraries of concurrent data structures.}},
  url = {https://doi.org/10.1145/3178487.3178488},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup940,
  title = {RockSalt: better, faster, stronger SFI for the x86}},
  author = {Morrisett, Greg and Tan, Gang and Tassarotti, Joseph and Tristan, Jean-Baptiste and Gan, Edward}},
  year = {2012}},
  journal = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Software-based fault isolation (SFI), as used in Google's Native Client (NaCl), relies upon a conceptually simple machine-code analysis to enforce a security policy. But for complicated architectures such as the x86, it is all too easy to get the details of the analysis wrong. We have built a new checker that is smaller, faster, and has a much reduced trusted computing base when compared to Google's original analysis. The key to our approach is automatically generating the bulk of the analysis from a declarative description which we relate to a formal model of a subset of the x86 instruction set architecture. The x86 model, developed in Coq, is of independent interest and should be usable for a wide range of machine-level verification tasks.}},
  url = {https://doi.org/10.1145/2254064.2254111},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup941,
  title = {Precise and scalable static analysis of jQuery using a regular expression domain}},
  author = {Park, Changhee and Im, Hyeonseung and Ryu, Sukyoung}},
  year = {2016}},
  journal = {Proceedings of the 12th Symposium on Dynamic Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {jQuery is the most popular JavaScript library but the state-of-the-art static analyzers for JavaScript applications fail to analyze simple programs that use jQuery. In this paper, we present a novel abstract string domain whose elements are simple regular expressions that can represent prefix, infix, and postfix substrings of a string and even their sets. We formalize the new domain in the abstract interpretation framework with abstract models of strings and objects commonly used in the existing JavaScript analyzers. For practical use of the domain, we present polynomial-time inclusion decision rules between the regular expressions and prove that the rules exactly capture the actual inclusion relation. We have implemented the domain as an extension of the open-source JavaScript analyzer, SAFE, and we show that the extension significantly improves the scalability and precision of the baseline analyzer in analyzing programs that use jQuery.}},
  url = {https://doi.org/10.1145/2989225.2989228},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup942,
  title = {Debugging heterogeneous distributed systems using event-based models of behavior}},
  author = {Bates, Peter}},
  year = {1988}},
  journal = {Proceedings of the 1988 ACM SIGPLAN and SIGOPS Workshop on Parallel and Distributed Debugging}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Event Based Behavioral Abstraction (EBBA) is a high-level debugging approach which treats debugging as a process of creating models of actual behavior from the activity of the system and comparing these to models of expected system behavior. The differences between the actual and expected models are used to characterize erroneous system behavior and direct further investigation.A set of EBBA-based tools has been implemented that users can employ to construct libraries of behavior models and investigate the behavior of an errorful system through these models. EBBA evolves naturally as a cooperative distributed program that can take better advantage of computational power available in a network computer system to enhance debugging tool transparency, reduce latency and uncertainty for fundamental debugging activities and accommodate diverse, heterogeneous architectures.}},
  url = {https://doi.org/10.1145/68210.69217},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup943,
  title = {Reducing the barriers to writing verified specifications}},
  author = {Schiller, Todd W. and Ernst, Michael D.}},
  year = {2012}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Formally verifying a program requires significant skill not only because of complex interactions between program subcomponents, but also because of deficiencies in current verification interfaces. These skill barriers make verification economically unattractive by preventing the use of less-skilled (less-expensive) workers and distributed workflows (i.e., crowdsourcing). This paper presents VeriWeb, a web-based IDE for verification that decomposes the task of writing verifiable specifications into manageable subproblems. To overcome the information loss caused by task decomposition, and to reduce the skill required to verify a program, VeriWeb incorporates several innovative user interface features: drag and drop condition construction, concrete counterexamples, and specification inlining.To evaluate VeriWeb, we performed three experiments. First, we show that VeriWeb lowers the time and monetary cost of verification by performing a comparative study of VeriWeb and a traditional tool using 14 paid subjects contracted hourly from Exhedra Solution's vWorker online marketplace. Second, we demonstrate the dearth and insufficiency of current ad-hoc labor marketplaces for verification by recruiting workers from Amazon's Mechanical Turk to perform verification with VeriWeb. Finally, we characterize the minimal communication overhead incurred when VeriWeb is used collaboratively by observing two pairs of developers each use the tool simultaneously to verify a single program.}},
  url = {https://doi.org/10.1145/2384616.2384624},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup944,
  title = {Using regular approximations for generalisation during partial evalution}},
  author = {Gallagher, John P. and Peralta, Julio C.}},
  year = {1999}},
  journal = {Proceedings of the 2000 ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Based Program Manipulation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {On-line partial evaluation algorithms include a generalisation step, which is needed to ensure termination. In partial evaluation of logic and functional programs, the usual generalisation operation applied to computation states is the most specific generalisation (msg) of expressions. This can cause loss of information, which is especially serious in programs whose computations first build some internal data structure, which is then used to control a subsequent phase of execution - a common pattern of computation. If the size of the intermediate data is unbounded at partial evaluation time then the msg will lose almost all information about its structure. Hence the second phase of computation cannot be effectively specialised.In this paper a generalisation based on regular approximations is presented. Regular approximations are recursive descriptions of term structure closely related to tree automata. A regular approximation of computation states can be built during partial evaluation. The critical point is that when generalisation is performed, the upper bound on regular descriptions can be combined with the msg, thus preserving structural information including recursively defined structure. The domain of regular approximations is infinite and hence a widening is incorporated in the generalisation to ensure termination. An algorithm for partial evaluation of logic programs, enhanced with regular approximations, along with some examples of its use will be presented.}},
  url = {https://doi.org/10.1145/328690.328698},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup945,
  title = {Causal consistency: beyond memory}},
  author = {Perrin, Matthieu and Mostefaoui, Achour and Jard, Claude}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In distributed systems where strong consistency is costly when not impossible, causal consistency provides a valuable abstraction to represent program executions as partial orders. In addition to the sequential program order of each computing entity, causal order also contains the semantic links between the events that affect the shared objects -- messages emission and reception in a communication channel, reads and writes on a shared register. Usual approaches based on semantic links are very difficult to adapt to other data types such as queues or counters because they require a specific analysis of causal dependencies for each data type. This paper presents a new approach to define causal consistency for any abstract data type based on sequential specifications. It explores, formalizes and studies the differences between three variations of causal consistency and highlights them in the light of PRAM, eventual consistency and sequential consistency: weak causal consistency, that captures the notion of causality preservation when focusing on convergence; causal convergence that mixes weak causal consistency and convergence; and causal consistency, that coincides with causal memory when applied to shared memory.}},
  url = {https://doi.org/10.1145/2851141.2851170},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup946,
  title = {Composing concurrency control}},
  author = {Ziv, Ofri and Aiken, Alex and Golan-Gueta, Guy and Ramalingam, G. and Sagiv, Mooly}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Concurrency control poses significant challenges when composing computations over multiple data-structures (objects) with different concurrency-control implementations. We formalize the usually desired requirements (serializability, abort-safety, deadlock-safety, and opacity) as well as stronger versions of these properties that enable composition. We show how to compose protocols satisfying these properties so that the resulting combined protocol also satisfies these properties. Our approach generalizes well-known protocols (such as two-phase-locking and two-phase-commit) and leads to new protocols. We apply this theory to show how we can safely compose optimistic and pessimistic concurrency control. For example, we show how we can execute a transaction that accesses two objects, one controlled by an STM and another by locking.}},
  url = {https://doi.org/10.1145/2737924.2737970},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup947,
  title = {Types, bytes, and separation logic}},
  author = {Tuch, Harvey and Klein, Gerwin and Norrish, Michael}},
  year = {2007}},
  journal = {Proceedings of the 34th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a formal model of memory that both captures the low-level features of C's pointers and memory, and that forms the basis for an expressive implementation of separation logic. At the low level, we do not commit common oversimplifications, but correctly deal with C's model of programming language values and the heap. At the level of separation logic, we are still able to reason abstractly and efficiently. We implement this framework in the theorem prover Isabelle/HOL and demonstrate it on two case studies. We show that the divide between detailed and abstract does not impose undue verification overhead, and that simple programs remain easy to verify. We also show that the framework is applicable to real, security- and safety-critical code by formally verifying the memory allocator of the L4 microkernel.}},
  url = {https://doi.org/10.1145/1190216.1190234},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup948,
  title = {Recaf: Java dialects as libraries}},
  author = {Biboudis, Aggelos and Inostroza, Pablo and Storm, Tijs van der}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Mainstream programming languages like Java have limited support for language extensibility. Without mechanisms for syntactic abstraction, new programming styles can only be embedded in the form of libraries, limiting expressiveness. In this paper, we present Recaf, a lightweight tool for creating Java dialects; effectively extending Java with new language constructs and user defined semantics. The Recaf compiler generically transforms designated method bodies to code that is parameterized by a semantic factory (Object Algebra), defined in plain Java. The implementation of such a factory defines the desired runtime semantics. We applied our design to produce several examples from a diverse set of programming styles and two case studies: we define i) extensions for generators, asynchronous computations and asynchronous streams and ii) a Domain-Specific Language (DSL) for Parsing Expression Grammars (PEGs), in a few lines of code.}},
  url = {https://doi.org/10.1145/2993236.2993239},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup949,
  title = {A constraint-based architecture for local search}},
  author = {Michel, Laurent and Hentenryck, Pascal Van}},
  year = {2002}},
  journal = {Proceedings of the 17th ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Combinatorial optimization problems are ubiquitous in numerous practical applications. Yet most of them are challenging, both from computational complexity and programming standpoints. Local search is one of the main approaches to address these problems. However, it often requires sophisticated incremental algorithms and data structures, and considerable experimentation. This paper proposes a constraint-based, object-oriented, architecture to reduce the development time of local search algorithms significantly. The architecture consists of declarative and search components. The declarative component includes invariants, which maintain complex expressions incrementally, and differentiable objects, which maintain properties that can be queried to evaluate the effect of local moves. Differentiable objects are high-level modeling concepts, such as constraints and functions, that capture combinatorial substructures arising in many applications. The search component supports various abstractions to specify heuristics and meta-heuristics. We illustrate the architecture with the language Comet and several applications, such as car sequencing and the progressive party problem. The applications indicate that the architecture allows for very high-level modeling of local search algorithms, while preserving excellent performance.}},
  url = {https://doi.org/10.1145/582419.582430},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup950,
  title = {Algorithms for algebraic path properties in concurrent systems of constant treewidth components}},
  author = {Chatterjee, Krishnendu and Goharshady, Amir Kafshdar and Ibsen-Jensen, Rasmus and Pavlogiannis, Andreas}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We study algorithmic questions for concurrent systems where the transitions are labeled from a complete, closed semiring, and path properties are algebraic with semiring operations. The algebraic path properties can model dataflow analysis problems, the shortest path problem, and many other natural problems that arise in program analysis. We consider that each component of the concurrent system is a graph with constant treewidth, a property satisfied by the controlflow graphs of most programs. We allow for multiple possible queries, which arise naturally in demand driven dataflow analysis. The study of multiple queries allows us to consider the tradeoff between the resource usage of the one-time preprocessing and for each individual query. The traditional approach constructs the product graph of all components and applies the best-known graph algorithm on the product. In this approach, even the answer to a single query requires the transitive closure (i.e., the results of all possible queries), which provides no room for tradeoff between preprocessing and query time. Our main contributions are algorithms that significantly improve the worst-case running time of the traditional approach, and provide various tradeoffs depending on the number of queries. For example, in a concurrent system of two components, the traditional approach requires hexic time in the worst case for answering one query as well as computing the transitive closure, whereas we show that with one-time preprocessing in almost cubic time, each subsequent query can be answered in at most linear time, and even the transitive closure can be computed in almost quartic time. Furthermore, we establish conditional optimality results showing that the worst-case running time of our algorithms cannot be improved without achieving major breakthroughs in graph algorithms (i.e., improving the worst-case bound for the shortest path problem in general graphs). Preliminary experimental results show that our algorithms perform favorably on several benchmarks.}},
  url = {https://doi.org/10.1145/2837614.2837624},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup951,
  title = {Putting in all the stops: execution control for JavaScript}},
  author = {Baxter, Samuel and Nigam, Rachit and Politz, Joe Gibbs and Krishnamurthi, Shriram and Guha, Arjun}},
  year = {2018}},
  journal = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Scores of compilers produce JavaScript, enabling programmers to use many languages on the Web, reuse existing code, and even use Web IDEs. Unfortunately, most compilers inherit the browser's compromised execution model, so long-running programs freeze the browser tab, infinite loops crash IDEs, and so on. The few compilers that avoid these problems suffer poor performance and are difficult to engineer. This paper presents Stopify, a source-to-source compiler that extends JavaScript with debugging abstractions and blocking operations, and easily integrates with existing compilers. We apply Stopify to ten programming languages and develop a Web IDE that supports stopping, single-stepping, breakpointing, and long-running computations. For nine languages, Stopify requires no or trivial compiler changes. For eight, our IDE is the first that provides these features. Two of our subject languages have compilers with similar features. Stopify's performance is competitive with these compilers and it makes them dramatically simpler. Stopify's abstractions rely on first-class continuations, which it provides by compiling JavaScript to JavaScript. We also identify sub-languages of JavaScript that compilers implicitly use, and exploit these to improve performance. Finally, Stopify needs to repeatedly interrupt and resume program execution. We use a sampling-based technique to estimate program speed that outperforms other systems.}},
  url = {https://doi.org/10.1145/3192366.3192370},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup952,
  title = {The marriage of bisimulations and Kripke logical relations}},
  author = {Hur, Chung-Kil and Dreyer, Derek and Neis, Georg and Vafeiadis, Viktor}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {There has been great progress in recent years on developing effective techniques for reasoning about program equivalence in ML-like languages---that is, languages that combine features like higher-order functions, recursive types, abstract types, and general mutable references. Two of the most prominent types of techniques to have emerged are *bisimulations* and *Kripke logical relations (KLRs)*. While both approaches are powerful, their complementary advantages have led us and other researchers to wonder whether there is an essential tradeoff between them. Furthermore, both approaches seem to suffer from fundamental limitations if one is interested in scaling them to inter-language reasoning. In this paper, we propose *relation transition systems (RTSs)*, which marry together some of the most appealing aspects of KLRs and bisimulations. In particular, RTSs show how bisimulations' support for reasoning about recursive features via *coinduction* can be synthesized with KLRs' support for reasoning about local state via *state transition systems*. Moreover, we have designed RTSs to avoid the limitations of KLRs and bisimulations that preclude their generalization to inter-language reasoning. Notably, unlike KLRs, RTSs are transitively composable.}},
  url = {https://doi.org/10.1145/2103656.2103666},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup953,
  title = {Cache-tries: concurrent lock-free hash tries with constant-time operations}},
  author = {Prokopec, Aleksandar}},
  year = {2018}},
  journal = {Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Concurrent non-blocking hash tries have good cache locality, and horizontally scalable operations. However, operations on most existing concurrent hash tries run in O(log n) time.In this paper, we show that the concurrent hash trie operations can run in expected constant time. We present a novel lock-free concurrent hash trie design that exerts less pressure on the memory allocator. This hash trie is augmented with a quiescently consistent cache, which permits the basic operations to run in expected O(1) time. We show a statistical analysis for the constant-time bound, which, to the best of our knowledge, is the first such proof for hash tries. We also prove the safety, lock-freedom and linearizability properties. On typical workloads, our implementation demonstrates up to 5X performance improvements with respect to the previous hash trie variants.}},
  url = {https://doi.org/10.1145/3178487.3178498},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup954,
  title = {Finding root causes of floating point error}},
  author = {Sanchez-Stern, Alex and Panchekha, Pavel and Lerner, Sorin and Tatlock, Zachary}},
  year = {2018}},
  journal = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Floating-point arithmetic plays a central role in science, engineering, and finance by enabling developers to approximate real arithmetic. To address numerical issues in large floating-point applications, developers must identify root causes, which is difficult because floating-point errors are generally non-local, non-compositional, and non-uniform. This paper presents Herbgrind, a tool to help developers identify and address root causes in numerical code written in low-level languages like C/C++ and Fortran. Herbgrind dynamically tracks dependencies between operations and program outputs to avoid false positives and abstracts erroneous computations to simplified program fragments whose improvement can reduce output error. We perform several case studies applying Herbgrind to large, expert-crafted numerical programs and show that it scales to applications spanning hundreds of thousands of lines, correctly handling the low-level details of modern floating point hardware and mathematical libraries and tracking error across function boundaries and through the heap.}},
  url = {https://doi.org/10.1145/3192366.3192411},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup955,
  title = {GPS: navigating weak memory with ghosts, protocols, and separation}},
  author = {Turon, Aaron and Vafeiadis, Viktor and Dreyer, Derek}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \& Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Weak memory models formalize the inconsistent behaviors that one can expect to observe in multithreaded programs running on modern hardware. In so doing, however, they complicate the already-difficult task of reasoning about correctness of concurrent code. Worse, they render impotent the sophisticated formal methods that have been developed to tame concurrency, which almost universally assume a strong (i.e. sequentially consistent) memory model.This paper introduces GPS, the first program logic to provide a full-fledged suite of modern verification techniques - including ghost state, protocols, and separation logic - for high-level, structured reasoning about weak memory. We demonstrate the effectiveness of GPS by applying it to challenging examples drawn from the Linux kernel as well as lock-free data structures. We also define the semantics of GPS and prove in Coq that it is sound with respect to the axiomatic C11 weak memory model.}},
  url = {https://doi.org/10.1145/2660193.2660243},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup956,
  title = {Protocol-based verification of message-passing parallel programs}},
  author = {L\'{o}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present ParTypes, a type-based methodology for the verification of Message Passing Interface (MPI) programs written in the C programming language. The aim is to statically verify programs against protocol specifications, enforcing properties such as fidelity and absence of deadlocks. We develop a protocol language based on a dependent type system for message-passing parallel programs, which includes various communication operators, such as point-to-point messages, broadcast, reduce, array scatter and gather. For the verification of a program against a given protocol, the protocol is first translated into a representation read by VCC, a software verifier for C. We successfully verified several MPI programs in a running time that is independent of the number of processes or other input parameters. This contrasts with alternative techniques, notably model checking and runtime verification, that suffer from the state-explosion problem or that otherwise depend on parameters to the program itself. We experimentally evaluated our approach against state-of-the-art tools for MPI to conclude that our approach offers a scalable solution.}},
  url = {https://doi.org/10.1145/2814270.2814302},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup957,
  title = {Are web applications ready for parallelism?}},
  author = {Radoi, Cosmin and Herhut, Stephan and Sreeram, Jaswanth and Dig, Danny}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In recent years, web applications have become pervasive. Their backbone is JavaScript, the only programming language supported by all major web browsers. Most browsers run on desktop or mobile devices with parallel hardware. However, JavaScript is by design sequential, and current web applications make little use of hardware parallelism. Are web applications ready to exploit parallel hardware? We answer the question in two steps: First, we survey 174 web developers about the potential and challenges of using parallelism. Then, we study the performance and computation shape of a set of web applications that are representative for the emerging web. Our findings indicate that emerging web applications do have latent data parallelism, and JavaScript developers' programming style is not a significant impediment to exploiting this parallelism.}},
  url = {https://doi.org/10.1145/2688500.2700995},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup958,
  title = {System f-omega with equirecursive types for datatype-generic programming}},
  author = {Cai, Yufei and Giarrusso, Paolo G. and Ostermann, Klaus}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Traversing an algebraic datatype by hand requires boilerplate code which duplicates the structure of the datatype. Datatype-generic programming (DGP) aims to eliminate such boilerplate code by decomposing algebraic datatypes into type constructor applications from which generic traversals can be synthesized. However, different traversals require different decompositions, which yield isomorphic but unequal types. This hinders the interoperability of different DGP techniques. In this paper, we propose Fωμ, an extension of the higher-order polymorphic lambda calculus Fω with records, variants, and equirecursive types. We prove the soundness of the type system, and show that type checking for first-order recursive types is decidable with a practical type checking algorithm. In our soundness proof we define type equality by interpreting types as infinitary λ-terms (in particular, Berarducci-trees). To decide type equality we β-normalize types, and then use an extension of equivalence checking for usual equirecursive types. Thanks to equirecursive types, new decompositions for a datatype can be added modularly and still interoperate with each other, allowing multiple DGP techniques to work together. We sketch how generic traversals can be synthesized, and apply these components to some examples. Since the set of datatype decomposition becomes extensible, System Fωμ enables using DGP techniques incrementally, instead of planning for them upfront or doing invasive refactoring.}},
  url = {https://doi.org/10.1145/2837614.2837660},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup959,
  title = {Automatic patch generation by learning correct code}},
  author = {Long, Fan and Rinard, Martin}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present Prophet, a novel patch generation system that works with a set of successful human patches obtained from open- source software repositories to learn a probabilistic, application-independent model of correct code. It generates a space of candidate patches, uses the model to rank the candidate patches in order of likely correctness, and validates the ranked patches against a suite of test cases to find correct patches. Experimental results show that, on a benchmark set of 69 real-world defects drawn from eight open-source projects, Prophet significantly outperforms the previous state-of-the-art patch generation system.}},
  url = {https://doi.org/10.1145/2837614.2837617},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup960,
  title = {SATCheck: SAT-directed stateless model checking for SC and TSO}},
  author = {Demsky, Brian and Lam, Patrick}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Writing low-level concurrent code is well known to be challenging and error prone. The widespread deployment of multi-core hardware and the shift towards using low-level concurrent data structures has moved the problem into the mainstream. Finding bugs in such code may require finding a specific bug-revealing thread interleaving out of a huge space of parallel executions. Model-checking is a powerful technique for exhaustively testing code. However, scaling model checking presents a significant challenge. In this paper we present a new and more scalable technique for model checking concurrent code, based on concrete execution. Our technique observes concrete behaviors, builds a model of these behaviors, encodes the model in SAT, and leverages SAT solver technology to find executions that reveal new behaviors. It then runs the new execution, incorporates the newly observed behavior, and repeats the process until it has explored all reachable behaviors. We have implemented a prototype of our approach in the SATCheck tool. Our tool supports both the Total Store Ordering (TSO) and Sequentially Consistent (SC) memory models. We evaulate SATCheck by testing several concurrent data structure implementations and comparing its performance to the original DPOR stateless model checking algorithm implemented in CDSChecker, the source DPOR algorithm implemented in Nidhugg, and CheckFence. Our experiments show that SATCheck scales better than previous approaches while at the same time operating on concrete executions.}},
  url = {https://doi.org/10.1145/2814270.2814297},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup961,
  title = {Automated synthesis of symbolic instruction encodings from I/O samples}},
  author = {Godefroid, Patrice and Taly, Ankur}},
  year = {2012}},
  journal = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Symbolic execution is a key component of precise binary program analysis tools. We discuss how to automatically boot-strap the construction of a symbolic execution engine for a processor instruction set such as x86, x64 or ARM. We show how to automatically synthesize symbolic representations of individual processor instructions from input/output examples and express them as bit-vector constraints. We present and compare various synthesis algorithms and instruction sampling strategies. We introduce a new synthesis algorithm based on smart sampling which we show is one to two orders of magnitude faster than previous synthesis algorithms in our context. With this new algorithm, we can automatically synthesize bit-vector circuits for over 500 x86 instructions (8/16/32-bits, outputs, EFLAGS) using only 6 synthesis templates and in less than two hours using the Z3 SMT solver on a regular machine. During this work, we also discovered several inconsistencies across x86 processors, errors in the x86 Intel spec, and several bugs in previous manually-written x86 instruction handlers.}},
  url = {https://doi.org/10.1145/2254064.2254116},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup962,
  title = {A kripke logical relation for effect-based program transformations}},
  author = {Thamsborg, Jacob and Birkedal, Lars}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a Kripke logical relation for showing the correctness of program transformations based on a type-and-effect system for an ML-like programming language with higher-order store and dynamic allocation.We show how to use our model to verify a number of interesting program transformations that rely on effect annotations.Our model is constructed as a step-indexed model over the standard operational semantics of the programming language. It extends earlier work [7, 8]that has considered, respectively, dynamically allocated first-order references and higher-order store for global variables (but no dynamic allocation). It builds on ideas from region-based memory management [21], and on Kripke logical relations for higher-order store [12, 14].Our type-and-effect system is region-based and includes a region-masking rule which allows to hide local effects. One of the key challenges in the model construction for dynamically allocated higher-order store is that the meaning of a type may change since references, conceptually speaking, may become dangling due to region-masking. We explain how our Kripke model can be used to show correctness of program transformations for programs involving references that, conceptually, are dangling.}},
  url = {https://doi.org/10.1145/2034773.2034831},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup963,
  title = {Meta-theory \`{a}},
  author = {Delaware, Benjamin and d. S. Oliveira, Bruno C. and Schrijvers, Tom}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Formalizing meta-theory, or proofs about programming languages, in a proof assistant has many well-known benefits. Unfortunately, the considerable effort involved in mechanizing proofs has prevented it from becoming standard practice. This cost can be amortized by reusing as much of existing mechanized formalizations as possible when building a new language or extending an existing one. One important challenge in achieving reuse is that the inductive definitions and proofs used in these formalizations are closed to extension. This forces language designers to cut and paste existing definitions and proofs in an ad-hoc manner and to expend considerable effort to patch up the results.The key contribution of this paper is the development of an induction technique for extensible Church encodings using a novel reinterpretation of the universal property of folds. These encodings provide the foundation for a framework, formalized in Coq, which uses type classes to automate the composition of proofs from modular components. This framework enables a more structured approach to the reuse of meta-theory formalizations through the composition of modular inductive definitions and proofs.Several interesting language features, including binders and general recursion, illustrate the capabilities of our framework. We reuse these features to build fully mechanized definitions and proofs for a number of languages, including a version of mini-ML. Bounded induction enables proofs of properties for non-inductive semantic functions, and mediating type classes enable proof adaptation for more feature-rich languages.}},
  url = {https://doi.org/10.1145/2429069.2429094},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup964,
  title = {Rigorous floating-point mixed-precision tuning}},
  author = {Chiang, Wei-Fan and Baranowski, Mark and Briggs, Ian and Solovyev, Alexey and Gopalakrishnan, Ganesh and Rakamari\'{c}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Virtually all real-valued computations are carried out using floating-point data types and operations. The precision of these data types must be set with the goals of reducing the overall round-off error, but also emphasizing performance improvements. Often, a mixed-precision allocation achieves this optimum; unfortunately, there are no techniques available to compute such allocations and conservatively meet a given error target across all program inputs. In this work, we present a rigorous approach to precision allocation based on formal analysis via Symbolic Taylor Expansions, and error analysis based on interval functions. This approach is implemented in an automated tool called FPTuner that generates and solves a quadratically constrained quadratic program to obtain a precision-annotated version of the given expression. FPTuner automatically introduces all the requisite precision up and down casting operations. It also allows users to flexibly control precision allocation using constraints to cap the number of high precision operators as well as group operators to allocate the same precision to facilitate vectorization. We evaluate FPTuner by tuning several benchmarks and measuring the proportion of lower precision operators allocated as we increase the error threshold. We also measure the reduction in energy consumption resulting from executing mixed-precision tuned code on a real hardware platform. We observe significant energy savings in response to mixed-precision tuning, but also observe situations where unexpected compiler behaviors thwart intended optimizations.}},
  url = {https://doi.org/10.1145/3009837.3009846},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup965,
  title = {Scalable race detection for Android applications}},
  author = {Bielik, Pavol and Raychev, Veselin and Vechev, Martin}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a complete end-to-end dynamic analysis system for finding data races in mobile Android applications. The capabilities of our system significantly exceed the state of the art: our system can analyze real-world application interactions in minutes rather than hours, finds errors inherently beyond the reach of existing approaches, while still (critically) reporting very few false positives. Our system is based on three key concepts: (i) a thorough happens-before model of Android-specific concurrency, (ii) a scalable analysis algorithm for efficiently building and querying the happens-before graph, and (iii) an effective set of domain-specific filters that reduce the number of reported data races by several orders of magnitude. We evaluated the usability and performance of our system on 354 real-world Android applications (e.g., Facebook). Our system analyzes a minute of end-user interaction with the application in about 24 seconds, while current approaches take hours to complete. Inspecting the results for 8 large open-source applications revealed 15 harmful bugs of diverse kinds. Some of the bugs we reported were confirmed and fixed by developers.}},
  url = {https://doi.org/10.1145/2814270.2814303},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup966,
  title = {Abstract acceleration of general linear loops}},
  author = {Jeannet, Bertrand and Schrammel, Peter and Sankaranarayanan, Sriram}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present abstract acceleration techniques for computing loop invariants for numerical programs with linear assignments and conditionals. Whereas abstract interpretation techniques typically over-approximate the set of reachable states iteratively, abstract acceleration captures the effect of the loop with a single, non-iterative transfer function applied to the initial states at the loop head. In contrast to previous acceleration techniques, our approach applies to any linear loop without restrictions. Its novelty lies in the use of the Jordan normal form decomposition of the loop body to derive symbolic expressions for the entries of the matrix modeling the effect of η ≥ Ο iterations of the loop. The entries of such a matrix depend on η through complex polynomial, exponential and trigonometric functions. Therefore, we introduces an abstract domain for matrices that captures the linear inequality relations between these complex expressions. This results in an abstract matrix for describing the fixpoint semantics of the loop.Our approach integrates smoothly into standard abstract interpreters and can handle programs with nested loops and loops containing conditional branches. We evaluate it over small but complex loops that are commonly found in control software, comparing it with other tools for computing linear loop invariants. The loops in our benchmarks typically exhibit polynomial, exponential and oscillatory behaviors that present challenges to existing approaches. Our approach finds non-trivial invariants to prove useful bounds on the values of variables for such loops, clearly outperforming the existing approaches in terms of precision while exhibiting good performance.}},
  url = {https://doi.org/10.1145/2535838.2535843},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup967,
  title = {Sound predictive race detection in polynomial time}},
  author = {Smaragdakis, Yannis and Evans, Jacob and Sadowski, Caitlin and Yi, Jaeheon and Flanagan, Cormac}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Data races are among the most reliable indicators of programming errors in concurrent software. For at least two decades, Lamport's happens-before (HB) relation has served as the standard test for detecting races--other techniques, such as lockset-based approaches, fail to be sound, as they may falsely warn of races. This work introduces a new relation, causally-precedes (CP), which generalizes happens-before to observe more races without sacrificing soundness. Intuitively, CP tries to capture the concept of happens-before ordered events that must occur in the observed order for the program to observe the same values. What distinguishes CP from past predictive race detection approaches (which also generalize an observed execution to detect races in other plausible executions) is that CP-based race detection is both sound and of polynomial complexity. We demonstrate that the unique aspects of CP result in practical benefit. Applying CP to real-world programs, we successfully analyze server-level applications (e.g., Apache FtpServer) and show that traces longer than in past predictive race analyses can be analyzed in mere seconds to a few minutes. For these programs, CP race detection uncovers races that are hard to detect by repeated execution and HB race detection: a single run of CP race detection produces several races not discovered by 10 separate rounds of happens-before race detection.}},
  url = {https://doi.org/10.1145/2103656.2103702},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup968,
  title = {Interactively verifying absence of explicit information flows in Android apps}},
  author = {Bastani, Osbert and Anand, Saswat and Aiken, Alex}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {App stores are increasingly the preferred mechanism for distributing software, including mobile apps (Google Play), desktop apps (Mac App Store and Ubuntu Software Center), computer games (the Steam Store), and browser extensions (Chrome Web Store). The centralized nature of these stores has important implications for security. While app stores have unprecedented ability to audit apps, users now trust hosted apps, making them more vulnerable to malware that evades detection and finds its way onto the app store. Sound static explicit information flow analysis has the potential to significantly aid human auditors, but it is handicapped by high false positive rates. Instead, auditors currently rely on a combination of dynamic analysis (which is unsound) and lightweight static analysis (which cannot identify information flows) to help detect malicious behaviors. We propose a process for producing apps certified to be free of malicious explicit information flows. In practice, imprecision in the reachability analysis is a major source of false positive information flows that are difficult to understand and discharge. In our approach, the developer provides tests that specify what code is reachable, allowing the static analysis to restrict its search to tested code. The app hosted on the store is instrumented to enforce the provided specification (i.e., executing untested code terminates the app). We use abductive inference to minimize the necessary instrumentation, and then interact with the developer to ensure that the instrumentation only cuts unreachable code. We demonstrate the effectiveness of our approach in verifying a corpus of 77 Android apps—our interactive verification process successfully discharges 11 out of the 12 false positives.}},
  url = {https://doi.org/10.1145/2814270.2814274},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup969,
  title = {An operational semantics for Simulink's simulation engine}},
  author = {Bouissou, Olivier and Chapoutot, Alexandre}},
  year = {2012}},
  journal = {Proceedings of the 13th ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, Tools and Theory for Embedded Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The industrial tool Matlab/Simulink is widely used in the design of embedded systems. The main feature of this tool is its ability to model in a common formalism the software and its physical environment. This makes it very useful for validating the design of embedded software using numerical simulation. However, the formal verification of such models is still problematic as Simulink is a programming language for which no formal semantics exists. In this article, we present an operational semantics of a representative subset of Simulink which includes both continuous-time and discrete-time blocks. We believe that this work gives a better understanding of Simulink and it defines the foundations of a general framework to apply formal methods on Simulink's high level descriptions of embedded systems.}},
  url = {https://doi.org/10.1145/2248418.2248437},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup970,
  title = {MIX10: compiling MATLAB to X10 for high performance}},
  author = {Kumar, Vineet and Hendren, Laurie}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {MATLAB is a popular dynamic array-based language commonly used by students, scientists and engineers who appreciate the interactive development style, the rich set of array operators, the extensive builtin library, and the fact that they do not have to declare static types. Even though these users like to program in MATLAB, their computations are often very compute-intensive and are better suited for emerging high performance computing systems. This paper reports on MIX10, a source-to-source compiler that automatically translates MATLAB programs to X10, a language designed for "Performance and Productivity at Scale"; thus, helping scientific programmers make better use of high performance computing systems.There is a large semantic gap between the array-based dynamically-typed nature of MATLAB and the object-oriented, statically-typed, and high-level array abstractions of X10. This paper addresses the major challenges that must be overcome to produce sequential X10 code that is competitive with state-of-the-art static compilers for MATLAB which target more conventional imperative languages such as C and Fortran. Given that efficient basis, the paper then provides a translation for the MATLAB parfor construct that leverages the powerful concurrency constructs in X10.The MIX10 compiler has been implemented using the McLab compiler tools, is open source, and is available both for compiler researchers and end-user MATLAB programmers. We have used the implementation to perform many empirical measurements on a set of 17 MATLAB benchmarks. We show that our best MIX10-generated code is significantly faster than the de facto Mathworks' MATLAB system, and that our results are competitive with state-of-the-art static compilers that target C and Fortran. We also show the importance of finding the correct approach to representing the arrays in the generated X10 code, and the necessity of an IntegerOkay' analysis that determines which double variables can be safely represented as integers. Finally, we show that our X10-based handling of the MATLAB parfor greatly outperforms the de facto MATLAB implementation.}},
  url = {https://doi.org/10.1145/2660193.2660218},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup971,
  title = {A framework for practical parallel fast matrix multiplication}},
  author = {Benson, Austin R. and Ballard, Grey}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Matrix multiplication is a fundamental computation in many scientific disciplines. In this paper, we show that novel fast matrix multiplication algorithms can significantly outperform vendor implementations of the classical algorithm and Strassen's fast algorithm on modest problem sizes and shapes. Furthermore, we show that the best choice of fast algorithm depends not only on the size of the matrices but also the shape. We develop a code generation tool to automatically implement multiple sequential and shared-memory parallel variants of each fast algorithm, including our novel parallelization scheme. This allows us to rapidly benchmark over 20 fast algorithms on several problem sizes. Furthermore, we discuss a number of practical implementation issues for these algorithms on shared-memory machines that can direct further research on making fast algorithms practical.}},
  url = {https://doi.org/10.1145/2688500.2688513},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup972,
  title = {Semantic subtyping with an SMT solver}},
  author = {Bierman, Gavin M. and Gordon, Andrew D. and Hri\c{t}},
  year = {2010}},
  journal = {Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We study a first-order functional language with the novel combination of the ideas of refinement type (the subset of a type to satisfy a Boolean expression) and type-test (a Boolean expression testing whether a value belongs to a type). Our core calculus can express a rich variety of typing idioms; for example, intersection, union, negation, singleton, nullable, variant, and algebraic types are all derivable. We formulate a semantics in which expressions denote terms, and types are interpreted as first-order logic formulas. Subtyping is defined as valid implication between the semantics of types. The formulas are interpreted in a specific model that we axiomatize using standard first-order theories. On this basis, we present a novel type-checking algorithm able to eliminate many dynamic tests and to detect many errors statically. The key idea is to rely on an SMT solver to compute subtyping efficiently. Moreover, interpreting types as formulas allows us to call the SMT solver at run-time to compute instances of types.}},
  url = {https://doi.org/10.1145/1863543.1863560},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup973,
  title = {QR factorization with Morton-ordered quadtree matrices for memory re-use and parallelism}},
  author = {Frens, Jeremy D. and Wise, David S.}},
  year = {2003}},
  journal = {Proceedings of the Ninth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Quadtree matrices using Morton-order storage provide natural blocking on every level of a memory hierarchy. Writing the natural recursive algorithms to take advantage of this blocking results in code that honors the memory hierarchy without the need for transforming the code. Furthermore, the divide-and-conquer algorithm breaks problems down into independent computations. These independent computations can be dispatched in parallel for straightforward parallel processing.Proof-of-concept is given by an algorithm for QR factorization based on Givens rotations for quadtree matrices in Morton-order storage. The algorithms deliver positive results, competing with and even beating the LAPACK equivalent.}},
  url = {https://doi.org/10.1145/781498.781525},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup974,
  title = {Extending a C-like language for portable SIMD programming}},
  author = {Lei\ss{}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {SIMD instructions are common in CPUs for years now. Using these instructions effectively requires not only vectorization of code, but also modifications to the data layout. However, automatic vectorization techniques are often not powerful enough and suffer from restricted scope of applicability; hence, programmers often vectorize their programs manually by using intrinsics: compiler-known functions that directly expand to machine instructions. They significantly decrease programmer productivity by enforcing a very error-prone and hard-to-read assembly-like programming style. Furthermore, intrinsics are not portable because they are tied to a specific instruction set.In this paper, we show how a C-like language can be extended to allow for portable and efficient SIMD programming. Our extension puts the programmer in total control over where and how control-flow vectorization is triggered. We present a type system and a formal semantics of our extension and prove the soundness of the type system. Using our prototype implementation IVL that targets Intel's MIC architecture and SSE instruction set, we show that the generated code is roughly on par with handwritten intrinsic code.}},
  url = {https://doi.org/10.1145/2145816.2145825},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup975,
  title = {Meta-level features in an industrial-strength theorem prover}},
  author = {Moore, J. Strother}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The ACL2 theorem prover---the current incarnation of "the" Boyer-Moore theorem prover---is a theorem prover for an extension of a first-order, applicative subset of Common Lisp. The ACL2 system provides a useful specification and modeling language as well as a useful mechanical theorem proving environment. ACL2 is in use at several major microprocessor manufacturers to verify functional correctness of important components of commercial designs. This talk explores the design of ACL2 and the tradeoffs that have turned out to be pivotal to its success.}},
  url = {https://doi.org/10.1145/2103656.2103707},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup976,
  title = {Full abstraction for nominal Scott domains}},
  author = {L\"{o}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We develop a domain theory within nominal sets and present programming language constructs and results that can be gained from this approach. The development is based on the concept of orbit-finite subset, that is, a subset of a nominal sets that is both finitely supported and contained in finitely many orbits. This concept appears prominently in the recent research programme of Bojanczyk et al. on automata over infinite languages, and our results establish a connection between their work and a characterisation of topological compactness discovered, in a quite different setting, by Winskel and Turner as part of a nominal domain theory for concurrency. We use this connection to derive a notion of Scott domain within nominal sets. The functionals for existential quantification over names and `definite description' over names turn out to be compact in the sense appropriate for nominal Scott domains. Adding them, together with parallel-or, to a programming language for recursively defined higher-order functions with name abstraction and locally scoped names, we prove a full abstraction result for nominal Scott domains analogous to Plotkin's classic result about PCF and conventional Scott domains: two program phrases have the same observable operational behaviour in all contexts if and only if they denote equal elements of the nominal Scott domain model. This is the first full abstraction result we know of for higher-order functions with local names that uses a domain theory based on ordinary extensional functions, rather than using the more intensional approach of game semantics.}},
  url = {https://doi.org/10.1145/2429069.2429073},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup977,
  title = {Toward compositional verification of interruptible OS kernels and device drivers}},
  author = {Chen, Hao and Wu, Xiongnan (Newman) and Shao, Zhong and Lockerman, Joshua and Gu, Ronghui}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {An operating system (OS) kernel forms the lowest level of any system software stack. The correctness of the OS kernel is the basis for the correctness of the entire system. Recent efforts have demonstrated the feasibility of building formally verified general-purpose kernels, but it is unclear how to extend their work to verify the functional correctness of device drivers, due to the non-local effects of interrupts. In this paper, we present a novel compositional framework for building certified interruptible OS kernels with device drivers. We provide a general device model that can be instantiated with various hardware devices, and a realistic formal model of interrupts, which can be used to reason about interruptible code. We have realized this framework in the Coq proof assistant. To demonstrate the effectiveness of our new approach, we have successfully extended an existing verified non-interruptible kernel with our framework and turned it into an interruptible kernel with verified device drivers. To the best of our knowledge, this is the first verified interruptible operating system with device drivers.}},
  url = {https://doi.org/10.1145/2908080.2908101},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup978,
  title = {Proving acceptability properties of relaxed nondeterministic approximate programs}},
  author = {Carbin, Michael and Kim, Deokhwan and Misailovic, Sasa and Rinard, Martin C.}},
  year = {2012}},
  journal = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Approximate program transformations such as skipping tasks [29, 30], loop perforation [21, 22, 35], reduction sampling [38], multiple selectable implementations [3, 4, 16, 38], dynamic knobs [16], synchronization elimination [20, 32], approximate function memoization [11],and approximate data types [34] produce programs that can execute at a variety of points in an underlying performance versus accuracy tradeoff space. These transformed programs have the ability to trade accuracy of their results for increased performance by dynamically and nondeterministically modifying variables that control their execution.We call such transformed programs relaxed programs because they have been extended with additional nondeterminism to relax their semantics and enable greater flexibility in their execution.We present language constructs for developing and specifying relaxed programs. We also present proof rules for reasoning about properties [28] which the program must satisfy to be acceptable. Our proof rules work with two kinds of acceptability properties: acceptability properties [28], which characterize desired relationships between the values of variables in the original and relaxed programs, and unary acceptability properties, which involve values only from a single (original or relaxed) program. The proof rules support a staged reasoning approach in which the majority of the reasoning effort works with the original program. Exploiting the common structure that the original and relaxed programs share, relational reasoning transfers reasoning effort from the original program to prove properties of the relaxed program.We have formalized the dynamic semantics of our target programming language and the proof rules in Coq and verified that the proof rules are sound with respect to the dynamic semantics. Our Coq implementation enables developers to obtain fully machine-checked verifications of their relaxed programs.}},
  url = {https://doi.org/10.1145/2254064.2254086},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup979,
  title = {A hierarchy of mendler style recursion combinators: taming inductive datatypes with negative occurrences}},
  author = {Ahn, Ki Yung and Sheard, Tim}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The Mendler style catamorphism (which corresponds to weak induction) always terminates even for negative inductive datatypes. The Mendler style histomorphism (which corresponds to strong induction) is known to terminate for positive inductive datatypes. To our knowledge, the literature is silent on its termination properties for negative datatypes. In this paper, we prove that histomorphisms do not always termintate by showing a counter-example. We also enrich the Mendler collection of recursion combinators by defining a new form of Mendler style catamorphism (msfcata), which terminates for all inductive datatypes, that is more expressive than the original. We organize the collection of combinators by placing them into a hierarchy of ever increasing generality, and describing the termination properties of each point on the hierarchy. We also provide many examples (including a case study on a negative inductive datatype), which illustrate both the expressive power and beauty of the Mendler style. One lesson we learn from this work is that weak induction applies to negative inductive datatypes but strong induction is problematic. We provide a proof of weak induction by exhibiting an embedding of our new combinator into Fω. We pose the open question: Is there a safe way to apply strong induction to negative inductive datatypes?}},
  url = {https://doi.org/10.1145/2034773.2034807},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup980,
  title = {Pick your contexts well: understanding object-sensitivity}},
  author = {Smaragdakis, Yannis and Bravenboer, Martin and Lhot\'{a}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Object-sensitivity has emerged as an excellent context abstraction for points-to analysis in object-oriented languages. Despite its practical success, however, object-sensitivity is poorly understood. For instance, for a context depth of 2 or higher, past scalable implementations deviate significantly from the original definition of an object-sensitive analysis. The reason is that the analysis has many degrees of freedom, relating to which context elements are picked at every method call and object creation. We offer a clean model for the analysis design space, and discuss a formal and informal understanding of object-sensitivity and of how to create good object-sensitive analyses. The results are surprising in their extent. We find that past implementations have made a sub-optimal choice of contexts, to the severe detriment of precision and performance. We define a "full-object-sensitive" analysis that results in significantly higher precision, and often performance, for the exact same context depth. We also introduce "type-sensitivity" as an explicit approximation of object-sensitivity that preserves high context quality at substantially reduced cost. A type-sensitive points-to analysis makes an unconventional use of types as context: the context types are not dynamic types of objects involved in the analysis, but instead upper bounds on the dynamic types of their allocator objects. Our results expose the influence of context choice on the quality of points-to analysis and demonstrate type-sensitivity to be an idea with major impact: It decisively advances the state-of-the-art with a spectrum of analyses that simultaneously enjoy speed (several times faster than an analogous object-sensitive analysis), scalability (comparable to analyses with much less context-sensitivity), and precision (comparable to the best object-sensitive analysis with the same context depth).}},
  url = {https://doi.org/10.1145/1926385.1926390},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup981,
  title = {Partial evaluation of machine code}},
  author = {Srinivasan, Venkatesh and Reps, Thomas}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents an algorithm for off-line partial evaluation of machine code. The algorithm follows the classical two-phase approach of binding-time analysis (BTA) followed by specialization. However, machine-code partial evaluation presents a number of new challenges, and it was necessary to devise new techniques for use in each phase. - Our BTA algorithm makes use of an instruction-rewriting method that ``decouples'' multiple updates performed by a single instruction. This method counters the cascading imprecision that would otherwise occur with a more naive approach to BTA. - Our specializer specializes an explicit representation of the semantics of an instruction, and emits residual code via machine-code synthesis. Moreover, to create code that allows the stack and heap to be at different positions at run-time than at specialization-time, the specializer represents specialization-time addresses using symbolic constants, and uses a symbolic state for specialization. Our experiments show that our algorithm can be used to specialize binaries with respect to commonly used inputs to produce faster binaries, as well as to extract an executable component from a bloated binary.}},
  url = {https://doi.org/10.1145/2814270.2814321},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup982,
  title = {A theory of indirection via approximation}},
  author = {Hobor, Aquinas and Dockins, Robert and Appel, Andrew W.}},
  year = {2010}},
  journal = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Building semantic models that account for various kinds of indirect reference has traditionally been a difficult problem. Indirect reference can appear in many guises, such as heap pointers, higher-order functions, object references, and shared-memory mutexes.We give a general method to construct models containing indirect reference by presenting a "theory of indirection". Our method can be applied in a wide variety of settings and uses only simple, elementary mathematics. In addition to various forms of indirect reference, the resulting models support powerful features such as impredicative quantification and equirecursion; moreover they are compatible with the kind of powerful substructural accounting required to model (higher-order) separation logic. In contrast to previous work, our model is easy to apply to new settings and has a simple axiomatization, which is complete in the sense that all models of it are isomorphic. Our proofs are machine-checked in Coq.}},
  url = {https://doi.org/10.1145/1706299.1706322},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup983,
  title = {RRB vector: a practical general purpose immutable sequence}},
  author = {Stucki, Nicolas and Rompf, Tiark and Ureche, Vlad and Bagwell, Phil}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {State-of-the-art immutable collections have wildly differing performance characteristics across their operations, often forcing programmers to choose different collection implementations for each task. Thus, changes to the program can invalidate the choice of collections, making code evolution costly. It would be desirable to have a collection that performs well for a broad range of operations. To this end, we present the RRB-Vector, an immutable sequence collection that offers good performance across a large number of sequential and parallel operations. The underlying innovations are: (1) the Relaxed-Radix-Balanced (RRB) tree structure, which allows efficient structural reorganization, and (2) an optimization that exploits spatio-temporal locality on the RRB data structure in order to offset the cost of traversing the tree. In our benchmarks, the RRB-Vector speedup for parallel operations is lower bounded by 7x when executing on 4 CPUs of 8 cores each. The performance for discrete operations, such as appending on either end, or updating and removing elements, is consistently good and compares favorably to the most important immutable sequence collections in the literature and in use today. The memory footprint of RRB-Vector is on par with arrays and an order of magnitude less than competing collections.}},
  url = {https://doi.org/10.1145/2784731.2784739},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup984,
  title = {The process trellis architecture for real-time monitors}},
  author = {Factor, M.}},
  year = {1990}},
  journal = {Proceedings of the Second ACM SIGPLAN Symposium on Principles \&amp; Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The process trellis is a parallel software architecture for building heuristic real-time monitors. These programs, for example Intelligent Cardiovascular Monitors, must process massive quantities of data in real time. It is natural to turn to parallelism to meet these computational requirements. The process trellis software architecture is intended to simplify the creation and maintenance of heuristic real-time monitors. To do this it must be 1) modular, 2) efficient and 3) predictable.This paper presents the goals and an overview of the process trellis. We have implemented a process trellis shell, which we describe. It is in use as the frame for an Intelligent Cardiovascular Monitor (ICM) which we are building with colleagues from the Yale School of Medicine. An analytical model is able to produce an upper bound on the time required by arbitrary trellis programs. Finally, we report on the predicted and actual performance of the ICM and synthetic programs consisting of roughly 7000 processes.}},
  url = {https://doi.org/10.1145/99163.99180},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup985,
  title = {Versatile yet lightweight record-and-replay for Android}},
  author = {Hu, Yongjian and Azim, Tanzirul and Neamtiu, Iulian}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Recording and replaying the execution of smartphone apps is useful in a variety of contexts, from reproducing bugs to profiling and testing. Achieving effective record-and-replay is a balancing act between accuracy and overhead. On smartphones, the act is particularly complicated, because smartphone apps receive a high-bandwidth stream of input (e.g., network, GPS, camera, microphone, touchscreen) and concurrency events, but the stream has to be recorded and replayed with minimal overhead, to avoid interfering with app execution. Prior record-and-replay approaches have focused on replaying machine instructions or system calls, which is not a good fit on smartphones. We propose a novel, stream-oriented record-and-replay approach which achieves high-accuracy and low-overhead by aiming at a sweet spot: recording and replaying sensor and network input, event schedules, and inter-app communication via intents. To demonstrate the versatility of our approach, we have constructed a tool named VALERA that supports record-and-replay on the Android platform. VALERA works with apps running directly on the phone, and does not require access to the app source code. Through an evaluation on 50 popular Android apps, we show that: VALERA's replay fidelity far exceeds current record-and-replay approaches for Android; VALERA's precise timing control and low overhead (about 1\% for either record or replay) allows it to replay high-throughput, timing-sensitive apps such as video/audio capture and recognition; and VALERA's support for event schedule replay enables the construction of useful analyses, such as reproducing event-driven race bugs.}},
  url = {https://doi.org/10.1145/2814270.2814320},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup986,
  title = {A theory of gradual effect systems}},
  author = {Ba\~{n}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Effect systems have the potential to help software developers, but their practical adoption has been very limited. We conjecture that this limited adoption is due in part to the difficulty of transitioning from a system where effects are implicit and unrestricted to a system with a static effect discipline, which must settle for conservative checking in order to be decidable. To address this hindrance, we develop a theory of gradual effect checking, which makes it possible to incrementally annotate and statically check effects, while still rejecting statically inconsistent programs. We extend the generic type-and-effect framework of Marino and Millstein with a notion of unknown effects, which turns out to be significantly more subtle than unknown types in traditional gradual typing. We appeal to abstract interpretation to develop and validate the concepts of gradual effect checking. We also demonstrate how an effect system formulated in Marino and Millstein's framework can be automatically extended to support gradual checking.}},
  url = {https://doi.org/10.1145/2628136.2628149},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup987,
  title = {On the power of coercion abstraction}},
  author = {Cretin, Julien and R\'{e}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Erasable coercions in System F-eta, also known as retyping functions, are well-typed eta-expansions of the identity. They may change the type of terms without changing their behavior and can thus be erased before reduction. Coercions in F-eta can model subtyping of known types and some displacement of quantifiers, but not subtyping assumptions nor certain forms of delayed type instantiation. We generalize F-eta by allowing abstraction over retyping functions. We follow a general approach where computing with coercions can be seen as computing in the lambda-calculus but keeping track of which parts of terms are coercions. We obtain a language where coercions do not contribute to the reduction but may block it and are thus not erasable. We recover erasable coercions by choosing a weak reduction strategy and restricting coercion abstraction to value-forms or by restricting abstraction to coercions that are polymorphic in their domain or codomain. The latter variant subsumes F-eta, F-sub, and MLF in a unified framework.}},
  url = {https://doi.org/10.1145/2103656.2103699},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup988,
  title = {Online-ABFT: an online algorithm based fault tolerance scheme for soft error detection in iterative methods}},
  author = {Chen, Zizhong}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Soft errors are one-time events that corrupt the state of a computing system but not its overall functionality. Large supercomputers are especially susceptible to soft errors because of their large number of components. Soft errors can generally be detected offline through the comparison of the final computation results of two duplicated computations, but this approach often introduces significant overhead. This paper presents Online-ABFT, a simple but efficient online soft error detection technique that can detect soft errors in the widely used Krylov subspace iterative methods in the middle of the program execution so that the computation efficiency can be improved through the termination of the corrupted computation in a timely manner soon after a soft error occurs. Based on a simple verification of orthogonality and residual, Online-ABFT is easy to implement and highly efficient. Experimental results demonstrate that, when this online error detection approach is used together with checkpointing, it improves the time to obtain correct results by up to several orders of magnitude over the traditional offline approach.}},
  url = {https://doi.org/10.1145/2442516.2442533},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup989,
  title = {Scrap your boilerplate with object algebras}},
  author = {Zhang, Haoyuan and Chu, Zewei and Oliveira, Bruno C. d. S. and Storm, Tijs van der}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Traversing complex Abstract Syntax Trees (ASTs) typically requires large amounts of tedious boilerplate code. For many operations most of the code simply walks the structure, and only a small portion of the code implements the functionality that motivated the traversal in the first place. This paper presents a type-safe Java framework called Shy that removes much of this boilerplate code. In Shy object algebras are used to describe complex and extensible AST structures. Using Java annotations Shy generates generic boilerplate code for various types of traversals. For a concrete traversal, users of Shy can then inherit from the generated code and override only the interesting cases. Consequently, the amount of code that users need to write is significantly smaller. Moreover, traversals using the Shy framework are also much more structure shy, becoming more adaptive to future changes or extensions to the AST structure. To prove the effectiveness of the approach, we applied Shy in the implementation of a domain-specific questionnaire language. Our results show that for a large number of traversals there was a significant reduction in the amount of user-defined code.}},
  url = {https://doi.org/10.1145/2814270.2814279},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup990,
  title = {Recursion principles for syntax with bindings and substitution}},
  author = {Popescu, Andrei and Gunter, Elsa L.}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We characterize the data type of terms with bindings, freshness and substitution, as an initial model in a suitable Horn theory. This characterization yields a convenient recursive definition principle, which we have formalized in Isabelle/HOL and employed in a series of case studies taken from the λ-calculus literature.}},
  url = {https://doi.org/10.1145/2034773.2034819},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup991,
  title = {Preventing glitches and short circuits in high-level self-timed chip specifications}},
  author = {Longfield, Stephen and Nkounkou, Brittany and Manohar, Rajit and Tate, Ross}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Self-timed chip designs are commonly specified in a high-level message-passing language called CHP. This language is closely related to Hoare's CSP except it admits erroneous behavior due to the necessary limitations of efficient hardware implementations. For example, two processes sending on the same channel at the same time causes glitches and short circuits in the physical chip implementation. If a CHP program maintains certain invariants, such as only one process is sending on any given channel at a time, it can guarantee an error-free execution that behaves much like a CSP program would. In this paper, we present an inferable effect system for ensuring that these invariants hold, drawing from model-checking methodologies while exploiting language-usage patterns and domain-specific specializations to achieve efficiency. This analysis is sound, and is even complete for the common subset of CHP programs without data-sensitive synchronization. We have implemented the analysis and demonstrated that it scales to validate even microprocessors.}},
  url = {https://doi.org/10.1145/2737924.2737967},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup992,
  title = {Sound compilation of reals}},
  author = {Darulova, Eva and Kuncak, Viktor}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Writing accurate numerical software is hard because of many sources of unavoidable uncertainties, including finite numerical precision of implementations. We present a programming model where the user writes a program in a real-valued implementation and specification language that explicitly includes different types of uncertainties. We then present a compilation algorithm that generates a finite-precision implementation that is guaranteed to meet the desired precision with respect to real numbers. Our compilation performs a number of verification steps for different candidate precisions. It generates verification conditions that treat all sources of uncertainties in a unified way and encode reasoning about finite-precision roundoff errors into reasoning about real numbers. Such verification conditions can be used as a standardized format for verifying the precision and the correctness of numerical programs. Due to their non-linear nature, precise reasoning about these verification conditions remains difficult and cannot be handled using state-of-the art SMT solvers alone. We therefore propose a new procedure that combines exact SMT solving over reals with approximate and sound affine and interval arithmetic. We show that this approach overcomes scalability limitations of SMT solvers while providing improved precision over affine and interval arithmetic. Our implementation gives promising results on several numerical models, including dynamical systems, transcendental functions, and controller implementations.}},
  url = {https://doi.org/10.1145/2535838.2535874},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup993,
  title = {When to use a compilation service?}},
  author = {Palm, Jeffrey and Lee, Han and Diwan, Amer and Moss, J. Eliot B.}},
  year = {2002}},
  journal = {Proceedings of the Joint Conference on Languages, Compilers and Tools for Embedded Systems: Software and Compilers for Embedded Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Modern handheld computers are certainly capable of running general purpose applications, such as Java virtual machines. However, short battery life rather than computational capability often limits the usefulness of handheld computers. This paper considers how to reduce the energy consumption of Java applications.Broadly speaking, there are three interleaved steps in running Java programs in a compiled environment: downloading the bytecodes, compiling and possibly optimizing the bytecodes, and running the compiled code. Optimized code typically runs faster than non-optimized code but the optimization process itself may consume significant energy. We consider the possibility of moving compilation (optimizing or non-optimizing) to a tethered server. We demonstrate that there is a significant benefit to moving compilation to a server (up to 67\% reduction in energy for a realistic handheld configuration). We also demonstrate that there is no single best compilation strategy for all methods.}},
  url = {https://doi.org/10.1145/513829.513862},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup994,
  title = {The type discipline of behavioral separation}},
  author = {Caires, Lu\'{\i}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We introduce the concept of behavioral separation as a general principle for disciplining interference in higher-order imperative concurrent programs, and present a type-based approach that systematically develops the concept in the context of an ML-like language extended with concurrency and synchronization primitives. Behavioral separation builds on notions originally introduced for behavioral type systems and separation logics, but shifts the focus from the separation of static program state properties towards the separation of dynamic usage behaviors of runtime values. Behavioral separation types specify how values may be safely used by client code, and can enforce fine-grained interference control disciplines while preserving compositionality, information hiding, and flexibility. We illustrate how our type system, even if based on a small set of general primitives, is already able to tackle fairly challenging program idioms, involving aliasing at various types, concurrency with first-class threads, manipulation of linked data structures, behavioral borrowing, and invariant-based separation.}},
  url = {https://doi.org/10.1145/2429069.2429103},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup995,
  title = {Stateless model checking of event-driven applications}},
  author = {Jensen, Casper S. and M\o{}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Modern event-driven applications, such as, web pages and mobile apps, rely on asynchrony to ensure smooth end-user experience. Unfortunately, even though these applications are executed by a single event-loop thread, they can still exhibit nondeterministic behaviors depending on the execution order of interfering asynchronous events. As in classic shared-memory concurrency, this nondeterminism makes it challenging to discover errors that manifest only in specific schedules of events. In this work we propose the first stateless model checker for event-driven applications, called R4. Our algorithm systematically explores the nondeterminism in the application and concisely exposes its overall effect, which is useful for bug discovery. The algorithm builds on a combination of three key insights: (i) a dynamic partial order reduction (DPOR) technique for reducing the search space, tailored to the domain of event-driven applications, (ii) conflict-reversal bounding based on a hypothesis that most errors occur with a small number of event reorderings, and (iii) approximate replay of event sequences, which is critical for separating harmless from harmful nondeterminism. We instantiate R4 for the domain of client-side web applications and use it to analyze event interference in a number of real-world programs. The experimental results indicate that the precision and overall exploration capabilities of our system significantly exceed that of existing techniques.}},
  url = {https://doi.org/10.1145/2814270.2814282},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup996,
  title = {Analysing the complexity of functional programs: higher-order meets first-order}},
  author = {Avanzini, Martin and Dal Lago, Ugo and Moser, Georg}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We show how the complexity of higher-order functional programs can be analysed automatically by applying program transformations to a defunctionalised versions of them, and feeding the result to existing tools for the complexity analysis of first-order term rewrite systems. This is done while carefully analysing complexity preservation and reflection of the employed transformations such that the complexity of the obtained term rewrite system reflects on the complexity of the initial program. Further, we describe suitable strategies for the application of the studied transformations and provide ample experimental data for assessing the viability of our method.}},
  url = {https://doi.org/10.1145/2784731.2784753},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup997,
  title = {Data-only flattening for nested data parallelism}},
  author = {Bergstrom, Lars and Fluet, Matthew and Rainey, Mike and Reppy, John and Rosen, Stephen and Shaw, Adam}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Data parallelism has proven to be an effective technique for high-level programming of a certain class of parallel applications, but it is not well suited to irregular parallel computations. Blelloch and others proposed nested data parallelism (NDP) as a language mechanism for programming irregular parallel applications in a declarative data-parallel style. The key to this approach is a compiler transformation that flattens the NDP computation and data structures into a form that can be executed efficiently on a wide-vector SIMD architecture. Unfortunately, this technique is ill suited to execution on today's multicore machines. We present a new technique, called data-only flattening, for the compilation of NDP, which is suitable for multicore architectures. Data-only flattening transforms nested data structures in order to expose programs to various optimizations while leaving control structures intact. We present a formal semantics of data-only flattening in a core language with a rewriting system. We demonstrate the effectiveness of this technique in the Parallel ML implementation and we report encouraging experimental results across various benchmark applications.}},
  url = {https://doi.org/10.1145/2442516.2442525},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup998,
  title = {MemSAT: checking axiomatic specifications of memory models}},
  author = {Torlak, Emina and Vaziri, Mandana and Dolby, Julian}},
  year = {2010}},
  journal = {Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Memory models are hard to reason about due to their complexity, which stems from the need to strike a balance between ease-of-programming and allowing compiler and hardware optimizations. In this paper, we present an automated tool, MemSAT, that helps in debugging and reasoning about memory models. Given an axiomatic specification of a memory model and a multi-threaded test program containing assertions, MemSAT outputs a trace of the program in which both the assertions and the memory model axioms are satisfied, if one can be found. The tool is fully automatic and is based on a SAT solver. If it cannot find a trace, it outputs a minimal subset of the memory model and program constraints that are unsatisfiable. We used MemSAT to check several existing memory models against their published test cases, including the current Java Memory Model by Manson et al. and a revised version of it by Sevcik and Aspinall. We found subtle discrepancies between what was expected and the actual results of test programs.}},
  url = {https://doi.org/10.1145/1806596.1806635},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup999,
  title = {Chlorophyll: synthesis-aided compiler for low-power spatial architectures}},
  author = {Phothilimthana, Phitchaya Mangpo and Jelvis, Tikhon and Shah, Rohin and Totla, Nishant and Chasins, Sarah and Bodik, Rastislav}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We developed Chlorophyll, a synthesis-aided programming model and compiler for the GreenArrays GA144, an extremely minimalist low-power spatial architecture that requires partitioning the program into fragments of no more than 256 instructions and 64 words of data. This processor is 100-times more energy efficient than its competitors, but currently can only be programmed using a low-level stack-based language.The Chlorophyll programming model allows programmers to provide human insight by specifying partial partitioning of data and computation. The Chlorophyll compiler relies on synthesis, sidestepping the need to develop classical optimizations, which may be challenging given the unusual architecture. To scale synthesis to real problems, we decompose the compilation into smaller synthesis subproblems---partitioning, layout, and code generation. We show that the synthesized programs are no more than 65\% slower than highly optimized expert-written programs and are faster than programs produced by a heuristic, non-synthesizing version of our compiler.}},
  url = {https://doi.org/10.1145/2594291.2594339},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1000,
  title = {Atomicity refinement for verified compilation}},
  author = {Jagannathan, Suresh and Laporte, Vincent and Petri, Gustavo and Pichardie, David and Vitek, Jan}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We consider the verified compilation of high-level managed languages like Java or C# whose intermediate representations provide support for shared-memory synchronization and automatic memory management. In this environment, the interactions between application threads and the language runtime (e.g., the garbage collector) are regulated by compiler-injected code snippets. Example of snippets include allocation fast paths among others. In our TOPLAS paper we propose a refinement-based proof methodology that precisely relates concurrent code expressed at different abstraction levels, cognizant throughout of the relaxed memory semantics of the underlying processor. Our technique allows the compiler writer to reason compositionally about the atomicity of low-level concurrent code used to implement managed services. We illustrate our approach with examples taken from the verification of a concurrent garbage collector.}},
  url = {https://doi.org/10.1145/2594291.2594346},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1001,
  title = {Program-ing finger trees in Coq}},
  author = {Sozeau, Matthieu}},
  year = {2007}},
  journal = {Proceedings of the 12th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Finger Trees (Hinze \&amp; Paterson, 2006) are a general purpose persistent data structure with good performance. Their genericity permits developing a wealth of structures like ordered sequences or interval trees on top of a single implementation. However, the type systems used by current functional languages do not guarantee the coherent parameterization and specialization of Finger Trees, let alone the correctness of their implementation. We present a certified implementation of Finger Trees solving these problems using the Program extension of Coq. We not only implement the structure but also prove its invariants along the way, which permit building certified structures on top of Finger Trees in an elegant way.}},
  url = {https://doi.org/10.1145/1291151.1291156},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1002,
  title = {Simple and compositional reification of monadic embedded languages}},
  author = {Svenningsson, Josef David and Svensson, Bo Joel}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {When writing embedded domain specific languages in Haskell, it is often convenient to be able to make an instance of the Monad class to take advantage of the do-notation and the extensive monad libraries. Commonly it is desirable to compile such languages rather than just interpret them. This introduces the problem of monad reification, i.e. observing the structure of the monadic computation. We present a solution to the monad reification problem and illustrate it with a small robot control language. Monad reification is not new but the novelty of our approach is in its directness, simplicity and compositionality.}},
  url = {https://doi.org/10.1145/2500365.2500611},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1003,
  title = {Architecture-Adaptive Code Variant Tuning}},
  author = {Muralidharan, Saurav and Roy, Amit and Hall, Mary and Garland, Michael and Rai, Piyush}},
  year = {2016}},
  journal = {Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Code variants represent alternative implementations of a computation, and are common in high-performance libraries and applications to facilitate selecting the most appropriate implementation for a specific execution context (target architecture and input dataset). Automating code variant selection typically relies on machine learning to construct a model during an offline learning phase that can be quickly queried at runtime once the execution context is known. In this paper, we define a new approach called architecture-adaptive code variant tuning, where the variant selection model is learned on a set of source architectures, and then used to predict variants on a new target architecture without having to repeat the training process. We pose this as a multi-task learning problem, where each source architecture corresponds to a task; we use device features in the construction of the variant selection model. This work explores the effectiveness of multi-task learning and the impact of different strategies for device feature selection. We evaluate our approach on a set of benchmarks and a collection of six NVIDIA GPU architectures from three distinct generations. We achieve performance results that are mostly comparable to the previous approach of tuning for a single GPU architecture without having to repeat the learning phase.}},
  url = {https://doi.org/10.1145/2872362.2872411},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1004,
  title = {Nested refinements: a logic for duck typing}},
  author = {Chugh, Ravi and Rondon, Patrick M. and Jhala, Ranjit}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Programs written in dynamic languages make heavy use of features --- run-time type tests, value-indexed dictionaries, polymorphism, and higher-order functions --- that are beyond the reach of type systems that employ either purely syntactic or purely semantic reasoning. We present a core calculus, System D, that merges these two modes of reasoning into a single powerful mechanism of nested refinement types wherein the typing relation is itself a predicate in the refinement logic. System D coordinates SMT-based logical implication and syntactic subtyping to automatically typecheck sophisticated dynamic language programs. By coupling nested refinements with McCarthy's theory of finite maps, System D can precisely reason about the interaction of higher-order functions, polymorphism, and dictionaries. The addition of type predicates to the refinement logic creates a circularity that leads to unique technical challenges in the metatheory, which we solve with a novel stratification approach that we use to prove the soundness of System D.}},
  url = {https://doi.org/10.1145/2103656.2103686},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1005,
  title = {Automatic program inversion using symbolic transducers}},
  author = {Hu, Qinheping and D'Antoni, Loris}},
  year = {2017}},
  journal = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We propose a fully-automated technique for inverting functional programs that operate over lists such as string encoders and decoders. We consider programs that can be modeled using symbolic extended finite transducers (), an expressive model that can describe complex list-manipulating programs while retaining several decidable properties. Concretely, given a program P expressed as an , we propose techniques for: (1) checking whether P is injective and, if that is the case, (2) building an P-1 describing its inverse. We first show that it is undecidable to check whether an is injective and propose an algorithm for checking injectivity for a restricted, but a practical class of . We then propose an algorithm for inverting based on the following idea: if an is injective, inverting it amounts to inverting all its individual transitions. We leverage recent advances program synthesis and show that the transition inversion problem can be expressed as an instance of the syntax-guided synthesis framework. Finally, we implement the proposed techniques in a tool called and show that can invert 13 out 14 real complex string encoders and decoders, producing inverse programs that are substantially identical to manually written ones.}},
  url = {https://doi.org/10.1145/3062341.3062345},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1006,
  title = {Complete functional synthesis}},
  author = {Kuncak, Viktor and Mayer, Mika\"{e}},
  year = {2010}},
  journal = {Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Synthesis of program fragments from specifications can make programs easier to write and easier to reason about. To integrate synthesis into programming languages, synthesis algorithms should behave in a predictable way - they should succeed for a well-defined class of specifications. They should also support unbounded data types such as numbers and data structures. We propose to generalize decision procedures into predictable and complete synthesis procedures. Such procedures are guaranteed to find code that satisfies the specification if such code exists. Moreover, we identify conditions under which synthesis will statically decide whether the solution is guaranteed to exist, and whether it is unique. We demonstrate our approach by starting from decision procedures for linear arithmetic and data structures and transforming them into synthesis procedures. We establish results on the size and the efficiency of the synthesized code. We show that such procedures are useful as a language extension with implicit value definitions, and we show how to extend a compiler to support such definitions. Our constructs provide the benefits of synthesis to programmers, without requiring them to learn new concepts or give up a deterministic execution model.}},
  url = {https://doi.org/10.1145/1806596.1806632},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1007,
  title = {Learning a strategy for adapting a program analysis via bayesian optimisation}},
  author = {Oh, Hakjoo and Yang, Hongseok and Yi, Kwangkeun}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Building a cost-effective static analyser for real-world programs is still regarded an art. One key contributor to this grim reputation is the difficulty in balancing the cost and the precision of an analyser. An ideal analyser should be adaptive to a given analysis task, and avoid using techniques that unnecessarily improve precision and increase analysis cost. However, achieving this ideal is highly nontrivial, and it requires a large amount of engineering efforts. In this paper we present a new approach for building an adaptive static analyser. In our approach, the analyser includes a sophisticated parameterised strategy that decides, for each part of a given program, whether to apply a precision-improving technique to that part or not. We present a method for learning a good parameter for such a strategy from an existing codebase via Bayesian optimisation. The learnt strategy is then used for new, unseen programs. Using our approach, we developed partially flow- and context-sensitive variants of a realistic C static analyser. The experimental results demonstrate that using Bayesian optimisation is crucial for learning from an existing codebase. Also, they show that among all program queries that require flow- or context-sensitivity, our partially flow- and context-sensitive analysis answers the 75\% of them, while increasing the analysis cost only by 3.3x of the baseline flow- and context-insensitive analysis, rather than 40x or more of the fully sensitive version.}},
  url = {https://doi.org/10.1145/2814270.2814309},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1008,
  title = {Prototyping symbolic execution engines for interpreted languages}},
  author = {Bucur, Stefan and Kinder, Johannes and Candea, George}},
  year = {2014}},
  journal = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Symbolic execution is being successfully used to automatically test statically compiled code. However, increasingly more systems and applications are written in dynamic interpreted languages like Python. Building a new symbolic execution engine is a monumental effort, and so is keeping it up-to-date as the target language evolves. Furthermore, ambiguous language specifications lead to their implementation in a symbolic execution engine potentially differing from the production interpreter in subtle ways.We address these challenges by flipping the problem and using the interpreter itself as a specification of the language semantics. We present a recipe and tool (called Chef) for turning a vanilla interpreter into a sound and complete symbolic execution engine. Chef symbolically executes the target program by symbolically executing the interpreter's binary while exploiting inferred knowledge about the program's high-level structure.Using Chef, we developed a symbolic execution engine for Python in 5 person-days and one for Lua in 3 person-days. They offer complete and faithful coverage of language features in a way that keeps up with future language versions at near-zero cost. Chef-produced engines are up to 1000 times more performant than if directly executing the interpreter symbolically without Chef.}},
  url = {https://doi.org/10.1145/2541940.2541977},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1009,
  title = {Making numerical program analysis fast}},
  author = {Singh, Gagandeep and P\"{u}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Numerical abstract domains are a fundamental component in modern static program analysis and are used in a wide range of scenarios (e.g. computing array bounds, disjointness, etc). However, analysis with these domains can be very expensive, deeply affecting the scalability and practical applicability of the static analysis. Hence, it is critical to ensure that these domains are made highly efficient. In this work, we present a complete approach for optimizing the performance of the Octagon numerical abstract domain, a domain shown to be particularly effective in practice. Our optimization approach is based on two key insights: i) the ability to perform online decomposition of the octagons leading to a massive reduction in operation counts, and ii) leveraging classic performance optimizations from linear algebra such as vectorization, locality of reference, scalar replacement and others, for improving the key bottlenecks of the domain. Applying these ideas, we designed new algorithms for the core Octagon operators with better asymptotic runtime than prior work and combined them with the optimization techniques to achieve high actual performance. We implemented our approach in the Octagon operators exported by the popular APRON C library, thus enabling existing static analyzers using APRON to immediately benefit from our work. To demonstrate the performance benefits of our approach, we evaluated our framework on three published static analyzers showing massive speed-ups for the time spent in Octagon analysis (e.g., up to 146x) as well as significant end-to-end program analysis speed-ups (up to 18.7x). Based on these results, we believe that our framework can serve as a new basis for static analysis with the Octagon numerical domain.}},
  url = {https://doi.org/10.1145/2737924.2738000},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1010,
  title = {Step-indexed kripke models over recursive worlds}},
  author = {Birkedal, Lars and Reus, Bernhard and Schwinghammer, Jan and St\o{}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Over the last decade, there has been extensive research on modelling challenging features in programming languages and program logics, such as higher-order store and storable resource invariants. A recent line of work has identified a common solution to some of these challenges: Kripke models over worlds that are recursively defined in a category of metric spaces. In this paper, we broaden the scope of this technique from the original domain-theoretic setting to an elementary, operational one based on step indexing. The resulting method is widely applicable and leads to simple, succinct models of complicated language features, as we demonstrate in our semantics of Chargu\'{e}},
  url = {https://doi.org/10.1145/1926385.1926401},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1011,
  title = {Correctness of an STM Haskell implementation}},
  author = {Schmidt-Schau\ss{}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A concurrent implementation of software transactional memory in Concurrent Haskell using a call-by-need functional language with processes and futures is given. The description of the small-step operational semantics is precise and explicit, and employs an early abort of conflicting transactions. A proof of correctness of the implementation is given for a contextual semantics with may- and should-convergence. This implies that our implementation is a correct evaluator for an abstract specification equipped with a big-step semantics.}},
  url = {https://doi.org/10.1145/2500365.2500585},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1012,
  title = {Some desirable properties of data abstraction facilities}},
  author = {Horning, J. J.}},
  year = {1976}},
  journal = {Proceedings of the 1976 Conference on Data : Abstraction, Definition and Structure}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {It is currently popular to say that programming languages need “data abstraction facilities,” and to assert that the provision of such facilities would provide conceptual and practical advantages in the domain of data structures akin to the advantages provided by procedures in the domain of computational structures. This note explores some of the implications of this metaphor, without attempting to make it precise. I shall use the term capsule to refer to the data analog of procedure. [Those who are familiar with the SIMULA class, the CLU cluster, or the ALPHARD form, may use any of these as an approximation to capsule; I use a neutral term to avoid implying the details of any particular language.]First, what are the advantages provided by procedures (subroutines, functions, macros)? I can think of at least eight (highly interrelated) categories: 1) avoidance of repetition, 2) modular program structure, 3) a basis for structured programming, 4) conceptual units for understanding and reasoning about programs, 5) clearly defined interfaces that may be precisely specified, 6) units of maintenance and improvement, 7) a language extension mechanism, and 8) units for separate compilation. Let us consider each of these in turn.}},
  url = {https://doi.org/10.1145/800237.807119},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1013,
  title = {Parametric polymorphism and semantic subtyping: the logical connection}},
  author = {Gesbert, Nils and Genev\`{e}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We consider a type algebra equipped with recursive, product, function, intersection, union, and complement types together with type variables and implicit universal quantification over them. We consider the subtyping relation recently defined by Castagna and Xu over such type expressions and show how this relation can be decided in EXPTIME, answering an open question. The novelty, originality and strength of our solution reside in introducing a logical modeling for the semantic subtyping framework. We model semantic subtyping in a tree logic and use a satisfiability-testing algorithm in order to decide subtyping. We report on practical experiments made with a full implementation of the system. This provides a powerful polymorphic type system aiming at maintaining full static type-safety of functional programs that manipulate trees, even with higher-order functions, which is particularly useful in the context of XML.}},
  url = {https://doi.org/10.1145/2034773.2034789},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1014,
  title = {StarL: Towards a Unified Framework for Programming, Simulating and Verifying Distributed Robotic Systems}},
  author = {Lin, Yixiao and Mitra, Sayan}},
  year = {2015}},
  journal = {Proceedings of the 16th ACM SIGPLAN/SIGBED Conference on Languages, Compilers and Tools for Embedded Systems 2015 CD-ROM}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We developed StarL as a framework for programming, simulating, and verifying distributed systems that interacts with physical processes. StarL framework has (a) a collection of distributed primitives for coordination, such as mutual exclusion, registration and geocast that can be used to build sophisticated applications, (b) theory libraries for verifying StarL applications in the PVS theorem prover, and (c) an execution environment that can be used to deploy the applications on hardware or to execute them in a discrete event simulator. The primitives have (i) abstract, nondeterministic specifications in terms of invariants, and assume-guarantee style progress properties, (ii) implementations in Java/Android that always satisfy the invariants and attempt progress using best effort strategies. The PVS theories specify the invariant and progress properties of the primitives, and have to be appropriately instantiated and composed with the application's state machine to prove properties about the application. We have built two execution environments: one for deploying applications on Android/iRobot Create platform and a second one for simulating large instantiations of the applications in a discrete even simulator. The capabilities are illustrated with a StarL application for vehicle to vehicle coordination in an automatic intersection that uses primitives for point-to-point motion, mutual exclusion, and registration.}},
  url = {https://doi.org/10.1145/2670529.2754966},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1015,
  title = {Run your research: on the effectiveness of lightweight mechanization}},
  author = {Klein, Casey and Clements, John and Dimoulas, Christos and Eastlund, Carl and Felleisen, Matthias and Flatt, Matthew and McCarthy, Jay A. and Rafkind, Jon and Tobin-Hochstadt, Sam and Findler, Robert Bruce}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Formal models serve in many roles in the programming language community. In its primary role, a model communicates the idea of a language design; the architecture of a language tool; or the essence of a program analysis. No matter which role it plays, however, a faulty model doesn't serve its purpose.One way to eliminate flaws from a model is to write it down in a mechanized formal language. It is then possible to state theorems about the model, to prove them, and to check the proofs. Over the past nine years, PLT has developed and explored a lightweight version of this approach, dubbed Redex. In a nutshell, Redex is a domain-specific language for semantic models that is embedded in the Racket programming language. The effort of creating a model in Redex is often no more burdensome than typesetting it with LaTeX; the difference is that Redex comes with tools for the semantics engineering life cycle.}},
  url = {https://doi.org/10.1145/2103656.2103691},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1016,
  title = {S2E: a platform for in-vivo multi-path analysis of software systems}},
  author = {Chipounov, Vitaly and Kuznetsov, Volodymyr and Candea, George}},
  year = {2011}},
  journal = {Proceedings of the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents S2E, a platform for analyzing the properties and behavior of software systems. We demonstrate S2E's use in developing practical tools for comprehensive performance profiling, reverse engineering of proprietary software, and bug finding for both kernel-mode and user-mode binaries. Building these tools on top of S2E took less than 770 LOC and 40 person-hours each.S2E's novelty consists of its ability to scale to large real systems, such as a full Windows stack. S2E is based on two new ideas: selective symbolic execution, a way to automatically minimize the amount of code that has to be executed symbolically given a target analysis, and relaxed execution consistency models, a way to make principled performance/accuracy trade-offs in complex analyses. These techniques give S2E three key abilities: to simultaneously analyze entire families of execution paths, instead of just one execution at a time; to perform the analyses in-vivo within a real software stack--user programs, libraries, kernel, drivers, etc.--instead of using abstract models of these layers; and to operate directly on binaries, thus being able to analyze even proprietary software.Conceptually, S2E is an automated path explorer with modular path analyzers: the explorer drives the target system down all execution paths of interest, while analyzers check properties of each such path (e.g., to look for bugs) or simply collect information (e.g., count page faults). Desired paths can be specified in multiple ways, and S2E users can either combine existing analyzers to build a custom analysis tool, or write new analyzers using the S2E API.}},
  url = {https://doi.org/10.1145/1950365.1950396},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1017,
  title = {Quantitative relaxation of concurrent data structures}},
  author = {Henzinger, Thomas A. and Kirsch, Christoph M. and Payer, Hannes and Sezgin, Ali and Sokolova, Ana}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {There is a trade-off between performance and correctness in implementing concurrent data structures. Better performance may be achieved at the expense of relaxing correctness, by redefining the semantics of data structures. We address such a redefinition of data structure semantics and present a systematic and formal framework for obtaining new data structures by quantitatively relaxing existing ones. We view a data structure as a sequential specification S containing all "legal" sequences over an alphabet of method calls. Relaxing the data structure corresponds to defining a distance from any sequence over the alphabet to the sequential specification: the k-relaxed sequential specification contains all sequences over the alphabet within distance k from the original specification. In contrast to other existing work, our relaxations are semantic (distance in terms of data structure states). As an instantiation of our framework, we present two simple yet generic relaxation schemes, called out-of-order and stuttering relaxation, along with several ways of computing distances. We show that the out-of-order relaxation, when further instantiated to stacks, queues, and priority queues, amounts to tolerating bounded out-of-order behavior, which cannot be captured by a purely syntactic relaxation (distance in terms of sequence manipulation, e.g. edit distance). We give concurrent implementations of relaxed data structures and demonstrate that bounded relaxations provide the means for trading correctness for performance in a controlled way. The relaxations are monotonic which further highlights the trade-off: increasing k increases the number of permitted sequences, which as we demonstrate can lead to better performance. Finally, since a relaxed stack or queue also implements a pool, we actually have new concurrent pool implementations that outperform the state-of-the-art ones.}},
  url = {https://doi.org/10.1145/2429069.2429109},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1018,
  title = {FlumeJava: easy, efficient data-parallel pipelines}},
  author = {Chambers, Craig and Raniwala, Ashish and Perry, Frances and Adams, Stephen and Henry, Robert R. and Bradshaw, Robert and Weizenbaum, Nathan}},
  year = {2010}},
  journal = {Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {MapReduce and similar systems significantly ease the task of writing data-parallel code. However, many real-world computations require a pipeline of MapReduces, and programming and managing such pipelines can be difficult. We present FlumeJava, a Java library that makes it easy to develop, test, and run efficient data-parallel pipelines. At the core of the FlumeJava library are a couple of classes that represent immutable parallel collections, each supporting a modest number of operations for processing them in parallel. Parallel collections and their operations present a simple, high-level, uniform abstraction over different data representations and execution strategies. To enable parallel operations to run efficiently, FlumeJava defers their evaluation, instead internally constructing an execution plan dataflow graph. When the final results of the parallel operations are eventually needed, FlumeJava first optimizes the execution plan, and then executes the optimized operations on appropriate underlying primitives (e.g., MapReduces). The combination of high-level abstractions for parallel data and computation, deferred evaluation and optimization, and efficient parallel primitives yields an easy-to-use system that approaches the efficiency of hand-optimized pipelines. FlumeJava is in active use by hundreds of pipeline developers within Google.}},
  url = {https://doi.org/10.1145/1806596.1806638},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1019,
  title = {Software is discrete mathematics}},
  author = {Page, Rex L.}},
  year = {2003}},
  journal = {Proceedings of the Eighth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A three-year study collected information bearing on the question of whether studying mathematics improves programming skills. An analysis of the data revealed significant differences in the programming effectiveness of two populations of students: (1) those who studied discrete mathematics through examples focused on reasoning about software and (2) those who studied the same mathematical topics illustrated with more traditional examples. Functional programming played a central role in the study because it provides a straightforward framework for the presentation of concepts such as predicate logic and proof by induction. Such topics can be covered in depth, staying almost entirely within the context of reasoning about software. The intricate complexities in logic that mutable variables carry with them need not arise, early on, to confuse novices struggling to understand new ideas. In addition, because functional languages provide useful and compact ways to express mathematical concepts, and because the choice of notation in mathematics courses is often at the discretion of the instructor (in contrast to the notational restrictions often fiercely guarded by the faculty in programming courses), discrete mathematics courses, as they are found in most computer science programs, provide an easy opportunity to enhance the education of students by exposing them to functional programming concepts.}},
  url = {https://doi.org/10.1145/944705.944713},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1020,
  title = {NDSeq: runtime checking for nondeterministic sequential specifications of parallel correctness}},
  author = {Burnim, Jacob and Elmas, Tayfun and Necula, George and Sen, Koushik}},
  year = {2011}},
  journal = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We propose to specify the correctness of a program's parallelism using a sequential version of the program with controlled nondeterminism. Such a nondeterministic sequential specification allows (1) the correctness of parallel interference to be verified independently of the program's functional correctness, and (2) the functional correctness of a program to be understood and verified on a sequential version of the program, one with controlled nondeterminism but no interleaving of parallel threads.We identify a number of common patterns for writing nondeterministic sequential specifications. We apply these patterns to specify the parallelism correctness for a variety of parallel Java benchmarks, even in cases when the functional correctness is far too complex to feasibly specify.We describe a sound runtime checking technique to validate that an execution of a parallel program conforms to its nondeterministic sequential specification. The technique uses a novel form of conflict-serializability checking to identify, for a given interleaved execution of a parallel program, an equivalent nondeterministic sequential execution. Our experiments show a significant reduction in the number of false positives versus traditional conflict-serializability in checking for parallelization bugs.}},
  url = {https://doi.org/10.1145/1993498.1993545},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1021,
  title = {Generational reference counting: a reduced-communication distributed storage reclamation scheme}},
  author = {Goldberg, B.}},
  year = {1989}},
  journal = {Proceedings of the ACM SIGPLAN 1989 Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper describes generational reference counting, a new distributed storage reclamation scheme for loosely-coupled multiprocessors. It has a significantly lower communication overhead than distributed versions of conventional reference counting. Although generational reference counting has greater computational and space requirements than ordinary reference counting, it may provide a significant saving in overall execution time on machines in which message passing is expensive.The communication overhead for generational reference counting is one message for each copy of an interprocessor reference (pointer). Unlike conventional reference counting, when a reference to an object is copied no message is sent to the processor on which the object lies. A message is sent only when a reference is discarded. Unfortunately, generational reference counting shares conventional reference counting's inability to reclaim cyclical structures.In this paper, we present the generational reference counting algorithm, prove it correct, and discuss some refinements that make it more efficient. We also compare it with weighted reference counting, another distributed reference counting scheme described in the literature.}},
  url = {https://doi.org/10.1145/73141.74846},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1022,
  title = {Remote-scope promotion: clarified, rectified, and verified}},
  author = {Wickerson, John and Batty, Mark and Beckmann, Bradford M. and Donaldson, Alastair F.}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Modern accelerator programming frameworks, such as OpenCL, organise threads into work-groups. Remote-scope promotion (RSP) is a language extension recently proposed by AMD researchers that is designed to enable applications, for the first time, both to optimise for the common case of intra-work-group communication (using memory scopes to provide consistency only within a work-group) and to allow occasional inter-work-group communication (as required, for instance, to support the popular load-balancing idiom of work stealing). We present the first formal, axiomatic memory model of OpenCL extended with RSP. We have extended the Herd memory model simulator with support for OpenCL kernels that exploit RSP, and used it to discover bugs in several litmus tests and a work-stealing queue, that have been used previously in the study of RSP. We have also formalised the proposed GPU implementation of RSP. The formalisation process allowed us to identify bugs in the description of RSP that could result in well-synchronised programs experiencing memory inconsistencies. We present and prove sound a new implementation of RSP that incorporates bug fixes and requires less non-standard hardware than the original implementation. This work, a collaboration between academia and industry, clearly demonstrates how, when designing hardware support for a new concurrent language feature, the early application of formal tools and techniques can help to prevent errors, such as those we have found, from making it into silicon.}},
  url = {https://doi.org/10.1145/2814270.2814283},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1023,
  title = {Silent Data Corruption Resilient Two-sided Matrix Factorizations}},
  author = {Wu, Panruo and DeBardeleben, Nathan and Guan, Qiang and Blanchard, Sean and Chen, Jieyang and Tao, Dingwen and Liang, Xin and Ouyang, Kaiming and Chen, Zizhong}},
  year = {2017}},
  journal = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents an algorithm based fault tolerance method to harden three two-sided matrix factorizations against soft errors: reduction to Hessenberg form, tridiagonal form, and bidiagonal form. These two sided factorizations are usually the prerequisites to computing eigenvalues/eigenvectors and singular value decomposition. Algorithm based fault tolerance has been shown to work on three main one-sided matrix factorizations: LU, Cholesky, and QR, but extending it to cover two sided factorizations is non-trivial because there are no obvious textit{offline, problem}},
  url = {https://doi.org/10.1145/3018743.3018750},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1024,
  title = {From object algebras to attribute grammars}},
  author = {Rendel, Tillmann and Brachth\"{a}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Oliveira and Cook (2012) and Oliveira et al. (2013) have recently introduced object algebras as a program structuring technique to improve the modularity and extensibility of programs. We analyze the relationship between object algebras and attribute grammars (AGs), a formalism to augment context-free grammars with attributes. We present an extension of the object algebra technique with which the full class of L-attributed grammars - an important class of AGs that corresponds to one-pass compilers - can be encoded in Scala. The encoding is modular (attributes can be defined and type-checked separately), scalable (the size of the encoding is linear in the size of the AG specification) and compositional (each AG artifact is represented as a semantic object of the host language). To evaluate these claims, we have formalized the encoding and re-implemented a one-pass compiler for a subset of C with our technique. We also discuss how advanced features of modern AG systems, such as higher-order and parameterized attributes, reference attributes, and forwarding can be supported.}},
  url = {https://doi.org/10.1145/2660193.2660237},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1025,
  title = {Asynchronous assertions}},
  author = {Aftandilian, Edward E. and Guyer, Samuel Z. and Vechev, Martin and Yahav, Eran}},
  year = {2011}},
  journal = {Proceedings of the 2011 ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Assertions are a familiar and widely used bug detection technique. Traditional assertion checking, however, is performed synchronously, imposing its full cost on the runtime of the program. As a result, many useful kinds of checks, such as data structure invariants and heap analyses, are impractical because they lead to extreme slowdowns. We present a solution that decouples assertion evaluation from program execution: assertions are checked asynchronously by separate checking threads while the program continues to execute. Our technique guarantees that asynchronous evaluation always produces the same result as synchronous evaluation, even if the program concurrently modifies the program state. The checking threads evaluate each assertion on a consistent snapshot of the program state as it existed at the moment the assertion started.We implemented our technique in a system called Strobe, which supports asynchronous assertion checking in both single-and multi-threaded Java applications. Strobe runs inside the Java virtual machine and uses copy-on-write to construct snapshots incrementally, on-the-fly. Our system includes all necessary synchronization to support multiple concurrent checking threads, and to prevent data races with the main program threads. We find that asynchronous checking significantly outperforms synchronous checking, incurring tolerable overheads -- in the range of 10\% to 50\% over no checking at all -- even for heavy-weight assertions that would otherwise result in crushing slowdowns.}},
  url = {https://doi.org/10.1145/2048066.2048090},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1026,
  title = {RAIVE: runtime assessment of floating-point instability by vectorization}},
  author = {Lee, Wen-Chuan and Bao, Tao and Zheng, Yunhui and Zhang, Xiangyu and Vora, Keval and Gupta, Rajiv}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Floating point representation has limited precision and inputs to floating point programs may also have errors. Consequently, during execution, errors are introduced, propagated, and accumulated, leading to unreliable outputs. We call this the instability problem. We propose RAIVE, a technique that identifies output variations of a floating point execution in the presence of instability. RAIVE transforms every floating point value to a vector of multiple values – the values added to create the vector are obtained by introducing artifi- cial errors that are upper bounds of actual errors. The propagation of artificial errors models the propagation of actual errors. When values in vectors result in discrete execution differences (e.g., following different paths), the execution is forked to capture the resulting output variations. Our evaluation shows that RAIVE can precisely capture output variations. Its overhead (340\%) is 2.43 times lower than the state of the art}},
  url = {https://doi.org/10.1145/2814270.2814299},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1027,
  title = {Predicate abstraction and refinement for verifying multi-threaded programs}},
  author = {Gupta, Ashutosh and Popeea, Corneliu and Rybalchenko, Andrey}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Automated verification of multi-threaded programs requires explicit identification of the interplay between interacting threads, so-called environment transitions, to enable scalable, compositional reasoning. Once the environment transitions are identified, we can prove program properties by considering each program thread in isolation, as the environment transitions keep track of the interleaving with other threads. Finding adequate environment transitions that are sufficiently precise to yield conclusive results and yet do not overwhelm the verifier with unnecessary details about the interleaving with other threads is a major challenge. In this paper we propose a method for safety verification of multi-threaded programs that applies (transition) predicate abstraction-based discovery of environment transitions, exposing a minimal amount of information about the thread interleaving. The crux of our method is an abstraction refinement procedure that uses recursion-free Horn clauses to declaratively state abstraction refinement queries. Then, the queries are resolved by a corresponding constraint solving algorithm. We present preliminary experimental results for mutual exclusion protocols and multi-threaded device drivers.}},
  url = {https://doi.org/10.1145/1926385.1926424},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1028,
  title = {A library for portable and composable data locality optimizations for NUMA systems}},
  author = {Majo, Zoltan and Gross, Thomas R.}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many recent multiprocessor systems are realized with a non-uniform memory architecture (NUMA) and accesses to remote memory locations take more time than local memory accesses. Optimizing NUMA memory system performance is difficult and costly for three principal reasons: (1) today's programming languages/libraries have no explicit support for NUMA systems, (2) NUMA optimizations are not~portable, and (3) optimizations are not~composable (i.e., they can become ineffective or worsen performance in environments that support composable parallel software). This paper presents TBB-NUMA, a parallel programming library based on Intel Threading Building Blocks (TBB) that supports portable and composable NUMA-aware programming. TBB-NUMA provides a model of task affinity that captures a programmer's insights on mapping tasks to resources. NUMA-awareness affects all layers of the library (i.e., resource management, task scheduling, and high-level parallel algorithm templates) and requires close coupling between all these layers. Optimizations implemented with TBB-NUMA (for a set of standard benchmark programs) result in up to 44\% performance improvement over standard TBB, but more important, optimized programs are portable across different NUMA architectures and preserve data locality also when composed with other parallel computations.}},
  url = {https://doi.org/10.1145/2688500.2688509},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1029,
  title = {Formalizing the LLVM intermediate representation for verified program transformations}},
  author = {Zhao, Jianzhou and Nagarakatte, Santosh and Martin, Milo M.K. and Zdancewic, Steve}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents Vellvm (verified LLVM), a framework for reasoning about programs expressed in LLVM's intermediate representation and transformations that operate on it. Vellvm provides a mechanized formal semantics of LLVM's intermediate representation, its type system, and properties of its SSA form. The framework is built using the Coq interactive theorem prover. It includes multiple operational semantics and proves relations among them to facilitate different reasoning styles and proof techniques.To validate Vellvm's design, we extract an interpreter from the Coq formal semantics that can execute programs from LLVM test suite and thus be compared against LLVM reference implementations. To demonstrate Vellvm's practicality, we formalize and verify a previously proposed transformation that hardens C programs against spatial memory safety violations. Vellvm's tools allow us to extract a new, verified implementation of the transformation pass that plugs into the real LLVM infrastructure; its performance is competitive with the non-verified, ad-hoc original.}},
  url = {https://doi.org/10.1145/2103656.2103709},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1030,
  title = {An abstract interpretation framework for refactoring with application to extract methods with contracts}},
  author = {Cousot, Patrick M. and Cousot, Radhia and Logozzo, Francesco and Barnett, Michael}},
  year = {2012}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Method extraction is a common refactoring feature provided by most modern IDEs. It replaces a user-selected piece of code with a call to an automatically generated method. We address the problem of automatically inferring contracts (precondition, postcondition) for the extracted method. We require the inferred contract: (a) to be valid for the extracted method (validity); (b) to guard the language and programmer assertions in the body of the extracted method by an opportune precondition (safety); (c) to preserve the proof of correctness of the original code when analyzing the new method separately (completeness); and (d) to be the most general possible (generality). These requirements rule out trivial solutions (e.g., inlining, projection, etc). We propose two theoretical solutions to the problem. The first one is simple and optimal. It is valid, safe, complete and general but unfortunately not effectively computable (except for unrealistic finiteness/decidability hypotheses). The second one is based on an iterative forward/backward method. We show it to be valid, safe, and, under reasonable assumptions, complete and general. We prove that the second solution subsumes the first. All justifications are provided with respect to a new, set-theoretic version of Hoare logic (hence without logic), and abstractions of Hoare logic, revisited to avoid surprisingly unsound inference rules.We have implemented the new algorithms on the top of two industrial-strength tools (CCCheck and the Microsoft Roslyn CTP). Our experience shows that the analysis is both fast enough to be used in an interactive environment and precise enough to generate good annotations.}},
  url = {https://doi.org/10.1145/2384616.2384633},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1031,
  title = {Selective control-flow abstraction via jumping}},
  author = {Blackshear, Sam and Chang, Bor-Yuh Evan and Sridharan, Manu}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present jumping, a form of selective control-flow abstraction useful for improving the scalability of goal-directed static analyses. Jumping is useful for analyzing programs with complex control-flow such as event-driven systems. In such systems, accounting for orderings between certain events is important for precision, yet analyzing the product graph of all possible event orderings is intractable. Jumping solves this problem by allowing the analysis to selectively abstract away control-flow between events irrelevant to a goal query while preserving information about the ordering of relevant events. We present a framework for designing sound jumping analyses and create an instantiation of the framework for per- forming precise inter-event analysis of Android applications. Our experimental evaluation showed that using jumping to augment a precise goal-directed analysis with inter-event reasoning enabled our analysis to prove 90–97\% of dereferences safe across our benchmarks.}},
  url = {https://doi.org/10.1145/2814270.2814293},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1032,
  title = {A relational modal logic for higher-order stateful ADTs}},
  author = {Dreyer, Derek and Neis, Georg and Rossberg, Andreas and Birkedal, Lars}},
  year = {2010}},
  journal = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The method of logical relations is a classic technique for proving the equivalence of higher-order programs that implement the same observable behavior but employ different internal data representations. Although it was originally studied for pure, strongly normalizing languages like System F, it has been extended over the past two decades to reason about increasingly realistic languages. In particular, Appel and McAllester's idea of step-indexing has been used recently to develop syntactic Kripke logical relations for ML-like languages that mix functional and imperative forms of data abstraction. However, while step-indexed models are powerful tools, reasoning with them directly is quite painful, as one is forced to engage in tedious step-index arithmetic to derive even simple results.In this paper, we propose a logic LADR for equational reasoning about higher-order programs in the presence of existential type abstraction, general recursive types, and higher-order mutable state. LADR exhibits a novel synthesis of features from Plotkin-Abadi logic, G\"{o}},
  url = {https://doi.org/10.1145/1706299.1706323},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1033,
  title = {Prescient memory: exposing weak memory model behavior by looking into the future}},
  author = {Cao, Man and Roemer, Jake and Sengupta, Aritra and Bond, Michael D.}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Symposium on Memory Management}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Shared-memory parallel programs are hard to get right. A major challenge is that language and hardware memory models allow unexpected, erroneous behaviors for executions containing data races. Researchers have introduced dynamic analyses that expose weak memory model behaviors, but these approaches cannot expose behaviors due to loading a "future value" -- a value written by a program store that executes after the program load that uses the value. This paper presents prescient memory (PM), a novel dynamic analysis that exposes behaviors due to future values. PM speculatively returns a future value at a program load, and tries to validate the speculative value at a later store. To enable PM to expose behaviors due to future values in real application executions, we introduce a novel approach that increases the chances of using and successfully validating future values, by profiling and predicting future values and guiding execution. Experiments show that our approach is able to uncover a few previously unknown behaviors due to future values in benchmarked versions of real applications. Overall, PM overcomes a key limitation of existing approaches, broadening the scope of program behaviors that dynamic analyses can expose.}},
  url = {https://doi.org/10.1145/2926697.2926700},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1034,
  title = {Concurrent libraries with foresight}},
  author = {Golan-Gueta, Guy and Ramalingam, G. and Sagiv, Mooly and Yahav, Eran}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Linearizable libraries provide operations that appear to execute atomically. Clients, however, may need to execute a sequence of operations (a composite operation) atomically. We consider the problem of extending a linearizable library to support arbitrary atomic composite operations by clients. We introduce a novel approach in which the concurrent library ensures atomicity of composite operations by exploiting information (foresight) provided by its clients. We use a correctness condition, based on a notion of dynamic right-movers, that guarantees that composite operations execute atomically without deadlocks, and without using rollbacks.We present a static analysis to infer the foresight information required by our approach, allowing a compiler to automatically insert the foresight information into the client. This relieves the client programmer of this burden and simplifies writing client code.We present a generic technique for extending the library implementation to realize foresight-based synchronization. This technique is used to implement a general-purpose Java library for Map data structures -- the library permits composite operations to simultaneously work with multiple instances of Map data structures.We use the Maps library and the static analysis to enforce atomicity of a wide selection of real-life Java composite operations. Our experiments indicate that our approach enables realizing efficient and scalable synchronization for real-life composite operations.}},
  url = {https://doi.org/10.1145/2491956.2462172},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1035,
  title = {A Formally-Verified C Static Analyzer}},
  author = {Jourdan, Jacques-Henri and Laporte, Vincent and Blazy, Sandrine and Leroy, Xavier and Pichardie, David}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper reports on the design and soundness proof, using the Coq proof assistant, of Verasco, a static analyzer based on abstract interpretation for most of the ISO C 1999 language (excluding recursion and dynamic allocation). Verasco establishes the absence of run-time errors in the analyzed programs. It enjoys a modular architecture that supports the extensible combination of multiple abstract domains, both relational and non-relational. Verasco integrates with the CompCert formally-verified C compiler so that not only the soundness of the analysis results is guaranteed with mathematical certitude, but also the fact that these guarantees carry over to the compiled code.}},
  url = {https://doi.org/10.1145/2676726.2676966},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1036,
  title = {A stream compiler for communication-exposed architectures}},
  author = {Gordon, Michael I. and Thies, William and Karczmarek, Michal and Lin, Jasper and Meli, Ali S. and Lamb, Andrew A. and Leger, Chris and Wong, Jeremy and Hoffmann, Henry and Maze, David and Amarasinghe, Saman}},
  year = {2002}},
  journal = {Proceedings of the 10th International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {With the increasing miniaturization of transistors, wire delays are becoming a dominant factor in microprocessor performance. To address this issue, a number of emerging architectures contain replicated processing units with software-exposed communication between one unit and another (e.g., Raw, SmartMemories, TRIPS). However, for their use to be widespread, it will be necessary to develop compiler technology that enables a portable, high-level language to execute efficiently across a range of wire-exposed architectures.In this paper, we describe our compiler for StreamIt: a high-level, architecture-independent language for streaming applications. We focus on our backend for the Raw processor. Though StreamIt exposes the parallelism and communication patterns of stream programs, some analysis is needed to adapt a stream program to a software-exposed processor. We describe a partitioning algorithm that employs fission and fusion transformations to adjust the granularity of a stream graph, a layout algorithm that maps a stream graph to a given network topology, and a scheduling strategy that generates a fine-grained static communication pattern for each computational element.We have implemented a fully functional compiler that parallelizes StreamIt applications for Raw, including several load-balancing transformations. Using the cycle-accurate Raw simulator, we demonstrate that the StreamIt compiler can automatically map a high-level stream abstraction to Raw without losing performance. We consider this work to be a first step towards a portable programming model for communication-exposed architectures.}},
  url = {https://doi.org/10.1145/605397.605428},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1037,
  title = {Barrier invariants: a shared state abstraction for the analysis of data-dependent GPU kernels}},
  author = {Chong, Nathan and Donaldson, Alastair F. and Kelly, Paul H.J. and Ketema, Jeroen and Qadeer, Shaz}},
  year = {2013}},
  journal = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Data-dependent GPU kernels, whose data or control flow are dependent on the input of the program, are difficult to verify because they require reasoning about shared state manipulated by many parallel threads. Existing verification techniques for GPU kernels achieve soundness and scalability by using a two-thread reduction and making the contents of the shared state nondeterministic each time threads synchronise at a barrier, to account for all possible thread interactions. This coarse abstraction prohibits verification of data-dependent kernels. We present barrier invariants, a novel abstraction technique which allows key properties about the shared state of a kernel to be preserved across barriers during formal reasoning. We have integrated barrier invariants with the GPUVerify tool, and present a detailed case study showing how they can be used to verify three prefix sum algorithms, allowing efficient modular verification of a stream compaction kernel, a key building block for GPU programming. This analysis goes significantly beyond what is possible using existing verification techniques for GPU kernels.}},
  url = {https://doi.org/10.1145/2509136.2509517},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1038,
  title = {Predicate abstraction and CEGAR for higher-order model checking}},
  author = {Kobayashi, Naoki and Sato, Ryosuke and Unno, Hiroshi}},
  year = {2011}},
  journal = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Higher-order model checking (more precisely, the model checking of higher-order recursion schemes) has been extensively studied recently, which can automatically decide properties of programs written in the simply-typed λ-calculus with recursion and finite data domains. This paper formalizes predicate abstraction and counterexample-guided abstraction refinement (CEGAR) for higher-order model checking, enabling automatic verification of programs that use infinite data domains such as integers. A prototype verifier for higher-order functional programs based on the formalization has been implemented and tested for several programs.}},
  url = {https://doi.org/10.1145/1993498.1993525},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1039,
  title = {Static analysis of event-driven Node.js JavaScript applications}},
  author = {Madsen, Magnus and Tip, Frank and Lhot\'{a}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many JavaScript programs are written in an event-driven style. In particular, in server-side Node.js applications, operations involving sockets, streams, and files are typically performed in an asynchronous manner, where the execution of listeners is triggered by events. Several types of programming errors are specific to such event-based programs (e.g., unhandled events, and listeners that are registered too late). We present the event-based call graph, a program representation that can be used to detect bugs related to event handling. We have designed and implemented three analyses for constructing event-based call graphs. Our results show that these analyses are capable of detecting problems reported on StackOverflow. Moreover, we show that the number of false positives reported by the analysis on a suite of small Node.js applications is manageable.}},
  url = {https://doi.org/10.1145/2814270.2814272},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1040,
  title = {Provably correct peephole optimizations with alive}},
  author = {Lopes, Nuno P. and Menendez, David and Nagarakatte, Santosh and Regehr, John}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Compilers should not miscompile. Our work addresses problems in developing peephole optimizations that perform local rewriting to improve the efficiency of LLVM code. These optimizations are individually difficult to get right, particularly in the presence of undefined behavior; taken together they represent a persistent source of bugs. This paper presents Alive, a domain-specific language for writing optimizations and for automatically either proving them correct or else generating counterexamples. Furthermore, Alive can be automatically translated into C++ code that is suitable for inclusion in an LLVM optimization pass. Alive is based on an attempt to balance usability and formal methods; for example, it captures---but largely hides---the detailed semantics of three different kinds of undefined behavior in LLVM. We have translated more than 300 LLVM optimizations into Alive and, in the process, found that eight of them were wrong.}},
  url = {https://doi.org/10.1145/2737924.2737965},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1041,
  title = {Cause clue clauses: error localization using maximum satisfiability}},
  author = {Jose, Manu and Majumdar, Rupak}},
  year = {2011}},
  journal = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Much effort is spent by programmers everyday in trying to reduce long, failing execution traces to the cause of the error. We present an algorithm for error cause localization based on a reduction to the maximal satisfiability problem (MAX-SAT), which asks what is the maximum number of clauses of a Boolean formula that can be simultaneously satisfied by an assignment. At an intuitive level, our algorithm takes as input a program and a failing test, and comprises the following three steps. First, using bounded model checking, and a bound obtained from the execution of the test, we encode the semantics of a bounded unrolling of the program as a Boolean trace formula. Second, for a failing program execution (e.g., one that violates an assertion or a post-condition), we construct an unsatisfiable formula by taking the formula and additionally asserting that the input is the failing test and that the assertion condition does hold at the end. Third, using MAX-SAT, we find a maximal set of clauses in this formula that can be satisfied together, and output the complement set as a potential cause of the error.We have implemented our algorithm in a tool called BugAssist that performs error localization for C programs. We demonstrate the effectiveness of BugAssist on a set of benchmark examples with injected faults, and show that in most cases, BugAssist can quickly and precisely isolate a few lines of code whose change eliminates the error. We also demonstrate how our algorithm can be modified to automatically suggest fixes for common classes of errors such as off-by-one.We have implemented our algorithm in a tool called BugAssist that performs error localization for C programs. We demonstrate the effectiveness of BugAssist on a set of benchmark examples with injected faults, and show that in most cases, BugAssist can quickly and precisely isolate a few lines of code whose change eliminates the error. We also demonstrate how our algorithm can be modified to automatically suggest fixes for common classes of errors such as off-by-one.}},
  url = {https://doi.org/10.1145/1993498.1993550},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1042,
  title = {Generalising and dualising the third list-homomorphism theorem: functional pearl}},
  author = {Mu, Shin-Cheng and Morihata, Akimasa}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The third list-homomorphism theorem says that a function is a list homomorphism if it can be described as an instance of both a foldr and a foldl. We prove a dual theorem for unfolds and generalise both theorems to trees: if a function generating a list can be described both as an unfoldr and an unfoldl, the list can be generated from the middle, and a function that processes or builds a tree both upwards and downwards may independently process/build a subtree and its one-hole context. The point-free, relational formalism helps to reveal the beautiful symmetry hidden in the theorem.}},
  url = {https://doi.org/10.1145/2034773.2034824},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1043,
  title = {Controlled temporal non-determinism for reasoning with a machine of finite speed}},
  author = {Ennals, Robert}},
  year = {1998}},
  journal = {Proceedings of the Third ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {},
  url = {https://doi.org/10.1145/289423.289467},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1044,
  title = {An Efficient Abortable-locking Protocol for Multi-level NUMA Systems}},
  author = {Chabbi, Milind and Amer, Abdelhalim and Wen, Shasha and Liu, Xu}},
  year = {2017}},
  journal = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The popularity of Non-Uniform Memory Access (NUMA) architectures has led to numerous locality-preserving hierarchical lock designs, such as HCLH, HMCS, and cohort locks. Locality-preserving locks trade fairness for higher throughput. Hence, some instances of acquisitions can incur long latencies, which may be intolerable for certain applications. Few locks admit a waiting thread to abandon its protocol on a timeout. State-of-the-art abortable locks are not fully locality aware, introduce high overheads, and unsuitable for frequent aborts. Enhancing locality-aware locks with lightweight timeout capability is critical for their adoption. In this paper, we design and evaluate the HMCS-T lock, a Hierarchical MCS (HMCS) lock variant that admits a timeout. HMCS-T maintains the locality benefits of HMCS while ensuring aborts to be lightweight. HMCS-T offers the progress guarantee missing in most abortable queuing locks. Our evaluations show that HMCS-T offers the timeout feature at a moderate overhead over its HMCS analog. HMCS-T, used in an MPI runtime lock, mitigated the poor scalability of an MPI+OpenMP BFS code and resulted in 4.3x superior scaling.}},
  url = {https://doi.org/10.1145/3018743.3018768},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1045,
  title = {Fast algorithm for creating space efficient dispatching tables with application to multi-dispatching}},
  author = {Zibin, Yoav and Gil, Joseph Yossi}},
  year = {2002}},
  journal = {Proceedings of the 17th ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The dispatching problem can be solved very efficiently in the single-inheritance~(SI) setting. In this paper we show how to extend one such solution to the multiple-inheritance~(MI) setting. This generalization comes with an increase to the space requirement by a small factor of κ This factor can be thought of as a metric of the complexity of the topology of the inheritance hierarchy.On a data set of~35 hierarchies totaling some~64 thousand types, our dispatching data structure, based on a novel type slicing technique, exhibits very significant improvements over previous dispatching techniques, not only in terms of the time for creating the underlying data structure, but also in terms of total space used.The cost is in the dispatching time, which is no longer constant, but doubly logarithmic in the number of types. Conversely, by using a simple binary search, dispatching time is logarithmic in the number of different implementations. In practice dispatching uses one indirect branch and, on average, only~2.5 binary branches.Our results also have applications to the space-efficient implementation of the more general problem of dispatching multi-methods.A by-product of our type slicing technique is an incremental algorithm for constant-time subtyping tests with favorable memory requirements. (The incremental version of the subtyping problem is to maintain the subtyping data structure in presence of additions of types to the inheritance hierarchy.)}},
  url = {https://doi.org/10.1145/582419.582434},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1046,
  title = {A tool to analyze the performance of multithreaded programs on NUMA architectures}},
  author = {Liu, Xu and Mellor-Crummey, John}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Almost all of today's microprocessors contain memory controllers and directly attach to memory. Modern multiprocessor systems support non-uniform memory access (NUMA): it is faster for a microprocessor to access memory that is directly attached than it is to access memory attached to another processor. Without careful distribution of computation and data, a multithreaded program running on such a system may have high average memory access latency. To use multiprocessor systems efficiently, programmers need performance tools to guide the design of NUMA-aware codes. To address this need, we enhanced the HPCToolkit performance tools to support measurement and analysis of performance problems on multiprocessor systems with multiple NUMA domains. With these extensions, HPCToolkit helps pinpoint, quantify, and analyze NUMA bottlenecks in executions of multithreaded programs. It computes derived metrics to assess the severity of bottlenecks, analyzes memory accesses, and provides a wealth of information to guide NUMA optimization, including information about how to distribute data to reduce access latency and minimize contention. This paper describes the design and implementation of our extensions to HPCToolkit. We demonstrate their utility by describing case studies in which we use these capabilities to diagnose NUMA bottlenecks in four multithreaded applications.}},
  url = {https://doi.org/10.1145/2555243.2555271},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1047,
  title = {CheckCell: data debugging for spreadsheets}},
  author = {Barowy, Daniel W. and Gochev, Dimitar and Berger, Emery D.}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Testing and static analysis can help root out bugs in programs, but not in data. This paper introduces data debugging, an approach that combines program analysis and statistical analysis to automatically find potential data errors. Since it is impossible to know a priori whether data are erroneous, data debugging instead locates data that has a disproportionate impact on the computation. Such data is either very important, or wrong. Data debugging is especially useful in the context of data-intensive programming environments that intertwine data with programs in the form of queries or formulas.We present the first data debugging tool, CheckCell, an add-in for Microsoft Excel. CheckCell identifies cells that have an unusually high impact on the spreadsheet's computations. We show that CheckCell is both analytically and empirically fast and effective. We show that it successfully finds injected typographical errors produced by a generative model trained with data entry from 169,112 Mechanical Turk tasks. CheckCell is more precise and efficient than standard outlier detection techniques. CheckCell also automatically identifies a key flaw in the infamous Reinhart and Rogoff spreadsheet.}},
  url = {https://doi.org/10.1145/2660193.2660207},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1048,
  title = {Contracts made manifest}},
  author = {Greenberg, Michael and Pierce, Benjamin C. and Weirich, Stephanie}},
  year = {2010}},
  journal = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Since Findler and Felleisen introduced higher-order contracts , many variants have been proposed. Broadly, these fall into two groups: some follow Findler and Felleisen in using latent contracts, purely dynamic checks that are transparent to the type system; others use manifest contracts, where refinement types record the most recent check that has been applied to each value. These two approaches are commonly assumed to be equivalent---different ways of implementing the same idea, one retaining a simple type system, and the other providing more static information. Our goal is to formalize and clarify this folklore understanding.Our work extends that of Gronski and Flanagan, who defined a latent calculus λ C and a manifest calculus λ H , gave a translation φ from λ C to λ H , and proved that, if a λ C term reduces to a constant, then so does its φ-image. We enrich their account with a translation Ψ from λ H to λ C and prove an analogous theorem.We then generalize the whole framework to dependent contracts , whose predicates can mention free variables. This extension is both pragmatically crucial, supporting a much more interesting range of contracts, and theoretically challenging. We define dependent versions of λ H and two dialects ("lax" and "picky") of λ C , establish type soundness---a substantial result in itself, for λ H ---and extend φ and Ψ accordingly. Surprisingly, the intuition that the latent and manifest systems are equivalent now breaks down: the extended translations preserve behavior in one direction but, in the other, sometimes yield terms that blame more.}},
  url = {https://doi.org/10.1145/1706299.1706341},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1049,
  title = {Steering symbolic execution to less traveled paths}},
  author = {Li, You and Su, Zhendong and Wang, Linzhang and Li, Xuandong}},
  year = {2013}},
  journal = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Symbolic execution is a promising testing and analysis methodology. It systematically explores a program's execution space and can generate test cases with high coverage. One significant practical challenge for symbolic execution is how to effectively explore the enormous number of program paths in real-world programs. Various heuristics have been proposed for guiding symbolic execution, but they are generally inefficient and ad-hoc. In this paper, we introduce a novel, unified strategy to guide symbolic execution to less explored parts of a program. Our key idea is to exploit a specific type of path spectra, namely the length-n subpath program spectra, to systematically approximate full path information for guiding path exploration. In particular, we use frequency distributions of explored length-n subpaths to prioritize "less traveled" parts of the program to improve test coverage and error detection. We have implemented our general strategy in KLEE, a state-of-the-art symbolic execution engine. Evaluation results on the GNU Coreutils programs show that (1) varying the length n captures program-specific information and exhibits different degrees of effectiveness, and (2) our general approach outperforms traditional strategies in both coverage and error detection.}},
  url = {https://doi.org/10.1145/2509136.2509553},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1050,
  title = {Nominal system T}},
  author = {Pitts, Andrew M.}},
  year = {2010}},
  journal = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper introduces a new recursion principle for inductive data modulo α-equivalence of bound names. It makes use of Odersky-style local names when recursing over bound names. It is formulated in an extension of G\"{o}},
  url = {https://doi.org/10.1145/1706299.1706321},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1051,
  title = {FlowDroid: precise context, flow, field, object-sensitive and lifecycle-aware taint analysis for Android apps}},
  author = {Arzt, Steven and Rasthofer, Siegfried and Fritz, Christian and Bodden, Eric and Bartel, Alexandre and Klein, Jacques and Le Traon, Yves and Octeau, Damien and McDaniel, Patrick}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Today's smartphones are a ubiquitous source of private and confidential data. At the same time, smartphone users are plagued by carelessly programmed apps that leak important data by accident, and by malicious apps that exploit their given privileges to copy such data intentionally. While existing static taint-analysis approaches have the potential of detecting such data leaks ahead of time, all approaches for Android use a number of coarse-grain approximations that can yield high numbers of missed leaks and false alarms.In this work we thus present FlowDroid, a novel and highly precise static taint analysis for Android applications. A precise model of Android's lifecycle allows the analysis to properly handle callbacks invoked by the Android framework, while context, flow, field and object-sensitivity allows the analysis to reduce the number of false alarms. Novel on-demand algorithms help FlowDroid maintain high efficiency and precision at the same time.We also propose DroidBench, an open test suite for evaluating the effectiveness and accuracy of taint-analysis tools specifically for Android apps. As we show through a set of experiments using SecuriBench Micro, DroidBench, and a set of well-known Android test applications, FlowDroid finds a very high fraction of data leaks while keeping the rate of false positives low. On DroidBench, FlowDroid achieves 93\% recall and 86\% precision, greatly outperforming the commercial tools IBM AppScan Source and Fortify SCA. FlowDroid successfully finds leaks in a subset of 500 apps from Google Play and about 1,000 malware apps from the VirusShare project.}},
  url = {https://doi.org/10.1145/2594291.2594299},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1052,
  title = {A parallel algorithm for global states enumeration in concurrent systems}},
  author = {Chang, Yen-Jung and Garg, Vijay K.}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Verifying the correctness of the executions of a concurrent program is difficult because of its nondeterministic behavior. One of the verification methods is predicate detection, which predicts whether the user specified condition (predicate) could become true in any global states of the program. The method is predictive because it generates inferred execution paths from the observed execution path and then checks the predicate on the global states of inferred paths. One important part of predicate detection is global states enumeration, which generates the global states on inferred paths. Cooper and Marzullo gave the first enumeration algorithm based on a breadth first strategy (BFS). Later, many algorithms have been proposed to improve space and time complexity. None of them, however, takes parallelism into consideration. In this paper, we present the first parallel and online algorithm, named ParaMount, for global state enumeration. Our experimental results show that ParaMount speeds up the existing sequential algorithms by a factor of 6 with 8 threads. We have implemented an online predicate detector using ParaMount. For predicate detection, our detector based on ParaMount is 10 to 50 times faster than RV runtime (a verification tool that uses Cooper and Marzullo’s BFS enumeration algorithm).}},
  url = {https://doi.org/10.1145/2688500.2688520},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1053,
  title = {CLAP: recording local executions to reproduce concurrency failures}},
  author = {Huang, Jeff and Zhang, Charles and Dolby, Julian}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present CLAP, a new technique to reproduce concurrency bugs. CLAP has two key steps. First, it logs thread local execution paths at runtime. Second, offline, it computes memory dependencies that accord with the logged execution and are able to reproduce the observed bug. The second step works by combining constraints from the thread paths and constraints based on a memory model, and computing an execution with a constraint solver.CLAP has four major advantages. First, logging purely local execution of each thread is substantially cheaper than logging memory interactions, which enables CLAP to be efficient compared to previous approaches. Second, our logging does not require any synchronization and hence with no added memory barriers or fences; this minimizes perturbation and missed bugs due to extra synchronizations foreclosing certain racy behaviors. Third, since it uses no synchronization, we extend CLAP to work on a range of relaxed memory models, such as TSO and PSO, in addition to sequential consistency. Fourth, CLAP can compute a much simpler execution than the original one, that reveals the bug with minimal thread context switches. To mitigate the scalability issues, we also present an approach to parallelize constraint solving, which theoretically scales our technique to programs with arbitrary execution length.Experimental results on a variety of multithreaded benchmarks and real world concurrent applications validate these advantages by showing that our technique is effective in reproducing concurrency bugs even under relaxed memory models; furthermore, it is significantly more efficient than a state-of-the-art technique that records shared memory dependencies, reducing execution time overhead by 45\% and log size by 88\% on average.}},
  url = {https://doi.org/10.1145/2491956.2462167},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1054,
  title = {Summarizing procedures in concurrent programs}},
  author = {Qadeer, Shaz and Rajamani, Sriram K. and Rehof, Jakob}},
  year = {2004}},
  journal = {Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The ability to summarize procedures is fundamental to building scalable interprocedural analyses. For sequential programs, procedure summarization is well-understood and used routinely in a variety of compiler optimizations and software defect-detection tools. However, the benefit of summarization is not available to multithreaded programs, for which a clear notion of summaries has so far remained unarticulated in the research literature.In this paper, we present an intuitive and novel notion of procedure summaries for multithreaded programs. We also present a model checking algorithm for these programs that uses procedure summarization as an essential component. Our algorithm can also be viewed as a precise interprocedural dataflow analysis for multithreaded programs. Our method for procedure summarization is based on the insight that in well-synchronized programs, any computation of a thread can be viewed as a sequence of transactions, each of which appears to execute atomically to other threads. We summarize within each transaction; the summary of a procedure comprises the summaries of all transactions within the procedure. We leverage the theory of reduction [17] to infer boundaries of these transactions.The procedure summaries computed by our algorithm allow reuse of analysis results across different call sites in a multithreaded program, a benefit that has hitherto been available only to sequential programs. Although our algorithm is not guaranteed to terminate on multithreaded programs that use recursion (reachability analysis for multithreaded programs with recursive procedures is undecidable [18]), there is a large class of programs for which our algorithm does terminate. We give a formal characterization of this class, which includes programs that use shared variables, synchronization, and recursion.}},
  url = {https://doi.org/10.1145/964001.964022},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1055,
  title = {Deriving an efficient FPGA implementation of a low density parity check forward error corrector}},
  author = {Gill, Andy and Farmer, Andrew}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Creating correct hardware is hard. Though there is much talk of using formal and semi-formal methods to develop designs and implementations, in practice most implementations are written without the support of any formal or semi-formal methodology. Having such a methodology brings many benefits, including improved likelihood of a correct implementation, lowering the cost of design exploration and lowering the cost of certification. In this paper, we introduce a semi formal methodology for connecting executable specifications written in the functional language Haskell to efficient VHDL implementations. The connection is performed by manual edits, using semi-formal equational reasoning facilitated by the worker/wrapper transformation, and directed using commutable functors. We explain our methodology on a full-scale example, an efficient Low-Density Parity Check forward error correcting code, which has been implemented on a Virtex-5 FPGA.}},
  url = {https://doi.org/10.1145/2034773.2034804},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1056,
  title = {Combining proofs and programs in a dependently typed language}},
  author = {Casinghino, Chris and Sj\"{o}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Most dependently-typed programming languages either require that all expressions terminate (e.g. Coq, Agda, and Epigram), or allow infinite loops but are inconsistent when viewed as logics (e.g. Haskell, ATS, Ωmega. Here, we combine these two approaches into a single dependently-typed core language. The language is composed of two fragments that share a common syntax and overlapping semantics: a logic that guarantees total correctness, and a call-by-value programming language that guarantees type safety but not termination. The two fragments may interact: logical expressions may be used as programs; the logic may soundly reason about potentially nonterminating programs; programs can require logical proofs as arguments; and "mobile" program values, including proofs computed at runtime, may be used as evidence by the logic. This language allows programmers to work with total and partial functions uniformly, providing a smooth path from functional programming to dependently-typed programming.}},
  url = {https://doi.org/10.1145/2535838.2535883},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1057,
  title = {Shared subtypes: subtyping recursive parametrized algebraic data types}},
  author = {Ahn, Ki Yung and Sheard, Tim}},
  year = {2008}},
  journal = {Proceedings of the First ACM SIGPLAN Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A newtype declaration in Haskell introduces a new type renaming an existing type. The two types are viewed by the programmer as semantically different, but share the same runtime representation. When operations on the two semantic views coincide, the run-time cost of conversion between the two types is reduced to zero (in both directions) because of this common representation.We describe a new language feature called Shared Subtypes (SSubtypes), which generalizes these properties of the newtype declaration. SSubtypes allow programmers to specify subtype rules between types and sharing rules between data constructors. A value of a type T, where T is a subtype of U, can always be cast, at no cost, to value of type U. This free up-casting allows library functions that consume the supertype to be applied without cost to subtypes. Yet any semantic interpretations desired by the programmer can be enforced by the compiler. SSubtype declarations work particularly well with GADTs. GADTs use differing type indexes to make explicit semantic differences, by using a different index for each way of viewing the data. Shared subtypes allow GADTs to share the same runtime representation as a reference type, of which the GADT is a refinement.}},
  url = {https://doi.org/10.1145/1411286.1411297},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1058,
  title = {Detecting silent data corruption through data dynamic monitoring for scientific applications}},
  author = {Bautista Gomez, Leonardo and Cappello, Franck}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Parallel programming has become one of the best ways to express scientific models that simulate a wide range of natural phenomena. These complex parallel codes are deployed and executed on large-scale parallel computers, making them important tools for scientific discovery. As supercomputers get faster and larger, the increasing number of components is leading to higher failure rates. In particular, the miniaturization of electronic components is expected to lead to a dramatic rise in soft errors and data corruption. Moreover, soft errors can corrupt data silently and generate large inaccuracies or wrong results at the end of the computation. In this paper we propose a novel technique to detect silent data corruption based on data monitoring. Using this technique, an application can learn the normal dynamics of its datasets, allowing it to quickly spot anomalies. We evaluate our technique with synthetic benchmarks and we show that our technique can detect up to 50\% of injected errors while incurring only negligible overhead.}},
  url = {https://doi.org/10.1145/2555243.2555279},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1059,
  title = {A theorem prover for Boolean BI}},
  author = {Park, Jonghyun and Seo, Jeongbong and Park, Sungwoo}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {While separation logic is acknowledged as an enabling technology for large-scale program verification, most of the existing verification tools use only a fragment of separation logic that excludes separating implication. As the first step towards a verification tool using full separation logic, we develop a nested sequent calculus for Boolean BI (Bunched Implications), the underlying theory of separation logic, as well as a theorem prover based on it. A salient feature of our nested sequent calculus is that its sequent may have not only smaller child sequents but also multiple parent sequents, thus producing a graph structure of sequents instead of a tree structure. Our theorem prover is based on backward search in a refinement of the nested sequent calculus in which weakening and contraction are built into all the inference rules. We explain the details of designing our theorem prover and provide empirical evidence of its practicality.}},
  url = {https://doi.org/10.1145/2429069.2429095},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1060,
  title = {A simple, verified validator for software pipelining}},
  author = {Tristan, Jean-Baptiste and Leroy, Xavier}},
  year = {2010}},
  journal = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Software pipelining is a loop optimization that overlaps the execution of several iterations of a loop to expose more instruction-level parallelism. It can result in first-class performance characteristics, but at the cost of significant obfuscation of the code, making this optimization difficult to test and debug. In this paper, we present a translation validation algorithm that uses symbolic evaluation to detect semantics discrepancies between a loop and its pipelined version. Our algorithm can be implemented simply and efficiently, is provably sound, and appears to be complete with respect to most modulo scheduling algorithms. A conclusion of this case study is that it is possible and effective to use symbolic evaluation to reason about loop transformations.}},
  url = {https://doi.org/10.1145/1706299.1706311},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1061,
  title = {Geometry of synthesis iv: compiling affine recursion into static hardware}},
  author = {Ghica, Dan R. and Smith, Alex and Singh, Satnam}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Abramsky's Geometry of Interaction interpretation (GoI) is a logical-directed way to reconcile the process and functional views of computation, and can lead to a dataflow-style semantics of programming languages that is both operational (i.e. effective) and denotational (i.e. inductive on the language syntax). The key idea of Ghica's Geometry of Synthesis (GoS) approach is that for certain programming languages (namely Reynolds's affine Syntactic Control of Interference - SCI) the GoI processes-like interpretation of the language can be given a finitary representation, for both internal state and tokens. A physical realisation of this representation becomes a semantics-directed compiler for SCI into hardware. In this paper we examine the issue of compiling affine recursive programs into hardware using the GoS method. We give syntax and compilation techniques for unfolding recursive computation in space or in time and we illustrate it with simple benchmark-style examples. We examine the performance of the benchmarks against conventional CPU-based execution models.}},
  url = {https://doi.org/10.1145/2034773.2034805},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1062,
  title = {Space-time memory: a parallel programming abstraction for interactive multimedia applications}},
  author = {Ramachandran, Umakishore and Nikhil, Rishiyur S. and Harel, Nissim and Rehg, James M. and Knobe, Kathleen}},
  year = {1999}},
  journal = {Proceedings of the Seventh ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Realistic interactive multimedia involving vision, animation, and multimedia collaboration is likely to become an important aspect of future computer applications. The scalable parallelism inherent in such applications coupled with their computational demands make them ideal candidates for SMPs and clusters of SMPs. These applications have novel requirements that offer new kinds of challenges for parallel system design.We have designed a programming system called Stampede that offers many functionalities needed to simplify development of such applications (such as high-level data sharing abstractions, dynamic cluster-wide threads, and multiple address spaces). We have built Stampede and it runs on clusters of SMPs. To date we have implemented two applications on Stampede, one of which is discussed herein.In this paper we describe a part of Stampede called Space-Time Memory (STM). It is a novel data sharing abstraction that enables interactive multimedia applications to manage a collection of time-sequenced data items simply, efficiently, and transparently across a cluster. STM relieves the application programmer from low level synchronization and data communication by providing a high level interface that subsumes buffer management, inter-thread synchronization, and location transparency for data produced and accessed anywhere in the cluster. STM also automatically handles garbage collection of data items that will no longer be accessed by any of the application threads. We discuss ease of use issues for developing applications using STM, and present preliminary performance results to show that STM's overhead is low.}},
  url = {https://doi.org/10.1145/301104.301121},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1063,
  title = {Specifying and verifying sparse matrix codes}},
  author = {Arnold, Gilad and H\"{o}},
  year = {2010}},
  journal = {Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Sparse matrix formats are typically implemented with low-level imperative programs. The optimized nature of these implementations hides the structural organization of the sparse format and complicates its verification. We define a variable-free functional language (LL) in which even advanced formats can be expressed naturally, as a pipeline-style composition of smaller construction steps. We translate LL programs to Isabelle/HOL and describe a proof system based on parametric predicates for tracking relationship between mathematical vectors and their concrete representations. This proof theory automatically verifies full functional correctness of many formats. We show that it is reusable and extensible to hierarchical sparse formats.}},
  url = {https://doi.org/10.1145/1863543.1863581},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1064,
  title = {Summary-Based Context-Sensitive Data-Dependence Analysis in Presence of Callbacks}},
  author = {Tang, Hao and Wang, Xiaoyin and Zhang, Lingming and Xie, Bing and Zhang, Lu and Mei, Hong}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Building a summary for library code is a common approach to speeding up the analysis of client code. In presence of callbacks, some reachability relationships between library nodes cannot be obtained during library-code summarization. Thus, the library code may have to be analyzed again during the analysis of the client code with the library summary. In this paper, we propose to summarize library code with tree-adjoining-language (TAL) reachability. Compared with the summary built with context-free-language (CFL) reachability, the summary built with TAL reachability further contains conditional reachability relationships. The conditional reachability relationships can lead to much lighter analysis of the library code during the client code analysis with the TAL-reachability-based library summary. We also performed an experimental comparison of context-sensitive data-dependence analysis with the TAL-reachability-based library summary and context-sensitive data-dependence analysis with the CFL-reachability-based library summary using 15 benchmark subjects. Our experimental results demonstrate that the former has an 8X speed-up over the latter on average.}},
  url = {https://doi.org/10.1145/2676726.2676997},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1065,
  title = {Automatic modular abstractions for linear constraints}},
  author = {Monniaux, David P.}},
  year = {2009}},
  journal = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We propose a method for automatically generating abstract transformers for static analysis by abstract interpretation. The method focuses on linear constraints on programs operating on rational, real or floating-point variables and containing linear assignments and tests.In addition to loop-free code, the same method also applies for obtaining least fixed points as functions of the precondition, which permits the analysis of loops and recursive functions. Our algorithms are based on new quantifier elimination and symbolic manipulation techniques.Given the specification of an abstract domain, and a program block, our method automatically outputs an implementation of the corresponding abstract transformer. It is thus a form of program transformation.The motivation of our work is data-flow synchronous programming languages, used for building control-command embedded systems, but it also applies to imperative and functional programming.}},
  url = {https://doi.org/10.1145/1480881.1480899},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1066,
  title = {Compositional certified resource bounds}},
  author = {Carbonneaux, Quentin and Hoffmann, Jan and Shao, Zhong}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents a new approach for automatically deriving worst-case resource bounds for C programs. The described technique combines ideas from amortized analysis and abstract interpretation in a unified framework to address four challenges for state-of-the-art techniques: compositionality, user interaction, generation of proof certificates, and scalability. Compositionality is achieved by incorporating the potential method of amortized analysis. It enables the derivation of global whole-program bounds with local derivation rules by naturally tracking size changes of variables in sequenced loops and function calls. The resource consumption of functions is described abstractly and a function call can be analyzed without access to the function body. User interaction is supported with a new mechanism that clearly separates qualitative and quantitative verification. A user can guide the analysis to derive complex non-linear bounds by using auxiliary variables and assertions. The assertions are separately proved using established qualitative techniques such as abstract interpretation or Hoare logic. Proof certificates are automatically generated from the local derivation rules. A soundness proof of the derivation system with respect to a formal cost semantics guarantees the validity of the certificates. Scalability is attained by an efficient reduction of bound inference to a linear optimization problem that can be solved by off-the-shelf LP solvers. The analysis framework is implemented in the publicly-available tool C4B. An experimental evaluation demonstrates the advantages of the new technique with a comparison of C4B with existing tools on challenging micro benchmarks and the analysis of more than 2900 lines of C code from the cBench benchmark suite.}},
  url = {https://doi.org/10.1145/2737924.2737955},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1067,
  title = {Efficient synthesis of network updates}},
  author = {McClurg, Jedidiah and Hojjat, Hossein and \v{C}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Software-defined networking (SDN) is revolutionizing the networking industry, but current SDN programming platforms do not provide automated mechanisms for updating global configurations on the fly. Implementing updates by hand is challenging for SDN programmers because networks are distributed systems with hundreds or thousands of interacting nodes. Even if initial and final configurations are correct, naively updating individual nodes can lead to incorrect transient behaviors, including loops, black holes, and access control violations. This paper presents an approach for automatically synthesizing updates that are guaranteed to preserve specified properties. We formalize network updates as a distributed programming problem and develop a synthesis algorithm based on counterexample-guided search and incremental model checking. We describe a prototype implementation, and present results from experiments on real-world topologies and properties demonstrating that our tool scales to updates involving over one-thousand nodes.}},
  url = {https://doi.org/10.1145/2737924.2737980},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1068,
  title = {GPUs as an opportunity for offloading garbage collection}},
  author = {Maas, Martin and Reames, Philip and Morlan, Jeffrey and Asanovi\'{c}},
  year = {2012}},
  journal = {Proceedings of the 2012 International Symposium on Memory Management}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {GPUs have become part of most commodity systems. Nonetheless, they are often underutilized when not executing graphics-intensive or special-purpose numerical computations, which are rare in consumer workloads. Emerging architectures, such as integrated CPU/GPU combinations, may create an opportunity to utilize these otherwise unused cycles for offloading traditional systems tasks. Garbage collection appears to be a particularly promising candidate for offloading, due to the popularity of managed languages on consumer devices.We investigate the challenges for offloading garbage collection to a GPU, by examining the performance trade-offs for the mark phase of a mark \&amp; sweep garbage collector. We present a theoretical analysis and an algorithm that demonstrates the feasibility of this approach. We also discuss a number of algorithmic design trade-offs required to leverage the strengths and capabilities of the GPU hardware. Our algorithm has been integrated into the Jikes RVM and we present promising performance results.}},
  url = {https://doi.org/10.1145/2258996.2259002},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1069,
  title = {Faster Algorithms for Algebraic Path Properties in Recursive State Machines with Constant Treewidth}},
  author = {Chatterjee, Krishnendu and Ibsen-Jensen, Rasmus and Pavlogiannis, Andreas and Goyal, Prateesh}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Interprocedural analysis is at the heart of numerous applications in programming languages, such as alias analysis, constant propagation, etc. Recursive state machines (RSMs) are standard models for interprocedural analysis. We consider a general framework with RSMs where the transitions are labeled from a semiring, and path properties are algebraic with semiring operations. RSMs with algebraic path properties can model interprocedural dataflow analysis problems, the shortest path problem, the most probable path problem, etc. The traditional algorithms for interprocedural analysis focus on path properties where the starting point is fixed as the entry point of a specific method. In this work, we consider possible multiple queries as required in many applications such as in alias analysis. The study of multiple queries allows us to bring in a very important algorithmic distinction between the resource usage of the one-time preprocessing vs for each individual query. The second aspect that we consider is that the control flow graphs for most programs have constant treewidth.Our main contributions are simple and implementable algorithms that support multiple queries for algebraic path properties for RSMs that have constant treewidth. Our theoretical results show that our algorithms have small additional one-time preprocessing, but can answer subsequent queries significantly faster as compared to the current best-known solutions for several important problems, such as interprocedural reachability and shortest path. We provide a prototype implementation for interprocedural reachability and intraprocedural shortest path that gives a significant speed-up on several benchmarks.}},
  url = {https://doi.org/10.1145/2676726.2676979},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1070,
  title = {Tree dependence analysis}},
  author = {Weijiang, Yusheng and Balakrishna, Shruthi and Liu, Jianqiao and Kulkarni, Milind}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We develop a new framework for analyzing recursive methods that perform traversals over trees, called tree dependence analysis. This analysis translates dependence analysis techniques for regular programs to the irregular space, identifying the structure of dependences within a recursive method that traverses trees. We develop a dependence test that exploits the dependence structure of such programs, and can prove that several locality- and parallelism- enhancing transformations are legal. In addition, we extend our analysis with a novel path-dependent, conditional analysis to refine the dependence test and prove the legality of transformations for a wider range of algorithms. We then use these analyses to show that several common algorithms that manipulate trees recursively are amenable to several locality- and parallelism-enhancing transformations. This work shows that classical dependence analysis techniques, which have largely been confined to nested loops over array data structures, can be extended and translated to work for complex, recursive programs that operate over pointer-based data structures.}},
  url = {https://doi.org/10.1145/2737924.2737972},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1071,
  title = {Verified heap theorem prover by paramodulation}},
  author = {Stewart, Gordon and Beringer, Lennart and Appel, Andrew W.}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present VeriStar, a verified theorem prover for a decidable subset of separation logic. Together with VeriSmall [3], a proved-sound Smallfoot-style program analysis for C minor, VeriStar demonstrates that fully machine-checked static analyses equipped with efficient theorem provers are now within the reach of formal methods. As a pair, VeriStar and VeriSmall represent the first application of the Verified Software Toolchain [4], a tightly integrated collection of machine-verified program logics and compilers giving foundational correctness guarantees.VeriStar is (1) purely functional, (2) machine-checked, (3) end-to-end, (4) efficient and (5) modular. By purely functional, we mean it is implemented in Gallina, the pure functional programming language embedded in the Coq theorem prover. By machine-checked, we mean it has a proof in Coq that when the prover says "valid", the checked entailment holds in a proved-sound separation logic for C minor. By end-to-end, we mean that when the static analysis+theorem prover says a C minor program is safe, the program will be compiled to a semantically equivalent assembly program that runs on real hardware. By efficient, we mean that the prover implements a state-of-the-art algorithm for deciding heap entailments and uses highly tuned verified functional data structures. By modular, we mean that VeriStar can be retrofitted to other static analyses as a plug-compatible entailment checker and its soundness proof can easily be ported to other separation logics.}},
  url = {https://doi.org/10.1145/2364527.2364531},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1072,
  title = {Sound Modular Verification of C Code Executing in an Unverified Context}},
  author = {Agten, Pieter and Jacobs, Bart and Piessens, Frank}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Over the past decade, great progress has been made in the static modular verification of C code by means of separation logic-based program logics. However, the runtime guarantees offered by such verification are relatively limited when the verified modules are part of a whole program that also contains unverified modules. In particular, a memory safety error in an unverified module can corrupt the runtime state, leading to assertion failures or invalid memory accesses in the verified modules. This paper develops runtime checks to be inserted at the boundary between the verified and the unverified part of a program, to guarantee that no assertion failures or invalid memory accesses can occur at runtime in any verified module. One of the key challenges is enforcing the separation logic frame rule, which we achieve by checking the integrity of the footprint of the verified part of the program on each control flow transition from the unverified to the verified part. This in turn requires the presence of some support for module-private memory at runtime. We formalize our approach and prove soundness. We implement the necessary runtime checks by means of a program transformation that translates C code with separation logic annotations into plain C, and that relies on a protected module architecture for providing module-private memory and restricted module entry points. Benchmarks show the performance impact of this transformation depends on the choice of boundary between the verified and unverified parts of the program, but is below 4\% for real-world applications.}},
  url = {https://doi.org/10.1145/2676726.2676972},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1073,
  title = {Multivariate amortized resource analysis}},
  author = {Hoffmann, Jan and Aehlig, Klaus and Hofmann, Martin}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We study the problem of automatically analyzing the worst-case resource usage of procedures with several arguments. Existing automatic analyses based on amortization, or sized types bound the resource usage or result size of such a procedure by a sum of unary functions of the sizes of the arguments.In this paper we generalize this to arbitrary multivariate polynomial functions thus allowing bounds of the form mn which had to be grossly overestimated by m2+n2 before. Our framework even encompasses bounds like ∗i,j≤n m_i mj where the mi are the sizes of the entries of a list of length n.This allows us for the first time to derive useful resource bounds for operations on matrices that are represented as lists of lists and to considerably improve bounds on other super-linear operations on lists such as longest common subsequence and removal of duplicates from lists of lists.Furthermore, resource bounds are now closed under composition which improves accuracy of the analysis of composed programs when some or all of the components exhibit super-linear resource or size behavior.The analysis is based on a novel multivariate amortized resource analysis. We present it in form of a type system for a simple first-order functional language with lists and trees, prove soundness, and describe automatic type inference based on linear programming.We have experimentally validated the automatic analysis on a wide range of examples from functional programming with lists and trees. The obtained bounds were compared with actual resource consumption. All bounds were asymptotically tight, and the constants were close or even identical to the optimal ones.}},
  url = {https://doi.org/10.1145/1926385.1926427},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1074,
  title = {A logical theory of concurrent objects}},
  author = {Meseguer, Jos\'{e}},
  year = {1990}},
  journal = {Proceedings of the European Conference on Object-Oriented Programming on Object-Oriented Programming Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A new theory of concurrent objects is presented. The theory has the important advantage of being based directly on a logic called rewriting logic in which concurrent object-oriented computation exactly corresponds to logical deduction. This deduction is performed by concurrent rewriting modulo structural axioms of associativity, commutativity and identity that capture abstractly the essential aspects of communication in a distributed object-oriented configuration made up of concurrent objects and messages. Thanks to this axiomatization, it becomes possible to study the behavior of concurrent objects by formal methods in a logic intrinsic to their computation. The relationship with Actors and with other models of concurrent computation is also discussed. A direct fruit of this theory is a new language, called Maude, to program concurrent object-oriented modules in an entirely declarative way using rewriting logic; modules written in this language are used to illustrate the main ideas with examples. Maude contains OBJ3 as a functional sublanguage and provides a simple and semantically rigorous integration of functional programming and concurrent object-oriented programming.}},
  url = {https://doi.org/10.1145/97945.97958},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1075,
  title = {Modeling the semantics of smalltalk-80 with Petri nets}},
  author = {Christodoulakis, D. N.}},
  year = {1988}},
  journal = {Proceedings of the 1988 ACM SIGPLAN Workshop on Object-Based Concurrent Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Over the last ten years, as the use of concurrent programming languages increased with a corresponding increase in the effort spent for building parallel machines, the importance of Petri nets grow. During this period Petri nets have been widely used for the specification, design and verification of nonsequential systems, and lots of effort has been spent for the development of software tools supporting Petri nets. Examples include the GRASPIN software engineering environment which supports Petri nets for graphical representation and formal specification of nonsequential systems [5], different net editors, etc.Since about one year we have started to use Petri nets in order to describe the semantics of object oriented languages (O-O for short), and in particular as a semantic model of the most representative O-O language, namely Smalltalk-80 [1]. Recent attempts towards possible extensions of Smalltalk-80 with concurrency have shown that semantic models of Smalltalk-80 like the above presented here, are not only useful for compiler writers, but their aid also towards identification and control of concurrency in the existing Smalltalk-80 systems; to specify possible computational models for a "concurrent" Smalltalk-80; and to indicate a possible semantic model for O-O distributed databases. Finally, since Petri nets provide the necessary theory for verification purposes, such a semantic model can be utilised for the verification of Smalltalk-80 programs as well.Main objective of this paper is to give an outline of the semantic function, described in next section. Subsequentialy, we discuss very short how the semantic function can be used for concurrency identiffication and control, and program verification.For the ongoing discussion we suppose that the reader is familiar with basic notions from Petri net theory [2, 3, 4].}},
  url = {https://doi.org/10.1145/67386.67430},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1076,
  title = {Iago attacks: why the system call API is a bad untrusted RPC interface}},
  author = {Checkoway, Stephen and Shacham, Hovav}},
  year = {2013}},
  journal = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In recent years, researchers have proposed systems for running trusted code on an untrusted operating system. Protection mechanisms deployed by such systems keep a malicious kernel from directly manipulating a trusted application's state. Under such systems, the application and kernel are, conceptually, peers, and the system call API defines an RPC interface between them.We introduce Iago attacks, attacks that a malicious kernel can mount in this model. We show how a carefully chosen sequence of integer return values to Linux system calls can lead a supposedly protected process to act against its interests, and even to undertake arbitrary computation at the malicious kernel's behest.Iago attacks are evidence that protecting applications from malicious kernels is more difficult than previously realized.}},
  url = {https://doi.org/10.1145/2451116.2451145},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1077,
  title = {Probabilistic, modular and scalable inference of typestate specifications}},
  author = {Beckman, Nels E. and Nori, Aditya V.}},
  year = {2011}},
  journal = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Static analysis tools aim to find bugs in software that correspond to violations of specifications. Unfortunately, for large and complex software, these specifications are usually either unavailable or sophisticated, and hard to write.This paper presents ANEK, a tool and accompanying methodology for inferring specifications useful for modular typestate checking of programs. In particular, these specifications consist of pre and postconditions along with aliasing annotations known as access permissions. A novel feature of ANEK is that it can generate program specifications even when the code under analysis gives rise to conflicting constraints, a situation that typically occurs when there are bugs. The design of ANEK also makes it easy to add heuristic constraints that encode intuitions gleaned from several years of experience writing such specifications, and this allows it to infer specifications that are better in a subjective sense. The ANEK algorithm is based on a modular analysis that makes it fast and scalable, while producing reliable specifications. All of these features are enabled by its underlying probabilistic analysis that produces specifications that are very likely.Our implementation of ANEK infers access permissions specifications used by the PLURAL [5] modular typestate checker for Java programs. We have run ANEK on a number of Java benchmark programs, including one large open-source program(approximately 38K lines of code), to infer specifications that were then checked using PLURAL. The results for the large benchmark show that ANEK can quickly infer specifications that are both accurate and qualitatively similar to those written by hand, and at 5\% of the time taken to manually discover and hand-code the specifications.}},
  url = {https://doi.org/10.1145/1993498.1993524},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1078,
  title = {Extending monads with pattern matching}},
  author = {Petricek, Tomas and Mycroft, Alan and Syme, Don}},
  year = {2011}},
  journal = {Proceedings of the 4th ACM Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Sequencing of effectful computations can be neatly captured using monads and elegantly written using do notation. In practice such monads often allow additional ways of composing computations, which have to be written explicitly using combinators.We identify joinads, an abstract notion of computation that is stronger than monads and captures many such ad-hoc extensions. In particular, joinads are monads with three additional operations: one of type m a -&gt; m b -&gt; m (a, b) captures various forms of parallel composition, one of type m a -&gt; m a -&gt; m a that is inspired by choice and one of type m a -&gt; m (m a) that captures aliasing of computations. Algebraically, the first two operations form a near-semiring with commutative multiplication.We introduce docase notation that can be viewed as a monadic version of case. Joinad laws imply various syntactic equivalences of programs written using docase that are analogous to equivalences about case. Examples of joinads that benefit from the notation include speculative parallelism, waiting for a combination of user interface events, but also encoding of validation rules using the intersection of parsers.}},
  url = {https://doi.org/10.1145/2034675.2034677},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1079,
  title = {Lag, drag, void and use—heap profiling and space-efficient compilation revisited}},
  author = {R\"{o}},
  year = {1996}},
  journal = {Proceedings of the First ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The context for this paper is functional computation by graph reduction. Our overall aim is more efficient use of memory. The specific topic is the detection of dormant cells in the live graph --- those retained in heap memory though not actually playing a useful role in computation. We describe a profiler that can identify heap consumption by such 'useless' cells. Unlike heap profilers based on traversals of the live heap, this profiler works by examining cells postmortem. The new profiler has revealed a surprisingly large proportion of 'useless' cells, even in some programs that previously seemed space-efficient such as the boot-strapping Haskell compiler nhc.}},
  url = {https://doi.org/10.1145/232627.232633},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1080,
  title = {Tractable Refinement Checking for Concurrent Objects}},
  author = {Bouajjani, Ahmed and Emmi, Michael and Enea, Constantin and Hamza, Jad}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Efficient implementations of concurrent objects such as semaphores, locks, and atomic collections are essential to modern computing. Yet programming such objects is error prone: in minimizing the synchronization overhead between concurrent object invocations, one risks the conformance to reference implementations --- or in formal terms, one risks violating observational refinement. Testing this refinement even within a single execution is intractable, limiting existing approaches to executions with very few object invocations.We develop a polynomial-time (per execution) approximation to refinement checking. The approximation is parameterized by an accuracy k∈N representing the degree to which refinement violations are visible. In principle, more violations are detectable as k increases, and in the limit, all are detectable. Our insight for this approximation arises from foundational properties on the partial orders characterizing the happens-before relations between object invocations: they are interval orders, with a well defined measure of complexity, i.e., their length. Approximating the happens-before relation with a possibly-weaker interval order of bounded length can be efficiently implemented by maintaining a bounded number of integer counters. In practice, we find that refinement violations can be detected with very small values of k, and that our approach scales far beyond existing refinement-checking approaches.}},
  url = {https://doi.org/10.1145/2676726.2677002},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1081,
  title = {Abstracting gradual typing}},
  author = {Garcia, Ronald and Clark, Alison M. and Tanter, \'{E}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Language researchers and designers have extended a wide variety of type systems to support gradual typing, which enables languages to seamlessly combine dynamic and static checking. These efforts consistently demonstrate that designing a satisfactory gradual counterpart to a static type system is challenging, and this challenge only increases with the sophistication of the type system. Gradual type system designers need more formal tools to help them conceptualize, structure, and evaluate their designs. In this paper, we propose a new formal foundation for gradual typing, drawing on principles from abstract interpretation to give gradual types a semantics in terms of pre-existing static types. Abstracting Gradual Typing (AGT for short) yields a formal account of consistency---one of the cornerstones of the gradual typing approach---that subsumes existing notions of consistency, which were developed through intuition and ad hoc reasoning. Given a syntax-directed static typing judgment, the AGT approach induces a corresponding gradual typing judgment. Then the type safety proof for the underlying static discipline induces a dynamic semantics for gradual programs defined over source-language typing derivations. The AGT approach does not resort to an externally justified cast calculus: instead, run-time checks naturally arise by deducing evidence for consistent judgments during proof reduction. To illustrate the approach, we develop a novel gradually-typed counterpart for a language with record subtyping. Gradual languages designed with the AGT approach satisfy by construction the refined criteria for gradual typing set forth by Siek and colleagues.}},
  url = {https://doi.org/10.1145/2837614.2837670},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1082,
  title = {Static analysis for independent app developers}},
  author = {Brutschy, Lucas and Ferrara, Pietro and M\"{u}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Mobile app markets have lowered the barrier to market entry for software producers. As a consequence, an increasing number of independent app developers offer their products, and recent platforms such as the MIT App Inventor and Microsoft's TouchDevelop enable even lay programmers to develop apps and distribute them in app markets.A major challenge in this distribution model is to ensure the quality of apps. Besides the usual sources of software errors, mobile apps are susceptible to errors caused by the non-determinism of an event-based execution model, a volatile environment, diverse hardware, and others. Many of these errors are difficult to detect during testing, especially for independent app developers, who are not supported by test teams and elaborate test infrastructures.To address this problem, we propose a static program analysis that captures the specifics of mobile apps and is efficient enough to provide feedback during the development process. Experiments involving 51,456 published TouchDevelop scripts show that our analysis analyzes 98\% of the scripts in under a minute, and five seconds on average. Manual inspection of the analysis results for a selection of all scripts shows that most of the alarms are real errors.}},
  url = {https://doi.org/10.1145/2660193.2660219},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1083,
  title = {Automatic generation of staged geometric predicates}},
  author = {Nanevski, Aleksandar and Blelloch, Guy and Harper, Robert}},
  year = {2001}},
  journal = {Proceedings of the Sixth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Algorithms in Computational Geometry and Computer Aided Design are often developed for the Real RAM model of computation, which assumes exactness of all the input arguments and operations. In practice, however, the exactness imposes tremendous limitations on the algorithms --- even the basic operations become uncomputable, or prohibitively slow. When the computations of interest are limited to determining the sign of polynomial expressions over floating point numbers, faster approaches are available. One can evaluate the polynomial in floating-point first, together with some estimate of the rounding error, and fall back to exact arithmetic only if this error is too big to determine the sign reliably. A particularly efficient variation on this approach has been used by Shewchuk in his robust implementations of Orient and InSphere geometric predicates. We extend Shewchuk's method to arbitrary polynomial expressions. The expressions are given as programs in a suitable source language featuring basic arithmetic operations of addition, subtraction, multiplication and squaring, which are to be perceived by the programmer as exact. The source language also allows for anonymous functions, and thus enables the common functional programming technique of staging. The method is presented formally through several judgments that govern the compilation of the source expression into target code, which is then easily transformed into SML or, in case of single-stage expressions, into C.}},
  url = {https://doi.org/10.1145/507635.507662},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1084,
  title = {Finding resume and restart errors in Android applications}},
  author = {Shan, Zhiyong and Azim, Tanzirul and Neamtiu, Iulian}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Smartphone apps create and handle a large variety of ``instance'' data that has to persist across runs, such as the current navigation route, workout results, antivirus settings, or game state. Due to the nature of the smartphone platform, an app can be paused, sent into background, or killed at any time. If the instance data is not saved and restored between runs, in addition to data loss, partially-saved or corrupted data can crash the app upon resume or restart. While smartphone platforms offer API support for data-saving and data-retrieving operations, the use of this API is ad-hoc: left to the programmer, rather than enforced by the compiler. We have observed that several categories of bugs---including data loss, failure to resume/restart or resuming/restarting in the wrong state---are due to incorrect handling of instance data and are easily triggered by just pressing the `Home' or `Back' buttons. To help address this problem, we have constructed a tool chain for Android (the KREfinder static analysis and the KREreproducer input generator) that helps find and reproduce such incorrect handling. We have evaluated our approach by running the static analysis on 324 apps, of which 49 were further analyzed manually. Results indicate that our approach is (i) effective, as it has discovered 49 bugs, including in popular Android apps, and (ii) efficient, completing on average in 61 seconds per app. More generally, our approach helps determine whether an app saves too much or too little state.}},
  url = {https://doi.org/10.1145/2983990.2984011},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1085,
  title = {Learning minimal abstractions}},
  author = {Liang, Percy and Tripp, Omer and Naik, Mayur}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Static analyses are generally parametrized by an abstraction which is chosen from a family of abstractions. We are interested in flexible families of abstractions with many parameters, as these families can allow one to increase precision in ways tailored to the client without sacrificing scalability. For example, we consider k-limited points-to analyses where each call site and allocation site in a program can have a different k value. We then ask a natural question in this paper: What is the minimal (coarsest) abstraction in a given family which is able to prove a set of queries? In addressing this question, we make the following two contributions: (i) We introduce two machine learning algorithms for efficiently finding a minimal abstraction; and (ii) for a static race detector backed by a k-limited points-to analysis, we show empirically that minimal abstractions are actually quite coarse: It suffices to provide context/object sensitivity to a very small fraction (0.4-2.3\%) of the sites to yield equally precise results as providing context/object sensitivity uniformly to all sites.}},
  url = {https://doi.org/10.1145/1926385.1926391},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1086,
  title = {Making prophecies with decision predicates}},
  author = {Cook, Byron and Koskinen, Eric}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We describe a new algorithm for proving temporal properties expressed in LTL of infinite-state programs. Our approach takes advantage of the fact that LTL properties can often be proved more efficiently using techniques usually associated with the branching-time logic CTL than they can with native LTL algorithms. The caveat is that, in certain instances, nondeterminism in the system's transition relation can cause CTL methods to report counter examples that are spurious with respect to the original LTL formula. To address this problem we describe an algorithm that, as it attempts to apply CTL proof methods, finds and then removes problematic nondeterminism via an analysis on the potentially spurious counterexamples. Problematic nondeterminism is characterized using decision predicates, and removed using a partial, symbolic determinization procedure which introduces new prophecy variables to predict the future outcome of these choices. We demonstrate---using examples taken from the PostgreSQL database server, Apache web server, and Windows OS kernel---that our method can yield enormous performance improvements in comparison to known tools, allowing us to automatically prove properties of programs where we could not prove them before.}},
  url = {https://doi.org/10.1145/1926385.1926431},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1087,
  title = {ShamDroid: gracefully degrading functionality in the presence of limited resource access}},
  author = {Brutschy, Lucas and Ferrara, Pietro and Tripp, Omer and Pistoia, Marco}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Given a program whose functionality depends on access to certain external resources, we investigate the question of how to gracefully degrade functionality when a subset of those resources is unavailable. The concrete setting motivating this problem statement is mobile applications, which rely on contextual data (e.g., device identifiers, user location and contacts, etc.) to fulfill their functionality. In particular, we focus on the Android platform, which mediates access to resources via an installation-time permission model. On the one hand, granting an app the permission to access a resource (e.g., the device ID) entails privacy threats (e.g., releasing the device ID to advertising servers). On the other hand, denying access to a resource could render the app useless (e.g., if inability to read the device ID is treated as an error state). Our goal is to specialize an existing Android app in such a way that it is disabled from accessing certain sensitive resources (or contextual data) as specified by the user, while still being able to execute functionality that does not depend on those resources. We present ShamDroid, a program transformation algorithm, based on specialized forms of program slicing, backwards static analysis and constraint solving, that enables the use of Android apps with partial permissions. We rigorously state the guarantees provided by ShamDroid w.r.t. functionality maximization. We provide an evaluation over the top 500 Google Play apps and report on an extensive comparative evaluation of ShamDroid against three other state-of-the-art solutions (APM, XPrivacy, and Google App Ops) that mediate resource access at the system (rather than app) level. ShamDroid performs better than all of these tools by a significant margin, leading to abnormal behavior in only 1 out of 27 apps we manually investigated, compared to the other solutions, which cause crashes and abnormalities in 9 or more of the apps. This demonstrates the importance of performing app-sensitive mocking.}},
  url = {https://doi.org/10.1145/2814270.2814296},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1088,
  title = {Software data-triggered threads}},
  author = {Tseng, Hung-Wei and Tullsen, Dean Michael}},
  year = {2012}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The data-triggered threads (DTT) programming and execution model can increase parallelism and eliminate redundant computation. However, the initial proposal requires significant architecture support, which impedes existing applications and architectures from taking advantage of this model. This work proposes a pure software solution that supports the DTT model without any hardware support. This research uses a prototype compiler and runtime libraries running on top of existing machines. Several enhancements to the initial software implementation are presented, which further improve the performance.The software runtime system improves the performance of serial C SPEC benchmarks by 15\% on a Nehalem processor, but by over 7X over the full suite of single-thread applications. It is shown that the DTT model can work in conjunction with traditional parallelism. The DTT model provides up to 64X speedup over parallel applications exploiting traditional parallelism.}},
  url = {https://doi.org/10.1145/2384616.2384668},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1089,
  title = {Maximal causality reduction for TSO and PSO}},
  author = {Huang, Shiyou and Huang, Jeff}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Verifying concurrent programs is challenging due to the exponentially large thread interleaving space. The problem is exacerbated by relaxed memory models such as Total Store Order (TSO) and Partial Store Order (PSO) which further explode the interleaving space by reordering instructions. A recent advance, Maximal Causality Reduction (MCR), has shown great promise to improve verification effectiveness by maximally reducing redundant explorations. However, the original MCR only works for the Sequential Consistency (SC) memory model, but not for TSO and PSO. In this paper, we develop novel extensions to MCR by solving two key problems under TSO and PSO: 1) generating interleavings that can reach new states by encoding the operational semantics of TSO and PSO with first-order logical constraints and solving them with SMT solvers, and 2) enforcing TSO and PSO interleavings by developing novel replay algorithms that allow executions out of the program order. We show that our approach successfully enables MCR to effectively explore TSO and PSO interleavings. We have compared our approach with a recent Dynamic Partial Order Reduction (DPOR) algorithm for TSO and PSO and a SAT-based stateless model checking approach. Our results show that our approach is much more effective than the other approaches for both state-space exploration and bug finding – on average it explores 5-10X fewer executions and finds many bugs that the other tools cannot find.}},
  url = {https://doi.org/10.1145/2983990.2984025},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1090,
  title = {Constrained kinds}},
  author = {Tardieu, Olivier and Nystrom, Nathaniel and Peshansky, Igor and Saraswat, Vijay}},
  year = {2012}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Modern object-oriented languages such as X10 require a rich framework for types capable of expressing both value-dependency and genericity, and supporting pluggable, domain-specific extensions. In earlier work, we presented a framework for constrained types in object-oriented languages, parametrized by an underlying constraint system. Types are viewed as formulas C{c}},
  url = {https://doi.org/10.1145/2384616.2384675},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1091,
  title = {Hybrid STM/HTM for nested transactions on OpenJDK}},
  author = {Chapman, Keith and Hosking, Antony L. and Moss, J. Eliot B.}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Transactional memory (TM) has long been advocated as a promising pathway to more automated concurrency control for scaling concurrent programs running on parallel hardware. Software TM (STM) has the benefit of being able to run general transactional programs, but at the significant cost of overheads imposed to log memory accesses, mediate access conflicts, and maintain other transaction metadata. Recently, hardware manufacturers have begun to offer commodity hardware TM (HTM) support in their processors wherein the transaction metadata is maintained "for free" in hardware. However, HTM approaches are only best-effort: they cannot successfully run all transactional programs, whether because of hardware capacity issues (causing large transactions to fail), or compatibility restrictions on the processor instructions permitted within hardware transactions (causing transactions that execute those instructions to fail). In such cases, programs must include failure-handling code to attempt the computation by some other software means, since retrying the transaction would be futile. Thus, a canonical use of HTM is lock elision: replacing lock regions with transactions, retrying some number of times in the case of conflicts, but falling back to locking when HTM fails for other reasons. Here, we describe how software and hardware schemes can combine seamlessly into a hybrid system in support of transactional programs, allowing use of low-cost HTM when it works, but reverting to STM when it doesn't. We describe heuristics used to make this choice dynamically and automatically, but allowing the transition back to HTM opportunistically. Our implementation is for an extension of Java having syntax for both open and closed nested transactions, and boosting, running on the OpenJDK, with dynamic injection of STM mechanisms (into code variants used under STM) and HTM instructions (into code variants used under HTM). Both schemes are compatible to allow different threads to run concurrently with either mechanism, while preserving transaction safety. Using a standard synthetic benchmark we demonstrate that HTM offers significant acceleration of both closed and open nested transactions, while yielding parallel scaling up to the limits of the hardware, whereupon scaling in software continues but with the penalty to throughput imposed by software mechanisms.}},
  url = {https://doi.org/10.1145/2983990.2984029},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1092,
  title = {Typed self-interpretation by pattern matching}},
  author = {Jay, Barry and Palsberg, Jens}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Self-interpreters can be roughly divided into two sorts: self-recognisers that recover the input program from a canonical representation, and self-enactors that execute the input program. Major progress for statically-typed languages was achieved in 2009 by Rendel, Ostermann, and Hofer who presented the first typed self-recogniser that allows representations of different terms to have different types. A key feature of their type system is a type:type rule that renders the kind system of their language inconsistent.In this paper we present the first statically-typed language that not only allows representations of different terms to have different types, and supports a self-recogniser, but also supports a self-enactor. Our language is a factorisation calculus in the style of Jay and Given-Wilson, a combinatory calculus with a factorisation operator that is powerful enough to support the pattern-matching functions necessary for a self-interpreter. This allows us to avoid a type:type rule. Indeed, the types of System F are sufficient. We have implemented our approach and our experiments support the theory.}},
  url = {https://doi.org/10.1145/2034773.2034808},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1093,
  title = {Implementation issues for a source level symbolic debugger}},
  author = {Johnson, John D. and Kenney, Gary W.}},
  year = {1983}},
  journal = {Proceedings of the Symposium on High-Level Debugging}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper deals with issuces that have emerged as a result of a successful implementation of a source level symbolic debugger for HP-1000 computer systems. By analyzing a user's thought processes during a debugging session we created a powerful and easy to use tool for program analysis.}},
  url = {https://doi.org/10.1145/1006147.1006180},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1094,
  title = {kb-anonymity: a model for anonymized behaviour-preserving test and debugging data}},
  author = {Budi, Aditya and Lo, David and Jiang, Lingxiao and Lucia}},
  year = {2011}},
  journal = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {It is often very expensive and practically infeasible to generate test cases that can exercise all possible program states in a program. This is especially true for a medium or large industrial system. In practice, industrial clients of the system often have a set of input data collected either before the system is built or after the deployment of a previous version of the system. Such data are highly valuable as they represent the operations that matter in a client's daily business and may be used to extensively test the system. However, such data often carries sensitive information and cannot be released to third-party development houses. For example, a healthcare provider may have a set of patient records that are strictly confidential and cannot be used by any third party. Simply masking sensitive values alone may not be sufficient, as the correlation among fields in the data can reveal the masked information. Also, masked data may exhibit different behavior in the system and become less useful than the original data for testing and debugging.For the purpose of releasing private data for testing and debugging, this paper proposes the kb-anonymity model, which combines the k-anonymity model commonly used in the data mining and database areas with the concept of program behavior preservation. Like k-anonymity, kb-anonymity replaces some information in the original data to ensure privacy preservation so that the replaced data can be released to third-party developers. Unlike k-anonymity, kb-anonymity ensures that the replaced data exhibits the same kind of program behavior exhibited by the original data so that the replaced data may still be useful for the purposes of testing and debugging. We also provide a concrete version of the model under three particular configurations and have successfully applied our prototype implementation to three open source programs, demonstrating the utility and scalability of our prototype.}},
  url = {https://doi.org/10.1145/1993498.1993551},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1095,
  title = {Safe and efficient hybrid memory management for Java}},
  author = {Stancu, Codru\c{t}},
  year = {2015}},
  journal = {Proceedings of the 2015 International Symposium on Memory Management}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Java uses automatic memory management, usually implemented as a garbage-collected heap. That lifts the burden of manually allocating and deallocating memory, but it can incur significant runtime overhead and increase the memory footprint of applications. We propose a hybrid memory management scheme that utilizes region-based memory management to deallocate objects automatically on region exits. Static program analysis detects allocation sites that are safe for region allocation, i.e., the static analysis proves that the objects allocated at such a site are not reachable after the region exit. A regular garbage-collected heap is used for objects that are not region allocatable. The region allocation exploits the temporal locality of object allocation. Our analysis uses coarse-grain source code annotations to disambiguate objects with non-overlapping lifetimes, and maps them to different memory scopes. Region-allocated memory does not require garbage collection as the regions are simply deallocated when they go out of scope. The region allocation technique is backed by a garbage collector that manages memory that is not region allocated. We provide a detailed description of the analysis, provide experimental results showing that as much as 78\% of the memory is region allocatable and discuss how our hybrid memory management system can be implemented efficiently with respect to both space and time.}},
  url = {https://doi.org/10.1145/2754169.2754185},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1096,
  title = {Lightweight, flexible object-oriented generics}},
  author = {Zhang, Yizhou and Loring, Matthew C. and Salvaneschi, Guido and Liskov, Barbara and Myers, Andrew C.}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The support for generic programming in modern object-oriented programming languages is awkward and lacks desirable expressive power. We introduce an expressive genericity mechanism that adds expressive power and strengthens static checking, while remaining lightweight and simple in common use cases. Like type classes and concepts, the mechanism allows existing types to model type constraints retroactively. For expressive power, we expose models as named constructs that can be defined and selected explicitly to witness constraints; in common uses of genericity, however, types implicitly witness constraints without additional programmer effort. Models are integrated into the object-oriented style, with features like model generics, model-dependent types, model enrichment, model multimethods, constraint entailment, model inheritance, and existential quantification further extending expressive power in an object-oriented setting. We introduce the new genericity features and show that common generic programming idioms, including current generic libraries, can be expressed more precisely and concisely. The static semantics of the mechanism and a proof of a key decidability property can be found in an associated technical report.}},
  url = {https://doi.org/10.1145/2737924.2738008},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1097,
  title = {Scalable verification of border gateway protocol configurations with an SMT solver}},
  author = {Weitz, Konstantin and Woos, Doug and Torlak, Emina and Ernst, Michael D. and Krishnamurthy, Arvind and Tatlock, Zachary}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Internet Service Providers (ISPs) use the Border Gateway Protocol (BGP) to announce and exchange routes for de- livering packets through the internet. ISPs must carefully configure their BGP routers to ensure traffic is routed reli- ably and securely. Correctly configuring BGP routers has proven challenging in practice, and misconfiguration has led to worldwide outages and traffic hijacks. This paper presents Bagpipe, a system that enables ISPs to declaratively express BGP policies and that automatically verifies that router configurations implement such policies. The novel initial network reduction soundly reduces policy verification to a search for counterexamples in a finite space. An SMT-based symbolic execution engine performs this search efficiently. Bagpipe reduces the size of its search space using predicate abstraction and parallelizes its search using symbolic variable hoisting. Bagpipe's policy specification language is expressive: we expressed policies inferred from real AS configurations, policies from the literature, and policies for 10 Juniper TechLibrary configuration scenarios. Bagpipe is efficient: we ran it on three ASes with a total of over 240,000 lines of Cisco and Juniper BGP configuration. Bagpipe is effective: it revealed 19 policy violations without issuing any false positives.}},
  url = {https://doi.org/10.1145/2983990.2984012},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1098,
  title = {SPIRIT: a runtime system for distributed irregular tree applications}},
  author = {Hegde, Nikhil and Liu, Jianqiao and Kulkarni, Milind}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Repeated, depth-first traversal of trees is a common algorithmic pattern in an important set of applications from diverse domains such as cosmological simulations, data mining, and computer graphics. As these applications operate over massive data sets, it is often necessary to distribute the trees to process all of the data.In this work, we introduce SPIRIT, a runtime system to ease the writing of distributed tree applications. SPIRIT automates the challenging tasks of tree distribution, optimizing communication and parallelizing independent computations. The common algorithmic pattern in tree traversals is exploited to effectively schedule parallel computations and improve locality. As a result, pipeline parallelism in distributed traversals is identified, which is complemented by load-balancing, and locality-enhancing, message aggregation optimizations. Evaluation of SPIRIT on tree traversal in Point Correlation (PC) shows a scalable system, achieving speedups upto 38x on a 16-node, 64 process system compared to a 1-node, baseline configuration. We also find that SPIRIT results in substantially less communication and achieves significant performance improvements over implementations in other distributed graph systems.}},
  url = {https://doi.org/10.1145/2851141.2851177},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1099,
  title = {McErlang: a model checker for a distributed functional programming language}},
  author = {Fredlund, Lars-\r{A}},
  year = {2007}},
  journal = {Proceedings of the 12th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a model checker for verifying distributed programs written in the Erlang programming language. Providing a model checker for Erlang is especially rewarding since the language is by now being seen as a very capable platform for developing industrial strength distributed applications with excellent failure tolerance characteristics. In contrast to most other Erlang verification attempts, we provide support for a very substantial part of the language. The model checker has full Erlang data type support, support for general process communication, node semantics (inter-process behave subtly different from intra-process communication), fault detection and fault tolerance through process linking, and can verify programs written using the OTP Erlang component library (used by most modern Erlang programs).As the model checking tool is itself implemented in Erlang we benefit from the advantages that a (dynamically typed) functional programming language offers: easy prototyping and experimentation with new verification algorithms, rich executable models that use complex data structures directly programmed in Erlang, the ability to treat executable models interchangeably as programs (to be executed directly by the Erlang interpreter) and data, and not least the possibility to cleanly structure and to cleanly combine various verification sub-tasks. In the paper we discuss the design of the tool and provide early indications on its performance.}},
  url = {https://doi.org/10.1145/1291151.1291171},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1100,
  title = {Declarative programming over eventually consistent data stores}},
  author = {Sivaramakrishnan, KC and Kaki, Gowtham and Jagannathan, Suresh}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {User-facing online services utilize geo-distributed data stores to minimize latency and tolerate partial failures, with the intention of providing a fast, always-on experience. However, geo-distribution does not come for free; application developers have to contend with weak consistency behaviors, and the lack of abstractions to composably construct high-level replicated data types, necessitating the need for complex application logic and invariably exposing inconsistencies to the user. Some commercial distributed data stores and several academic proposals provide a lattice of consistency levels, with stronger consistency guarantees incurring increased latency and throughput costs. However, correctly assigning the right consistency level for an operation requires subtle reasoning and is often an error-prone task. In this paper, we present QUELEA, a declarative programming model for eventually consistent data stores (ECDS), equipped with a contract language, capable of specifying fine-grained application - level consistency properties. A contract enforcement system analyses contracts, and automatically generates the appropriate consistency protocol for the method protected by the contract. We describe an implementation of QUELEA on top of an off-the-shelf ECDS that provides support for coordination-free transactions. Several benchmarks including two large web applications, illustrate the effectiveness of our approach.}},
  url = {https://doi.org/10.1145/2737924.2737981},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1101,
  title = {Thread Data Sharing in Cache: Theory and Measurement}},
  author = {Luo, Hao and Li, Pengcheng and Ding, Chen}},
  year = {2017}},
  journal = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {On modern multi-core processors, independent workloads often interfere with each other by competing for shared cache space. However, for multi-threaded workloads, where a single copy of data can be accessed by multiple threads, the threads can cooperatively share cache. Because data sharing consolidates the collective working set of threads, the effective size of shared cache becomes larger than it would have been when data are not shared. This paper presents a new theory of data sharing. It includes (1) a new metric called the shared footprint to mathematically compute the amount of data shared by any group of threads in any size cache, and (2) a linear-time algorithm to measure shared footprint by scanning the memory trace of a multi-threaded program. The paper presents the practical implementation and evaluates the new theory using 14 PARSEC and SPEC OMP benchmarks, including an example use of shared footprint in program optimization.}},
  url = {https://doi.org/10.1145/3018743.3018759},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1102,
  title = {Extending sized type with collection analysis}},
  author = {Chin, Wei-Ngan and Khoo, Siau-Cheng and Xu, Dana N.}},
  year = {2003}},
  journal = {Proceedings of the 2003 ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Based Program Manipulation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many program optimizations and analyses, such as array-bounds checking, termination analysis, depend on knowing the size of a function's input and output. However, size information can be difficult to compute. Firstly, accurate size computation requires detecting a size relation between different inputs of a function. Secondly, size information may also be contained inside a collection (data structure with multiple elements). In this paper, we introduce some techniques to derive universal and existential size properties over collections of elements of recursive data structures. We shall show how a mixed constraint system could support the enhanced size type, and highlight examples where collection analysis are useful.}},
  url = {https://doi.org/10.1145/777388.777397},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1103,
  title = {The tasks with effects model for safe concurrency}},
  author = {Heumann, Stephen T. and Adve, Vikram S. and Wang, Shengjie}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Today's widely-used concurrent programming models either provide weak safety guarantees, making it easy to write code with subtle errors, or are limited in the class of programs that they can express. We propose a new concurrent programming model based on tasks with effects that offers strong safety guarantees while still providing the flexibility needed to support the many ways that concurrency is used in complex applications. The core unit of work in our model is a dynamically-created task. The model's key feature is that each task has programmer-specified effects, and a run-time scheduler is used to ensure that two tasks are run concurrently only if they have non-interfering effects. Through the combination of statically verifying the declared effects of tasks and using an effect-aware run-time scheduler, our model is able to guarantee strong safety properties, including data race freedom and atomicity. It is also possible to use our model to write programs and computations that can be statically proven to behave deterministically. We describe the tasks with effects programming model and provide a formal dynamic semantics for it. We also describe our implementation of this model in an extended version of Java and evaluate its use in several programs exhibiting various patterns of concurrency.}},
  url = {https://doi.org/10.1145/2442516.2442540},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1104,
  title = {A calculus of atomic actions}},
  author = {Elmas, Tayfun and Qadeer, Shaz and Tasiran, Serdar}},
  year = {2009}},
  journal = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a proof calculus and method for the static verification of assertions and procedure specifications in shared-memory concurrent programs. The key idea in our approach is to use atomicity as a proof tool and to simplify the verification of assertions by rewriting programs to consist of larger atomic actions. We propose a novel, iterative proof style in which alternating use of abstraction and reduction is exploited to compute larger atomic code blocks in a sound manner. This makes possible the verification of assertions in the transformed program by simple sequential reasoning within atomic blocks, or significantly simplified application of existing concurrent program verification techniques such as the Owicki-Gries or rely-guarantee methods. Our method facilitates a clean separation of concerns where at each phase of the proof, the user worries only about only either the sequential properties or the concurrency control mechanisms in the program. We implemented our method in a tool called QED. We demonstrate the simplicity and effectiveness of our approach on a number of benchmarks including ones with intricate concurrency protocols.}},
  url = {https://doi.org/10.1145/1480881.1480885},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1105,
  title = {Static analysis of interrupt-driven programs synchronized via the priority ceiling protocol}},
  author = {Schwarz, Martin D. and Seidl, Helmut and Vojdani, Vesal and Lammich, Peter and M\"{u}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We consider programs for embedded real-time systems which use priority-driven preemptive scheduling with task priorities adjusted dynamically according to the immediate ceiling priority protocol. For these programs, we provide static analyses for detecting data races between tasks running at different priorities as well as methods to guarantee transactional execution of procedures. Beyond that, we demonstrate how general techniques for value analyses can be adapted to this setting by developing a precise analysis of affine equalities.}},
  url = {https://doi.org/10.1145/1926385.1926398},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1106,
  title = {Adaptive LL(*) parsing: the power of dynamic analysis}},
  author = {Parr, Terence and Harwell, Sam and Fisher, Kathleen}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Despite the advances made by modern parsing strategies such as PEG, LL(*), GLR, and GLL, parsing is not a solved problem. Existing approaches suffer from a number of weaknesses, including difficulties supporting side-effecting embedded actions, slow and/or unpredictable performance, and counter-intuitive matching strategies. This paper introduces the ALL(*) parsing strategy that combines the simplicity, efficiency, and predictability of conventional top-down LL(k) parsers with the power of a GLR-like mechanism to make parsing decisions. The critical innovation is to move grammar analysis to parse-time, which lets ALL(*) handle any non-left-recursive context-free grammar. ALL(*) is O(n4) in theory but consistently performs linearly on grammars used in practice, outperforming general strategies such as GLL and GLR by orders of magnitude. ANTLR 4 generates ALL(*) parsers and supports direct left-recursion through grammar rewriting. Widespread ANTLR 4 use (5000 downloads/month in 2013) provides evidence that ALL(*) is effective for a wide variety of applications.}},
  url = {https://doi.org/10.1145/2660193.2660202},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1107,
  title = {Directed synthesis of failing concurrent executions}},
  author = {Samak, Malavika and Tripp, Omer and Ramanathan, Murali Krishna}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Detecting concurrency-induced bugs in multithreaded libraries can be challenging due to the intricacies associated with their manifestation. This includes invocation of multiple methods, synthesis of inputs to the methods to reach the failing location, and crafting of thread interleavings that cause the erroneous behavior. Neither fuzzing-based testing techniques nor over-approximate static analyses are well positioned to detect such subtle defects while retaining high accuracy alongside satisfactory coverage. In this paper, we propose a directed, iterative and scalable testing engine that combines the strengths of static and dynamic analysis to help synthesize concurrent executions to expose complex concurrency-induced bugs. Our engine accepts as input the library, its client (either sequential or concurrent) and a specification of correctness. Then, it iteratively refines the client to generate an execution that can break the input specification. Each step of the iterative process includes statically identifying sub-goals towards the goal of failing the specification, generating a plan toward meeting these goals, and merging of the paths traversed dynamically with the plan computed statically via constraint solving to generate a new client. The engine reports full reproduction scenarios, guaranteed to be true, for the bugs it finds. We have created a prototype of our approach named MINION. We validated MINION by applying it to well-tested concurrent classes from popular Java libraries, including the latest versions of openjdk and google-guava. We were able to detect 31 real crashes across 10 classes in a total of 23 minutes, including previously unknown bugs. Comparison with three other tools reveals that combined, they report only 9 of the 31 crashes (and no other crashes beyond MINION). This is because several of these bugs manifest under deeply nested path conditions (observed maximum of 11), deep nesting of method invocations (observed maximum of 6) and multiple refinement iterations to generate the crash-inducing client.}},
  url = {https://doi.org/10.1145/2983990.2984040},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1108,
  title = {Concurrent NetCore: from policies to pipelines}},
  author = {Schlesinger, Cole and Greenberg, Michael and Walker, David}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In a Software-Defined Network (SDN), a central, computationally powerful controller manages a set of distributed, computationally simple switches. The controller computes a policy describing how each switch should route packets and populates packet-processing tables on each switch with rules to enact the routing policy. As network conditions change, the controller continues to add and remove rules from switches to adjust the policy as needed.Recently, the SDN landscape has begun to change as several proposals for new, reconfigurable switching architectures, such as RMT [5] and FlexPipe [14] have emerged. These platforms provide switch programmers with many, flexible tables for storing packet-processing rules, and they offer programmers control over the packet fields that each table can analyze and act on. These reconfigurable switch architectures support a richer SDN model in which a switch configuration phase precedes the rule population phase [4]. In the configuration phase, the controller sends the switch a graph describing the layout and capabilities of the packet processing tables it will require during the population phase. Armed with this foreknowledge, the switch can allocate its hardware (or software) resources more efficiently.We present a new, typed language, called Concurrent NetCore, for specifying routing policies and graphs of packet-processing tables. Concurrent NetCore includes features for specifying sequential, conditional and concurrent control-flow between packet-processing tables. We develop a fine-grained operational model for the language and prove this model coincides with a higher-level denotational model when programs are well-typed. We also prove several additional properties of well-typed programs, including strong normalization and determinism. To illustrate the utility of the language, we develop linguistic models of both the RMT and FlexPipe architectures and we give a multi-pass compilation algorithm that translates graphs and routing policies to the RMT model.}},
  url = {https://doi.org/10.1145/2628136.2628157},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1109,
  title = {Strictly declarative specification of sophisticated points-to analyses}},
  author = {Bravenboer, Martin and Smaragdakis, Yannis}},
  year = {2009}},
  journal = {Proceedings of the 24th ACM SIGPLAN Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present the DOOP framework for points-to analysis of Java programs. DOOP builds on the idea of specifying pointer analysis algorithms declaratively, using Datalog: a logic-based language for defining (recursive) relations. We carry the declarative approach further than past work by describing the full end-to-end analysis in Datalog and optimizing aggressively using a novel technique specifically targeting highly recursive Datalog programs.As a result, DOOP achieves several benefits, including full order-of-magnitude improvements in runtime. We compare DOOP with Lhotak and Hendren's PADDLE, which defines the state of the art for context-sensitive analyses. For the exact same logical points-to definitions (and, consequently, identical precision) DOOP is more than 15x faster than PADDLE for a 1-call-site sensitive analysis of the DaCapo benchmarks, with lower but still substantial speedups for other important analyses. Additionally, DOOP scales to very precise analyses that are impossible with PADDLE and Whaley et al.'s bddbddb, directly addressing open problems in past literature. Finally, our implementation is modular and can be easily configured to analyses with a wide range of characteristics, largely due to its declarativeness.}},
  url = {https://doi.org/10.1145/1640089.1640108},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1110,
  title = {Automatic detection of floating-point exceptions}},
  author = {Barr, Earl T. and Vo, Thanh and Le, Vu and Su, Zhendong}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {It is well-known that floating-point exceptions can be disastrous and writing exception-free numerical programs is very difficult. Thus, it is important to automatically detect such errors. In this paper, we present Ariadne, a practical symbolic execution system specifically designed and implemented for detecting floating-point exceptions. Ariadne systematically transforms a numerical program to explicitly check each exception triggering condition. Ariadne symbolically executes the transformed program using real arithmetic to find candidate real-valued inputs that can reach and trigger an exception. Ariadne converts each candidate input into a floating-point number, then tests it against the original program. In general, approximating floating-point arithmetic with real arithmetic can change paths from feasible to infeasible and vice versa. The key insight of this work is that, for the problem of detecting floating-point exceptions, this approximation works well in practice because, if one input reaches an exception, many are likely to, and at least one of them will do so over both floating-point and real arithmetic. To realize Ariadne, we also devised a novel, practical linearization technique to solve nonlinear constraints. We extensively evaluated Ariadne over 467 scalar functions in the widely used GNU Scientific Library (GSL). Our results show that Ariadne is practical and identifies a large number of real runtime exceptions in GSL. The GSL developers confirmed our preliminary findings and look forward to Ariadne's public release, which we plan to do in the near future.}},
  url = {https://doi.org/10.1145/2429069.2429133},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1111,
  title = {What can the GC compute efficiently? a language for heap assertions at GC time}},
  author = {Reichenbach, Christoph and Immerman, Neil and Smaragdakis, Yannis and Aftandilian, Edward E. and Guyer, Samuel Z.}},
  year = {2010}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present the DeAL language for heap assertions that are efficiently evaluated during garbage collection time. DeAL is a rich, declarative, logic-based language whose programs are guaranteed to be executable with good whole-heap locality, i.e., within a single traversal over every live object on the heap and a finite neighborhood around each object. As a result, evaluating DeAL programs incurs negligible cost: for simple assertion checking at each garbage collection, the end-to-end execution slowdown is below 2\%. DeAL is integrated into Java as a VM extension and we demonstrate its efficiency and expressiveness with several applications and properties from the past literature.Compared to past systems for heap assertions, DeAL is distinguished by its very attractive expressiveness/efficiency tradeoff: it o ers a significantly richer class of assertions than what past systems could check with a single traversal. Conversely, past systems that can express the same (or more) complex assertions as DeAL do so only by su ering orders-of-magnitude higher costs.}},
  url = {https://doi.org/10.1145/1869459.1869482},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1112,
  title = {Finding minimum type error sources}},
  author = {Pavlinovic, Zvonimir and King, Tim and Wies, Thomas}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Automatic type inference is a popular feature of functional programming languages. If a program cannot be typed, the compiler typically reports a single program location in its error message. This location is the point where the type inference failed, but not necessarily the actual source of the error. Other potential error sources are not even considered. Hence, the compiler often misses the true error source, which increases debugging time for the programmer. In this paper, we present a general framework for automatic localization of type errors. Our algorithm finds all minimum error sources, where the exact definition of minimum is given in terms of a compiler-specific ranking criterion. Compilers can use minimum error sources to produce more meaningful error reports, and for automatic error correction. Our approach works by reducing the search for minimum error sources to an optimization problem that we formulate in terms of weighted maximum satisfiability modulo theories (MaxSMT). The reduction to weighted MaxSMT allows us to build on SMT solvers to support rich type systems and at the same time abstract from the concrete criterion that is used for ranking the error sources. We have implemented an instance of our framework targeted at Hindley-Milner type systems and evaluated it on existing OCaml benchmarks for type error localization. Our evaluation shows that our approach has the potential to significantly improve the quality of type error reports produced by state of the art compilers.}},
  url = {https://doi.org/10.1145/2660193.2660230},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1113,
  title = {Fault tolerance via idempotence}},
  author = {Ramalingam, Ganesan and Vaswani, Kapil}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Building distributed services and applications is challenging due to the pitfalls of distribution such as process and communication failures. A natural solution to these problems is to detect potential failures, and retry the failed computation and/or resend messages. Ensuring correctness in such an environment requires distributed services and applications to be idempotent.In this paper, we study the inter-related aspects of process failures, duplicate messages, and idempotence. We first introduce a simple core language (based on lambda calculus inspired by modern distributed computing platforms. This language formalizes the notions of a service, duplicate requests, process failures, data partitioning, and local atomic transactions that are restricted to a single store.We then formalize a desired (generic) correctness criterion for applications written in this language, consisting of idempotence (which captures the desired safety properties) and failure-freedom (which captures the desired progress properties).We then propose language support in the form of a monad that automatically ensures failfree idempotence. A key characteristic of our implementation is that it is decentralized and does not require distributed coordination. We show that the language support can be enriched with other useful constructs, such as compensations, while retaining the coordination-free decentralized nature of the implementation.We have implemented the idempotence monad (and its variants) in F# and C# and used our implementation to build realistic applications on Windows Azure. We find that the monad has low runtime overheads and leads to more declarative applications.}},
  url = {https://doi.org/10.1145/2429069.2429100},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1114,
  title = {From Hilbert space to Dilbert space: context semantics as a language for games and flow analysis}},
  author = {Mairson, Harry G.}},
  year = {2003}},
  journal = {Proceedings of the Eighth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We give a tutorial and first-principles description of the context semantics of Gonthier, Abadi, and L\'{e}},
  url = {https://doi.org/10.1145/944705.944717},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1115,
  title = {Deadlock-freedom-by-design: multiparty asynchronous global programming}},
  author = {Carbone, Marco and Montesi, Fabrizio}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Over the last decade, global descriptions have been successfully employed for the verification and implementation of communicating systems, respectively as protocol specifications and choreographies. In this work, we bring these two practices together by proposing a purely-global programming model. We show a novel interpretation of asynchrony and parallelism in a global setting and develop a typing discipline that verifies choreographies against protocol specifications, based on multiparty sessions. Exploiting the nature of global descriptions, our type system defines a new class of deadlock-free concurrent systems (deadlock-freedom-by-design), provides type inference, and supports session mobility. We give a notion of Endpoint Projection (EPP) which generates correct entity code (as pi-calculus terms) from a choreography. Finally, we evaluate our approach by providing a prototype implementation for a concrete programming language and by applying it to some examples from multicore and service-oriented programming.}},
  url = {https://doi.org/10.1145/2429069.2429101},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1116,
  title = {Pushdown control-flow analysis for free}},
  author = {Gilray, Thomas and Lyde, Steven and Adams, Michael D. and Might, Matthew and Van Horn, David}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Traditional control-flow analysis (CFA) for higher-order languages introduces spurious connections between callers and callees, and different invocations of a function may pollute each other's return flows. Recently, three distinct approaches have been published that provide perfect call-stack precision in a computable manner: CFA2, PDCFA, and AAC. Unfortunately, implementing CFA2 and PDCFA requires significant engineering effort. Furthermore, all three are computationally expensive. For a monovariant analysis, CFA2 is in O(2^n), PDCFA is in O(n^6), and AAC is in O(n^8). In this paper, we describe a new technique that builds on these but is both straightforward to implement and computationally inexpensive. The crucial insight is an unusual state-dependent allocation strategy for the addresses of continuations. Our technique imposes only a constant-factor overhead on the underlying analysis and costs only O(n^3) in the monovariant case. We present the intuitions behind this development, benchmarks demonstrating its efficacy, and a proof of the precision of this analysis.}},
  url = {https://doi.org/10.1145/2837614.2837631},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1117,
  title = {Optimal inference of fields in row-polymorphic records}},
  author = {Simon, Axel}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Flexible records are a powerful concept in type systems that form the basis of, for instance, objects in dynamically typed languages. One caveat of using flexible records is that a program may try to access a record field that does not exist. We present a type inference algorithm that checks for these runtime errors. The novelty of our algorithm is that it satisfies a clear notion of completeness: The inferred types are optimal in the sense that type annotations cannot increase the set of typeable programs. Under certain assumptions, our algorithm guarantees the following stronger property: it rejects a program if and only if it contains a path from an empty record to a field access on which the field has not been added. We derive this optimal algorithm by abstracting a semantics to types. The derived inference rules use a novel combination of type terms and Boolean functions that retains the simplicity of unification-based type inference but adds the ability of Boolean functions to express implications, thereby addressing the challenge of combining implications and types. By following our derivation method, we show how various operations such as record concatenation and branching if a field exists lead to Boolean satisfiability problems of different complexity. Analogously, we show that more expressive type systems give rise to SMT problems. On the practical side, we present an implementation of the select and update operations and give practical evidence that these are sufficient in real-world applications.}},
  url = {https://doi.org/10.1145/2594291.2594313},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1118,
  title = {A scheduling framework for general-purpose parallel languages}},
  author = {Fluet, Matthew and Rainey, Mike and Reppy, John}},
  year = {2008}},
  journal = {Proceedings of the 13th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The trend in microprocessor design toward multicore and manycore processors means that future performance gains in software will largely come from harnessing parallelism. To realize such gains, we need languages and implementations that can enable parallelism at many different levels. For example, an application might use both explicit threads to implement course-grain parallelism for independent tasks and implicit threads for fine-grain data-parallel computation over a large array. An important aspect of this requirement is supporting a wide range of different scheduling mechanisms for parallel computation.In this paper, we describe the scheduling framework that we have designed and implemented for Manticore, a strict parallel functional language. We take a micro-kernel approach in our design: the compiler and runtime support a small collection of scheduling primitives upon which complex scheduling policies can be implemented. This framework is extremely flexible and can support a wide range of different scheduling policies. It also supports the nesting of schedulers, which is key to both supporting multiple scheduling policies in the same application and to hierarchies of speculative parallel computations.In addition to describing our framework, we also illustrate its expressiveness with several popular scheduling techniques. We present a (mostly) modular approach to extending our schedulers to support cancellation. This mechanism is essential for implementing eager and speculative parallelism. We finally evaluate our framework with a series of benchmarks and an analysis.}},
  url = {https://doi.org/10.1145/1411204.1411239},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1119,
  title = {Parallel programming with big operators}},
  author = {Park, Changhee and Steele, Guy L. and Tristan, Jean-Baptiste}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In the sciences, it is common to use the so-called "big operator" notation to express the iteration of a binary operator (the reducer) over a collection of values. Such a notation typically assumes that the reducer is associative and abstracts the iteration process. Consequently, from a programming point-of-view, we can organize the reducer operations to minimize the depth of the overall reduction, allowing a potentially parallel evaluation of a big operator expression. We believe that the big operator notation is indeed an effective construct to express parallel computations in the Generate/Map/Reduce programming model, and our goal is to introduce it in programming languages to support parallel programming. The effective definition of such a big operator expression requires a simple way to generate elements, and a simple way to declare algebraic properties of the reducer (such as its identity, or its commutativity). In this poster, we want to present an extension of Scala with support for big operator expressions. We show how big operator expressions are defined and how the API is organized to support the simple definition of reducers with their algebraic properties.}},
  url = {https://doi.org/10.1145/2442516.2442551},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1120,
  title = {Program verification as probabilistic inference}},
  author = {Gulwani, Sumit and Jojic, Nebojsa}},
  year = {2007}},
  journal = {Proceedings of the 34th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this paper, we propose a new algorithm for proving the validity or invalidity of a pre/postcondition pair for a program. The algorithm is motivated by the success of the algorithms for probabilistic inference developed in the machine learning community for reasoning in graphical models. The validity or invalidity proof consists of providing an invariant at each program point that can be locally verified. The algorithm works by iteratively randomly selecting a program point and updating the current abstract state representation to make it more locally consistent (with respect to the abstractions at the neighboring points). We show that this simple algorithm has some interesting aspects: (a) It brings together the complementary powers of forward and backward analyses; (b) The algorithm has the ability to recover itself from excessive under-approximation or over-approximation that it may make. (Because the algorithm does not distinguish between the forward and backward information, the information could get both under-approximated and over-approximated at any step.) (c) The randomness in the algorithm ensures that the correct choice of updates is eventually made as there is no single deterministic strategy that would provably work for any interesting class of programs. In our experiments we use this algorithm to produce the proof of correctness of a small (but non-trivial) example. In addition, we empirically illustrate several important properties of the algorithm.}},
  url = {https://doi.org/10.1145/1190216.1190258},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1121,
  title = {Biorthogonality, step-indexing and compiler correctness}},
  author = {Benton, Nick and Hur, Chung-Kil}},
  year = {2009}},
  journal = {Proceedings of the 14th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We define logical relations between the denotational semantics of a simply typed functional language with recursion and the operational behaviour of low-level programs in a variant SECD machine. The relations, which are defined using biorthogonality and stepindexing, capture what it means for a piece of low-level code to implement a mathematical, domain-theoretic function and are used to prove correctness of a simple compiler. The results have been formalized in the Coq proof assistant.}},
  url = {https://doi.org/10.1145/1596550.1596567},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1122,
  title = {The Push/Pull model of transactions}},
  author = {Koskinen, Eric and Parkinson, Matthew}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a general theory of serializability, unifying a wide range of transactional algorithms, including some that are yet to come. To this end, we provide a compact semantics in which concurrent transactions PUSH their effects into the shared view (or UNPUSH to recall effects) and PULL the effects of potentially uncommitted concurrent transactions into their local view (or UNPULL to detangle). Each operation comes with simple criteria given in terms of commutativity (Lipton's left-movers and right-movers). The benefit of this model is that most of the elaborate reasoning (coinduction, simulation, subtle invariants, etc.) necessary for proving the serializability of a transactional algorithm is already proved within the semantic model. Thus, proving serializability (or opacity) amounts simply to mapping the algorithm on to our rules, and showing that it satisfies the rules' criteria.}},
  url = {https://doi.org/10.1145/2737924.2737995},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1123,
  title = {Efficient subcubic alias analysis for C}},
  author = {Zhang, Qirun and Xiao, Xiao and Zhang, Charles and Yuan, Hao and Su, Zhendong}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Inclusion-based alias analysis for C can be formulated as a context-free language (CFL) reachability problem. It is well known that the traditional cubic CFL-reachability algorithm does not scale well in practice. We present a highly scalable and efficient CFL-reachability-based alias analysis for C. The key novelty of our algorithm is to propagate reachability information along only original graph edges and bypass a large portion of summary edges, while the traditional CFL-reachability algorithm propagates along all summary edges. We also utilize the Four Russians' Trick - a key enabling technique in the subcubic CFL-reachability algorithm - in our alias analysis. We have implemented our subcubic alias analysis and conducted extensive experiments on widely-used C programs from the pointer analysis literature. The results demonstrate that our alias analysis scales extremely well in practice. In particular, it can analyze the recent Linux kernel (which consists of 10M SLOC) in about 30 seconds.}},
  url = {https://doi.org/10.1145/2660193.2660213},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1124,
  title = {Combining concern input with program analysis for bloat detection}},
  author = {Bhattacharya, Suparna and Gopinath, Kanchi and Nanda, Mangala Gowri}},
  year = {2013}},
  journal = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Framework based software tends to get bloated by accumulating optional features (or concerns) just-in-case they are needed. The good news is that such feature bloat need not always cause runtime execution bloat. The bad news is that often enough, only a few statements from an optional concern may cause execution bloat that may result in as much as 50\% runtime overhead.We present a novel technique to analyze the connection between optional concerns and the potential sources of execution bloat induced by them. Our analysis automatically answers questions such as (1) whether a given set of optional concerns could lead to execution bloat and (2) which particular statements are the likely sources of bloat when those concerns are not required. The technique combines coarse grain concern input from an external source with a fine-grained static analysis. Our experimental evaluation highlights the effectiveness of such concern augmented program analysis in execution bloat assessment of ten programs.}},
  url = {https://doi.org/10.1145/2509136.2509522},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1125,
  title = {Composable thread coloring}},
  author = {Sutherland, Dean F. and Scherlis, William L.}},
  year = {2010}},
  journal = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper introduces the language-independent concept of ``thread usage policy.'' Many multi-threaded software systems contain policies that regulate associations among threads, executable code, and potentially shared state. A system, for example, may constrain which threads are permitted to execute particular code segments, usually as a means to constrain those threads from accessing or writing particular elements of state. These policies ensure properties such as state confinement or reader/writer constraints, often without recourse to locking or transaction discipline.Our approach allows developers to concisely document their thread usage policies in a manner that enables the use of sound scalable analysis to assess consistency of policy and as-written code. This paper identifies the key semantic concepts of our thread coloring language and illustrates how to use its succinct source-level annotations to express models of thread usage policies, following established annotation conventions for Java.We have built a prototype static analysis tool, implemented as an integrated development environment plug-in (for the Eclipse IDE), that notifies developers of discrepancies between policy annotations and as-written code. Our analysis technique uses several underlying algorithms based on abstract interpretation, call-graphs, and type inference. The resulting overall analysis is both sound and composable. We have used this prototype analysis tool in case studies to model and analyze more than a million lines of code.Our validation process included field trials on a wide variety of complex large-scale production code selected by the host organizations. Our in-field experience led us to focus on potential adoptability by real-world developers. We have developed techniques that can reduce annotation density to less than one line per thousand lines of code (KLOC). In addition, the prototype analysis tool supports an incremental and iterative approach to modeling and analysis. This approach enabled field trial partners to directly target areas of greatest concern and to achieve useful results within a few hours.}},
  url = {https://doi.org/10.1145/1693453.1693485},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1126,
  title = {DCatch: Automatically Detecting Distributed Concurrency Bugs in Cloud Systems}},
  author = {Liu, Haopeng and Li, Guangpu and Lukman, Jeffrey F. and Li, Jiaxin and Lu, Shan and Gunawi, Haryadi S. and Tian, Chen}},
  year = {2017}},
  journal = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In big data and cloud computing era, reliability of distributed systems is extremely important. Unfortunately, distributed concurrency bugs, referred to as DCbugs, widely exist. They hide in the large state space of distributed cloud systems and manifest non-deterministically depending on the timing of distributed computation and communication. Effective techniques to detect DCbugs are desired. This paper presents a pilot solution, DCatch, in the world of DCbug detection. DCatch predicts DCbugs by analyzing correct execution of distributed systems. To build DCatch, we design a set of happens-before rules that model a wide variety of communication and concurrency mechanisms in real-world distributed cloud systems. We then build runtime tracing and trace analysis tools to effectively identify concurrent conflicting memory accesses in these systems. Finally, we design tools to help prune false positives and trigger DCbugs. We have evaluated DCatch on four representative open-source distributed cloud systems, Cassandra, Hadoop MapReduce, HBase, and ZooKeeper. By monitoring correct execution of seven workloads on these systems, DCatch reports 32 DCbugs, with 20 of them being truly harmful.}},
  url = {https://doi.org/10.1145/3037697.3037735},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1127,
  title = {A simpler, safer programming and execution model for intermittent systems}},
  author = {Lucia, Brandon and Ransford, Benjamin}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Energy harvesting enables novel devices and applications without batteries, but intermittent operation under energy harvesting poses new challenges to memory consistency that threaten to leave applications in failed states not reachable in continuous execution. This paper presents analytical models that aid in reasoning about intermittence. Using these, we develop DINO (Death Is Not an Option), a programming and execution model that simplifies programming for intermittent systems and ensures volatile and nonvolatile data consistency despite near-constant interruptions. DINO is the first system to address these consistency problems in the context of intermittent execution. We evaluate DINO on three energy-harvesting hardware platforms running different applications. The applications fail and exhibit error without DINO, but run correctly with DINO’s modest 1.8–2.7\texttimes{}},
  url = {https://doi.org/10.1145/2737924.2737978},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1128,
  title = {Rehearsal: a configuration verification tool for puppet}},
  author = {Shambaugh, Rian and Weiss, Aaron and Guha, Arjun}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Large-scale data centers and cloud computing have turned system configuration into a challenging problem. Several widely-publicized outages have been blamed not on software bugs, but on configuration bugs. To cope, thousands of organizations use system configuration languages to manage their computing infrastructure. Of these, Puppet is the most widely used with thousands of paying customers and many more open-source users. The heart of Puppet is a domain-specific language that describes the state of a system. Puppet already performs some basic static checks, but they only prevent a narrow range of errors. Furthermore, testing is ineffective because many errors are only triggered under specific machine states that are difficult to predict and reproduce. With several examples, we show that a key problem with Puppet is that configurations can be non-deterministic. This paper presents Rehearsal, a verification tool for Puppet configurations. Rehearsal implements a sound, complete, and scalable determinacy analysis for Puppet. To develop it, we (1) present a formal semantics for Puppet, (2) use several analyses to shrink our models to a tractable size, and (3) frame determinism-checking as decidable formulas for an SMT solver. Rehearsal then leverages the determinacy analysis to check other important properties, such as idempotency. Finally, we apply Rehearsal to several real-world Puppet configurations.}},
  url = {https://doi.org/10.1145/2908080.2908083},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1129,
  title = {Symbolic execution of multithreaded programs from arbitrary program contexts}},
  author = {Bergan, Tom and Grossman, Dan and Ceze, Luis}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We describe an algorithm to perform symbolic execution of a multithreaded program starting from an arbitrary program context. We argue that this can enable more efficient symbolic exploration of deep code paths in multithreaded programs by allowing the symbolic engine to jump directly to program contexts of interest.The key challenge is modeling the initial context with reasonable precision - an overly approximate model leads to exploration of many infeasible paths during symbolic execution, while a very precise model would be so expensive to compute that computing it would defeat the purpose of jumping directly to the initial context in the first place. We propose a context-specific dataflow analysis that approximates the initial context cheaply, but precisely enough to avoid some common causes of infeasible-path explosion. This model is necessarily approximate - it may leave portions of the memory state unconstrained, leaving our symbolic execution unable to answer simple questions such as "which thread holds lock A?". For such cases, we describe a novel algorithm for evaluating symbolic synchronization during symbolic execution. Our symbolic execution semantics are sound and complete up to the limits of the underlying SMT solver. We describe initial experiments on an implementation in Cloud 9.}},
  url = {https://doi.org/10.1145/2660193.2660200},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1130,
  title = {Generic discrimination: sorting and paritioning unshared data in linear time}},
  author = {Henglein, Fritz}},
  year = {2008}},
  journal = {Proceedings of the 13th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We introduce the notion of discrimination as a generalization of both sorting and partitioning and show that worst-case linear-time discrimination functions (discriminators) can be defined generically, by (co-)induction on an expressive language of order denotations. The generic definition yields discriminators that generalize both distributive sorting and multiset discrimination. The generic discriminator can be coded compactly using list comprehensions, with order denotations specified using Generalized Algebraic Data Types (GADTs). A GADT-free combinator formulation of discriminators is also given.We give some examples of the uses of discriminators, including a new most-significant-digit lexicographic sorting algorithm.Discriminators generalize binary comparison functions: They operate on n arguments at a time, but do not expose more information than the underlying equivalence, respectively ordering relation on the arguments. We argue that primitive types with equality (such as references in ML) and ordered types (such as the machine integer type), should expose their equality, respectively standard ordering relation, as discriminators: Having only a binary equality test on a type requires Θ(n2) time to find all the occurrences of an element in a list of length n, for each element in the list, even if the equality test takes only constant time. A discriminator accomplishes this in linear time. Likewise, having only a (constant-time) comparison function requires Θ(n log n) time to sort a list of n elements. A discriminator can do this in linear time.}},
  url = {https://doi.org/10.1145/1411204.1411220},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1131,
  title = {A formally verified compiler for Lustre}},
  author = {Bourke, Timothy and Brun, L\'{e}},
  year = {2017}},
  journal = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The correct compilation of block diagram languages like Lustre, Scade, and a discrete subset of Simulink is important since they are used to program critical embedded control software. We describe the specification and verification in an Interactive Theorem Prover of a compilation chain that treats the key aspects of Lustre: sampling, nodes, and delays. Building on CompCert, we show that repeated execution of the generated assembly code faithfully implements the dataflow semantics of source programs. We resolve two key technical challenges. The first is the change from a synchronous dataflow semantics, where programs manipulate streams of values, to an imperative one, where computations manipulate memory sequentially. The second is the verified compilation of an imperative language with encapsulated state to C code where the state is realized by nested records. We also treat a standard control optimization that eliminates unnecessary conditional statements.}},
  url = {https://doi.org/10.1145/3062341.3062358},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1132,
  title = {Stratified synthesis: automatically learning the x86-64 instruction set}},
  author = {Heule, Stefan and Schkufza, Eric and Sharma, Rahul and Aiken, Alex}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The x86-64 ISA sits at the bottom of the software stack of most desktop and server software. Because of its importance, many software analysis and verification tools depend, either explicitly or implicitly, on correct modeling of the semantics of x86-64 instructions. However, formal semantics for the x86-64 ISA are difficult to obtain and often written manually through great effort. We describe an automatically synthesized formal semantics of the input/output behavior for a large fraction of the x86-64 Haswell ISA’s many thousands of instruction variants. The key to our results is stratified synthesis, where we use a set of instructions whose semantics are known to synthesize the semantics of additional instructions whose semantics are unknown. As the set of formally described instructions increases, the synthesis vocabulary expands, making it possible to synthesize the semantics of increasingly complex instructions. Using this technique we automatically synthesized formal semantics for 1,795 instruction variants of the x86-64 Haswell ISA. We evaluate the learned semantics against manually written semantics (where available) and find that they are formally equivalent with the exception of 50 instructions, where the manually written semantics contain an error. We further find the learned formulas to be largely as precise as manually written ones and of similar size.}},
  url = {https://doi.org/10.1145/2908080.2908121},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1133,
  title = {Test-driven repair of data races in structured parallel programs}},
  author = {Surendran, Rishi and Raman, Raghavan and Chaudhuri, Swarat and Mellor-Crummey, John and Sarkar, Vivek}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A common workflow for developing parallel software is as follows: 1) start with a sequential program, 2) identify subcomputations that should be converted to parallel tasks, 3) insert synchronization to achieve the same semantics as the sequential program, and repeat steps 2) and 3) as needed to improve performance. Though this is not the only approach to developing parallel software, it is sufficiently common to warrant special attention as parallel programming becomes ubiquitous. This paper focuses on automating step 3), which is usually the hardest step for developers who lack expertise in parallel programming.Past solutions to the problem of repairing parallel programs have used static-only or dynamic-only approaches, both of which incur significant limitations in practice. Static approaches can guarantee soundness in many cases but are limited in precision when analyzing medium or large-scale software with accesses to pointer-based data structures in multiple procedures. Dynamic approaches are more precise, but their proposed repairs are limited to a single input and are not reflected back in the original source program. In this paper, we introduce a hybrid static+dynamic test-driven approach to repairing data races in structured parallel programs. Our approach includes a novel coupling between static and dynamic analyses. First, we execute the program on a concrete test input and determine the set of data races for this input dynamically. Next, we compute a set of "finish" placements that prevent these races and also respects the static scoping rules of the program while maximizing parallelism. Empirical results on standard benchmarks and student homework submissions from a parallel computing course establish the effectiveness of our approach with respect to compile-time overhead, precision, and performance of the repaired code.}},
  url = {https://doi.org/10.1145/2594291.2594335},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1134,
  title = {Environment classifiers}},
  author = {Taha, Walid and Nielsen, Michael Florentin}},
  year = {2003}},
  journal = {Proceedings of the 30th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper proposes and develops the basic theory for a new approach to typing multi-stage languages based a notion of environment classifiers. This approach involves explicit but lightweight tracking -- at type-checking time -- of the origination environment for future-stage computations. Classification is less restrictive than the previously proposed notions of closedness, and allows for both a more expressive typing of the "run" construct and for a unifying account of typed multi-stage programmin.The proposed approach to typing requires making cross-stage persistence (CSP) explicit in the language. At the same time, it offers concrete new insights into the notion of levels and in turn into CSP itself. Type safety is established in the simply-typed setting. As a first step toward introducing classifiers to the Hindley-Milner setting, we propose an approach to integrating the two, and prove type preservation in this setting.}},
  url = {https://doi.org/10.1145/604131.604134},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1135,
  title = {Partial method compilation using dynamic profile information}},
  author = {Whaley, John}},
  year = {2001}},
  journal = {Proceedings of the 16th ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The traditional tradeoff when performing dynamic compilation is that of fast compilation time versus fast code performance. Most dynamic compilation systems for Java perform selective compilation and/or optimization at a method granularity. This is the not the optimal granularity level. However, compiling at a sub-method granularity is thought to be too complicated to be practical. This paper describes a straightforward technique for performing compilation and optimizations at a finer, sub-method granularity. We utilize dynamic profile data to determine intra-method code regions that are rarely or never executed, and compile and optimize the code without those regions. If a branch that was predicted to be rare is actually taken at run time, we fall back to the interpreter or dynamically compile another version of the code. By avoiding compiling and optimizing code that is rarely executed, we are able to decrease compile time significantly, with little to no degradation in performance. Futhermore, ignoring rarely-executed code can open up more optimization opportunities on the common paths. We present two optimizations---partial dead code elimination and rare-path-sensitive pointer and escape analysis---that take advantage of rare path information. Using these optimizations, our technique is able to improve performance beyond the compile time improvements}},
  url = {https://doi.org/10.1145/504282.504295},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1136,
  title = {On abstraction refinement for program analyses in Datalog}},
  author = {Zhang, Xin and Mangal, Ravi and Grigore, Radu and Naik, Mayur and Yang, Hongseok}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A central task for a program analysis concerns how to efficiently find a program abstraction that keeps only information relevant for proving properties of interest. We present a new approach for finding such abstractions for program analyses written in Datalog. Our approach is based on counterexample-guided abstraction refinement: when a Datalog analysis run fails using an abstraction, it seeks to generalize the cause of the failure to other abstractions, and pick a new abstraction that avoids a similar failure. Our solution uses a boolean satisfiability formulation that is general, complete, and optimal: it is independent of the Datalog solver, it generalizes the failure of an abstraction to as many other abstractions as possible, and it identifies the cheapest refined abstraction to try next. We show the performance of our approach on a pointer analysis and a typestate analysis, on eight real-world Java benchmark programs.}},
  url = {https://doi.org/10.1145/2594291.2594327},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1137,
  title = {Mallacc: Accelerating Memory Allocation}},
  author = {Kanev, Svilen and Xi, Sam Likun and Wei, Gu-Yeon and Brooks, David}},
  year = {2017}},
  journal = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Recent work shows that dynamic memory allocation consumes nearly 7\% of all cycles in Google datacenters. With the trend towards increased specialization of hardware, we propose Mallacc, an in-core hardware accelerator designed for broad use across a number of high-performance, modern memory allocators. The design of Mallacc is quite different from traditional throughput-oriented hardware accelerators. Because memory allocation requests tend to be very frequent, fast, and interspersed inside other application code, accelerators must be optimized for latency rather than throughput and area overheads must be kept to a bare minimum. Mallacc accelerates the three primary operations of a typical memory allocation request: size class computation, retrieval of a free memory block, and sampling of memory usage. Our results show that malloc latency can be reduced by up to 50\% with a hardware cost of less than 1500 um2 of silicon area, less than 0.006\% of a typical high-performance processor core.}},
  url = {https://doi.org/10.1145/3037697.3037736},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1138,
  title = {Supercompilation by evaluation}},
  author = {Bolingbroke, Maximilian and Peyton Jones, Simon}},
  year = {2010}},
  journal = {Proceedings of the Third ACM Haskell Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper shows how call-by-need supercompilation can be recast to be based explicitly on an evaluator, contrasting with standard presentations which are specified as algorithms that mix evaluation rules with reductions that are unique to supercompilation. Building on standard operational-semantics technology for call-by-need languages, we show how to extend the supercompilation algorithm to deal with recursive let expressions.}},
  url = {https://doi.org/10.1145/1863523.1863540},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1139,
  title = {Introspective analysis: context-sensitivity, across the board}},
  author = {Smaragdakis, Yannis and Kastrinis, George and Balatsouras, George}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Context-sensitivity is the primary approach for adding more precision to a points-to analysis, while hopefully also maintaining scalability. An oft-reported problem with context-sensitive analyses, however, is that they are bi-modal: either the analysis is precise enough that it manipulates only manageable sets of data, and thus scales impressively well, or the analysis gets quickly derailed at the first sign of imprecision and becomes orders-of-magnitude more expensive than would be expected given the program's size. There is currently no approach that makes precise context-sensitive analyses (of any flavor: call-site-, object-, or type-sensitive) scale across the board at a level comparable to that of a context-insensitive analysis. To address this issue, we propose introspective analysis: a technique for uniformly scaling context-sensitive analysis by eliminating its performance-detrimental behavior, at a small precision expense. Introspective analysis consists of a common adaptivity pattern: first perform a context-insensitive analysis, then use the results to selectively refine (i.e., analyze context-sensitively) program elements that will not cause explosion in the running time or space. The technical challenge is to appropriately identify such program elements. We show that a simple but principled approach can be remarkably effective, achieving scalability (often with dramatic speedup) for benchmarks previously completely out-of-reach for deep context-sensitive analyses.}},
  url = {https://doi.org/10.1145/2594291.2594320},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1140,
  title = {Formal certification of code-based cryptographic proofs}},
  author = {Barthe, Gilles and Gr\'{e}},
  year = {2009}},
  journal = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {As cryptographic proofs have become essentially unverifiable, cryptographers have argued in favor of developing techniques that help tame the complexity of their proofs. Game-based techniques provide a popular approach in which proofs are structured as sequences of games and in which proof steps establish the validity of transitions between successive games. Code-based techniques form an instance of this approach that takes a code-centric view of games, and that relies on programming language theory to justify proof steps. While code-based techniques contribute to formalize the security statements precisely and to carry out proofs systematically, typical proofs are so long and involved that formal verification is necessary to achieve a high degree of confidence. We present Certicrypt, a framework that enables the machine-checked construction and verification of code-based proofs. Certicrypt is built upon the general-purpose proof assistant Coq, and draws on many areas, including probability, complexity, algebra, and semantics of programming languages. Certicrypt provides certified tools to reason about the equivalence of probabilistic programs, including a relational Hoare logic, a theory of observational equivalence, verified program transformations, and game-based techniques such as reasoning about failure events. The usefulness of Certicrypt is demonstrated through various examples, including a proof of semantic security of OAEP (with a bound that improves upon existing published results), and a proof of existential unforgeability of FDH signatures. Our work provides a first yet significant step towards Halevi's ambitious programme of providing tool support for cryptographic proofs.}},
  url = {https://doi.org/10.1145/1480881.1480894},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1141,
  title = {Scaling symbolic execution using ranged analysis}},
  author = {Siddiqui, Junaid Haroon and Khurshid, Sarfraz}},
  year = {2012}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper introduces a novel approach to scale symbolic execution --- a program analysis technique for systematic exploration of bounded execution paths---for test input generation. While the foundations of symbolic execution were developed over three decades ago, recent years have seen a real resurgence of the technique, specifically for systematic bug finding. However, scaling symbolic execution remains a primary technical challenge due to the inherent complexity of the path-based exploration that lies at core of the technique.Our key insight is that the state of the analysis can be represented highly compactly: a test input is all that is needed to effectively encode the state of a symbolic execution run. We present ranged symbolic execution, which embodies this insight and uses two test inputs to define a range, i.e., the beginning and end, for a symbolic execution run. As an application of our approach, we show how it enables scalability by distributing the path exploration---both in a sequential setting with a single worker node and in a parallel setting with multiple workers. As an enabling technology, we leverage the open-source, state-of-the-art symbolic execution tool KLEE. Experimental results using 71 programs chosen from the widely deployed GNU Coreutils set of Unix utilities show that our approach provides a significant speedup over KLEE. For example, using 10 worker cores, we achieve an average speed-up of 6.6X for the 71 programs.}},
  url = {https://doi.org/10.1145/2384616.2384654},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1142,
  title = {Formal verification of object layout for c++ multiple inheritance}},
  author = {Ramananandro, Tahina and Dos Reis, Gabriel and Leroy, Xavier}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Object layout - the concrete in-memory representation of objects - raises many delicate issues in the case of the C++ language, owing in particular to multiple inheritance, C compatibility and separate compilation. This paper formalizes a family of C++ object layout schemes and mechanically proves their correctness against the operational semantics for multiple inheritance of Wasserrab et al. This formalization is flexible enough to account for space-saving techniques such as empty base class optimization and tail-padding optimization. As an application, we obtain the first formal correctness proofs for realistic, optimized object layout algorithms, including one based on the popular "common vendor" Itanium C++ application binary interface. This work provides semantic foundations to discover and justify new layout optimizations; it is also a first step towards the verification of a C++ compiler front-end.}},
  url = {https://doi.org/10.1145/1926385.1926395},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1143,
  title = {Code motion for explicitly parallel programs}},
  author = {Knoop, Jens and Steffen, Bernhard}},
  year = {1999}},
  journal = {Proceedings of the Seventh ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In comparison to automatic parallelization, which is thoroughly studied in the literature [31, 33], classical analyses and optimizations of explicitly parallel programs were more or less neglected. This may be due to the fact that naive adaptations of the sequential techniques fail [24], and their straightforward correct ones have unacceptable costs caused by the interleavings, which manifest the possible executions of a parallel program. Recently, however, we showed that unidirectional bitvector analyses can be performed for parallel programs as easily and as efficiently as for sequential ones [17], a necessary condition for the successful transfer of the classical optimizations to the parallel setting.In this article we focus on possible subsequent code motion transformations, which turn out to require much more care than originally conjectured [17]. Essentially, this is due to the fact that interleaving semantics, although being adequate for correctness considerations, fails when it comes to reasoning about efficiency of parallel programs. This deficiency, however, can be overcome by strengthening the specific treatment of synchronization points.}},
  url = {https://doi.org/10.1145/301104.301106},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1144,
  title = {Verifying GPU kernels by test amplification}},
  author = {Leung, Alan and Gupta, Manish and Agarwal, Yuvraj and Gupta, Rajesh and Jhala, Ranjit and Lerner, Sorin}},
  year = {2012}},
  journal = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a novel technique for verifying properties of data parallel GPU programs via test amplification. The key insight behind our work is that we can use the technique of static information flow to amplify the result of a single test execution over the set of all inputs and interleavings that affect the property being verified. We empirically demonstrate the effectiveness of test amplification for verifying race-freedom and determinism over a large number of standard GPU kernels, by showing that the result of verifying a single dynamic execution can be amplified over the massive space of possible data inputs and thread interleavings.}},
  url = {https://doi.org/10.1145/2254064.2254110},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1145,
  title = {Efficient state merging in symbolic execution}},
  author = {Kuznetsov, Volodymyr and Kinder, Johannes and Bucur, Stefan and Candea, George}},
  year = {2012}},
  journal = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Symbolic execution has proven to be a practical technique for building automated test case generation and bug finding tools. Nevertheless, due to state explosion, these tools still struggle to achieve scalability. Given a program, one way to reduce the number of states that the tools need to explore is to merge states obtained on different paths. Alas, doing so increases the size of symbolic path conditions (thereby stressing the underlying constraint solver) and interferes with optimizations of the exploration process (also referred to as search strategies). The net effect is that state merging may actually lower performance rather than increase it.We present a way to automatically choose when and how to merge states such that the performance of symbolic execution is significantly increased. First, we present query count estimation, a method for statically estimating the impact that each symbolic variable has on solver queries that follow a potential merge point; states are then merged only when doing so promises to be advantageous. Second, we present dynamic state merging, a technique for merging states that interacts favorably with search strategies in automated test case generation and bug finding tools.Experiments on the 96 GNU Coreutils show that our approach consistently achieves several orders of magnitude speedup over previously published results. Our code and experimental data are publicly available at http://cloud9.epfl.ch.}},
  url = {https://doi.org/10.1145/2254064.2254088},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1146,
  title = {Subject-oriented composition rules}},
  author = {Ossher, Harold and Kaplan, Matthew and Harrison, William and Katz, Alexander and Kruskal, Vincent}},
  year = {1995}},
  journal = {Proceedings of the Tenth Annual Conference on Object-Oriented Programming Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Subject-oriented programming supports composition of object-oriented programs or program fragments called subjects. This paper presents an approach to the composition rules used to specify composition details. Rules can be generic, allowing different subrules to be "plugged into" higher-level rules, and they include a means of specifying exceptions to general rules. We give definitions of a number of useful, generic rules, including merge and override, as a first step towards a generally-useful composition rule library. We also outline an object-oriented framework for implementing rules, which we are currently building as part of our support for subject-oriented programming in C++.}},
  url = {https://doi.org/10.1145/217838.217864},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1147,
  title = {Parameterized verification of transactional memories}},
  author = {Emmi, Michael and Majumdar, Rupak and Manevich, Roman}},
  year = {2010}},
  journal = {Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We describe an automatic verification method to check whether transactional memories ensure strict serializability a key property assumed of the transactional interface. Our main contribution is a technique for effectively verifying parameterized systems. The technique merges ideas from parameterized hardware and protocol verification--verification by invisible invariants and symmetry reduction--with ideas from software verification--template-based invariant generation and satisfiability checking for quantified formul\ae{}},
  url = {https://doi.org/10.1145/1806596.1806613},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1148,
  title = {End-to-end verification of stack-space bounds for C programs}},
  author = {Carbonneaux, Quentin and Hoffmann, Jan and Ramananandro, Tahina and Shao, Zhong}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Verified compilers guarantee the preservation of semantic properties and thus enable formal verification of programs at the source level. However, important quantitative properties such as memory and time usage still have to be verified at the machine level where interactive proofs tend to be more tedious and automation is more challenging.This article describes a framework that enables the formal verification of stack-space bounds of compiled machine code at the C level. It consists of a verified CompCert-based compiler that preserves quantitative properties, a verified quantitative program logic for interactive stack-bound development, and a verified stack analyzer that automatically derives stack bounds during compilation.The framework is based on event traces that record function calls and returns. The source language is CompCert Clight and the target language is x86 assembly. The compiler is implemented in the Coq Proof Assistant and it is proved that crucial properties of event traces are preserved during compilation. A novel quantitative Hoare logic is developed to verify stack-space bounds at the CompCert Clight level. The quantitative logic is implemented in Coq and proved sound with respect to event traces generated by the small-step semantics of CompCert Clight. Stack-space bounds can be proved at the source level without taking into account low-level details that depend on the implementation of the compiler. The compiler fills in these low-level details during compilation and generates a concrete stack-space bound that applies to the produced machine code. The verified stack analyzer is guaranteed to automatically derive bounds for code with non-recursive functions. It generates a derivation in the quantitative logic to ensure soundness as well as interoperability with interactively developed stack bounds.In an experimental evaluation, the developed framework is used to obtain verified stack-space bounds for micro benchmarks as well as real system code. The examples include the verified operating-system kernel CertiKOS, parts of the MiBench embedded benchmark suite, and programs from the CompCert benchmarks. The derived bounds are close to the measured stack-space usage of executions of the compiled programs on a Linux x86 system.}},
  url = {https://doi.org/10.1145/2594291.2594301},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1149,
  title = {Boolean classes}},
  author = {McAllester, David and Zabih, Ramin}},
  year = {1986}},
  journal = {Conference Proceedings on Object-Oriented Programming Systems, Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We extend the notion of class so that any Boolean combinations of classes is also a class. Boolean classes allow greater precision and conciseness in naming the class of objects governed a particular method. A class can be viewed as a predicate which is either true or false of any given object. Unlike predicates however classes have an inheritance hierarchy which is known at compile time. Boolean classes extend the notion of class, making classes more like predicates, while preserving the compile time computable inheritance hierarchy.}},
  url = {https://doi.org/10.1145/28697.28740},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1150,
  title = {Probabilistic Termination: Soundness, Completeness, and Compositionality}},
  author = {Ferrer Fioriti, Luis Mar\'{\i}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We propose a framework to prove almost sure termination for probabilistic programs with real valued variables. It is based on ranking supermartingales, a notion analogous to ranking functions on non-probabilistic programs. The framework is proven sound and complete for a meaningful class of programs involving randomization and bounded nondeterminism. We complement this foundational insigh by a practical proof methodology, based on sound conditions that enable compositional reasoning and are amenable to a direct implementation using modern theorem provers. This is integrated in a small dependent type system, to overcome the problem that lexicographic ranking functions fail when combined with randomization. Among others, this compositional methodology enables the verification of probabilistic programs outside the complete class that admits ranking supermartingales.}},
  url = {https://doi.org/10.1145/2676726.2677001},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1151,
  title = {VeriPhy: verified controller executables from verified cyber-physical system models}},
  author = {Bohrer, Rose and Tan, Yong Kiam and Mitsch, Stefan and Myreen, Magnus O. and Platzer, Andr\'{e}},
  year = {2018}},
  journal = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present VeriPhy, a verified pipeline which automatically transforms verified high-level models of safety-critical cyber-physical systems (CPSs) in differential dynamic logic (dL) to verified controller executables. VeriPhy proves that all safety results are preserved end-to-end as it bridges abstraction gaps, including: i) the gap between mathematical reals in physical models and machine arithmetic in the implementation, ii) the gap between real physics and its differential-equation models, and iii) the gap between nondeterministic controller models and machine code. VeriPhy reduces CPS safety to the faithfulness of the physical environment, which is checked at runtime by synthesized, verified monitors. We use three provers in this effort: KeYmaera X, HOL4, and Isabelle/HOL. To minimize the trusted base, we cross-verify KeYmaeraX in Isabelle/HOL. We evaluate the resulting controller and monitors on commodity robotics hardware.}},
  url = {https://doi.org/10.1145/3192366.3192406},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1152,
  title = {From Communicating Machines to Graphical Choreographies}},
  author = {Lange, Julien and Tuosto, Emilio and Yoshida, Nobuko}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Graphical choreographies, or global graphs, are general multiparty session specifications featuring expressive constructs such as forking, merging, and joining for representing application-level protocols. Global graphs can be directly translated into modelling notations such as BPMN and UML. This paper presents an algorithm whereby a global graph can be constructed from asynchronous interactions represented by communicating finite-state machines (CFSMs). Our results include: a sound and complete characterisation of a subset of safe CFSMs from which global graphs can be constructed; an algorithm to translate CFSMs to global graphs; a time complexity analysis; and an implementation of our theory, as well as an experimental evaluation.}},
  url = {https://doi.org/10.1145/2676726.2676964},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1153,
  title = {Modular verification of linearizability with non-fixed linearization points}},
  author = {Liang, Hongjin and Feng, Xinyu}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Locating linearization points (LPs) is an intuitive approach for proving linearizability, but it is difficult to apply the idea in Hoare-style logic for formal program verification, especially for verifying algorithms whose LPs cannot be statically located in the code. In this paper, we propose a program logic with a lightweight instrumentation mechanism which can verify algorithms with non-fixed LPs, including the most challenging ones that use the helping mechanism to achieve lock-freedom (as in HSY elimination-based stack), or have LPs depending on unpredictable future executions (as in the lazy set algorithm), or involve both features. We also develop a thread-local simulation as the meta-theory of our logic, and show it implies contextual refinement, which is equivalent to linearizability. Using our logic we have successfully verified various classic algorithms, some of which are used in the java.util.concurrent package.}},
  url = {https://doi.org/10.1145/2491956.2462189},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1154,
  title = {JDMM: a java memory model for non-cache-coherent memory architectures}},
  author = {Zakkak, Foivos S. and Pratikakis, Polyvios}},
  year = {2014}},
  journal = {Proceedings of the 2014 International Symposium on Memory Management}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {As the number of cores continuously grows, processor designers are considering non coherent memories as more scalable and energy efficient alternatives to the current coherent ones. The Java Memory Model (JMM) requires that all cores can access the Java heap. It guarantees sequential consistency for data-race-free programs and no out-of-thin-air values for non data-race-free programs. To implement the Java Memory Model over non-cache-coherent and distributed architectures Java Virtual Machines (JVMs) are most likely to employ software caching.In this work, i) we provide a formalization of the Java Memory Model for non-cache-coherent and distributed memory architectures, ii) prove the adherence of our model with the Java Memory Model and iii) evaluate, regarding its compliance to the Java Memory Model, a state-of-the-art Java Virtual Machine implementation on a non-cache-coherent architecture.}},
  url = {https://doi.org/10.1145/2602988.2602999},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1155,
  title = {Distance makes the types grow stronger: a calculus for differential privacy}},
  author = {Reed, Jason and Pierce, Benjamin C.}},
  year = {2010}},
  journal = {Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We want assurances that sensitive information will not be disclosed when aggregate data derived from a database is published. Differential privacy offers a strong statistical guarantee that the effect of the presence of any individual in a database will be negligible, even when an adversary has auxiliary knowledge. Much of the prior work in this area consists of proving algorithms to be differentially private one at a time; we propose to streamline this process with a functional language whose type system automatically guarantees differential privacy, allowing the programmer to write complex privacy-safe query programs in a flexible and compositional way.The key novelty is the way our type system captures function sensitivity, a measure of how much a function can magnify the distance between similar inputs: well-typed programs not only can't go wrong, they can't go too far on nearby inputs. Moreover, by introducing a monad for random computations, we can show that the established definition of differential privacy falls out naturally as a special case of this soundness principle. We develop examples including known differentially private algorithms, privacy-aware variants of standard functional programming idioms, and compositionality principles for differential privacy.}},
  url = {https://doi.org/10.1145/1863543.1863568},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1156,
  title = {Automating formal proofs for reactive systems}},
  author = {Ricketts, Daniel and Robert, Valentin and Jang, Dongseok and Tatlock, Zachary and Lerner, Sorin}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Implementing systems in proof assistants like Coq and proving their correctness in full formal detail has consistently demonstrated promise for making extremely strong guarantees about critical software, ranging from compilers and operating systems to databases and web browsers. Unfortunately, these verifications demand such heroic manual proof effort, even for a single system, that the approach has not been widely adopted.We demonstrate a technique to eliminate the manual proof burden for verifying many properties within an entire class of applications, in our case reactive systems, while only expending effort comparable to the manual verification of a single system. A crucial insight of our approach is simultaneously designing both (1) a domain-specific language (DSL) for expressing reactive systems and their correctness properties and (2) proof automation which exploits the constrained language of both programs and properties to enable fully automatic, pushbutton verification. We apply this insight in a deeply embedded Coq DSL, dubbed Reflex, and illustrate Reflex's expressiveness by implementing and automatically verifying realistic systems including a modern web browser, an SSH server, and a web server. Using Reflex radically reduced the proof burden: in previous, similar versions of our benchmarks written in Coq by experts, proofs accounted for over 80\% of the code base; our versions require no manual proofs.}},
  url = {https://doi.org/10.1145/2594291.2594338},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1157,
  title = {Profile-guided static typing for dynamic scripting languages}},
  author = {Furr, Michael and An, Jong-hoon (David) and Foster, Jeffrey S.}},
  year = {2009}},
  journal = {Proceedings of the 24th ACM SIGPLAN Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many popular scripting languages such as Ruby, Python, and Perl include highly dynamic language constructs, such as an eval method that evaluates a string as program text. While these constructs allow terse and expressive code, they have traditionally obstructed static analysis. In this paper we present PRuby, an extension to Diamondback Ruby (DRuby), a static type inference system for Ruby. PRuby augments DRuby with a novel dynamic analysis and transformation that allows us to precisely type uses of highly dynamic constructs. PRuby's analysis proceeds in three steps. First, we use run-time instrumentation to gather per-application profiles of dynamic feature usage. Next, we replace dynamic features with statically analyzable alternatives based on the profile. We also add instrumentation to safely handle cases when subsequent runs do not match the profile. Finally, we run DRuby's static type inference on the transformed code to enforce type safety.We used PRuby to gather profiles for a benchmark suite of sample Ruby programs. We found that dynamic features are pervasive throughout the benchmarks and the libraries they include, but that most uses of these features are highly constrained and hence can be effectively profiled. Using the profiles to guide type inference, we found that DRuby can generally statically type our benchmarks modulo some refactoring, and we discovered several previously unknown type errors. These results suggest that profiling and transformation is a lightweight but highly effective approach to bring static typing to highly dynamic languages.}},
  url = {https://doi.org/10.1145/1640089.1640110},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1158,
  title = {Internally deterministic parallel algorithms can be fast}},
  author = {Blelloch, Guy E. and Fineman, Jeremy T. and Gibbons, Phillip B. and Shun, Julian}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The virtues of deterministic parallelism have been argued for decades and many forms of deterministic parallelism have been described and analyzed. Here we are concerned with one of the strongest forms, requiring that for any input there is a unique dependence graph representing a trace of the computation annotated with every operation and value. This has been referred to as internal determinism, and implies a sequential semantics---i.e., considering any sequential traversal of the dependence graph is sufficient for analyzing the correctness of the code. In addition to returning deterministic results, internal determinism has many advantages including ease of reasoning about the code, ease of verifying correctness, ease of debugging, ease of defining invariants, ease of defining good coverage for testing, and ease of formally, informally and experimentally reasoning about performance. On the other hand one needs to consider the possible downsides of determinism, which might include making algorithms (i) more complicated, unnatural or special purpose and/or (ii) slower or less scalable.In this paper we study the effectiveness of this strong form of determinism through a broad set of benchmark problems. Our main contribution is to demonstrate that for this wide body of problems, there exist efficient internally deterministic algorithms, and moreover that these algorithms are natural to reason about and not complicated to code. We leverage an approach to determinism suggested by Steele (1990), which is to use nested parallelism with commutative operations. Our algorithms apply several diverse programming paradigms that fit within the model including (i) a strict functional style (no shared state among concurrent operations), (ii) an approach we refer to as deterministic reservations, and (iii) the use of commutative, linearizable operations on data structures. We describe algorithms for the benchmark problems that use these deterministic approaches and present performance results on a 32-core machine. Perhaps surprisingly, for all problems, our internally deterministic algorithms achieve good speedup and good performance even relative to prior nondeterministic solutions.}},
  url = {https://doi.org/10.1145/2145816.2145840},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1159,
  title = {Architectural implications of nanoscale integrated sensing and computing}},
  author = {Pistol, Constantin and Chongchitmate, Wutichai and Dwyer, Christopher and Lebeck, Alvin R.}},
  year = {2009}},
  journal = {Proceedings of the 14th International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper explores the architectural implications of integrating computation and molecular probes to form nanoscale sensor processors (nSP). We show how nSPs may enable new computing domains and automate tasks that currently require expert scientific training and costly equipment. This new application domain severely constrains nSP size, which significantly impacts the architectural design space. In this context, we explore nSP architectures and present an nSP design that includes a simple accumulator-based ISA, sensors, limited memory and communication transceivers. To reduce the application memory footprint, we introduce the concept of instruction-fused sensing. We use simulation and analytical models to evaluate nSP designs executing a representative set of target applications. Furthermore, we propose a candidate nSP technology based on optical Resonance Energy Transfer (RET) logic that enables the small size required by the application domain; our smallest design is about the size of the largest known virus. We also show laboratory results that demonstrate initial steps towards a prototype.}},
  url = {https://doi.org/10.1145/1508244.1508247},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1160,
  title = {Separation and information hiding}},
  author = {O'Hearn, Peter W. and Yang, Hongseok and Reynolds, John C.}},
  year = {2004}},
  journal = {Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We investigate proof rules for information hiding, using the recent formalism of separation logic. In essence, we use the separating conjunction to partition the internal resources of a module from those accessed by the module's clients. The use of a logical connective gives rise to a form of dynamic partitioning, where we track the transfer of ownership of portions of heap storage between program components. It also enables us to enforce separation in the presence of mutable data structures with embedded addresses that may be aliased.}},
  url = {https://doi.org/10.1145/964001.964024},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1161,
  title = {Automatically learning shape specifications}},
  author = {Zhu, He and Petri, Gustavo and Jagannathan, Suresh}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents a novel automated procedure for discovering expressive shape specifications for sophisticated functional data structures. Our approach extracts potential shape predicates based on the definition of constructors of arbitrary user-defined inductive data types, and combines these predicates within an expressive first-order specification language using a lightweight data-driven learning procedure. Notably, this technique requires no programmer annotations, and is equipped with a type-based decision procedure to verify the correctness of discovered specifications. Experimental results indicate that our implementation is both efficient and effective, capable of automatically synthesizing sophisticated shape specifications over a range of complex data types, going well beyond the scope of existing solutions.}},
  url = {https://doi.org/10.1145/2908080.2908125},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1162,
  title = {Language-based control and mitigation of timing channels}},
  author = {Zhang, Danfeng and Askarov, Aslan and Myers, Andrew C.}},
  year = {2012}},
  journal = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We propose a new language-based approach to mitigating timing channels. In this language, well-typed programs provably leak only a bounded amount of information over time through external timing channels. By incorporating mechanisms for predictive mitigation of timing channels, this approach also permits a more expressive programming model. Timing channels arising from interaction with underlying hardware features such as instruction caches are controlled. Assumptions about the underlying hardware are explicitly formalized, supporting the design of hardware that efficiently controls timing channels. One such hardware design is modeled and used to show that timing channels can be controlled in some simple programs of real-world significance.}},
  url = {https://doi.org/10.1145/2254064.2254078},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1163,
  title = {SDNRacer: concurrency analysis for software-defined networks}},
  author = {El-Hassany, Ahmed and Miserez, Jeremie and Bielik, Pavol and Vanbever, Laurent and Vechev, Martin}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Concurrency violations are an important source of bugs in Software-Defined Networks (SDN), often leading to policy or invariant violations. Unfortunately, concurrency violations are also notoriously difficult to avoid, detect and debug. This paper presents a novel approach and a tool, SDNRacer, for detecting concurrency violations of SDNs. Our approach is enabled by three key ingredients: (i) a precise happens- before model for SDNs that captures when events can happen concurrently; (ii) a set of sound, domain-specific filters that reduce reported violations by orders of magnitude, and; (iii) a sound and complete dynamic analyzer, based on the above, that can ensure the network is free of harmful errors such as data races and per-packet incoherence. We evaluated SDNRacer on several real-world OpenFlow controllers, running both reactive and proactive applications in large networks. We show that SDNRacer is practically effective: it quickly pinpoints harmful concurrency violations without overwhelming the user with false positives.}},
  url = {https://doi.org/10.1145/2908080.2908124},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1164,
  title = {Synthesizing transformations on hierarchically structured data}},
  author = {Yaghmazadeh, Navid and Klinger, Christian and Dillig, Isil and Chaudhuri, Swarat}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents a new approach for synthesizing transformations on tree-structured data, such as Unix directories and XML documents. We consider a general abstraction for such data, called hierarchical data trees (HDTs) and present a novel example-driven synthesis algorithm for HDT transformations. Our central insight is to reduce the problem of synthesizing tree transformers to the synthesis of list transformations that are applied to the paths of the tree. The synthesis problem over lists is solved using a new algorithm that combines SMT solving and decision tree learning. We have implemented our technique in a system called HADES and show that HADES can automatically synthesize a variety of interesting transformations collected from online forums.}},
  url = {https://doi.org/10.1145/2908080.2908088},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1165,
  title = {Bringing back monad comprehensions}},
  author = {Giorgidze, George and Grust, Torsten and Schweinsberg, Nils and Weijers, Jeroen}},
  year = {2011}},
  journal = {Proceedings of the 4th ACM Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper is about a Glasgow Haskell Compiler (GHC) extension that generalises Haskell's list comprehension notation to monads. The monad comprehension notation implemented by the extension supports generator and filter clauses, as was the case in the Haskell 1.4 standard. In addition, the extension generalises the recently proposed parallel and SQL-like list comprehension notations to monads. The aforementioned generalisations are formally defined in this paper. The extension will be available in GHC 7.2.This paper gives several instructive examples that we hope will facilitate wide adoption of the extension by the Haskell community. We also argue why the do notation is not always a good fit for monadic libraries and embedded domain-specific languages, especially for those that are based on collection monads. Should the question of how to integrate the extension into the Haskell standard arise, the paper proposes a solution to the problem that led to the removal of the monad comprehension notation from the language standard.}},
  url = {https://doi.org/10.1145/2034675.2034678},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1166,
  title = {Specialization slicing}},
  author = {Aung, Min and Horwitz, Susan and Joiner, Rich and Reps, Thomas}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this paper, we investigate opportunities to be gained from broadening the definition of program slicing. A major inspiration for our work comes from the field of partial evaluation, in which a wide repertoire of techniques have been developed for specializing programs. While slicing can also be harnessed for specializing programs, the kind of specialization obtainable via slicing has heretofore been quite restricted, compared to the kind of specialization allowed in partial evaluation. In particular, most slicing algorithms are what the partial-evaluation community calls monovariant: each program element of the original program generates at most one element in the answer. In contrast, partial-evaluation algorithms can be polyvariant, i.e., one program element in the original program may correspond to more than one element in the specialized program.The full paper appears in ACM TOPLAS 36(2), 2014.}},
  url = {https://doi.org/10.1145/2594291.2594345},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1167,
  title = {Garbage collection for monitoring parametric properties}},
  author = {Jin, Dongyun and Meredith, Patrick O'Neil and Griffith, Dennis and Rosu, Grigore}},
  year = {2011}},
  journal = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Parametric properties are behavioral properties over program events that depend on one or more parameters. Parameters are bound to concrete data or objects at runtime, which makes parametric properties particularly suitable for stating multi-object relationships or protocols. Monitoring parametric properties independently of the employed formalism involves slicing traces with respect to parameter instances and sending these slices to appropriate non-parametric monitor instances. The number of such instances is theoretically unbounded and tends to be enormous in practice, to an extent that how to efficiently manage monitor instances has become one of the most challenging problems in runtime verification. The previous formalism-independent approach was only able to do the obvious, namely to garbage collect monitor instances when all bound parameter objects were garbage collected. This led to pathological behaviors where unnecessary monitor instances were kept for the entire length of a program. This paper proposes a new approach to garbage collecting monitor instances. Unnecessary monitor instances are collected lazily to avoid creating undue overhead. This lazy collection, along with some careful engineering, has resulted in RV, the most efficient parametric monitoring system to date. Our evaluation shows that the average overhead of RV in the DaCapo benchmark is 15\%, which is two times lower than that of JavaMOP and orders of magnitude lower than that of Tracematches.}},
  url = {https://doi.org/10.1145/1993498.1993547},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1168,
  title = {Transactional pointcuts: designation reification and advice of interrelated join points}},
  author = {Sadat-Mohtasham, Hossein and Hoover, H. James}},
  year = {2009}},
  journal = {Proceedings of the Eighth International Conference on Generative Programming and Component Engineering}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Aspect-oriented mechanisms are characterized by their join point models. A join point model has three components: join points, which are elements of language semantics; "a means of identifying join points"; and "a means of affecting the behaviour at those join points." A pointcut-advice model is a dynamic join point model in which join points are points in program execution. Pointcuts select a set of join points, and advice affects the behaviour of the selected join points. In this model, join points are typically selected and advised independently of each other. That is, the relationships between join points are not taken into account in join point selection and advice. In practice, join points are often not independent. Instead, they form part of a higher-level operation that implements the intent of the developer (e.g. managing a resource). There are natural situations in which join points should be selected only if they play a specific role in that operation.We propose a new join point model that takes join point interrelationships into account and allows the designation of more complex computations as join points. Based on the new model, we have designed an aspect-oriented construct called a transactional pointcut (transcut). Transcuts select sets of interrelated join points and reify them into higher-level join points that can be advised. They share much of the machinery and intuition of pointcuts, and can be viewed as their natural extension. We have implemented a transcuts prototype as an extension to the AspectJ language and integrated it into the abc compiler. We present an example where a transcut is applied to implement recommended resource handling practices in the presence of exceptions within method boundaries.}},
  url = {https://doi.org/10.1145/1621607.1621615},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1169,
  title = {Painless programming combining reduction and search: design principles for embedding decision procedures in high-level languages}},
  author = {Sheard, Timothy E.}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We describe the Funlogic system which extends a functional language with existentially quantified declarations. An existential declaration introduces a variable and a set of constraints that its value should meet. Existential variables are bound to conforming values by a decision procedure. Funlogic embeds multiple external decision procedures using a common framework. Design principles for embedding decision procedures are developed and illustrated for three different decision procedures from widely varying domains.}},
  url = {https://doi.org/10.1145/2364527.2364542},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1170,
  title = {Evolving RPC for active storage}},
  author = {Sivathanu, Muthian and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.}},
  year = {2002}},
  journal = {Proceedings of the 10th International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We introduce Scriptable RPC (SRPC), an RPC-based framework that enables distributed system services to take advantage of active components. Technology trends point to a world where each component in a system (whether disk, network interface, or memory) has substantial computational capabilities; however, traditional methods of building distributed services are not designed to take advantage of these new architectures, mandating wholesale change of the software base to exploit more powerful hardware. In contrast, SRPC provides a direct and simple migration path for traditional services into the active environment.We demonstrate the power and flexibility of the SRPC framework through a series of case studies, with a focus on active storage servers. Specifically, we find three advantages to our approach. First, SRPC improves the performance of distributed file servers, reducing latency by combining the execution of operations at the file server. Second, SRPC enables the ready addition of new functionality; for example, more powerful cache consistency models can be realized on top of a server that exports a simple NFS-like interface. Third, SRPC simplifies the construction of distributed services; operations that are difficult to coordinate across client and server can now be co-executed at the server, thus avoiding costly agreement and crash-recovery protocols.}},
  url = {https://doi.org/10.1145/605397.605425},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1171,
  title = {Multi-stage programming with explicit annotations}},
  author = {Taha, Walid and Sheard, Tim}},
  year = {1997}},
  journal = {Proceedings of the 1997 ACM SIGPLAN Symposium on Partial Evaluation and Semantics-Based Program Manipulation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We introduce MetaML, a statically-typed multi-stage programming language extending Nielson and Nielson's two stage notation to an arbitrary number of stages. MetaML extends previous work by introducing four distinct staging annotations which generalize those published previously [25, 12, 7, 6]We give a static semantics in which type checking is done once and for all before the first stage, and a dynamic semantics which introduces a new concept of cross-stage persistence, which requires that variables available in any stage are also available in all future stages.We illustrate that staging is a manual form of binding time analysis. We explain why, even in the presence of automatic binding time analysis, explicit annotations are useful, especially for programs with more than two stages.A thesis of this paper is that multi-stage languages are useful as programming languages in their own right, and should support features that make it possible for programmers to write staged computations without significantly changing their normal programming style. To illustrate this we provide a simple three stage example, and an extended two-stage example elaborating a number of practical issues.}},
  url = {https://doi.org/10.1145/258993.259019},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1172,
  title = {Operational semantics of programming languages}},
  author = {Wegner, Peter}},
  year = {1972}},
  journal = {Proceedings of ACM Conference on Proving Assertions about Programs}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A framework is developed for the operational characterization of the semantics of computational formalisms such as programming languages, digital computers and automata. Operational semantics is contrasted with input-output semantics and it is shown that input-output semantics is inappropriate to the study of certain important implementation-dependent attributes of computational formalisms. Notions of equivalence are developed for a very general class of operational models called information structure models. The structure of proofs of compiler correctness and interpreter equivalence is discussed independently of the form of the states and state transitions of a specific interpreter. These techniques are then applied to defining a correctness criterion for block structure implementations and to correctness proofs of interpreters for block structure languages.}},
  url = {https://doi.org/10.1145/800235.807081},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1173,
  title = {Empirical assessment of object-oriented implementations with multiple inheritance and static typing}},
  author = {Ducournau, Roland and Morandat, Flor\'{e}},
  year = {2009}},
  journal = {Proceedings of the 24th ACM SIGPLAN Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Object-oriented languages involve a threefold tradeoff between runtime efficiency, expressiveness (multiple inheritance), and modularity, i.e. open-world assumption (OWA). Runtime efficiency is conditioned by both the implementation technique and compilation scheme. The former specifies the data structures that support method invocation, attribute access and subtype testing. The latter consists of the production line of an executable from the source code. Many implementation techniques have been proposed and several compilation schemes can be considered from fully global compilation under the closed-world assumption (CWA) to separate compilation with dynamic loading under the OWA, with midway solutions. This article reviews a significant subset of possible combinations and presents a systematic, empirical comparison of their respective efficiencies with all other things being equal. The testbed consists of the Prm compiler that has been designed for this purpose. The considered techniques include C++ subobjects, coloring, perfect hashing, binary tree dispatch and caching. A variety of processors were considered. Qualitatively, these first results confirm the intuitive or theoretical abstract assessments of the tested approaches. As expected, efficiency increases as CWA strengthens. From a quantitative standpoint, the results are the first to precisely compare the efficiency of techniques that are closely associated with specific languages like C++ and Eiffel. They also confirm that perfect hashing should be considered for implementing Java and .Net interfaces.}},
  url = {https://doi.org/10.1145/1640089.1640093},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1174,
  title = {Butterfly analysis: adapting dataflow analysis to dynamic parallel monitoring}},
  author = {Goodstein, Michelle L. and Vlachos, Evangelos and Chen, Shimin and Gibbons, Phillip B. and Kozuch, Michael A. and Mowry, Todd C.}},
  year = {2010}},
  journal = {Proceedings of the Fifteenth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Online program monitoring is an effective technique for detecting bugs and security attacks in running applications. Extending these tools to monitor parallel programs is challenging because the tools must account for inter-thread dependences and relaxed memory consistency models. Existing tools assume sequential consistency and often slow down the monitored program by orders of magnitude. In this paper, we present a novel approach that avoids these pitfalls by not relying on strong consistency models or detailed inter-thread dependence tracking. Instead, we only assume that events in the distant past on all threads have become visible; we make no assumptions on (and avoid the overheads of tracking) the relative ordering of more recent events on other threads. To overcome the potential state explosion of considering all the possible orderings among recent events, we adapt two techniques from static dataflow analysis, reaching definitions and reaching expressions, to this new domain of dynamic parallel monitoring. Significant modifications to these techniques are proposed to ensure the correctness and efficiency of our approach. We show how our adapted analysis can be used in two popular memory and security tools. We prove that our approach does not miss errors, and sacrifices precision only due to the lack of a relative ordering among recent events. Moreover, our simulation study on a collection of Splash-2 and Parsec 2.0 benchmarks running a memory-checking tool on a hardware-assisted logging platform demonstrates the potential benefits in trading off a very low false positive rate for (i) reduced overhead and (ii) the ability to run on relaxed consistency models.}},
  url = {https://doi.org/10.1145/1736020.1736050},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1175,
  title = {Type-based parametric analysis of program families}},
  author = {Chen, Sheng and Erwig, Martin}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Previous research on static analysis for program families has focused on lifting analyses for single, plain programs to program families by employing idiosyncratic representations. The lifting effort typically involves a significant amount of work for proving the correctness of the lifted algorithm and demonstrating its scalability. In this paper, we propose a parameterized static analysis framework for program families that can automatically lift a class of type-based static analyses for plain programs to program families. The framework consists of a parametric logical specification and a parametric variational constraint solver. We prove that a lifted algorithm is correct provided that the underlying analysis algorithm is correct. An evaluation of our framework has revealed an error in a previous manually lifted analysis. Moreover, performance tests indicate that the overhead incurred by the general framework is bounded by a factor of 2.}},
  url = {https://doi.org/10.1145/2628136.2628155},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1176,
  title = {VeriCon: towards verifying controller programs in software-defined networks}},
  author = {Ball, Thomas and Bj\o{}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Software-defined networking (SDN) is a new paradigm for operating and managing computer networks. SDN enables logically-centralized control over network devices through a "controller" software that operates independently from the network hardware, and can be viewed as the network operating system. Network operators can run both inhouse and third-party SDN programs (often called applications) on top of the controller, e.g., to specify routing and access control policies. SDN opens up the possibility of applying formal methods to prove the correctness of computer networks. Indeed, recently much effort has been invested in applying finite state model checking to check that SDN programs behave correctly. However, in general, scaling these methods to large networks is challenging and, moreover, they cannot guarantee the absence of errors.We present VeriCon, the first system for verifying that an SDN program is correct on all admissible topologies and for all possible (infinite) sequences of network events. VeriCon either confirms the correctness of the controller program on all admissible network topologies or outputs a concrete counterexample. VeriCon uses first-order logic to specify admissible network topologies and desired network-wide invariants, and then implements classical Floyd-Hoare-Dijkstra deductive verification using Z3. Our preliminary experience indicates that VeriCon is able to rapidly verify correctness, or identify bugs, for a large repertoire of simple core SDN programs. VeriCon is compositional, in the sense that it verifies the correctness of execution of any single network event w.r.t. the specified invariant, and can thus scale to handle large programs. To relieve the burden of specifying inductive invariants from the programmer, VeriCon includes a separate procedure for inferring invariants, which is shown to be effective on simple controller programs. We view VeriCon as a first step en route to practical mechanisms for verifying network-wide invariants of SDN programs.}},
  url = {https://doi.org/10.1145/2594291.2594317},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1177,
  title = {Stochastic superoptimization}},
  author = {Schkufza, Eric and Sharma, Rahul and Aiken, Alex}},
  year = {2013}},
  journal = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We formulate the loop-free binary superoptimization task as a stochastic search problem. The competing constraints of transformation correctness and performance improvement are encoded as terms in a cost function, and a Markov Chain Monte Carlo sampler is used to rapidly explore the space of all possible programs to find one that is an optimization of a given target program. Although our method sacrifices completeness, the scope of programs we are able to consider, and the resulting quality of the programs that we produce, far exceed those of existing superoptimizers. Beginning from binaries compiled by llvm -O0 for 64-bit x86, our prototype implementation, STOKE, is able to produce programs which either match or outperform the code produced by gcc -O3, icc -O3, and in some cases, expert handwritten assembly.}},
  url = {https://doi.org/10.1145/2451116.2451150},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1178,
  title = {The implicit calculus: a new foundation for generic programming}},
  author = {Oliveira, Bruno C.d.S. and Schrijvers, Tom and Choi, Wontae and Lee, Wonchan and Yi, Kwangkeun}},
  year = {2012}},
  journal = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Generic programming (GP) is an increasingly important trend in programming languages. Well-known GP mechanisms, such as type classes and the C++0x concepts proposal, usually combine two features: 1) a special type of interfaces; and 2) implicit instantiation of implementations of those interfaces.Scala implicits are a GP language mechanism, inspired by type classes, that break with the tradition of coupling implicit instantiation with a special type of interface. Instead, implicits provide only implicit instantiation, which is generalized to work for any types. This turns out to be quite powerful and useful to address many limitations that show up in other GP mechanisms.This paper synthesizes the key ideas of implicits formally in a minimal and general core calculus called the implicit calculus (λ⇒), and it shows how to build source languages supporting implicit instantiation on top of it. A novelty of the calculus is its support for partial resolution and higher-order rules (a feature that has been proposed before, but was never formalized or implemented). Ultimately, the implicit calculus provides a formal model of implicits, which can be used by language designers to study and inform implementations of similar mechanisms in their own languages.}},
  url = {https://doi.org/10.1145/2254064.2254070},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1179,
  title = {Parallelizing top-down interprocedural analyses}},
  author = {Albarghouthi, Aws and Kumar, Rahul and Nori, Aditya V. and Rajamani, Sriram K.}},
  year = {2012}},
  journal = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Modularity is a central theme in any scalable program analysis. The core idea in a modular analysis is to build summaries at procedure boundaries, and use the summary of a procedure to analyze the effect of calling it at its calling context. There are two ways to perform a modular program analysis: (1) top-down and (2) bottomup. A bottom-up analysis proceeds upwards from the leaves of the call graph, and analyzes each procedure in the most general calling context and builds its summary. In contrast, a top-down analysis starts from the root of the call graph, and proceeds downward, analyzing each procedure in its calling context. Top-down analyses have several applications in verification and software model checking. However, traditionally, bottom-up analyses have been easier to scale and parallelize than top-down analyses.In this paper, we propose a generic framework, BOLT, which uses MapReduce style parallelism to scale top-down analyses. In particular, we consider top-down analyses that are demand driven, such as the ones used for software model checking. In such analyses, each intraprocedural analysis happens in the context of a reachability query. A query Q over a procedure P results in query tree that consists of sub-queries over the procedures called by P. The key insight in BOLT is that the query tree can be explored in parallel using MapReduce style parallelism -- the map stage can be used to run a set of enabled queries in parallel, and the reduce stage can be used to manage inter-dependencies between queries. Iterating the map and reduce stages alternately, we can exploit the parallelism inherent in top-down analyses. Another unique feature of BOLT is that it is parameterized by the algorithm used for intraprocedural analysis. Several kinds of analyses, including may analyses, mustanalyses, and may-must-analyses can be parallelized using BOLT.We have implemented the BOLT framework and instantiated the intraprocedural parameter with a may-must-analysis. We have run BOLT on a test suite consisting of 45 Microsoft Windows device drivers and 150 safety properties. Our results demonstrate an average speedup of 3.71x and a maximum speedup of 7.4x (with 8 cores) over a sequential analysis. Moreover, in several checks where a sequential analysis fails, BOLT is able to successfully complete its analysis.}},
  url = {https://doi.org/10.1145/2254064.2254091},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1180,
  title = {GPUVerify: a verifier for GPU kernels}},
  author = {Betts, Adam and Chong, Nathan and Donaldson, Alastair and Qadeer, Shaz and Thomson, Paul}},
  year = {2012}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a technique for verifying race- and divergence-freedom of GPU kernels that are written in mainstream kernel programming languages such as OpenCL and CUDA. Our approach is founded on a novel formal operational semantics for GPU programming termed synchronous, delayed visibility (SDV) semantics. The SDV semantics provides a precise definition of barrier divergence in GPU kernels and allows kernel verification to be reduced to analysis of a sequential program, thereby completely avoiding the need to reason about thread interleavings, and allowing existing modular techniques for program verification to be leveraged. We describe an efficient encoding for data race detection and propose a method for automatically inferring loop invariants required for verification. We have implemented these techniques as a practical verification tool, GPUVerify, which can be applied directly to OpenCL and CUDA source code. We evaluate GPUVerify with respect to a set of 163 kernels drawn from public and commercial sources. Our evaluation demonstrates that GPUVerify is capable of efficient, automatic verification of a large number of real-world kernels.}},
  url = {https://doi.org/10.1145/2384616.2384625},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1181,
  title = {Directing JavaScript with arrows}},
  author = {Khoo, Yit Phang and Hicks, Michael and Foster, Jeffrey S. and Sazawal, Vibha}},
  year = {2009}},
  journal = {Proceedings of the 5th Symposium on Dynamic Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {JavaScript programmers make extensive use of event-driven programming to help build responsive web applications. However, standard approaches to sequencing events are messy, and often lead to code that is difficult to understand and maintain. We have found that arrows, a generalization of monads, are an elegant solution to this problem. Arrows allow us to easily write asynchronous programs in small, modular units of code, and flexibly compose them in many different ways, while nicely abstracting the details of asynchronous program composition. In this paper, we present Arrowlets, a new JavaScript library that offers arrows to the everyday JavaScript programmer. We show how to use Arrowlets to construct a variety of state machines, including state machines that branch and loop. We also demonstrate how Arrowlets separate computation from composition with examples such as a drag-and-drop handler and a bubblesort animation.}},
  url = {https://doi.org/10.1145/1640134.1640143},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1182,
  title = {Maximal sharing in the Lambda calculus with letrec}},
  author = {Grabmayer, Clemens and Rochel, Jan}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Increasing sharing in programs is desirable to compactify the code, and to avoid duplication of reduction work at run-time, thereby speeding up execution. We show how a maximal degree of sharing can be obtained for programs expressed as terms in the lambda calculus with letrec. We introduce a notion of 'maximal compactness' for λletrec-terms among all terms with the same infinite unfolding. Instead of defined purely syntactically, this notion is based on a graph semantics. λletrec-terms are interpreted as first-order term graphs so that unfolding equivalence between terms is preserved and reflected through bisimilarity of the term graph interpretations. Compactness of the term graphs can then be compared via functional bisimulation.We describe practical and efficient methods for the following two problems: transforming a λletrec-term into a maximally compact form; and deciding whether two λletrec-terms are unfolding-equivalent. The transformation of a λletrec-terms L into maximally compact form L0 proceeds in three steps: (i) translate L into its term graph G = [[L]] ; (ii) compute the maximally shared form of G as its bisimulation collapse G0 ; (iii) read back a λletrec-term L0 from the term graph G0 with the property [[L0]] = G0. Then L0 represents a maximally shared term graph, and it has the same unfolding as L.The procedure for deciding whether two given λletrec-terms L1 and L2 are unfolding-equivalent computes their term graph interpretations [[L1]] and [[L2]], and checks whether these are bisimilar.For illustration, we also provide a readily usable implementation.}},
  url = {https://doi.org/10.1145/2628136.2628148},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1183,
  title = {Set-theoretic foundation of parametric polymorphism and subtyping}},
  author = {Castagna, Giuseppe and Xu, Zhiwu}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We define and study parametric polymorphism for a type system with recursive, product, union, intersection, negation, and function types. We first recall why the definition of such a system was considered hard "when not impossible" and then present the main ideas at the basis of our solution. In particular, we introduce the notion of "convexity" on which our solution is built up and discuss its connections with parametricity as defined by Reynolds to whose study our work sheds new light.}},
  url = {https://doi.org/10.1145/2034773.2034788},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1184,
  title = {Deriving specialized program analyses for certifying component-client conformance}},
  author = {Ramalingam, G. and Warshavsky, Alex and Field, John and Goyal, Deepak and Sagiv, Mooly}},
  year = {2002}},
  journal = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We are concerned with the problem of statically certifying (verifying) whether the client of a software component conforms to the component's constraints for correct usage. We show how conformance certification can be efficiently carried out in a staged fashion for certain classes of first-order safety (FOS) specifications, which can express relationship requirements among potentially unbounded collections of runtime objects. In the first stage of the certification process, we systematically derive an abstraction that is used to model the component state during analysis of arbitrary clients. In general, the derived abstraction will utilize first-order predicates, rather than the propositions often used by model checkers. In the second stage, the generated abstraction is incorporated into a static analysis engine to produce a certifier. In the final stage, the resulting certifier is applied to a client to conservatively determine whether the client violates the component's constraints. Unlike verification approaches that analyze a specification and client code together, our technique can take advantage of computationally-intensive symbolic techniques during the abstraction generation phase, without affecting the performance of client analysis. Using as a running example the Concurrent Modification Problem (CMP), which arises when certain classes defined by the Java Collections Framework are misused, we describe several different classes of certifiers with varying time/space/precision tradeoffs. Of particular note are precise, polynomial-time, flow- and context-sensitive certifiers for certain classes of FOS specifications and client programs. Finally, we evaluate a prototype implementation of a certifier for CMP on a variety of test programs. The results of the evaluation show that our approach, though conservative, yields very few "false alarms," with acceptable performance.}},
  url = {https://doi.org/10.1145/512529.512540},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1185,
  title = {Improving parallel shear-warp volume rendering on shared address space multiprocessors}},
  author = {Jiang, Dongming and Singh, Jaswinder Pal}},
  year = {1997}},
  journal = {Proceedings of the Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents a new parallel volume rendering algorithm and implementation, based on shear warp factorization, for shared address space multiprocessors. Starting from an existing parallel shear-warp renderer, we use increasingly detailed performance measurements on real machines and simulators to understand performance bottlenecks. This leads us to a new parallel implementation that substantially outperforms and out-scales the old one on a range of shared address space platforms, from bus-based centralized memory machine to hardware-coherent distributed memory machines to networks of computers connected by page-based shared virtual memory. The results demonstrate that real time volume rendering is promising on general purpose multiprocessors, and illustrate the utility of tool hierarchies in conjunction with algorithmic and application knowledge to understand memory system interactions and improve parallel algorithms.}},
  url = {https://doi.org/10.1145/263764.263798},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1186,
  title = {Lifeline-based global load balancing}},
  author = {Saraswat, Vijay A. and Kambadur, Prabhanjan and Kodali, Sreedhar and Grove, David and Krishnamoorthy, Sriram}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {On shared-memory systems, Cilk-style work-stealing has been used to effectively parallelize irregular task-graph based applications such as Unbalanced Tree Search (UTS). There are two main difficulties in extending this approach to distributed memory. In the shared memory approach, thieves (nodes without work) constantly attempt to asynchronously steal work from randomly chosen victims until they find work. In distributed memory, thieves cannot autonomously steal work from a victim without disrupting its execution. When work is sparse, this results in performance degradation. In essence, a direct extension of traditional work-stealing to distributed memory violates the work-first principle underlying work-stealing. Further, thieves spend useless CPU cycles attacking victims that have no work, resulting in system inefficiencies in multi-programmed contexts. Second, it is non-trivial to detect active distributed termination (detect that programs at all nodes are looking for work, hence there is no work). This problem is well-studied and requires careful design for good performance. Unfortunately, in most existing languages/frameworks, application developers are forced to implement their own distributed termination detection.In this paper, we develop a simple set of ideas that allow work-stealing to be efficiently extended to distributed memory. First, we introduce lifeline graphs: low-degree, low-diameter, fully connected directed graphs. Such graphs can be constructed from k-dimensional hypercubes. When a node is unable to find work after w unsuccessful steals, it quiesces after informing the outgoing edges in its lifeline graph. Quiescent nodes do not disturb other nodes. A quiesced node is reactivated when work arrives from a lifeline and itself shares this work with those of its incoming lifelines that are activated. Termination occurs precisely when computation at all nodes has quiesced. In a language such as X10, such passive distributed termination can be detected automatically using the finish construct -- no application code is necessary.Our design is implemented in a few hundred lines of X10. On the binomial tree described in olivier:08}},
  url = {https://doi.org/10.1145/1941553.1941582},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1187,
  title = {A constraint-based approach to solving games on infinite graphs}},
  author = {Beyene, Tewodros and Chaudhuri, Swarat and Popeea, Corneliu and Rybalchenko, Andrey}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a constraint-based approach to computing winning strategies in two-player graph games over the state space of infinite-state programs. Such games have numerous applications in program verification and synthesis, including the synthesis of infinite-state reactive programs and branching-time verification of infinite-state programs. Our method handles games with winning conditions given by safety, reachability, and general Linear Temporal Logic (LTL) properties. For each property class, we give a deductive proof rule that --- provided a symbolic representation of the game players --- describes a winning strategy for a particular player. Our rules are sound and relatively complete. We show that these rules can be automated by using an off-the-shelf Horn constraint solver that supports existential quantification in clause heads. The practical promise of the rules is demonstrated through several case studies, including a challenging "Cinderella-Stepmother game" that allows infinite alternation of discrete and continuous choices by two players, as well as examples derived from prior work on program repair and synthesis.}},
  url = {https://doi.org/10.1145/2535838.2535860},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1188,
  title = {Feature interactions, products, and composition}},
  author = {Batory, Don and H\"{o}},
  year = {2011}},
  journal = {Proceedings of the 10th ACM International Conference on Generative Programming and Component Engineering}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The relationship between feature modules and feature interactions is not well-understood. To explain classic examples of feature interaction, we show that features are not only composed sequentially, but also by cross-product and interaction operations that heretofore were implicit in the literature. Using the Colored IDE (CIDE) tool as our starting point, we (a) present a formal model of these operations, (b) show how it connects and explains previously unrelated results in Feature Oriented Software Development (FOSD), and (c) describe a tool, based on our formalism, that demonstrates how changes in composed documents can be back-propagated to their original feature module definitions, thereby improving FOSD tooling.}},
  url = {https://doi.org/10.1145/2047862.2047867},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1189,
  title = {The next 700 data description languages}},
  author = {Fisher, Kathleen and Mandelbaum, Yitzhak and Walker, David}},
  year = {2006}},
  journal = {Conference Record of the 33rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In the spirit of Landin, we present a calculus of dependent types to serve as the semantic foundation for a family of languages called data description languages. Such languages, which include pads, datascript, and packettypes, are designed to facilitate programming with ad hoc data, ie, data not in well-behaved relational or xml formats. In the calculus, each type describes the physical layout and semantic properties of a data source. In the semantics, we interpret types simultaneously as the in-memory representation of the data described and as parsers for the data source. The parsing functions are robust, automatically detecting and recording errors in the data stream without halting parsing. We show the parsers are type-correct, returning data whose type matches the simple-type interpretation of the specification. We also prove the parsers are "error-correct," accurately reporting the number of physical and semantic errors that occur in the returned data. We use the calculus to describe the features of various data description languages, and we discuss how we have used the calculus to improve PADS.}},
  url = {https://doi.org/10.1145/1111037.1111039},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1190,
  title = {Attribute grammars fly first-class: how to do aspect oriented programming in Haskell}},
  author = {Viera, Marcos and Swierstra, S. Doaitse and Swierstra, Wouter}},
  year = {2009}},
  journal = {Proceedings of the 14th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Attribute Grammars (AGs), a general-purpose formalism for describing recursive computations over data types, avoid the trade-off which arises when building software incrementally: should it be easy to add new data types and data type alternatives or to add new operations on existing data types? However, AGs are usually implemented as a pre-processor, leaving e.g. type checking to later processing phases and making interactive development, proper error reporting and debugging difficult. Embedding AG into Haskell as a combinator library solves these problems.Previous attempts at embedding AGs as a domain-specific language were based on extensible records and thus exploiting Haskell's type system to check the well formedness of the AG, but fell short in compactness and the possibility to abstract over oft occurring AG patterns. Other attempts used a very generic mapping for which the AG well-formedness could not be statically checked.We present a typed embedding of AG in Haskell satisfying all these requirements. The key lies in using HList-like typed heterogeneous collections (extensible polymorphic records) and expressing AG well-formedness conditions as type-level predicates (i.e., type-class constraints). By further type-level programming we can also express common programming patterns, corresponding to the typical use cases of monads such as Reader, Writer and State. The paper presents a realistic example of type-class-based type-level programming in Haskell.}},
  url = {https://doi.org/10.1145/1596550.1596586},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1191,
  title = {Conditional correlation analysis for safe region-based memory management}},
  author = {Wang, Xi and Xu, Zhilei and Liu, Xuezheng and Guo, Zhenyu and Wang, Xiaoge and Zhang, Zheng}},
  year = {2008}},
  journal = {Proceedings of the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Region-based memory management is a popular scheme in systems software for better organization and performance. In the scheme, a developer constructs a hierarchy of regions of different lifetimes and allocates objects in regions. When the developer deletes a region, the runtime will recursively delete all its subregions and simultaneously reclaim objects in the regions. The developer must construct a consistent placement of objects in regions; otherwise, if a region that contains pointers to other regions is not always deleted before pointees, an inconsistency will surface and cause dangling pointers, which may lead to either crashes or leaks.This paper presents a static analysis tool RegionWiz that can find such lifetime inconsistencies in large C programs using regions. The tool is based on an analysis framework that generalizes the relations and constraints over regions and objects as conditional correlations. This framework allows a succinct formalization of consistency rules for region lifetimes, preserving memory safety and avoiding dangling pointers. RegionWiz uses these consistency rules to implement an efficient static analysis to compute the conditional correlation and reason about region lifetime consistency; the analysis is based on a context-sensitive, field-sensitive pointer analysis with heap cloning.Experiments with applying RegionWiz to six real-world software packages (including the RC compiler, Apache web server, and Subversion version control system) with two different region-based memory management interfaces show that RegionWiz can reason about region lifetime consistency in large C programs. The experiments also show that RegionWiz can find several previously unknown inconsistency bugs in these packages.}},
  url = {https://doi.org/10.1145/1375581.1375588},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1192,
  title = {Journey to find bugs in JavaScript web applications in the wild}},
  author = {Ryu, Sukyoung}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Analyzing real-world JavaScript web applications is a challenging task. On top of understanding the semantics of JavaScript, it requires modeling of web documents, platform objects, and interactions between them. Not only the JavaScript language itself but also its usage patterns are extremely dynamic. JavaScript can generate code and run it during evaluation, and most web applications load JavaScript code dynamically. Such dynamic characteristics of JavaScript web applications make pure static analysis approaches inapplicable. In this talk, we present our attempts to analyze JavaScript web applications in the wild mostly statically using various approaches. From pure JavaScript programs to JavaScript web applications using platform-specific libraries and dynamic code loading, we explain technical challenges in analyzing each of them and how we built an open-source analysis framework for JavaScript, SAFE, that addresses the challenges incrementally. In spite of active research accomplishments in analysis of JavaScript web applications, many issues still remain to be resolved such as events, callback functions, and hybrid web applications. We discuss possible future research directions and open challenges.}},
  url = {https://doi.org/10.1145/2951913.2976747},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1193,
  title = {Release-to-release binary compatibility in SOM}},
  author = {Forman, Ira R. and Conner, Michael H. and Danforth, Scott H. and Raper, Larry K.}},
  year = {1995}},
  journal = {Proceedings of the Tenth Annual Conference on Object-Oriented Programming Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {SOM (IBM's System Object Model) removes a major impediment to reuse in Object-Oriented Programming by facilitating the programming of release-to-release binary compatible class libraries. This is accomplished by supporting a large number of compatibility preserving transformations. Taken together these transformations compose a discipline for programming evolving class libraries.}},
  url = {https://doi.org/10.1145/217838.217880},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1194,
  title = {Sound type-dependent syntactic language extension}},
  author = {Lorenzen, Florian and Erdweg, Sebastian}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Syntactic language extensions can introduce new facilities into a programming language while requiring little implementation effort and modest changes to the compiler. It is typical to desugar language extensions in a distinguished compiler phase after parsing or type checking, not affecting any of the later compiler phases. If desugaring happens before type checking, the desugaring cannot depend on typing information and type errors are reported in terms of the generated code. If desugaring happens after type checking, the code generated by the desugaring is not type checked and may introduce vulnerabilities. Both options are undesirable. We propose a system for syntactic extensibility where desugaring happens after type checking and desugarings are guaranteed to only generate well-typed code. A major novelty of our work is that desugarings operate on typing derivations instead of plain syntax trees. This provides desugarings access to typing information and forms the basis for the soundness guarantee we provide, namely that a desugaring generates a valid typing derivation. We have implemented our system for syntactic extensibility in a language-independent fashion and instantiated it for a substantial subset of Java, including generics and inheritance. We provide a sound Java extension for Scala-like for-comprehensions.}},
  url = {https://doi.org/10.1145/2837614.2837644},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1195,
  title = {Abstract conflict driven learning}},
  author = {D'Silva, Vijay and Haller, Leopold and Kroening, Daniel}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Modern satisfiability solvers implement an algorithm, called Conflict Driven Clause Learning, which combines search for a model with analysis of conflicts. We show that this algorithm can be generalised to solve the lattice-theoretic problem of determining if an additive transformer on a Boolean lattice is always bottom. Our generalised procedure combines overapproximations of greatest fixed points with underapproximation of least fixed points to obtain more precise results than computing fixed points in isolation. We generalise implication graphs used in satisfiability solvers to derive underapproximate transformers from overapproximate ones. Our generalisation provides a new method for static analysers that operate over non-distributive lattices to reason about properties that require disjunction.}},
  url = {https://doi.org/10.1145/2429069.2429087},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1196,
  title = {A foundation for flow-based program matching: using temporal logic and model checking}},
  author = {Brunel, Julien and Doligez, Damien and Hansen, Ren\'{e}},
  year = {2009}},
  journal = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Reasoning about program control-flow paths is an important functionality of a number of recent program matching languages and associated searching and transformation tools. Temporal logic provides a well-defined means of expressing properties of control-flow paths in programs, and indeed an extension of the temporal logic CTL has been applied to the problem of specifying and verifying the transformations commonly performed by optimizing compilers. Nevertheless, in developing the Coccinelle program transformation tool for performing Linux collateral evolutions in systems code, we have found that existing variants of CTL do not adequately support rules that transform subterms other than the ones matching an entire formula. Being able to transform any of the subterms of a matched term seems essential in the domain targeted by Coccinelle.In this paper, we propose an extension to CTL named CTLVW (CTL with variables and witnesses) that is a suitable basis for the semantics and implementation of the Coccinelles program matching language. Our extension to CTL includes existential quantification over program fragments, which allows metavariables in the program matching language to range over different values within different control-flow paths, and a notion of witnesses that record such existential bindings for use in the subsequent program transformation process. We formalize CTL-VW and describe its use in the context of Coccinelle. We then assess the performance of the approach in practice, using a transformation rule that fixes several reference count bugs in Linux code.}},
  url = {https://doi.org/10.1145/1480881.1480897},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1197,
  title = {Full functional verification of linked data structures}},
  author = {Zee, Karen and Kuncak, Viktor and Rinard, Martin}},
  year = {2008}},
  journal = {Proceedings of the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present the first verification of full functional correctness for a range of linked data structure implementations, including mutable lists, trees, graphs, and hash tables. Specifically, we present the use of the Jahob verification system to verify formal specifications, written in classical higher-order logic, that completely capture the desired behavior of the Java data structure implementations (with the exception of properties involving execution time and/or memory consumption). Given that the desired correctness properties include intractable constructs such as quantifiers, transitive closure, and lambda abstraction, it is a challenge to successfully prove the generated verification conditions.Our Jahob verification system uses integrated reasoning to split each verification condition into a conjunction of simpler subformulas, then apply a diverse collection of specialized decision procedures, first-order theorem provers, and, in the worst case, interactive theorem provers to prove each subformula. Techniques such as replacing complex subformulas with stronger but simpler alternatives, exploiting structure inherently present in the verification conditions, and, when necessary, inserting verified lemmas and proof hints into the imperative source code make it possible to seamlessly integrate all of the specialized decision procedures and theorem provers into a single powerful integrated reasoning system. By appropriately applying multiple proof techniques to discharge different subformulas, this reasoning system can effectively prove the complex and challenging verification conditions that arise in this context.}},
  url = {https://doi.org/10.1145/1375581.1375624},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1198,
  title = {A type system for well-founded recursion}},
  author = {Dreyer, Derek}},
  year = {2004}},
  journal = {Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In the interest of designing a recursive module extension to ML that is as simple and general as possible, we propose a novel type system for general recursion over effectful expressions. The presence of effects seems to necessitate a backpatching semantics for recursion similar to that of Scheme. Our type system ensures statically that recursion is well-founded---that the body of a recursive expression will evaluate without attempting to access the undefined recursive variable---which avoids some unnecessary run-time costs associated with backpatching. To ensure well-founded recursion in the presence of multiple recursive variables and separate compilation, we track the usage of individual recursive variables, represented statically by "names". So that our type system may eventually be integrated smoothly into ML's, reasoning involving names is only required inside code that uses our recursive construct and need not infect existing ML code, although instrumentation of some existing code can help to improve the precision of our type system.}},
  url = {https://doi.org/10.1145/964001.964026},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1199,
  title = {PHANTOM: predicting performance of parallel applications on large-scale parallel machines using a single node}},
  author = {Zhai, Jidong and Chen, Wenguang and Zheng, Weimin}},
  year = {2010}},
  journal = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {For designers of large-scale parallel computers, it is greatly desired that performance of parallel applications can be predicted at the design phase. However, this is difficult because the execution time of parallel applications is determined by several factors, including sequential computation time in each process, communication time and their convolution. Despite previous efforts, it remains an open problem to estimate sequential computation time in each process accurately and efficiently for large-scale parallel applications on non-existing target machines.This paper proposes a novel approach to predict the sequential computation time accurately and efficiently. We assume that there is at least one node of the target platform but the whole target system need not be available. We make two main technical contributions. First, we employ deterministic replay techniques to execute any process of a parallel application on a single node at real speed. As a result, we can simply measure the real sequential computation time on a target node for each process one by one. Second, we observe that computation behavior of processes in parallel applications can be clustered into a few groups while processes in each group have similar computation behavior. This observation helps us reduce measurement time significantly because we only need to execute representative parallel processes instead of all of them.We have implemented a performance prediction framework, called PHANTOM, which integrates the above computation-time acquisition approach with a trace-driven network simulator. We validate our approach on several platforms. For ASCI Sweep3D, the error of our approach is less than 5\% on 1024 processor cores. Compared to a recent regression-based prediction approach, PHANTOM presents better prediction accuracy across different platforms.}},
  url = {https://doi.org/10.1145/1693453.1693493},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1200,
  title = {Cruiser: concurrent heap buffer overflow monitoring using lock-free data structures}},
  author = {Zeng, Qiang and Wu, Dinghao and Liu, Peng}},
  year = {2011}},
  journal = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Security enforcement inlined into user threads often delays the protected programs; inlined resource reclamation may interrupt program execution and defer resource release. We propose software cruising, a novel technique that migrates security enforcement and resource reclamation from user threads to a concurrent monitor thread. The technique leverages the increasingly popular multicore and multiprocessor architectures and uses lock-free data structures to achieve non-blocking and efficient synchronization between the monitor and user threads. As a case study, software cruising is applied to the heap buffer overflow problem. Previous mitigation and detection techniques for this problem suffer from high performance overhead, legacy code compatibility, semantics loyalty, or tedious manual program transformation. We present a concurrent heap buffer overflow detector, Cruiser, in which a concurrent thread is added to the user program to monitor heap integrity, and custom lock-free data structures and algorithms are designed to achieve high efficiency and scalability. The experiments show that our approach is practical: it imposes an average of 5\% performance overhead on SPEC CPU2006, and the throughput slowdown on Apache is negligible on average.}},
  url = {https://doi.org/10.1145/1993498.1993541},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1201,
  title = {Fast synthesis of fast collections}},
  author = {Loncaric, Calvin and Torlak, Emina and Ernst, Michael D.}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many applications require specialized data structures not found in the standard libraries, but implementing new data structures by hand is tedious and error-prone. This paper presents a novel approach for synthesizing efficient implementations of complex collection data structures from high-level specifications that describe the desired retrieval operations. Our approach handles a wider range of data structures than previous work, including structures that maintain an order among their elements or have complex retrieval methods. We have prototyped our approach in a data structure synthesizer called Cozy. Four large, real-world case studies compare structures generated by Cozy against handwritten implementations in terms of correctness and performance. Structures synthesized by Cozy match the performance of handwritten data structures while avoiding human error.}},
  url = {https://doi.org/10.1145/2908080.2908122},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1202,
  title = {Simple relational correctness proofs for static analyses and program transformations}},
  author = {Benton, Nick}},
  year = {2004}},
  journal = {Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We show how some classical static analyses for imperative programs, and the optimizing transformations which they enable, may be expressed and proved correct using elementary logical and denotationaltechniques. The key ingredients are an interpretation of program properties as relations, rather than predicates, and a realization that although many program analyses are traditionally formulated in very intensional terms, the associated transformations are actually enabled by more liberal extensional properties.We illustrate our approach with formal systems for analysing and transforming while-programs. The first is a simple type system which tracks constancy and dependency information and can be used to perform dead-code elimination, constant propagation and program slicing as well as capturing a form of secure information flow. The second is a relational version of Hoare logic, which significantly generalizes our first type system and can also justify optimizations including hoisting loop invariants. Finally we show how a simple available expression analysis and redundancy elimination transformation may be justified by translation into relational Hoare logic.}},
  url = {https://doi.org/10.1145/964001.964003},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1203,
  title = {Taming release-acquire consistency}},
  author = {Lahav, Ori and Giannarakis, Nick and Vafeiadis, Viktor}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We introduce a strengthening of the release-acquire fragment of the C11 memory model that (i) forbids dubious behaviors that are not observed in any implementation; (ii) supports fence instructions that restore sequential consistency; and (iii) admits an equivalent intuitive operational semantics based on point-to-point communication. This strengthening has no additional implementation cost: it allows the same local optimizations as C11 release and acquire accesses, and has exactly the same compilation schemes to the x86-TSO and Power architectures. In fact, the compilation to Power is complete with respect to a recent axiomatic model of Power; that is, the compiled program exhibits exactly the same behaviors as the source one. Moreover, we provide criteria for placing enough fence instructions to ensure sequential consistency, and apply them to an efficient RCU implementation.}},
  url = {https://doi.org/10.1145/2837614.2837643},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1204,
  title = {Bidirectionalization transformation based on automatic derivation of view complement functions}},
  author = {Matsuda, Kazutaka and Hu, Zhenjiang and Nakano, Keisuke and Hamana, Makoto and Takeichi, Masato}},
  year = {2007}},
  journal = {Proceedings of the 12th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Bidirectional transformation is a pair of transformations: a view function and a backward transformation. A view function maps one data structure called source onto another called view. The corresponding backward transformation reflects changes in the view to the source. Its practically useful applications include replicated data synchronization, presentation-oriented editor development, tracing software development, and view updating in the database community. However, developing a bidirectional transformation is hard, because one has to give two mappings that satisfy the bidirectional properties for system consistency.In this paper, we propose a new framework for bidirectionalization that can automatically generate a useful backward transformation from a view function while guaranteeing that the two transformations satisfy the bidirectional properties. Our framework is based on two known approaches to bidirectionalization, namely the constant complement approach from the database community and the combinator approach from the programming language community, but it has three new features: (1) unlike the constant complement approach, it can deal with transformations between algebraic data structures rather than just tables; (2) unlike the combinator approach, in which primitive bidirectional transformations have to be explicitly given, it can derive them automatically; (3) it generates a view update checker to validate updates on views, which has not been well addressed so far. The new framework has been implemented and the experimental results show that our framework has promise.}},
  url = {https://doi.org/10.1145/1291151.1291162},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1205,
  title = {Polyhedra scanning revisited}},
  author = {Chen, Chun}},
  year = {2012}},
  journal = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents a new polyhedra scanning system called CodeGen+ to address the challenge of generating high-performance code for complex iteration spaces resulting from compiler optimization and autotuning systems. The strength of our approach lies in two new algorithms. First, a loop overhead removal algorithm provides precise control of trade-offs between loop overhead and code size based on actual loop nesting depth. Second, an if-statement simplification algorithm further reduces the number of comparisons in the code. These algorithms combined with the expressive power of Presburger arithmetic enable CodeGen+ to support complex optimization strategies expressed in iteration spaces. We compare with the state-of-the-art polyhedra scanning tool CLooG on five loop nest computations, demonstrating that CodeGen+ generates code that is simpler and up to 1.15x faster.}},
  url = {https://doi.org/10.1145/2254064.2254123},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1206,
  title = {Underspecified harnesses and interleaved bugs}},
  author = {Joshi, Saurabh and Lahiri, Shuvendu K. and Lal, Akash}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Static assertion checking of open programs requires setting up a precise harness to capture the environment assumptions. For instance, a library may require a file handle to be properly initialized before it is passed into it. A harness is used to set up or specify the appropriate preconditions before invoking methods from the program. In the absence of a precise harness, even the most precise automated static checkers are bound to report numerous false alarms. This often limits the adoption of static assertion checking in the hands of a user.In this work, we explore the possibility of automatically filtering away (or prioritizing) warnings that result from imprecision in the harness. We limit our attention to the scenario when one is interested in finding bugs due to concurrency. We define a warning to be an interleaved bug when it manifests on an input for which no sequential interleaving produces a warning. As we argue in the paper, limiting a static analysis to only consider interleaved bugs greatly reduces false positives during static concurrency analysis in the presence of an imprecise harness.We formalize interleaved bugs as a differential analysis between the original program and its sequential version and provide various techniques for finding them. Our implementation CBugs demonstrates that the scheme of finding interleaved bugs can alleviate the need to construct precise harnesses while checking real-life concurrent programs.}},
  url = {https://doi.org/10.1145/2103656.2103662},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1207,
  title = {Continuity analysis of programs}},
  author = {Chaudhuri, Swarat and Gulwani, Sumit and Lublinerman, Roberto}},
  year = {2010}},
  journal = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present an analysis to automatically determine if a program represents a continuous function, or equivalently, if infinitesimal changes to its inputs can only cause infinitesimal changes to its outputs. The analysis can be used to verify the robustness of programs whose inputs can have small amounts of error and uncertainty---e.g., embedded controllers processing slightly unreliable sensor data, or handheld devices using slightly stale satellite data.Continuity is a fundamental notion in mathematics. However, it is difficult to apply continuity proofs from real analysis to functions that are coded as imperative programs, especially when they use diverse data types and features such as assignments, branches, and loops. We associate data types with metric spaces as opposed to just sets of values, and continuity of typed programs is phrased in terms of these spaces. Our analysis reduces questions about continuity to verification conditions that do not refer to infinitesimal changes and can be discharged using off-the-shelf SMT solvers. Challenges arise in proving continuity of programs with branches and loops, as a small perturbation in the value of a variable often leads to divergent control-flow that can lead to large changes in values of variables. Our proof rules identify appropriate ``synchronization points'' between executions and their perturbed counterparts, and establish that values of certain variables converge back to the original results in spite of temporary divergence.We prove our analysis sound with respect to the traditional epsilon-delta definition of continuity. We demonstrate the precision of our analysis by applying it to a range of classic algorithms, including algorithms for array sorting, shortest paths in graphs, minimum spanning trees, and combinatorial optimization. A prototype implementation based on the Z3 SMT-solver is also presented.}},
  url = {https://doi.org/10.1145/1706299.1706308},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1208,
  title = {A certified type-preserving compiler from lambda calculus to assembly language}},
  author = {Chlipala, Adam}},
  year = {2007}},
  journal = {Proceedings of the 28th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a certified compiler from the simply-typed lambda calculus to assembly language. The compiler is certified in the sense that it comes with a machine-checked proof of semantics preservation, performed with the Coq proof assistant. The compiler and the terms of its several intermediate languages are given dependent types that guarantee that only well-typed programs are representable. Thus, type preservation for each compiler pass follows without any significant "proofs" of the usual kind. Semantics preservation is proved based on denotational semantics assigned to the intermediate languages. We demonstrate how working with a type-preserving compiler enables type-directed proof search to discharge large parts of our proof obligations automatically.}},
  url = {https://doi.org/10.1145/1250734.1250742},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1209,
  title = {CakeML: a verified implementation of ML}},
  author = {Kumar, Ramana and Myreen, Magnus O. and Norrish, Michael and Owens, Scott}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We have developed and mechanically verified an ML system called CakeML, which supports a substantial subset of Standard ML. CakeML is implemented as an interactive read-eval-print loop (REPL) in x86-64 machine code. Our correctness theorem ensures that this REPL implementation prints only those results permitted by the semantics of CakeML. Our verification effort touches on a breadth of topics including lexing, parsing, type checking, incremental and dynamic compilation, garbage collection, arbitrary-precision arithmetic, and compiler bootstrapping.Our contributions are twofold. The first is simply in building a system that is end-to-end verified, demonstrating that each piece of such a verification effort can in practice be composed with the others, and ensuring that none of the pieces rely on any over-simplifying assumptions. The second is developing novel approaches to some of the more challenging aspects of the verification. In particular, our formally verified compiler can bootstrap itself: we apply the verified compiler to itself to produce a verified machine-code implementation of the compiler. Additionally, our compiler proof handles diverging input programs with a lightweight approach based on logical timeout exceptions. The entire development was carried out in the HOL4 theorem prover.}},
  url = {https://doi.org/10.1145/2535838.2535841},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1210,
  title = {An abstract interpretation framework for termination}},
  author = {Cousot, Patrick and Cousot, Radhia}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Proof, verification and analysis methods for termination all rely on two induction principles: (1) a variant function or induction on data ensuring progress towards the end and (2) some form of induction on the program structure. The abstract interpretation design principle is first illustrated for the design of new forward and backward proof, verification and analysis methods for safety. The safety collecting semantics defining the strongest safety property of programs is first expressed in a constructive fixpoint form. Safety proof and checking/verification methods then immediately follow by fixpoint induction. Static analysis of abstract safety properties such as invariance are constructively designed by fixpoint abstraction (or approximation) to (automatically) infer safety properties. So far, no such clear design principle did exist for termination so that the existing approaches are scattered and largely not comparable with each other.For (1), we show that this design principle applies equally well to potential and definite termination. The trace-based termination collecting semantics is given a fixpoint definition. Its abstraction yields a fixpoint definition of the best variant function. By further abstraction of this best variant function, we derive the Floyd/Turing termination proof method as well as new static analysis methods to effectively compute approximations of this best variant function.For (2), we introduce a generalization of the syntactic notion of struc- tural induction (as found in Hoare logic) into a semantic structural induction based on the new semantic concept of inductive trace cover covering execution traces by segments, a new basis for formulating program properties. Its abstractions allow for generalized recursive proof, verification and static analysis methods by induction on both program structure, control, and data. Examples of particular instances include Floyd's handling of loop cutpoints as well as nested loops, Burstall's intermittent assertion total correctness proof method, and Podelski-Rybalchenko transition invariants.}},
  url = {https://doi.org/10.1145/2103656.2103687},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1211,
  title = {CommGuard: Mitigating Communication Errors in Error-Prone Parallel Execution}},
  author = {Yetim, Yavuz and Malik, Sharad and Martonosi, Margaret}},
  year = {2015}},
  journal = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {As semiconductor technology scales towards ever-smaller transistor sizes, hardware fault rates are increasing. Since important application classes (e.g., multimedia, streaming workloads) are data-error-tolerant, recent research has proposed techniques that seek to save energy or improve yield by exploiting error tolerance at the architecture/microarchitecture level. Even seemingly error-tolerant applications, however, will crash or hang due to control-flow/memory addressing errors. In parallel computation, errors involving inter-thread communication can have equally catastrophic effects. Our work explores techniques that mitigate the impact of potentially catastrophic errors in parallel computation, while still garnering power, cost, or yield benefits from data error tolerance. Our proposed CommGuard solution uses FSM-based checkers to pad and discard data in order to maintain semantic alignment between program control flow and the data communicated between processors. CommGuard techniques are low overhead and they exploit application information already provided by some parallel programming languages (e.g. StreamIt). By converting potentially catastrophic communication errors into potentially tolerable data errors, CommGuard allows important streaming applications like JPEG and MP3 decoding to execute without crashing and to sustain good output quality, even for errors as frequent as every 500μs.}},
  url = {https://doi.org/10.1145/2694344.2694354},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1212,
  title = {Message of thanks: on the receipt of the 2011 ACM SIGPLAN distinguished achievement award}},
  author = {Hoare, Tony}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {},
  url = {https://doi.org/10.1145/2103656.2103659},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1213,
  title = {Model-based kinematics generation for modular mechatronic toolkits}},
  author = {Bordignon, Mirko and Schultz, Ulrik Pagh and Stoy, Kasper}},
  year = {2010}},
  journal = {Proceedings of the Ninth International Conference on Generative Programming and Component Engineering}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Modular robots are mechatronic devices that enable the construction of highly versatile and flexible robotic systems whose mechanical structure can be dynamically modified. The key feature that enables this dynamic modification is the capability of the individual modules to connect to each other in multiple ways and thus generate a number of different mechanical systems, in contrast with the monolithics fixed structure of conventional robots. The mechatronic flexibility, however, complicates the development of models and programming abstractions for modular robots, since manually describing and enumerating the full set of possible interconnections is tedious and error-prone for real-world robots. In order to allow for a general formulation of spatial abstractions for modular robots and to ensure correct and streamlined generation of code dependent on mechanical properties, we have developed the Modular Mechatronics Modelling Language (M3L). M3L is a domain-specific language, which can model the kinematic structure of individual robot modules and declaratively describe their possible interconnections rather than requiring the user to enumerate them in their entirety. From this description, the M3L compiler generates the code that is needed to simulate the resulting robots within Webots, widely used commercial robot simulator, and the software component needed for spatial structure computations by a virtual machine-based runtime system, which we have developed and used for programming physical modular robots}},
  url = {https://doi.org/10.1145/1868294.1868318},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1214,
  title = {CDSchecker: checking concurrent data structures written with C/C++ atomics}},
  author = {Norris, Brian and Demsky, Brian}},
  year = {2013}},
  journal = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Writing low-level concurrent software has traditionally required intimate knowledge of the entire toolchain and often has involved coding in assembly. New language standards have extended C and C++ with support for low-level atomic operations and a weak memory model, enabling developers to write portable and efficient multithreaded code.Developing correct low-level concurrent code is well-known to be especially difficult under a weak memory model, where code behavior can be surprising. Building reliable concurrent software using C/C++ low-level atomic operations will likely require tools that help developers discover unexpected program behaviors.In this paper we present CDSChecker, a tool for exhaustively exploring the behaviors of concurrent code under the C/C++ memory model. We develop several novel techniques for modeling the relaxed behaviors allowed by the memory model and for minimizing the number of execution behaviors that CDSChecker must explore. We have used CDSChecker to exhaustively unit test several concurrent data structure implementations on specific inputs and have discovered errors in both a recently published C11 implementation of a work-stealing queue and a single producer, single consumer queue implementation.}},
  url = {https://doi.org/10.1145/2509136.2509514},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1215,
  title = {Safer unsafe code for .NET}},
  author = {Ferrara, Pietro and Logozzo, Francesco and F\"{a}},
  year = {2008}},
  journal = {Proceedings of the 23rd ACM SIGPLAN Conference on Object-Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The .NET intermediate language (MSIL) allows expressing both statically verifiable memory and type safe code (typically called managed), as well as unsafe code using direct pointer manipulations. Unsafe code can be expressed in C# by marking regions of code as unsafe. Writing unsafe code can be useful where the rules of managed code are too strict. The obvious drawback of unsafe code is that it opens the door to programming errors typical of C and C++, namely memory access errors such as buffer overruns. Worse, a single piece of unsafe code may corrupt memory and destabilize the entire runtime or allow attackers to compromise the security of the platform.We present a new static analysis based on abstract interpretation to check memory safety for unsafe code in the .NET framework. The core of the analysis is a new numerical abstract domain, Strp, which is used to efficiently compute memory invariants. Strp is combined with lightweight abstract domains to raise the precision, yet achieving scalability.We implemented this analysis in Clousot, a generic static analyzer for .NET. In combination with contracts expressed in FoxTrot, an MSIL based annotation language for .NET, our analysis provides static safety guarantees on memory accesses in unsafe code. We tested it on all the assemblies of the .NET framework. We compare our results with those obtained using existing domains, showing how they are either too imprecise (e.g., Intervals or Octagons) or too expensive (Polyhedra) to be used in practice.}},
  url = {https://doi.org/10.1145/1449764.1449791},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1216,
  title = {Relaxed memory models: an operational approach}},
  author = {Boudol, G\'{e}},
  year = {2009}},
  journal = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Memory models define an interface between programs written in some language and their implementation, determining which behaviour the memory (and thus a program) is allowed to have in a given model. A minimal guarantee memory models should provide to the programmer is that well-synchronized, that is, data-race free code has a standard semantics. Traditionally, memory models are defined axiomatically, setting constraints on the order in which memory operations are allowed to occur, and the programming language semantics is implicit as determining some of these constraints. In this work we propose a new approach to formalizing a memory model in which the model itself is part of a weak operational semantics for a (possibly concurrent) programming language. We formalize in this way a model that allows write operations to the store to be buffered. This enables us to derive the ordering constraints from the weak semantics of programs, and to prove, at the programming language level, that the weak semantics implements the usual interleaving semantics for data-race free programs, hence in particular that it implements the usual semantics for sequential code.}},
  url = {https://doi.org/10.1145/1480881.1480930},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1217,
  title = {Loop-oriented array- and field-sensitive pointer analysis for automatic SIMD vectorization}},
  author = {Sui, Yulei and Fan, XIaokang and Zhou, Hao and Xue, Jingling}},
  year = {2016}},
  journal = {Proceedings of the 17th ACM SIGPLAN/SIGBED Conference on Languages, Compilers, Tools, and Theory for Embedded Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Compiler-based auto-vectorization is a promising solution to automatically generate code that makes efficient use of SIMD processors in high performance platforms and embedded systems. Two main auto-vectorization techniques, superword-level parallelism vectorization (SLP) and loop-level vectorization (LLV), re- quire precise dependence analysis on arrays and structs in order to vectorize isomorphic scalar instructions and/or reduce dynamic dependence checks incurred at runtime. The alias analyses used in modern vectorizing compilers are either intra-procedural (without tracking inter-procedural data-flows) or inter-procedural (by using field-insensitive models, which are too imprecise in handling arrays and structs). This paper pro- poses an inter-procedural Loop-oriented Pointer Analysis, called LPA, for analyzing arrays and structs to support aggressive SLP and LLV optimizations. Unlike field-insensitive solutions that pre- allocate objects for each memory allocation site, our approach uses a fine-grained memory model to generate location sets based on how structs and arrays are accessed. LPA can precisely analyze ar- rays and nested aggregate structures to enable SIMD optimizations for large programs. By separating the location set generation as an independent concern from the rest of the pointer analysis, LPA is designed to reuse easily existing points-to resolution algorithms. We evaluate LPA using SLP and LLV, the two classic vectorization techniques on a set of 20 CPU2000/2006 benchmarks. For SLP, LPA enables it to vectorize a total of 133 more basic blocks, with an average of 12.09 per benchmark, resulting in the best speedup of 2.95\% for 173.applu. For LLV, LPA has reduced a total of 319 static bound checks, with an average of 22.79 per benchmark, resulting in the best speedup of 7.18\% for 177.mesa.}},
  url = {https://doi.org/10.1145/2907950.2907957},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1218,
  title = {A galois connection calculus for abstract interpretation}},
  author = {Cousot, Patrick` and Cousot, Radhia}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We introduce a Galois connection calculus for language independent specification of abstract interpretations used in programming language semantics, formal verification, and static analysis. This Galois connection calculus and its type system are typed by abstract interpretation.}},
  url = {https://doi.org/10.1145/2535838.2537850},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1219,
  title = {Sequential verification of serializability}},
  author = {Attiya, H. and Ramalingam, G. and Rinetzky, N.}},
  year = {2010}},
  journal = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Serializability is a commonly used correctness condition in concurrent programming. When a concurrent module is serializable, certain other properties of the module can be verified by considering only its sequential executions. In many cases, concurrent modules guarantee serializability by using standard locking protocols, such as tree locking or two-phase locking. Unfortunately, according to the existing literature, verifying that a concurrent module adheres to these protocols requires considering concurrent interleavings.In this paper, we show that adherence to a large class of locking protocols (including tree locking and two-phase locking) can be verified by considering only sequential executions. The main consequence of our results is that in many cases, the (manual or automatic) verification of serializability can itself be done using sequential reasoning .}},
  url = {https://doi.org/10.1145/1706299.1706305},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1220,
  title = {Bridging boolean and quantitative synthesis using smoothed proof search}},
  author = {Chaudhuri, Swarat and Clochard, Martin and Solar-Lezama, Armando}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a new technique for parameter synthesis under boolean and quantitative objectives. The input to the technique is a "sketch" --- a program with missing numerical parameters --- and a probabilistic assumption about the program's inputs. The goal is to automatically synthesize values for the parameters such that the resulting program satisfies: (1) a {boolean specification}},
  url = {https://doi.org/10.1145/2535838.2535859},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1221,
  title = {Fully reflexive intensional type analysis}},
  author = {Trifonov, Valery and Saha, Bratin and Shao, Zhong}},
  year = {2000}},
  journal = {Proceedings of the Fifth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Compilers for polymorphic languages can use runtime type inspection to support advanced implementation techniques such as tagless garbage collection, polymorphic marshalling, and flattened data structures. Intensional type analysis is a type-theoretic framework for expressing and certifying such type-analyzing computations. Unfortunately, existing approaches to intensional analysis do not work well on types with universal, existential, or fixpoint quantifiers. This makes it impossible to code applications such as garbage collection, persistence, or marshalling which must be able to examine the type of any runtime value.We present a typed intermediate language that supports fully reflexive intensional type analysis. By fully reflexive, we mean that type-analyzing operations are applicable to the type of any runtime value in the language. In particular, we provide both type-level and term-level constructs for analyzing quantified types. Our system supports structural induction on quantified types yet type checking remains decidable. We show how to use reflexive type analysis to support type-safe marshalling and how to generate certified type-analyzing object code.}},
  url = {https://doi.org/10.1145/351240.351248},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1222,
  title = {Reasoning about object-oriented programs that use subtypes}},
  author = {Leavens, Gary T. and Weihl, William E.}},
  year = {1990}},
  journal = {Proceedings of the European Conference on Object-Oriented Programming on Object-Oriented Programming Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Programmers informally reason about object-oriented programs by using subtype relationships to classify the behavior of objects of different types and by letting supertypes stand for all their subtypes. We describe formal specification and verification techniques for such programs that mimic these informal ideas. Our techniques are modular and extend standard techniques for reasoning about programs that use abstract data types. Semantic restrictions on subtype relationships guarantee the soundness of these techniques.}},
  url = {https://doi.org/10.1145/97945.97970},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1223,
  title = {Functional programming with structured graphs}},
  author = {Oliveira, Bruno C.d.S. and Cook, William R.}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents a new functional programming model for graph structures called structured graphs. Structured graphs extend conventional algebraic datatypes with explicit definition and manipulation of cycles and/or sharing, and offer a practical and convenient way to program graphs in functional programming languages like Haskell. The representation of sharing and cycles (edges) employs recursive binders and uses an encoding inspired by parametric higher-order abstract syntax. Unlike traditional approaches based on mutable references or node/edge lists, well-formedness of the graph structure is ensured statically and reasoning can be done with standard functional programming techniques. Since the binding structure is generic, we can define many useful generic combinators for manipulating structured graphs. We give applications and show how to reason about structured graphs.}},
  url = {https://doi.org/10.1145/2364527.2364541},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1224,
  title = {Symbolic finite state transducers: algorithms and applications}},
  author = {Veanes, Margus and Hooimeijer, Pieter and Livshits, Benjamin and Molnar, David and Bjorner, Nikolaj}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Finite automata and finite transducers are used in a wide range of applications in software engineering, from regular expressions to specification languages. We extend these classic objects with symbolic alphabets represented as parametric theories. Admitting potentially infinite alphabets makes this representation strictly more general and succinct than classical finite transducers and automata over strings. Despite this, the main operations, including composition, checking that a transducer is single-valued, and equivalence checking for single-valued symbolic finite transducers are effective given a decision procedure for the background theory. We provide novel algorithms for these operations and extend composition to symbolic transducers augmented with registers. Our base algorithms are unusual in that they are nonconstructive, therefore, we also supply a separate model generation algorithm that can quickly find counterexamples in the case two symbolic finite transducers are not equivalent. The algorithms give rise to a complete decidable algebra of symbolic transducers. Unlike previous work, we do not need any syntactic restriction of the formulas on the transitions, only a decision procedure. In practice we leverage recent advances in satisfiability modulo theory (SMT) solvers. We demonstrate our techniques on four case studies, covering a wide range of applications. Our techniques can synthesize string pre-images in excess of 8,000 bytes in roughly a minute, and we find that our new encodings significantly outperform previous techniques in succinctness and speed of analysis.}},
  url = {https://doi.org/10.1145/2103656.2103674},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1225,
  title = {Functional translation of a calculus of capabilities}},
  author = {Chargu\'{e}},
  year = {2008}},
  journal = {Proceedings of the 13th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Reasoning about imperative programs requires the ability to track aliasing and ownership properties. We present a type system that provides this ability, by using regions, capabilities, and singleton types. It is designed for a high-level calculus with higher-order functions, algebraic data structures, and references (mutable memory cells). The type system has polymorphism, yet does not require a value restriction, because capabilities act as explicit store typings.We exhibit a type-directed, type-preserving, and meaning-preserving translation of this imperative calculus into a pure calculus. Like the monadic translation, this is a store-passing translation. Here, however, the store is partitioned into multiple fragments, which are threaded through a computation only if they are relevant to it. Furthermore, the decomposition of the store into fragments can evolve dynamically to reflect ownership transfers.The translation offers deep insight about the inner workings and soundness of the type system. If coupled with a semantic model of its target calculus, it leads to a semantic model of its imperative source calculus. Furthermore, it provides a foundation for our long-term objective of designing a system for specifying and certifying imperative programs with dynamic memory allocation.}},
  url = {https://doi.org/10.1145/1411204.1411235},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1226,
  title = {Monadic functional reactive programming}},
  author = {van der Ploeg, Atze}},
  year = {2013}},
  journal = {Proceedings of the 2013 ACM SIGPLAN Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Functional Reactive Programming (FRP) is a way to program reactive systems in functional style, eliminating many of the problems that arise from imperative techniques. In this paper, we present an alternative FRP formulation that is based on the notion of a reactive computation: a monadic computation which may require the occurrence of external events to continue. A signal computation is a reactive computation that may also emit values. In contrast to signals in other FRP formulations, signal computations can end, leading to a monadic interface for sequencing signal phases. This interface has several advantages: routing is implicit, sequencing signal phases is easier and more intuitive than when using the switching combinators found in other FRP approaches, and dynamic lists require much less boilerplate code. In other FRP approaches, either the entire FRP expression is re-evaluated on each external stimulus, or impure techniques are used to prevent redundant re-computations. We show how Monadic FRP can be implemented straightforwardly in a purely functional way while preventing redundant re-computations.}},
  url = {https://doi.org/10.1145/2503778.2503783},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1227,
  title = {Calculating sized types}},
  author = {Chin, Wei-Ngan and Khoo, Siau-Cheng}},
  year = {1999}},
  journal = {Proceedings of the 2000 ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Based Program Manipulation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many program optimisations and analyses, such as array-bound checking, termination analysis, etc, depend on knowing the size of a function's input and output. However, size information can be difficult to compute. Firstly, accurate size computation requires detecting size relation between different inputs of a function. Secondly, different optimisations and analyses may require slightly different size information, and thus slightly different computation. Literature in size computation has mainly concentrated on size checking, instead of inferencing. In this paper, we provide a generic framework on which different size variants can be expressed and computed. We also describe an effective algorithm for inferring, instead of checking, size information. Size information are expressed in terms of Presburger formulae, and our algorithm utilises the Omega Calculator to compute as exact a size information as possible, within the linear arithmetic capability.}},
  url = {https://doi.org/10.1145/328690.328893},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1228,
  title = {Dynamic inference of static types for ruby}},
  author = {An, Jong-hoon (David) and Chaudhuri, Avik and Foster, Jeffrey S. and Hicks, Michael}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {There have been several efforts to bring static type inference to object-oriented dynamic languages such as Ruby, Python, and Perl. In our experience, however, such type inference systems are extremely difficult to develop, because dynamic languages are typically complex, poorly specified, and include features, such as eval and reflection, that are hard to analyze.In this paper, we introduce constraint-based dynamic type inference, a technique that infers static types based on dynamic program executions. In our approach, we wrap each run-time value to associate it with a type variable, and the wrapper generates constraints on this type variable when the wrapped value is used. This technique avoids many of the often overly conservative approximations of static tools, as constraints are generated based on how values are used during actual program runs. Using wrappers is also easy to implement, since we need only write a constraint resolution algorithm and a transformation to introduce the wrappers. The best part is that we can eat our cake, too: our algorithm will infer sound types as long as it observes every path through each method body---note that the number of such paths may be dramatically smaller than the number of paths through the program as a whole.We have developed Rubydust, an implementation of our algorithm for Ruby. Rubydust takes advantage of Ruby's dynamic features to implement wrappers as a language library. We applied Rubydust to a number of small programs and found it to be both easy to use and useful: Rubydust discovered 1 real type error, and all other inferred types were correct and readable.}},
  url = {https://doi.org/10.1145/1926385.1926437},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1229,
  title = {Verifying distributed systems: the operational approach}},
  author = {Ridge, Thomas}},
  year = {2009}},
  journal = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This work develops an integrated approach to the verification of behaviourally rich programs, founded directly on operational semantics. The power of the approach is demonstrated with a state-of-the-art verification of a core piece of distributed infrastructure, involving networking, a filesystem, and concurrent OCaml code. The formalization is in higher-order logic and proof support is provided by the HOL4 theorem prover.Difficult verification problems demand a wide range of techniques. Here these include ground and symbolic evaluation, local reasoning, separation, invariants, Hoare-style assertional reasoning, rely/guarantee, inductive reasoning about protocol correctness, multiple refinement, and linearizability. While each of these techniques is useful in isolation, they are even more so in combination. The first contribution of this paper is to present the operational approach and describe how existing techniques, including all those mentioned above, may be cleanly and precisely integrated in this setting.The second contribution is to show how to combine verifications of individual library functions with arbitrary and unknown user code in a compositional manner, focusing on the problems of private state and encapsulation.The third contribution is the example verification itself. The infrastructure must behave correctly under arbitrary patterns of host and network failure, whilst for performance reasons the code also includes data races on shared state. Both features make the verification particularly challenging.}},
  url = {https://doi.org/10.1145/1480881.1480934},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1230,
  title = {Optimizing abstract abstract machines}},
  author = {Glaze, Dionna and Labich, Nicholas and Might, Matthew and Van Horn, David}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The technique of abstracting abstract machines (AAM) provides a systematic approach for deriving computable approximations of evaluators that are easily proved sound. This article contributes a complementary step-by-step process for subsequently going from a naive analyzer derived under the AAM approach, to an efficient and correct implementation. The end result of the process is a two to three order-of-magnitude improvement over the systematically derived analyzer, making it competitive with hand-optimized implementations that compute fundamentally less precise results.}},
  url = {https://doi.org/10.1145/2500365.2500604},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1231,
  title = {Reflective facilities in Smalltalk-80}},
  author = {Foote, B. and Johnson, R. E.}},
  year = {1989}},
  journal = {Conference Proceedings on Object-Oriented Programming Systems, Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Computational reflection makes it easy to solve problems that are otherwise difficult to address in Smalltalk-80, such as the construction of monitors, distributed objects, and futures, and can allow experimentation with new inheritance, delegation, and protection schemes. Full reflection is expensive to implement. However, the ability to override method lookup can bring much of the power of reflection to languages like Smalltalk-80 at no cost in efficiency.}},
  url = {https://doi.org/10.1145/74877.74911},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1232,
  title = {Javari: adding reference immutability to Java}},
  author = {Tschantz, Matthew S. and Ernst, Michael D.}},
  year = {2005}},
  journal = {Proceedings of the 20th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper describes a type system that is capable of expressing and enforcing immutability constraints. The specific constraint expressed is that the abstract state of the object to which an immutable reference refers cannot be modified using that reference. The abstract state is (part of) the transitively reachable state: that is, the state of the object and all state reachable from it by following references. The type system permits explicitly excluding fields from the abstract state of an object. For a statically type-safe language, the type system guarantees reference immutability. If the language is extended with immutability downcasts, then run-time checks enforce the reference immutability constraints.This research builds upon previous research in language support for reference immutability. Improvements that are new in this paper include distinguishing the notions of assignability and mutability; integration with Java 5's generic types and with multi-dimensional arrays; a mutability polymorphism approach to avoiding code duplication; type-safe support for reflection and serialization; and formal type rules and type soundness proof for a core calculus. Furthermore, it retains the valuable features of the previous dialect, including usability by humans (as evidenced by experience with 160,000 lines of Javari code) and interoperability with Java and existing JVMs.}},
  url = {https://doi.org/10.1145/1094811.1094828},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1233,
  title = {On the simplicity of synthesizing linked data structure operations}},
  author = {Kurilova, Darya and Rayside, Derek}},
  year = {2013}},
  journal = {Proceedings of the 12th International Conference on Generative Programming: Concepts \&amp; Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We argue that synthesizing operations on recursive linked data structures is not as hard as it appears and is, in fact, within reach of current SAT-based synthesis techniques - with the addition of a simple approach that we describe to decompose the problem into smaller parts. To generate smaller pieces of code, i.e., shorter routines, is obviously easier than large and complex routines, and, also, there is more potential for automating the code synthesis.In this paper, we present a code generation algorithm for synthesizing operations of linked data structures and, as an example, describe how the proposed algorithm works to synthesize operations of an AVL tree.}},
  url = {https://doi.org/10.1145/2517208.2517225},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1234,
  title = {An experimental survey of energy management across the stack}},
  author = {Kambadur, Melanie and Kim, Martha A.}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Modern demand for energy-efficient computation has spurred research at all levels of the stack, from devices to microarchitecture, operating systems, compilers, and languages. Unfortunately, this breadth has resulted in a disjointed space, with technologies at different levels of the system stack rarely compared, let alone coordinated.This work begins to remedy the problem, conducting an experimental survey of the present state of energy management across the stack. Focusing on settings that are exposed to software, we measure the total energy, average power, and execution time of 41 benchmark applications in 220 configurations, across a total of 200,000 program executions.Some of the more important findings of the survey include that effective parallelization and compiler optimizations have the potential to save far more energy than Linux's frequency tuning algorithms; that certain non-complementary energy strategies can undercut each other's savings by half when combined; and that while the power impacts of most strategies remain constant across applications, the runtime impacts vary, resulting in inconsistent energy impacts.}},
  url = {https://doi.org/10.1145/2660193.2660196},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1235,
  title = {Inductive invariant generation via abductive inference}},
  author = {Dillig, Isil and Dillig, Thomas and Li, Boyang and McMillan, Ken}},
  year = {2013}},
  journal = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents a new method for generating inductive loop invariants that are expressible as boolean combinations of linear integer constraints. The key idea underlying our technique is to perform a backtracking search that combines Hoare-style verification condition generation with a logical abduction procedure based on quantifier elimination to speculate candidate invariants. Starting with true, our method iteratively strengthens loop invariants until they are inductive and strong enough to verify the program. A key feature of our technique is that it is lazy: It only infers those invariants that are necessary for verifying program correctness. Furthermore, our technique can infer arbitrary boolean combinations (including disjunctions) of linear invariants. We have implemented the proposed approach in a tool called HOLA. Our experiments demonstrate that HOLA can infer interesting invariants that are beyond the reach of existing state-of-the-art invariant generation tools.}},
  url = {https://doi.org/10.1145/2509136.2509511},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1236,
  title = {JET: exception checking in the Java native interface}},
  author = {Li, Siliang and Tan, Gang}},
  year = {2011}},
  journal = {Proceedings of the 2011 ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Java's type system enforces exception-checking rules that stipulate a checked exception thrown by a method must be declared in the throws clause of the method. Software written in Java often invokes native methods through the use of the Java Native Interface (JNI). Java's type system, however, cannot enforce the same exception-checking rules on Java exceptions raised in native methods. This gap makes Java software potentially buggy and often difficult to debug when an exception is raised in native code. In this paper, we propose a complete static-analysis framework called JET to extend exception-checking rules even on native code. The framework has a two-stage design where the first stage throws away a large portion of irrelevant code so that the second stage, a fine-grained analysis, can concentrate on a small set of code for accurate bug finding. This design achieves both high efficiency and accuracy. We have applied JET on a set of benchmark programs with a total over 227K lines of source code and identified 12 inconsistent native-method exception declarations.}},
  url = {https://doi.org/10.1145/2048066.2048095},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1237,
  title = {Class hierarchy complementation: soundly completing a partial type graph}},
  author = {Balatsouras, George and Smaragdakis, Yannis}},
  year = {2013}},
  journal = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present the problem of class hierarchy complementation: given a partially known hierarchy of classes together with subtyping constraints ("A has to be a transitive subtype of B") complete the hierarchy so that it satisfies all constraints. The problem has immediate practical application to the analysis of partial programs--e.g., it arises in the process of providing a sound handling of "phantom classes" in the Soot program analysis framework. We provide algorithms to solve the hierarchy complementation problem in the single inheritance and multiple inheritance settings. We also show that the problem in a language such as Java, with single inheritance but multiple subtyping and distinguished class vs. interface types, can be decomposed into separate single- and multiple-subtyping instances. We implement our algorithms in a tool, JPhantom, which complements partial Java bytecode programs so that the result is guaranteed to satisfy the Java verifier requirements. JPhantom is highly scalable and runs in mere seconds even for large input applications and complex constraints (with a maximum of 14s for a 19MB binary).}},
  url = {https://doi.org/10.1145/2509136.2509530},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1238,
  title = {Space-efficient implementation of nested parallelism}},
  author = {Narlikar, Girija J. and Blelloch, Guy E.}},
  year = {1997}},
  journal = {Proceedings of the Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many of today's high level parallel languages support dynamic, fine-grained parallelism. These languages allow the user to expose all the parallelism in the program, which is typically of a much higher degree than the number of processors. Hence an efficient scheduling algorithm is required to assign computations to processors at runtime. Besides having low overheads and good load balancing, it is important for the scheduling algorithm to minimize the space usage of the parallel program. This paper presents a scheduling algorithm that is provably space-efficient and time-efficient for nested parallel languages. In addition to proving the space and time bounds of the parallel schedule generated by the algorithm, we demonstrate that it is efficient in practice. We have implemented a runtime system that uses our algorithm to schedule parallel threads. The results of executing parallel programs on this system show that our scheduling algorithm significantly reduces memory usage compared to previous techniques, without compromising performance.}},
  url = {https://doi.org/10.1145/263764.263770},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1239,
  title = {Effective race detection for event-driven programs}},
  author = {Raychev, Veselin and Vechev, Martin and Sridharan, Manu}},
  year = {2013}},
  journal = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Like shared-memory multi-threaded programs, event-driven programs such as client-side web applications are susceptible to data races that are hard to reproduce and debug. Race detection for such programs is hampered by their pervasive use of ad hoc synchronization, which can lead to a prohibitive number of false positives. Race detection also faces a scalability challenge, as a large number of short-running event handlers can quickly overwhelm standard vector-clock-based techniques.This paper presents several novel contributions that address both of these challenges. First, we introduce race coverage, a systematic method for exposing ad hoc synchronization and other (potentially harmful) races to the user, significantly reducing false positives. Second, we present an efficient connectivity algorithm for computing race coverage. The algorithm is based on chain decomposition and leverages the structure of event-driven programs to dramatically decrease the overhead of vector clocks.We implemented our techniques in a tool called EventRacer and evaluated it on a number of public web sites. The results indicate substantial performance and precision improvements of our approach over the state-of-the-art. Using EventRacer, we found many harmful races, most of which are beyond the reach of current techniques.}},
  url = {https://doi.org/10.1145/2509136.2509538},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1240,
  title = {CFD — A FORTRAN-like language for the ILLIAC IV}},
  author = {Stevens, K. G.}},
  year = {1975}},
  journal = {Proceedings of the Conference on Programming Languages and Compilers for Parallel and Vector Machines}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A FORTRAN-like language, CFD, is described as it relates to the parallel hardware of ILLIAC IV. Included are the basic goals of the authors of CFD and some of the author's own experiences during the evolution of the language. CFD was developed for the ILLIAC IV to provide programmers with a language similar to commonly used versions of FORTRAN. Many of the deviations from FORTRAN found in CFD were dictated by the parallel hardware architecture of ILLIAC IV.}},
  url = {https://doi.org/10.1145/800026.808404},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1241,
  title = {Types, potency, and idempotency: why nonlinearity and amnesia make a type system work}},
  author = {Neergaard, Peter M\o{}},
  year = {2004}},
  journal = {Proceedings of the Ninth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Useful type inference must be faster than normalization. Otherwise, you could check safety conditions by running the program. We analyze the relationship between bounds on normalization and type inference. We show how the success of type inference is fundamentally related to the amnesia of the type system: the nonlinearity by which all instances of a variable are constrained to have the same type.Recent work on intersection types has advocated their usefulness for static analysis and modular compilation. We analyze System-I (and some instances of its descendant, System E), an intersection type system with a type inference algorithm. Because System-I lacks idempotency, each occurrence of a variable requires a distinct type. Consequently, type inference is equivalent to normalization in every single case, and time bounds on type inference and normalization are identical. Similar relationships hold for other intersection type systems without idempotency.The analysis is founded on an investigation of the relationship between linear logic and intersection types. We show a lockstep correspondence between normalization and type inference. The latter shows the promise of intersection types to facilitate static analyses of varied granularity, but also belies an immense challenge: to add amnesia to such analysis without losing all of its benefits.}},
  url = {https://doi.org/10.1145/1016850.1016871},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1242,
  title = {Symbolic optimization with SMT solvers}},
  author = {Li, Yi and Albarghouthi, Aws and Kincaid, Zachary and Gurfinkel, Arie and Chechik, Marsha}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The rise in efficiency of Satisfiability Modulo Theories (SMT) solvers has created numerous uses for them in software verification, program synthesis, functional programming, refinement types, etc. In all of these applications, SMT solvers are used for generating satisfying assignments (e.g., a witness for a bug) or proving unsatisfiability/validity(e.g., proving that a subtyping relation holds). We are often interested in finding not just an arbitrary satisfying assignment, but one that optimizes (minimizes/maximizes) certain criteria. For example, we might be interested in detecting program executions that maximize energy usage (performance bugs), or synthesizing short programs that do not make expensive API calls. Unfortunately, none of the available SMT solvers offer such optimization capabilities.In this paper, we present SYMBA, an efficient SMT-based optimization algorithm for objective functions in the theory of linear real arithmetic (LRA). Given a formula φ and an objective function t, SYMBA finds a satisfying assignment of φthat maximizes the value of t. SYMBA utilizes efficient SMT solvers as black boxes. As a result, it is easy to implement and it directly benefits from future advances in SMT solvers. Moreover, SYMBA can optimize a set of objective functions, reusing information between them to speed up the analysis. We have implemented SYMBA and evaluated it on a large number of optimization benchmarks drawn from program analysis tasks. Our results indicate the power and efficiency of SYMBA in comparison with competing approaches, and highlight the importance of its multi-objective-function feature.}},
  url = {https://doi.org/10.1145/2535838.2535857},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1243,
  title = {Towards a program logic for JavaScript}},
  author = {Gardner, Philippa Anne and Maffeis, Sergio and Smith, Gareth David}},
  year = {2012}},
  journal = {Proceedings of the 39th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {JavaScript has become the most widely used language for client-side web programming. The dynamic nature of JavaScript makes understanding its code notoriously difficult, leading to buggy programs and a lack of adequate static-analysis tools. We believe that logical reasoning has much to offer JavaScript: a simple description of program behaviour, a clear understanding of module boundaries, and the ability to verify security contracts. We introduce a program logic for reasoning about a broad subset of JavaScript, including challenging features such as prototype inheritance and "with". We adapt ideas from separation logic to provide tractable reasoning about JavaScript code: reasoning about easy programs is easy; reasoning about hard programs is possible. We prove a strong soundness result. All libraries written in our subset and proved correct with respect to their specifications will be well-behaved, even when called by arbitrary JavaScript code.}},
  url = {https://doi.org/10.1145/2103656.2103663},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1244,
  title = {Variance analyses from invariance analyses}},
  author = {Berdine, Josh and Chawdhary, Aziem and Cook, Byron and Distefano, Dino and O'Hearn, Peter}},
  year = {2007}},
  journal = {Proceedings of the 34th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {An invariance assertion for a program location l is a statement that always holds at l during execution of the program. Program invariance analyses infer invariance assertions that can be useful when trying to prove safety properties. We use the term variance assertion to mean a statement that holds between any state at l and any previous state that was also at l. This paper is concerned with the development of analyses for variance assertions and their application to proving termination and liveness properties. We describe a method of constructing program variance analyses from invariance analyses. If we change the underlying invariance analysis, we get a different variance analysis. We describe several applications of the method, including variance analyses using linear arithmetic and shape analysis. Using experimental results we demonstrate that these variance analyses give rise to a new breed of termination provers which are competitive with and sometimes better than today's state-of-the-art termination provers.}},
  url = {https://doi.org/10.1145/1190216.1190249},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1245,
  title = {On teaching *how to design programs*: observations from a newcomer}},
  author = {Ramsey, Norman}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents a personal, qualitative case study of a first course using How to Design Programs and its functional teaching languages. The paper reconceptualizes the book's six-step design process as an eight-step design process ending in a new "review and refactor" step. It recommends specific approaches to students' difficulties with function descriptions, function templates, data examples, and other parts of the design process. It&nbsp;connects the process to interactive "world programs." It recounts significant, informative missteps in course design and delivery. Finally, it identifies some unsolved teaching problems and some potential solutions.}},
  url = {https://doi.org/10.1145/2628136.2628137},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1246,
  title = {Efficient divide-and-conquer parsing of practical context-free languages}},
  author = {Bernardy, Jean-Philippe and Claessen, Koen}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a divide-and-conquer algorithm for parsing context-free languages efficiently. Our algorithm is an instance of Valiant's (1975), who reduced the problem of parsing to matrix multiplications. We show that, while the conquer step of Valiant's is O(n3) in the worst case, it improves to O(logn3), under certain conditions satisfied by many useful inputs. These conditions occur for example in program texts written by humans. The improvement happens because the multiplications involve an overwhelming majority of empty matrices. This result is relevant to modern computing: divide-and-conquer algorithms can be parallelized relatively easily.}},
  url = {https://doi.org/10.1145/2500365.2500576},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1247,
  title = {Depending on types}},
  author = {Weirich, Stephanie}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Is Haskell a dependently typed programming language? Should it be? GHC's many type-system features, such as Generalized Algebraic Datatypes (GADTs), datatype promotion, multiparameter type classes, and type families, give programmers the ability to encode domain-specific invariants in their types. Clever Haskell programmers have used these features to enhance the reasoning capabilities of static type checking. But really, how far have we come? Could we do more?In this talk, I will discuss dependently typed programming in Haskell, through examples, analysis and comparisons with modern full-spectrum dependently typed languages, such as Coq, Agda and Idris. What sorts of dependently typed programming can be done in Haskell now? What could GHC learn from these languages? Conversely, what lessons can GHC offer in return?}},
  url = {https://doi.org/10.1145/2628136.2631168},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1248,
  title = {Estimating types in binaries using predictive modeling}},
  author = {Katz, Omer and El-Yaniv, Ran and Yahav, Eran}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Reverse engineering is an important tool in mitigating vulnerabilities in binaries. As a lot of software is developed in object-oriented languages, reverse engineering of object-oriented code is of critical importance. One of the major hurdles in reverse engineering binaries compiled from object-oriented code is the use of dynamic dispatch. In the absence of debug information, any dynamic dispatch may seem to jump to many possible targets, posing a significant challenge to a reverse engineer trying to track the program flow. We present a novel technique that allows us to statically determine the likely targets of virtual function calls. Our technique uses object tracelets – statically constructed sequences of operations performed on an object – to capture potential runtime behaviors of the object. Our analysis automatically pre-labels some of the object tracelets by relying on instances where the type of an object is known. The resulting type-labeled tracelets are then used to train a statistical language model (SLM) for each type.We then use the resulting ensemble of SLMs over unlabeled tracelets to generate a ranking of their most likely types, from which we deduce the likely targets of dynamic dispatches.We have implemented our technique and evaluated it over real-world C++ binaries. Our evaluation shows that when there are multiple alternative targets, our approach can drastically reduce the number of targets that have to be considered by a reverse engineer.}},
  url = {https://doi.org/10.1145/2837614.2837674},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1249,
  title = {A very modal model of a modern, major, general type system}},
  author = {Appel, Andrew W. and Melli\`{e}},
  year = {2007}},
  journal = {Proceedings of the 34th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a model of recursive and impredicatively quantified types with mutable references. We interpret in this model all of the type constructors needed for typed intermediate languages and typed assembly languages used for object-oriented and functional languages. We establish in this purely semantic fashion a soundness proof of the typing systems underlying these TILs and TALs---ensuring that every well-typed program is safe. The technique is generic, and applies to any small-step semantics including λ-calculus, labeled transition systems, and von Neumann machines. It is also simple, and reduces mainly to defining a Kripke semantics of the G\"{o}},
  url = {https://doi.org/10.1145/1190216.1190235},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1250,
  title = {Control-flow analysis of function calls and returns by abstract interpretation}},
  author = {Midtgaard, Jan and Jensen, Thomas P.}},
  year = {2009}},
  journal = {Proceedings of the 14th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We derive a control-flow analysis that approximates the interprocedural control-flow of both function calls and returns in the presence of first-class functions and tail-call optimization. In addition to an abstract environment, our analysis computes for each expression an abstract control stack, effectively approximating where function calls return across optimized tail calls. The analysis is systematically calculated by abstract interpretation of the stack-based CaEK abstract machine of Flanagan et al. using a series of Galois connections. Abstract interpretation provides a unifying setting in which we 1) prove the analysis equivalent to the composition of a continuation-passing style (CPS) transformation followed by an abstract interpretation of a stack-less CPS machine, and 2) extract an equivalent constraint-based formulation, thereby providing a rational reconstruction of a constraint-based control-flow analysis from abstract interpretation principles.}},
  url = {https://doi.org/10.1145/1596550.1596592},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1251,
  title = {Don't sweat the small stuff: formal verification of C code without the pain}},
  author = {Greenaway, David and Lim, Japheth and Andronick, June and Klein, Gerwin}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present an approach for automatically generating provably correct abstractions from C source code that are useful for practical implementation verification. The abstractions are easier for a human verification engineer to reason about than the implementation and increase the productivity of interactive code proof. We guarantee soundness by automatically generating proofs that the abstractions are correct.In particular, we show two key abstractions that are critical for verifying systems-level C code: automatically turning potentially overflowing machine-word arithmetic into ideal integers, and transforming low-level C pointer reasoning into separate abstract heaps. Previous work carrying out such transformations has either done so using unverified translations, or required significant proof engineering effort.We implement these abstractions in an existing proof-producing specification transformation framework named AutoCorres, developed in Isabelle/HOL, and demonstrate its effectiveness in a number of case studies. We show scalability on multiple OS microkernels, and we show how our changes to AutoCorres improve productivity for total correctness by porting an existing high-level verification of the Schorr-Waite algorithm to a low-level C implementation with minimal effort.}},
  url = {https://doi.org/10.1145/2594291.2594296},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1252,
  title = {Program verification using templates over predicate abstraction}},
  author = {Srivastava, Saurabh and Gulwani, Sumit}},
  year = {2009}},
  journal = {Proceedings of the 30th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We address the problem of automatically generating invariants with quantified and boolean structure for proving the validity of given assertions or generating pre-conditions under which the assertions are valid. We present three novel algorithms, having different strengths, that combine template and predicate abstraction based formalisms to discover required sophisticated program invariants using SMT solvers.Two of these algorithms use an iterative approach to compute fixed-points (one computes a least fixed-point and the other computes a greatest fixed-point), while the third algorithm uses a constraint based approach to encode the fixed-point. The key idea in all these algorithms is to reduce the problem of invariant discovery to that of finding optimal solutions for unknowns (over conjunctions of some predicates from a given set) in a template formula such that the formula is valid.Preliminary experiments using our implementation of these algorithms show encouraging results over a benchmark of small but complicated programs. Our algorithms can verify program properties that, to our knowledge, have not been automatically verified before. In particular, our algorithms can generate full correctness proofs for sorting algorithms (which requires nested universally-existentially quantified invariants) and can also generate preconditions required to establish worst-case upper bounds of sorting algorithms. Furthermore, for the case of previously considered properties, in particular sortedness in sorting algorithms, our algorithms take less time than reported by previous techniques.}},
  url = {https://doi.org/10.1145/1542476.1542501},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1253,
  title = {GPU-based NFA implementation for memory efficient high speed regular expression matching}},
  author = {Zu, Yuan and Yang, Ming and Xu, Zhonghu and Wang, Lin and Tian, Xin and Peng, Kunyang and Dong, Qunfeng}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Regular expression pattern matching is the foundation and core engine of many network functions, such as network intrusion detection, worm detection, traffic analysis, web applications and so on. DFA-based solutions suffer exponentially exploding state space and cannot be remedied without sacrificing matching speed. Given this scalability problem of DFA-based methods, there has been increasing interest in NFA-based methods for memory efficient regular expression matching. To achieve high matching speed using NFA, it requires potentially massive parallel processing, and hence represents an ideal programming task on Graphic Processor Unit (GPU). Based on in-depth understanding of NFA properties as well as GPU architecture, we propose effective methods for fitting NFAs into GPU architecture through proper data structure and parallel programming design, so that GPU's parallel processing power can be better utilized to achieve high speed regular expression matching. Experiment results demonstrate that, compared with the existing GPU-based NFA implementation method [9], our proposed methods can boost matching speed by 29~46 times, consistently yielding above 10Gbps matching speed on NVIDIA GTX-460 GPU. Meanwhile, our design only needs a small amount of memory space, growing exponentially more slowly than DFA size. These results make our design an effective solution for memory efficient high speed regular expression matching, and clearly demonstrate the power and potential of GPU as a platform for memory efficient high speed regular expression matching.}},
  url = {https://doi.org/10.1145/2145816.2145833},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1254,
  title = {Battery transition systems}},
  author = {Boker, Udi and Henzinger, Thomas A. and Radhakrishna, Arjun}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The analysis of the energy consumption of software is an important goal for quantitative formal methods. Current methods, using weighted transition systems or energy games, model the energy source as an ideal resource whose status is characterized by one number, namely the amount of remaining energy. Real batteries, however, exhibit behaviors that can deviate substantially from an ideal energy resource. Based on a discretization of a standard continuous battery model, we introduce {em battery transition systems}},
  url = {https://doi.org/10.1145/2535838.2535875},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1255,
  title = {Counting solutions to Presburger formulas: how and why}},
  author = {Pugh, William}},
  year = {1994}},
  journal = {Proceedings of the ACM SIGPLAN 1994 Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We describe methods that are able to count the number of integer solutions to selected free variables of a Presburger formula, or sum a polynomial over all integer solutions of selected free variables of a Presburger formula. This answer is given symbolically, in terms of symbolic constants (the remaining free variables in the Presburger formula).For example, we can create a Presburger formula who's solutions correspond to the iterations of a loop. By counting these, we obtain an estimate of the execution time of the loop.In more complicated applications, we can create Presburger formulas who's solutions correspond to the distinct memory locations or cache lines touched by a loop, the flops executed by a loop, or the array elements that need to be communicated at a particular point in a distributed computation. By counting the number of solutions, we can evaluate the computation/memory balance of a computation, determine if a loop is load balanced and evaluate message traffic and allocate message buffers.}},
  url = {https://doi.org/10.1145/178243.178254},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1256,
  title = {Learning refinement types}},
  author = {Zhu, He and Nori, Aditya V. and Jagannathan, Suresh}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We propose the integration of a random test generation system (capable of discovering program bugs) and a refinement type system (capable of expressing and verifying program invariants), for higher-order functional programs, using a novel lightweight learning algorithm as an effective intermediary between the two. Our approach is based on the well-understood intuition that useful, but difficult to infer, program properties can often be observed from concrete program states generated by tests; these properties act as likely invariants, which if used to refine simple types, can have their validity checked by a refinement type checker. We describe an implementation of our technique for a variety of benchmarks written in ML, and demonstrate its effectiveness in inferring and proving useful invariants for programs that express complex higher-order control and dataflow.}},
  url = {https://doi.org/10.1145/2784731.2784766},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1257,
  title = {Automating grammar comparison}},
  author = {Madhavan, Ravichandhran and Mayer, Mika\"{e}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We consider from a practical perspective the problem of checking equivalence of context-free grammars. We present techniques for proving equivalence, as well as techniques for finding counter-examples that establish non-equivalence. Among the key building blocks of our approach is a novel algorithm for efficiently enumerating and sampling words and parse trees from arbitrary context-free grammars; the algorithm supports polynomial time random access to words belonging to the grammar. Furthermore, we propose an algorithm for proving equivalence of context-free grammars that is complete for LL grammars, yet can be invoked on any context-free grammar, including ambiguous grammars. Our techniques successfully find discrepancies between different syntax specifications of several real-world languages, and are capable of detecting fine-grained incremental modifications performed on grammars. Our evaluation shows that our tool improves significantly on the existing available state of the art tools. In addition, we used these algorithms to develop an online tutoring system for grammars that we then used in an undergraduate course on computer language processing. On questions involving grammar constructions, our system was able to automatically evaluate the correctness of 95\% of the solutions submitted by students: it disproved 74\% of cases and proved 21\% of them.}},
  url = {https://doi.org/10.1145/2814270.2814304},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1258,
  title = {Casper: an efficient approach to call trace collection}},
  author = {Wu, Rongxin and Xiao, Xiao and Cheung, Shing-Chi and Zhang, Hongyu and Zhang, Charles}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Call traces, i.e., sequences of function calls and returns, are fundamental to a wide range of program analyses such as bug reproduction, fault diagnosis, performance analysis, and many others. The conventional approach to collect call traces that instruments each function call and return site incurs large space and time overhead. Our approach aims at reducing the recording overheads by instrumenting only a small amount of call sites while keeping the capability of recovering the full trace. We propose a call trace model and a logged call trace model based on an LL(1) grammar, which enables us to define the criteria of a feasible solution to call trace collection. Based on the two models, we prove that to collect call traces with minimal instrumentation is an NP-hard problem. We then propose an efficient approach to obtaining a suboptimal solution. We implemented our approach as a tool Casper and evaluated it using the DaCapo benchmark suite. The experiment results show that our approach causes significantly lower runtime (and space) overhead than two state-of-the-arts approaches.}},
  url = {https://doi.org/10.1145/2837614.2837619},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1259,
  title = {Dataflow analysis for concurrent programs using datarace detection}},
  author = {Chugh, Ravi and Voung, Jan W. and Jhala, Ranjit and Lerner, Sorin}},
  year = {2008}},
  journal = {Proceedings of the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Dataflow analyses for concurrent programs differ from their single-threaded counterparts in that they must account for shared memory locations being overwritten by concurrent threads. Existing dataflow analysis techniques for concurrent programs typically fall at either end of a spectrum: at one end, the analysis conservatively kills facts about all data that might possibly be shared by multiple threads; at the other end, a precise thread-interleaving analysis determines which data may be shared, and thus which dataflow facts must be invalidated. The former approach can suffer from imprecision, whereas the latter does not scale.We present RADAR, a framework that automatically converts a dataflow analysis for sequential programs into one that is correct for concurrent programs. RADAR uses a race detection engine to kill the dataflow facts, generated and propagated by the sequential analysis, that become invalid due to concurrent writes. Our approach of factoring all reasoning about concurrency into a race detection engine yields two benefits. First, to obtain analyses for code using new concurrency constructs, one need only design a suitable race detection engine for the constructs. Second, it gives analysis designers an easy way to tune the scalability and precision of the overall analysis by only modifying the race detection engine. We describe the RADAR framework and its implementation using a pre-existing race detection engine. We show how RADAR was used to generate a concurrent version of a null-pointer dereference analysis, and we analyze the result of running the generated concurrent analysis on several benchmarks.}},
  url = {https://doi.org/10.1145/1375581.1375620},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1260,
  title = {A dynamic evaluation of the precision of static heap abstractions}},
  author = {Liang, Percy and Tripp, Omer and Naik, Mayur and Sagiv, Mooly}},
  year = {2010}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The quality of a static analysis of heap-manipulating programs is largely determined by its heap abstraction. Object allocation sites are a commonly-used abstraction, but are too coarse for some clients. The goal of this paper is to investigate how various refinements of allocation sites can improve precision. In particular, we consider abstractions that use call stack, object recency, and heap connectivity information. We measure the precision of these abstractions dynamically for four different clients motivated by concurrency and on nine Java programs chosen from the DaCapo benchmark suite. Our dynamic results shed new light on aspects of heap abstractions that matter for precision, which allows us to more effectively navigate the large space of possible heap abstractions}},
  url = {https://doi.org/10.1145/1869459.1869494},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1261,
  title = {Abstract satisfaction}},
  author = {D'Silva, Vijay and Haller, Leopold and Kroening, Daniel}},
  year = {2014}},
  journal = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This article introduces an abstract interpretation framework that codifies the operations in SAT and SMT solvers in terms of lattices, transformers and fixed points. We develop the idea that a formula denotes a set of models in a universe of structures. This set of models has characterizations as fixed points of deduction, abduction and quantification transformers. A wide range of satisfiability procedures can be understood as computing and refining approximations of such fixed points. These include procedures in the DPLL family, those for preprocessing and inprocessing in SAT solvers, decision procedures for equality logics, weak arithmetics, and procedures for approximate quantification. Our framework provides a unified, mathematical basis for studying and combining program analysis and satisfiability procedures. A practical benefit of our work is a new, logic-agnostic architecture for implementing solvers.}},
  url = {https://doi.org/10.1145/2535838.2535868},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1262,
  title = {Detecting defects in object-oriented designs: using reading techniques to increase software quality}},
  author = {Travassos, Guilherme and Shull, Forrest and Fredericks, Michael and Basili, Victor R.}},
  year = {1999}},
  journal = {Proceedings of the 14th ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Inspections can be used to identify defects in software artifacts. In this way, inspection methods help to improve software quality, especially when used early in software development. Inspections of software design may be especially crucial since design defects (problems of correctness and completeness with respect to the requirements, internal consistency, or other quality attributes) can directly affect the quality of, and effort required for, the implementation.We have created a set of “reading techniques” (so called because they help a reviewer to “read” a design artifact for the purpose of finding relevant information) that gives specific and practical guidance for identifying defects in Object-Oriented designs. Each reading technique in the family focuses the reviewer on some aspect of the design, with the goal that an inspection team applying the entire family should achieve a high degree of coverage of the design defects.In this paper, we present an overview of this new set of reading techniques. We discuss how some elements of these techniques are based on empirical results concerning an analogous set of reading techniques that supports defect detection in requirements documents. We present an initial empirical study that was run to assess the feasibility of these new techniques, and discuss the changes made to the latest version of the techniques based on the results of this study.}},
  url = {https://doi.org/10.1145/320384.320389},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1263,
  title = {Online partial evaluation for shift and reset}},
  author = {Asai, Kenichi}},
  year = {2002}},
  journal = {Proceedings of the 2002 ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Based Program Manipulation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents an online partial evaluator for the λ-calculus with the delimited continuation constructs shift and reset. We first give the semantics of the delimited continuation constructs in two ways: one by writing a continuation passing style (CPS) interpreter and the other by transforming them into CPS. We then combine them to obtain a partial evaluator written in CPS which produces the result in CPS. By transforming this partial evaluator back into a direct style (DS) in two steps, we obtain a DS to DS partial evaluator written in DS. The correctness of the partial evaluator is not yet formally proven. The difficulty comes from the fact that the partial evaluator is written using shift and reset. The method for reasoning about such programs is not yet established. However, the development of the partial evaluator is detailed in the paper to give a degree of confidence that it behaves as we expect.}},
  url = {https://doi.org/10.1145/503032.503034},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1264,
  title = {Higher-order and tuple-based massively-parallel prefix sums}},
  author = {Maleki, Sepideh and Yang, Annie and Burtscher, Martin}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Prefix sums are an important parallel primitive, especially in massively-parallel programs. This paper discusses two orthogonal generalizations thereof, which we call higher-order and tuple-based prefix sums. Moreover, it describes and evaluates SAM, a GPU-friendly algorithm for computing prefix sums and other scans that directly supports higher orders and tuple values. Its templated CUDA implementation unifies all of these computations in a single 100-statement kernel. SAM is communication-efficient in the sense that it minimizes main-memory accesses. When computing prefix sums of a million or more values, it outperforms Thrust and CUDPP on both a Titan X and a K40 GPU. On the Titan X, SAM reaches memory-copy speeds for large input sizes, which cannot be surpassed. SAM outperforms CUB, the currently fastest conventional prefix sum implementation, by up to a factor of 2.9 on eighth-order prefix sums and by up to a factor of 2.6 on eight-tuple prefix sums.}},
  url = {https://doi.org/10.1145/2908080.2908089},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1265,
  title = {Calculating threesomes, with blame}},
  author = {Garcia, Ronald}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Coercions and threesomes both enable a language to combine static and dynamic types while avoiding cast-based space leaks. Coercion calculi elegantly specify space-efficient cast behavior, even when augmented with blame tracking, but implementing their semantics directly is difficult. Threesomes, on the other hand, have a straightforward recursive implementation, but endowing them with blame tracking is challenging. In this paper, we show that you can use that elegant spec to produce that straightforward implementation: we use the coercion calculus to derive threesomes with blame. In particular, we construct novel threesome calculi for blame tracking strategies that detect errors earlier, catch more errors, and reflect an intuitive conception of safe and unsafe casts based on traditional subtyping.}},
  url = {https://doi.org/10.1145/2500365.2500603},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1266,
  title = {Hybrid context-sensitivity for points-to analysis}},
  author = {Kastrinis, George and Smaragdakis, Yannis}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Context-sensitive points-to analysis is valuable for achieving high precision with good performance. The standard flavors of context-sensitivity are call-site-sensitivity (kCFA) and object-sensitivity. Combining both flavors of context-sensitivity increases precision but at an infeasibly high cost. We show that a selective combination of call-site- and object-sensitivity for Java points-to analysis is highly profitable. Namely, by keeping a combined context only when analyzing selected language features, we can closely approximate the precision of an analysis that keeps both contexts at all times. In terms of speed, the selective combination of both kinds of context not only vastly outperforms non-selective combinations but is also faster than a mere object-sensitive analysis. This result holds for a large array of analyses (e.g., 1-object-sensitive, 2-object-sensitive with a context-sensitive heap, type-sensitive) establishing a new set of performance/precision sweet spots.}},
  url = {https://doi.org/10.1145/2491956.2462191},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1267,
  title = {Voyeur: graphical views of parallel programs}},
  author = {Socha, David and Bailey, Mary L. and Notkin, David}},
  year = {1988}},
  journal = {Proceedings of the 1988 ACM SIGPLAN and SIGOPS Workshop on Parallel and Distributed Debugging}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Voyeur is a prototype system that facilitates the construction of application-specific, visual views of parallel programs. These views range from textual views showing the contents of variables to graphical maps of the state of the computational domain of the program. These views have been instrumental in quickly detecting bugs that would have been difficult to detect otherwise.}},
  url = {https://doi.org/10.1145/68210.69235},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1268,
  title = {Modular session types for distributed object-oriented programming}},
  author = {Gay, Simon J. and Vasconcelos, Vasco T. and Ravara, Ant\'{o}},
  year = {2010}},
  journal = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Session types allow communication protocols to be specified type-theoretically so that protocol implementations can be verified by static type-checking. We extend previous work on session types for distributed object-oriented languages in three ways. (1) We attach a session type to a class definition, to specify the possible sequences of method calls. (2) We allow a session type (protocol) implementation to be modularized , i.e. partitioned into separately-callable methods. (3) We treat session-typed communication channels as objects, integrating their session types with the session types of classes. The result is an elegant unification of communication channels and their session types, distributed object-oriented programming, and a form of typestates supporting non-uniform objects, i.e. objects that dynamically change the set of available methods. We define syntax, operational semantics, a sound type system, and a correct and complete type checking algorithm for a small distributed class-based object-oriented language. Static typing guarantees that both sequences of messages on channels, and sequences of method calls on objects, conform to type-theoretic specifications, thus ensuring type-safety. The language includes expected features of session types, such as delegation, and expected features of object-oriented programming, such as encapsulation of local state. We also describe a prototype implementation as an extension of Java.}},
  url = {https://doi.org/10.1145/1706299.1706335},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1269,
  title = {Partial-coherence abstractions for relaxed memory models}},
  author = {Kuperstein, Michael and Vechev, Martin and Yahav, Eran}},
  year = {2011}},
  journal = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present an approach for automatic verification and fence inference in concurrent programs running under relaxed memory models. Verification under relaxed memory models is a hard problem. Given a finite state program and a safety specification, verifying that the program satisfies the specification under a sufficiently relaxed memory model is undecidable. For stronger models, the problem is decidable but has non-primitive recursive complexity.In this paper, we focus on models that have store-buffer based semantics, e.g., SPARC TSO and PSO. We use abstract interpretation to provide an effective verification procedure for programs running under this type of models. Our main contribution is a family of novel partial-coherence abstractions, specialized for relaxed memory models, which partially preserve information required for memory coherence and consistency. We use our abstractions to automatically verify programs under relaxed memory models. In addition, when a program violates its specification but can be fixed by adding fences, our approach can automatically infer a correct fence placement that is optimal under the abstraction. We implemented our approach in a tool called BLENDER and applied it to verify and infer fences in several concurrent algorithms.}},
  url = {https://doi.org/10.1145/1993498.1993521},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1270,
  title = {Blame assignment for higher-order contracts with intersection and union}},
  author = {Keil, Matthias and Thiemann, Peter}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present an untyped calculus of blame assignment for a higher-order contract system with two new operators: intersection and union. The specification of these operators is based on the corresponding type theoretic constructions. This connection makes intersection and union contracts their inevitable dynamic counterparts with a range of desirable properties and makes them suitable for subsequent integration in a gradual type system. A denotational specification provides the semantics of a contract in terms of two sets: a set of terms satisfying the contract and a set of contexts respecting the contract. This kind of specification for contracts is novel and interesting in its own right. A nondeterministic operational semantics serves as the specification for contract monitoring and for proving its correctness. It is complemented by a deterministic semantics that is closer to an implementation and that is connected to the nondeterministic semantics by simulation. The calculus is the formal basis of TJS, a language embedded, higher-order contract system implemented for JavaScript.}},
  url = {https://doi.org/10.1145/2784731.2784737},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1271,
  title = {Resurrector: a tunable object lifetime profiling technique for optimizing real-world programs}},
  author = {Xu, Guoqing}},
  year = {2013}},
  journal = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Modern object-oriented applications commonly suffer from severe performance problems that need to be optimized away for increased efficiency and user satisfaction. Many existing optimization techniques (such as object pooling and pretenuring) require precise identification of object lifetimes. However, it is particularly challenging to obtain object lifetimes both precisely and efficiently: precise profiling techniques such as Merlin introduce several hundred times slowdown even for small programs while efficient approximation techniques often sacrifice precision and produce less useful lifetime information. This paper presents a tunable profiling technique, called Resurrector, that explores the middle ground between high precision and high efficiency to find the precision-efficiency sweetspot for various livenessbased optimization techniques. Our evaluation shows that Resurrector is both more precise and more efficient than the GC-based approximation, and it is orders-of-magnitude faster than Merlin. To demonstrate Resurrector's usefulness, we have developed client analyses to find allocation sites that create large data structures with disjoint lifetimes. By inspecting program source code and reusing data structures created from these allocation sites, we have achieved significant performance gains. We have also improved the precision of an existing optimization technique using the lifetime information collected by Resurrector.}},
  url = {https://doi.org/10.1145/2509136.2509512},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1272,
  title = {Push-pull constraint graph for efficient points-to analysis}},
  author = {Ratnakar, Bollu and Nasre., Rupesh}},
  year = {2014}},
  journal = {Proceedings of the 2014 International Symposium on Memory Management}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present techniques for efficient computation of points-to information for C programs. Pointer analysis is an important phase in the compilation process. The computed points-to information and the alias information is useful for client analyses from varied domains such as bug finding, data-flow analysis, identifying security vulnerabilities, and parallelization, to name a few. Former research on pointer analysis has indicated that the main bottleneck towards scalability is manifested by the presence of complex constraints (load p = *q and store *p = q constraints) in the program. Complex constraints add edges to the constraint graph in an unpredictable manner and are responsible for initiating propagation of large amounts of points-to information across edges. We identify that the root cause to this issue is in the homogeneous structure in the constraint graph, due to which existing analyses treat loads and stores in a uniform manner. To address these issues, we present two techniques. First, we represent a constraint graph in a non-homogeneous manner, treat loads and stores in different ways, and employ a push-pull model for non-uniform propagation. Second, we propose lazy propagation which propagates information in the constraint graph only when necessary. We illustrate the effectiveness of our techniques using six large open-source programs and show that they improve the analysis time over a state-of-the-art BDD-based analysis by 33\% and over Deep Propagation by 21\%.}},
  url = {https://doi.org/10.1145/2602988.2602989},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1273,
  title = {From MinX to MinC: semantics-driven decompilation of recursive datatypes}},
  author = {Robbins, Ed and King, Andy and Schrijvers, Tom}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Reconstructing the meaning of a program from its binary executable is known as reverse engineering; it has a wide range of applications in software security, exposing piracy, legacy systems, etc. Since reversing is ultimately a search for meaning, there is much interest in inferring a type (a meaning) for the elements of a binary in a consistent way. Unfortunately existing approaches do not guarantee any semantic relevance for their reconstructed types. This paper presents a new and semantically-founded approach that provides strong guarantees for the reconstructed types. Key to our approach is the derivation of a witness program in a high-level language alongside the reconstructed types. This witness has the same semantics as the binary, is type correct by construction, and it induces a (justifiable) type assignment on the binary. Moreover, the approach effectively yields a type-directed decompiler. We formalise and implement the approach for reversing MinX, an abstraction of x86, to MinC, a type-safe dialect of C with recursive datatypes. Our evaluation compiles a range of textbook C algorithms to MinX and then recovers the original structures.}},
  url = {https://doi.org/10.1145/2837614.2837633},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1274,
  title = {Log++ logging for a cloud-native world}},
  author = {Marron, Mark}},
  year = {2018}},
  journal = {Proceedings of the 14th ACM SIGPLAN International Symposium on Dynamic Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Logging is a fundamental part of the software development and deployment lifecycle but logging support is often provided as an afterthought via limited library APIs or third-party modules. Given the critical nature of logging in modern cloud, mobile, and IoT development workflows, the unique needs of the APIs involved, and the opportunities for optimization using semantic knowledge, we argue logging should be included as a central part of the language and runtime designs. This paper presents a rethinking of the logger for modern cloud-native workflows. Based on a set of design principles for modern logging we build a logging system, that supports near zero-cost for disabled log statements, low cost lazy-copying for enabled log statements, selective persistence of logging output, unified control of logging output across different libraries, and DevOps integration for use with modern cloud-based deployments. To evaluate these concepts we implemented the Log++ logger for Node.js hosted JavaScript applications.}},
  url = {https://doi.org/10.1145/3276945.3276952},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1275,
  title = {Typed iterators for XML}},
  author = {Castagna, Giuseppe and Nguyen, Kim}},
  year = {2008}},
  journal = {Proceedings of the 13th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {XML transformations are very sensitive to types: XML types describe the tags and attributes of XML elements as well as the number, kind, and order of their sub-elements. Therefore, operations, even simple ones, that modify these features may affect the types of documents. Operations on XML documents are performed by iterators that, to be useful, need to be typed by a kind of polymorphism that goes beyond what currently exists. For this reason these iterators are not programmed but, rather, hard-coded in the languages. However, this approach soon reaches its limits, as the hard-coded iterators cannot cover fairly standard usage scenarios.As a solution to this problem we propose a generic language to define iterators for XML data. This language can either be used as a compilation target (e.g., for XPATH) or it can be grafted on any statically typed host programming language (as long as this has product types) to endow it with XML processing capabilities. We show that our language mostly offers the required degree of polymorphism, study its formal properties, and show its expressiveness and practical impact by providing several usage examples and encodings.}},
  url = {https://doi.org/10.1145/1411204.1411210},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1276,
  title = {A theory of typed coercions and its applications}},
  author = {Swamy, Nikhil and Hicks, Michael and Bierman, Gavin M.}},
  year = {2009}},
  journal = {Proceedings of the 14th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A number of important program rewriting scenarios can be recast as type-directed coercion insertion. These range from more theoretical applications such as coercive subtyping and supporting overloading in type theories, to more practical applications such as integrating static and dynamically typed code using gradual typing, and inlining code to enforce security policies such as access control and provenance tracking. In this paper we give a general theory of type-directed coercion insertion. We specifically explore the inherent tradeoff between expressiveness and ambiguity--the more powerful the strategy for generating coercions, the greater the possibility of several, semantically distinct rewritings for a given program. We consider increasingly powerful coercion generation strategies, work out example applications supported by the increased power (including those mentioned above), and identify the inherent ambiguity problems of each setting, along with various techniques to tame the ambiguities.}},
  url = {https://doi.org/10.1145/1596550.1596598},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1277,
  title = {SPLLIFT: statically analyzing software product lines in minutes instead of years}},
  author = {Bodden, Eric and Tol\^{e}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A software product line (SPL) encodes a potentially large variety of software products as variants of some common code base. Up until now, re-using traditional static analyses for SPLs was virtually intractable, as it required programmers to generate and analyze all products individually. In this work, however, we show how an important class of existing inter-procedural static analyses can be transparently lifted to SPLs. Without requiring programmers to change a single line of code, our approach SPLLIFT automatically converts any analysis formulated for traditional programs within the popular IFDS framework for inter-procedural, finite, distributive, subset problems to an SPL-aware analysis formulated in the IDE framework, a well-known extension to IFDS. Using a full implementation based on Heros, Soot, CIDE and JavaBDD, we show that with SPLLIFT one can reuse IFDS-based analyses without changing a single line of code. Through experiments using three static analyses applied to four Java-based product lines, we were able to show that our approach produces correct results and outperforms the traditional approach by several orders of magnitude.}},
  url = {https://doi.org/10.1145/2491956.2491976},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1278,
  title = {Incremental distribution of timestamp packets: a new approach to distributed garbage collection}},
  author = {Schelvis, M.}},
  year = {1989}},
  journal = {Conference Proceedings on Object-Oriented Programming Systems, Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A new algorithm for distributed garbage collection is presented. This algorithm collects distributed garbage incrementally and concurrently with user activity. It is the first incremental algorithm that is capable of collecting cyclic distributed garbage. Computational and network communication overhead are acceptable. Hosts may be temporarily inaccessible and synchronization between hosts is not necessary. The algorithm is based on asynchronous distribution of timestamp packets each containing a list of last-access times of some relevant remotely referenced objects. Finally, the correctness and time complexity of the algorithm are discussed.}},
  url = {https://doi.org/10.1145/74877.74883},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1279,
  title = {Dynamically checking ownership policies in concurrent c/c++ programs}},
  author = {Martin, Jean-Phillipe and Hicks, Michael and Costa, Manuel and Akritidis, Periklis and Castro, Miguel}},
  year = {2010}},
  journal = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Concurrent programming errors arise when threads share data incorrectly. Programmers often avoid these errors by using synchronization to enforce a simple ownership policy: data is either owned exclusively by a thread that can read or write the data, or it is read owned by a set of threads that can read but not write the data. Unfortunately, incorrect synchronization often fails to enforce these policies and memory errors in languages like C and C++ can violate these policies even when synchronization is correct.In this paper, we present a dynamic analysis for checking ownership policies in concurrent C and C++ programs despite memory errors. The analysis can be used to find errors in commodity multi-threaded programs and to prevent attacks that exploit these errors. We require programmers to write ownership assertions that describe the sharing policies used by different parts of the program. These policies may change over time, as may the policies' means of enforcement, whether it be locks, barriers, thread joins, etc. Our compiler inserts checks in the program that signal an error if these policies are violated at runtime. We evaluated our tool on several benchmark programs. The run-time overhead was reasonable: between 0 and 49\% with an average of 26\%. We also found the tool easy to use: the total number of ownership assertions is small, and the asserted specification and implementation can be debugged together by running the instrumented program and addressing the errors that arise. Our approach enjoys a pleasing modular soundness property: if a thread executes a sequence of statements on variables it owns, the statements are serializable within a valid execution, and thus their effects can be reasoned about in isolation from other threads in the program.}},
  url = {https://doi.org/10.1145/1706299.1706351},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1280,
  title = {Unboundedness and downward closures of higher-order pushdown automata}},
  author = {Hague, Matthew and Kochems, Jonathan and Ong, C.-H. Luke}},
  year = {2016}},
  journal = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We show the diagonal problem for higher-order pushdown automata (HOPDA), and hence the simultaneous unboundedness problem, is decidable. From recent work by Zetzsche this means that we can construct the downward closure of the set of words accepted by a given HOPDA. This also means we can construct the downward closure of the Parikh image of a HOPDA. Both of these consequences play an important role in verifying concurrent higher-order programs expressed as HOPDA or safe higher-order recursion schemes.}},
  url = {https://doi.org/10.1145/2837614.2837627},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1281,
  title = {Trace driven dynamic deadlock detection and reproduction}},
  author = {Samak, Malavika and Ramanathan, Murali Krishna}},
  year = {2014}},
  journal = {Proceedings of the 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Dynamic analysis techniques have been proposed to detect potential deadlocks. Analyzing and comprehending each potential deadlock to determine whether the deadlock is feasible in a real execution requires significant programmer effort. Moreover, empirical evidence shows that existing analyses are quite imprecise. This imprecision of the analyses further void the manual effort invested in reasoning about non-existent defects.In this paper, we address the problems of imprecision of existing analyses and the subsequent manual effort necessary to reason about deadlocks. We propose a novel approach for deadlock detection by designing a dynamic analysis that intelligently leverages execution traces. To reduce the manual effort, we replay the program by making the execution follow a schedule derived based on the observed trace. For a real deadlock, its feasibility is automatically verified if the replay causes the execution to deadlock.We have implemented our approach as part of WOLF and have analyzed many large (upto 160KLoC) Java programs. Our experimental results show that we are able to identify 74\% of the reported defects as true (or false) positives automatically leaving very few defects for manual analysis. The overhead of our approach is negligible making it a compelling tool for practical adoption.}},
  url = {https://doi.org/10.1145/2555243.2555262},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1282,
  title = {Compositional shape analysis by means of bi-abduction}},
  author = {Calcagno, Cristiano and Distefano, Dino and O'Hearn, Peter and Yang, Hongseok}},
  year = {2009}},
  journal = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper describes a compositional shape analysis, where each procedure is analyzed independently of its callers. The analysis uses an abstract domain based on a restricted fragment of separation logic, and assigns a collection of Hoare triples to each procedure; the triples provide an over-approximation of data structure usage. Compositionality brings its usual benefits -- increased potential to scale, ability to deal with unknown calling contexts, graceful way to deal with imprecision -- to shape analysis, for the first time.The analysis rests on a generalized form of abduction (inference of explanatory hypotheses) which we call bi-abduction. Bi-abduction displays abduction as a kind of inverse to the frame problem: it jointly infers anti-frames (missing portions of state) and frames (portions of state not touched by an operation), and is the basis of a new interprocedural analysis algorithm. We have implemented our analysis algorithm and we report case studies on smaller programs to evaluate the quality of discovered specifications, and larger programs (e.g., an entire Linux distribution) to test scalability and graceful imprecision.}},
  url = {https://doi.org/10.1145/1480881.1480917},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1283,
  title = {MapReduce program synthesis}},
  author = {Smith, Calvin and Albarghouthi, Aws}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {By abstracting away the complexity of distributed systems, large-scale data processing platforms—MapReduce, Hadoop, Spark, Dryad, etc.—have provided developers with simple means for harnessing the power of the cloud. In this paper, we ask whether we can automatically synthesize MapReduce-style distributed programs from input–output examples. Our ultimate goal is to enable end users to specify large-scale data analyses through the simple interface of examples. We thus present a new algorithm and tool for synthesizing programs composed of efficient data-parallel operations that can execute on cloud computing infrastructure. We evaluate our tool on a range of real-world big-data analysis tasks and general computations. Our results demonstrate the efficiency of our approach and the small number of examples it requires to synthesize correct, scalable programs.}},
  url = {https://doi.org/10.1145/2908080.2908102},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1284,
  title = {How to combine widening and narrowing for non-monotonic systems of equations}},
  author = {Apinis, Kalmer and Seidl, Helmut and Vojdani, Vesal}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Non-trivial analysis problems require complete lattices with infinite ascending and descending chains. In order to compute reasonably precise post-fixpoints of the resulting systems of equations, Cousot and Cousot have suggested accelerated fixpoint iteration by means of widening and narrowing.The strict separation into phases, however, may unnecessarily give up precision that cannot be recovered later. While widening is also applicable if equations are non-monotonic, this is no longer the case for narrowing. A narrowing iteration to improve a given post-fixpoint, additionally, must assume that all right-hand sides are monotonic. The latter assumption, though, is not met in presence of widening. It is also not met by equation systems corresponding to context-sensitive interprocedural analysis, possibly combining context-sensitive analysis of local information with flow-insensitive analysis of globals.As a remedy, we present a novel operator that combines a given widening operator with a given narrowing operator. We present adapted versions of round-robin as well as of worklist iteration, local, and side-effecting solving algorithms for the combined operator and prove that the resulting solvers always return sound results and are guaranteed to terminate for monotonic systems whenever only finitely many unknowns (constraint variables) are encountered.}},
  url = {https://doi.org/10.1145/2491956.2462190},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1285,
  title = {State-dependent representation independence}},
  author = {Ahmed, Amal and Dreyer, Derek and Rossberg, Andreas}},
  year = {2009}},
  journal = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Mitchell's notion of representation independence is a particularly useful application of Reynolds' relational parametricity -- two different implementations of an abstract data type can be shown contextually equivalent so long as there exists a relation between their type representations that is preserved by their operations. There have been a number of methods proposed for proving representation independence in various pure extensions of System F (where data abstraction is achieved through existential typing), as well as in Algol- or Java-like languages (where data abstraction is achieved through the use of local mutable state). However, none of these approaches addresses the interaction of existential type abstraction and local state. In particular, none allows one to prove representation independence results for generative ADTs -- i.e. ADTs that both maintain some local state and define abstract types whose internal representations are dependent on that local state.In this paper, we present a syntactic, logical-relations-based method for proving representation independence of generative ADTs in a language supporting polymorphic types, existential types, general recursive types, and unrestricted ML-style mutable references. We demonstrate the effectiveness of our method by using it to prove several interesting contextual equivalences that involve a close interaction between existential typing and local state, as well as some well-known equivalences from the literature (such as Pitts and Stark's "awkward" example) that have caused trouble for previous logical-relations-based methods.The success of our method relies on two key technical innovations. First, in order to handle generative ADTs, we develop a possible-worlds model in which relational interpretations of types are allowed to grow over time in a manner that is tightly coupled with changes to some local state. Second, we employ a step-indexed stratification of possible worlds, which facilitates a simplified account of mutable references of higher type.}},
  url = {https://doi.org/10.1145/1480881.1480925},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1286,
  title = {Verdi: a framework for implementing and formally verifying distributed systems}},
  author = {Wilcox, James R. and Woos, Doug and Panchekha, Pavel and Tatlock, Zachary and Wang, Xi and Ernst, Michael D. and Anderson, Thomas}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Distributed systems are difficult to implement correctly because they must handle both concurrency and failures: machines may crash at arbitrary points and networks may reorder, drop, or duplicate packets. Further, their behavior is often too complex to permit exhaustive testing. Bugs in these systems have led to the loss of critical data and unacceptable service outages. We present Verdi, a framework for implementing and formally verifying distributed systems in Coq. Verdi formalizes various network semantics with different faults, and the developer chooses the most appropriate fault model when verifying their implementation. Furthermore, Verdi eases the verification burden by enabling the developer to first verify their system under an idealized fault model, then transfer the resulting correctness guarantees to a more realistic fault model without any additional proof burden. To demonstrate Verdi's utility, we present the first mechanically checked proof of linearizability of the Raft state machine replication algorithm, as well as verified implementations of a primary-backup replication system and a key-value store. These verified systems provide similar performance to unverified equivalents.}},
  url = {https://doi.org/10.1145/2737924.2737958},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1287,
  title = {LeakChaser: helping programmers narrow down causes of memory leaks}},
  author = {Xu, Guoqing and Bond, Michael D. and Qin, Feng and Rountev, Atanas}},
  year = {2011}},
  journal = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In large programs written in managed languages such as Java and C#, holding unnecessary references often results in memory leaks and bloat, degrading significantly their run-time performance and scalability. Despite the existence of many leak detectors for such languages, these detectors often target low-level objects; as a result, their reports contain many false warnings and lack sufficient semantic information to help diagnose problems. This paper introduces a specification-based technique called LeakChaser that can not only capture precisely the unnecessary references leading to leaks, but also explain, with high-level semantics, why these references become unnecessary.At the heart of LeakChaser is a three-tier approach that uses varying levels of abstraction to assist programmers with different skill levels and code familiarity to find leaks. At the highest tier of the approach, the programmer only needs to specify the boundaries of coarse-grained activities, referred to as transactions. The tool automatically infers liveness properties of these transactions, by monitoring the execution, in order to find unnecessary references. Diagnosis at this tier can be performed by any programmer after inspecting the APIs and basic modules of a program, without understanding of the detailed implementation of these APIs. At the middle tier, the programmer can introduce application-specific semantic information by specifying properties for the transactions. At the lowest tier of the approach is a liveness checker that does not rely on higher-level semantic information, but rather allows a programmer to assert lifetime relationships for pairs of objects. This task could only be performed by skillful programmers who have a clear understanding of data structures and algorithms in the program.We have implemented LeakChaser in Jikes RVM and used it to help us diagnose several real-world leaks. The implementation incurs a reasonable overhead for debugging and tuning. Our case studies indicate that the implementation is powerful in guiding programmers with varying code familiarity to find the root causes of several memory leaks---even someone who had not studied a leaking program can quickly find the cause after using LeakChaser's iterative process that infers and checks properties with different levels of semantic information.}},
  url = {https://doi.org/10.1145/1993498.1993530},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1288,
  title = {A program transformation and architecture support for quantum uncomputation}},
  author = {Schuchman, Ethan and Vijaykumar, T. N.}},
  year = {2006}},
  journal = {Proceedings of the 12th International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Quantum computing's power comes from new algorithms that exploit quantum mechanical phenomena for computation. Quantum algorithms are different from their classical counterparts in that quantum algorithms rely on algorithmic structures that are simply not present in classical computing. Just as classical program transformations and architectures have been designed for common classical algorithm structures, quantum program transformations and quantum architectures should be designed with quantum algorithms in mind. Because quantum algorithms come with these new algorithmic structures, resultant quantum program transformations and architectures may look very different from their classical counterparts.This paper focuses on uncomputation, a critical and prevalent structure in quantum algorithms, and considers how program transformations, and architecture support should be designed to accommodate uncomputation. In this paper,we show a simple quantum program transformation that exposes independence between uncomputation and later computation. We then propose a multicore architecture tailored to this exposed parallelism and propose a scheduling policy that efficiently maps such parallelism to the multicore architecture. Our policy achieves parallelism between uncomputation and later computation while reducing cumulative communication distance. Our scheduling and architecture allows significant speedup of quantum programs (between 1.8x and 2.8x speedup in Shor's factoring algorithm), while reducing cumulative communication distance 26\%.}},
  url = {https://doi.org/10.1145/1168857.1168889},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1289,
  title = {Minimizing row displacement dispatch tables}},
  author = {Driesen, Karel and H\"{o}},
  year = {1995}},
  journal = {Proceedings of the Tenth Annual Conference on Object-Oriented Programming Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Row displacement dispatch tables implement message dispatching for dynamically-typed languages with a run time overhead of one memory indirection plus an equality test. The technique is similar to virtual function table lookup, which is, however, restricted to statically typed languages like C++. We show how to reduce the space requirements of dispatch tables to approximately the same size as virtual function tables. The scheme is then generalized for multiple inheritance. Experiments on a number of class libraries from five different languages demonstrate that the technique is effective for a broad range of programs. Finally, we discuss optimizations of the row displacement algorithm that allow dispatch table construction of these large samples to take place in a few seconds.}},
  url = {https://doi.org/10.1145/217838.217851},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1290,
  title = {Synthesizing data structure transformations from input-output examples}},
  author = {Feser, John K. and Chaudhuri, Swarat and Dillig, Isil}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a method for example-guided synthesis of functional programs over recursive data structures. Given a set of input-output examples, our method synthesizes a program in a functional language with higher-order combinators like map and fold. The synthesized program is guaranteed to be the simplest program in the language to fit the examples. Our approach combines three technical ideas: inductive generalization, deduction, and enumerative search. First, we generalize the input-output examples into hypotheses about the structure of the target program. For each hypothesis, we use deduction to infer new input/output examples for the missing subexpressions. This leads to a new subproblem where the goal is to synthesize expressions within each hypothesis. Since not every hypothesis can be realized into a program that fits the examples, we use a combination of best-first enumeration and deduction to search for a hypothesis that meets our needs. We have implemented our method in a tool called λ2, and we evaluate this tool on a large set of synthesis problems involving lists, trees, and nested data structures. The experiments demonstrate the scalability and broad scope of λ2. A highlight is the synthesis of a program believed to be the world's earliest functional pearl.}},
  url = {https://doi.org/10.1145/2737924.2737977},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1291,
  title = {Implementation-Aware Model Analysis: The Case of Buffer-Throughput Tradeoff in Streaming Applications}},
  author = {Barijough, Kamyar Mirzazad and Hashemi, Matin and Khibin, Volodymyr and Ghiasi, Soheil}},
  year = {2015}},
  journal = {Proceedings of the 16th ACM SIGPLAN/SIGBED Conference on Languages, Compilers and Tools for Embedded Systems 2015 CD-ROM}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Models of computation abstract away a number of implementation details in favor of well-defined semantics. While this has unquestionable benefits, we argue that analysis of models solely based on operational semantics (implementation-oblivious analysis) is unfit to drive implementation design space exploration. Specifically, we study the tradeoff between buffer size and streaming throughput in applications modeled as synchronous data flow (SDF) graphs. We demonstrate the inherent inaccuracy of implementation-oblivious approach, which only considers SDF operational semantic. We propose a rigorous transformation, which equips the state of the art buffer-throughput tradeoff analysis technique with implementation awareness. Extensive empirical evaluation show that our approach results in significantly more accurate estimates in streaming throughput at the model level, while running two orders of magnitude faster than cycle-accurate simulation of implementations.}},
  url = {https://doi.org/10.1145/2670529.2754968},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1292,
  title = {Steal Tree: low-overhead tracing of work stealing schedulers}},
  author = {Lifflander, Jonathan and Krishnamoorthy, Sriram and Kale, Laxmikant V.}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Work stealing is a popular approach to scheduling task-parallel programs. The flexibility inherent in work stealing when dealing with load imbalance results in seemingly irregular computation structures, complicating the study of its runtime behavior. In this paper, we present an approach to efficiently trace async-finish parallel programs scheduled using work stealing. We identify key properties that allow us to trace the execution of tasks with low time and space overheads. We also study the usefulness of the proposed schemes in supporting algorithms for data-race detection and retentive stealing presented in the literature. We demonstrate that the perturbation due to tracing is within the variation in the execution time with 99\% confidence and the traces are concise, amounting to a few tens of kilobytes per thread in most cases. We also demonstrate that the traces enable significant reductions in the cost of detecting data races and result in low, stable space overheads in supporting retentive stealing for async-finish programs.}},
  url = {https://doi.org/10.1145/2491956.2462193},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1293,
  title = {A unification algorithm for Coq featuring universe polymorphism and overloading}},
  author = {Ziliani, Beta and Sozeau, Matthieu}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Unification is a core component of every proof assistant or programming language featuring dependent types. In many cases, it must deal with higher-order problems up to conversion. Since unification in such conditions is undecidable, unification algorithms may include several heuristics to solve common problems. However, when the stack of heuristics grows large, the result and complexity of the algorithm can become unpredictable. Our contributions are twofold: (1) We present a full description of a new unification algorithm for the Calculus of Inductive Constructions (the base logic of Coq), including universe polymorphism, canonical structures (the overloading mechanism baked into Coq's unification), and a small set of useful heuristics. (2) We implemented our algorithm, and tested it on several libraries, providing evidence that the selected set of heuristics suffices for large developments.}},
  url = {https://doi.org/10.1145/2784731.2784751},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1294,
  title = {Stateless model checking concurrent programs with maximal causality reduction}},
  author = {Huang, Jeff}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present maximal causality reduction (MCR), a new technique for stateless model checking. MCR systematically explores the state-space of concurrent programs with a provably minimal number of executions. Each execution corresponds to a distinct maximal causal model extracted from a given execution trace, which captures the largest possible set of causally equivalent executions. Moreover, MCR is embarrassingly parallel by shifting the runtime exploration cost to offline analysis. We have designed and implemented MCR using a constraint-based approach and compared with iterative context bounding (ICB) and dynamic partial order reduction (DPOR) on both benchmarks and real-world programs. MCR reduces the number of executions explored by ICB and ICB+DPOR by orders of magnitude, and significantly improves the scalability, efficiency, and effectiveness of the state-of-the-art for both state-space exploration and bug finding. In our experiments, MCR has also revealed several new data races and null pointer dereference errors in frequently studied real-world programs.}},
  url = {https://doi.org/10.1145/2737924.2737975},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1295,
  title = {Formally proving a compiler transformation safe}},
  author = {Breitner, Joachim}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We prove that the Call Arity analysis and transformation, as implemented in the Haskell compiler GHC, is safe, i.e. does not impede the performance of the program. We formalized syntax, semantics, the analysis and the transformation in the interactive theorem prover Isabelle to obtain a machine-checked proof and hence a level of rigor rarely obtained for compiler optimization safety theorems. The proof is modular and introduces trace trees as a suitable abstraction in abstract cardinality analyses. We discuss the breadth of the formalization gap.}},
  url = {https://doi.org/10.1145/2804302.2804312},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1296,
  title = {A practical type system and language for reference immutability}},
  author = {Birka, Adrian and Ernst, Michael D.}},
  year = {2004}},
  journal = {Proceedings of the 19th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper describes a type system that is capable of expressing and enforcing immutability constraints. The specific constraint expressed is that the abstract state of the object to which an immutable reference refers cannot be modified using that reference. The abstract state is (part of) the transitively reachable state: that is, the state of the object and all state reachable from it by following references. The type system permits explicitly excluding fields or objects from the abstract state of an object. For a statically type-safe language, the type system guarantees reference immutability. If the language is extended with immutability downcasts, then run-time checks enforce the reference immutability constraints.In order to better understand the usability and efficacy of the type system, we have implemented an extension to Java, called Javari, that includes all the features of our type system. Javari is interoperable with Java and existing JVMs. It can be viewed as a proposal for the semantics of the Java const keyword, though Javari's syntax uses readonly instead. This paper describes the design and implementation of Javari, including the type-checking rules for the language. This paper also discusses experience with 160,000 lines of Javari code. Javari was easy to use and provided a number of benefits, including detecting errors in well-tested code.}},
  url = {https://doi.org/10.1145/1028976.1028980},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1297,
  title = {Fast algorithms for Dyck-CFL-reachability with applications to alias analysis}},
  author = {Zhang, Qirun and Lyu, Michael R. and Yuan, Hao and Su, Zhendong}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The context-free language (CFL) reachability problem is a well-known fundamental formulation in program analysis. In practice, many program analyses, especially pointer analyses, adopt a restricted version of CFL-reachability, Dyck-CFL-reachability, and compute on edge-labeled bidirected graphs. Solving the all-pairs Dyck-CFL-reachability on such bidirected graphs is expensive. For a bidirected graph with n nodes and m edges, the traditional dynamic programming style algorithm exhibits a subcubic time complexity for the Dyck language with k kinds of parentheses. When the underlying graphs are restricted to bidirected trees, an algorithm with O(n log n log k) time complexity was proposed recently. This paper studies the Dyck-CFL-reachability problems on bidirected trees and graphs. In particular, it presents two fast algorithms with O(n) and O(n + m log m) time complexities on trees and graphs respectively. We have implemented and evaluated our algorithms on a state-of-the-art alias analysis for Java. Results on standard benchmarks show that our algorithms achieve orders of magnitude speedup and consume less memory.}},
  url = {https://doi.org/10.1145/2491956.2462159},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1298,
  title = {Recursive type generativity}},
  author = {Dreyer, Derek}},
  year = {2005}},
  journal = {Proceedings of the Tenth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Existential types provide a simple and elegant foundation for understanding generative abstract data types, of the kind supported by the Standard ML module system. However, in attempting to extend ML with support for recursive modules, we have found that the traditional existential account of type generativity does not work well in the presence of mutually recursive module definitions. The key problem is that, in recursive modules, one may wish to define an abstract type in a context where a name for the type already exists, but the existential type mechanism does not allow one to do so.We propose a novel account of recursive type generativity that resolves this problem. The basic idea is to separate the act of generating a name for an abstract type from the act of defining its underlying representation. To define several abstract types recursively, one may first "forward-declare" them by generating their names, and then define each one secretly within its own defining expression. Intuitively, this can be viewed as a kind of backpatching semantics for recursion at the level of types. Care must be taken to ensure that a type name is not defined more than once, and that cycles do not arise among "transparent" type definitions.In contrast to the usual continuation-passing interpretation of existential types in terms of universal types, our account of type generativity suggests a destination-passing interpretation. Briefly, instead of viewing a value of existential type as something that creates a new abstract type every time it is unpacked, we view it as a function that takes as input a pre-existing undefined abstract type and defines it. By leaving the creation of the abstract type name up to the client of the existential, our approach makes it significantly easier to link abstract data types together recursively.}},
  url = {https://doi.org/10.1145/1086365.1086372},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1299,
  title = {Energy types}},
  author = {Cohen, Michael and Zhu, Haitao Steve and Senem, Emgin Ezgi and Liu, Yu David}},
  year = {2012}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents a novel type system to promote and facilitate energy-aware programming. Energy Types is built upon a key insight into today's energy-efficient systems and applications: despite the popular perception that energy and power can only be described in joules and watts, real-world energy management is often based on discrete phases and modes, which in turn can be reasoned about by type systems very effectively. A phase characterizes a distinct pattern of program workload, and a mode represents an energy state the program is expected to execute in. This paper describes a programming model where phases and modes can be intuitively specified by programmers or inferred by the compiler as type information. It demonstrates how a type-based approach to reasoning about phases and modes can help promote energy efficiency. The soundness of our type system and the invariants related to inter-phase and inter-mode interactions are rigorously proved. Energy Types is implemented as the core of a prototyped object-oriented language ET for smartphone programming. Preliminary studies show ET can lead to significant energy savings for Android Apps.}},
  url = {https://doi.org/10.1145/2384616.2384676},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1300,
  title = {Understanding modern device drivers}},
  author = {Kadav, Asim and Swift, Michael M.}},
  year = {2012}},
  journal = {Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Device drivers are the single largest contributor to operating-system kernel code with over 5 million lines of code in the Linux kernel, and cause significant complexity, bugs and development costs. Recent years have seen a flurry of research aimed at improving the reliability and simplifying the development of drivers. However, little is known about what constitutes this huge body of code beyond the small set of drivers used for research.In this paper, we study the source code of Linux drivers to understand what drivers actually do, how current research applies to them and what opportunities exist for future research. We determine whether assumptions made by most driver research, such as that all drivers belong to a class, are indeed true. We also analyze driver code and abstractions to determine whether drivers can benefit from code re-organization or hardware trends. We develop a set of static-analysis tools to analyze driver code across various axes. Broadly, our study looks at three aspects of driver code (i) what are the characteristics of driver code functionality and how applicable is driver research to all drivers, (ii) how do drivers interact with the kernel, devices, and buses, and (iii) are there similarities that can be abstracted into libraries to reduce driver size and complexity?We find that many assumptions made by driver research do not apply to all drivers. At least 44\% of drivers have code that is not captured by a class definition, 28\% of drivers support more than one device per driver, and 15\% of drivers do significant computation over data. From the driver interactions study, we find USB bus offers an efficient bus interface with significant standardized code and coarse-grained access, ideal for executing drivers in isolation. We also find that drivers for different buses and classes have widely varying levels of device interaction, which indicates that the cost of isolation will vary by class. Finally, from our driver similarity study, we find 8\% of all driver code is substantially similar to code elsewhere and may be removed with new abstractions or libraries.}},
  url = {https://doi.org/10.1145/2150976.2150987},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1301,
  title = {Dynamic determinacy analysis}},
  author = {Sch\"{a}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present an analysis for identifying determinate variables and expressions that always have the same value at a given program point. This information can be exploited by client analyses and tools to, e.g., identify dead code or specialize uses of dynamic language constructs such as eval, replacing them with equivalent static constructs. Our analysis is completely dynamic and only needs to observe a single execution of the program, yet the determinacy facts it infers hold for any execution. We present a formal soundness proof of the analysis for a simple imperative language, and a prototype implementation that handles full JavaScript. Finally, we report on two case studies that explored how static analysis for JavaScript could leverage the information gathered by dynamic determinacy analysis. We found that in some cases scalability of static pointer analysis was improved dramatically, and that many uses of runtime code generation could be eliminated.}},
  url = {https://doi.org/10.1145/2491956.2462168},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1302,
  title = {Typed closure conversion preserves observational equivalence}},
  author = {Ahmed, Amal and Blume, Matthias}},
  year = {2008}},
  journal = {Proceedings of the 13th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Language-based security relies on the assumption that all potential attacks are bound by the rules of the language in question. When programs are compiled into a different language, this is true only if the translation process preserves observational equivalence.We investigate the problem of fully abstract compilation, i.e., compilation that both preserves and reflects observational equivalence. In particular, we prove that typed closure conversion for the polymorphic »-calculus with existential and recursive types is fully abstract. Our proof uses operational techniques in the form of a step-indexed logical relation and construction of certain wrapper terms that "back-translate" from target values to source values.Although typed closure conversion has been assumed to be fully abstract, we are not aware of any previous result that actually proves this.}},
  url = {https://doi.org/10.1145/1411204.1411227},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1303,
  title = {Natural proofs for structure, data, and separation}},
  author = {Qiu, Xiaokang and Garg, Pranav and \c{S}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We propose natural proofs for reasoning with programs that manipulate data-structures against specifications that describe the structure of the heap, the data stored within it, and separation and framing of sub-structures. Natural proofs are a subclass of proofs that are amenable to completely automated reasoning, that provide sound but incomplete procedures, and that capture common reasoning tactics in program verification. We develop a dialect of separation logic over heaps, called Dryad, with recursive definitions that avoids explicit quantification. We develop ways to reason with heaplets using classical logic over the theory of sets, and develop natural proofs for reasoning using proof tactics involving disciplined unfoldings and formula abstractions. Natural proofs are encoded into decidable theories of first-order logic so as to be discharged using SMT solvers.We also implement the technique and show that a large class of more than 100 correct programs that manipulate data-structures are amenable to full functional correctness using the proposed natural proof method. These programs are drawn from a variety of sources including standard data-structures, the Schorr-Waite algorithm for garbage collection, a large number of low-level C routines from the Glib library and OpenBSD library, the Linux kernel, and routines from a secure verified OS-browser project. Our work is the first that we know of that can handle such a wide range of full functional verification properties of heaps automatically, given pre/post and loop invariant annotations. We believe that this work paves the way for deductive verification technology to be used by programmers who do not (and need not) understand the internals of the underlying logic solvers, significantly increasing their applicability in building reliable systems.}},
  url = {https://doi.org/10.1145/2491956.2462169},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1304,
  title = {Almost-correct specifications: a modular semantic framework for assigning confidence to warnings}},
  author = {Blackshear, Sam and Lahiri, Shuvendu K.}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Modular assertion checkers are plagued with false alarms due to the need for precise environment specifications (preconditions and callee postconditions). Even the fully precise checkers report assertion failures under the most demonic environments allowed by unconstrained or partial specifications. The inability to preclude overly adversarial environments makes such checkers less attractive to developers and severely limits the adoption of such tools in the development cycle.In this work, we propose a parameterized framework for prioritizing the assertion failures reported by a modular verifier, with the goal of suppressing warnings from overly demonic environments. We formalize it almost-correct specifications as the minimal weakening of an angelic specification (over a set of predicates) that precludes any dead code intraprocedurally. Our work is inspired by and generalizes some aspects of semantic inconsistency detection. Our formulation allows us to lift this idea to a general class of warnings. We have developed a prototype acspec, which we use to explore a few instantiations of the framework and report preliminary findings on a diverse set of C benchmarks.}},
  url = {https://doi.org/10.1145/2491956.2462188},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1305,
  title = {Termination and non-termination specification inference}},
  author = {Le, Ton Chanh and Qin, Shengchao and Chin, Wei-Ngan}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Techniques for proving termination and non-termination of imperative programs are usually considered as orthogonal mechanisms. In this paper, we propose a novel mechanism that analyzes and proves both program termination and non-termination at the same time. We first introduce the concept of second-order termination constraints and accumulate a set of relational assumptions on them via a Hoare-style verification. We then solve these assumptions with case analysis to determine the (conditional) termination and non- termination scenarios expressed in some specification logic form. In contrast to current approaches, our technique can construct a summary of terminating and non-terminating behaviors for each method. This enables modularity and reuse for our termination and non-termination proving processes. We have tested our tool on sample programs from a recent termination competition, and compared favorably against state-of-the-art termination analyzers.}},
  url = {https://doi.org/10.1145/2737924.2737993},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1306,
  title = {Quantitative abstraction refinement}},
  author = {Cerny, Pavol and Henzinger, Thomas A. and Radhakrishna, Arjun}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We propose a general framework for abstraction with respect to quantitative properties, such as worst-case execution time, or power consumption. Our framework provides a systematic way for counter-example guided abstraction refinement for quantitative properties. The salient aspect of the framework is that it allows anytime verification, that is, verification algorithms that can be stopped at any time (for example, due to exhaustion of memory), and report approximations that improve monotonically when the algorithms are given more time.We instantiate the framework with a number of quantitative abstractions and refinement schemes, which differ in terms of how much quantitative information they keep from the original system. We introduce both state-based and trace-based quantitative abstractions, and we describe conditions that define classes of quantitative properties for which the abstractions provide over-approximations. We give algorithms for evaluating the quantitative properties on the abstract systems. We present algorithms for counter-example based refinements for quantitative properties for both state-based and segment-based abstractions. We perform a case study on worst-case execution time of executables to evaluate the anytime verification aspect and the quantitative abstractions we proposed.}},
  url = {https://doi.org/10.1145/2429069.2429085},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1307,
  title = {A fast compiler for NetKAT}},
  author = {Smolka, Steffen and Eliopoulos, Spiridon and Foster, Nate and Guha, Arjun}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {High-level programming languages play a key role in a growing number of networking platforms, streamlining application development and enabling precise formal reasoning about network behavior. Unfortunately, current compilers only handle "local" programs that specify behavior in terms of hop-by-hop forwarding behavior, or modest extensions such as simple paths. To encode richer "global" behaviors, programmers must add extra state -- something that is tricky to get right and makes programs harder to write and maintain. Making matters worse, existing compilers can take tens of minutes to generate the forwarding state for the network, even on relatively small inputs. This forces programmers to waste time working around performance issues or even revert to using hardware-level APIs. This paper presents a new compiler for the NetKAT language that handles rich features including regular paths and virtual networks, and yet is several orders of magnitude faster than previous compilers. The compiler uses symbolic automata to calculate the extra state needed to implement "global" programs, and an intermediate representation based on binary decision diagrams to dramatically improve performance. We describe the design and implementation of three essential compiler stages: from virtual programs (which specify behavior in terms of virtual topologies) to global programs (which specify network-wide behavior in terms of physical topologies), from global programs to local programs (which specify behavior in terms of single-switch behavior), and from local programs to hardware-level forwarding tables. We present results from experiments on real-world benchmarks that quantify performance in terms of compilation time and forwarding table size.}},
  url = {https://doi.org/10.1145/2784731.2784761},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1308,
  title = {Systematic design of program transformation frameworks by abstract interpretation}},
  author = {Cousot, Patrick and Cousot, Radhia}},
  year = {2002}},
  journal = {Proceedings of the 29th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We introduce a general uniform language-independent framework for designing online and offline source-to-source program transformations by abstract interpretation of program semantics. Iterative source-to-source program transformations are designed constructively by composition of source-to-semantics, semantics-to-transformed semantics and semantics-to-source abstractions applied to fixpoint trace semantics. The correctness of the transformations is expressed through observational and performance abstractions. The framework is illustrated on three examples: constant propagation, program specialization by online and offline partial evaluation and static program monitoring.}},
  url = {https://doi.org/10.1145/503272.503290},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1309,
  title = {Cache-related preemption delay analysis for FIFO caches}},
  author = {Ballabriga, Cl\'{e}},
  year = {2014}},
  journal = {Proceedings of the 2014 SIGPLAN/SIGBED Conference on Languages, Compilers and Tools for Embedded Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Hard real-time systems are typically composed of multiple tasks, subjected to timing constraints. To guarantee that these constraints will be respected, the Worst-Case Response Time (WCRT) of each task is needed. In the presence of systems supporting preemptible tasks, we need to take into account the time lost due to task preemption. A major part of this delay is the Cache-Related Preemption Delay (CRPD), which represents the penalties due to cache block evictions by preempting tasks. Previous works on CRPD have focused on caches with Least Recently used (LRU) replacement policy. However, for many real-world processors such as ARM9 or ARM11, the use of First-in-first-out (FIFO) cache replacement policy is common.In this paper, we propose an approach to compute CRPD in the presence of instruction caches with FIFO replacement policy. We use the result of a FIFO instruction cache categorization analysis to account for single-task cache misses, and we model as an Integer Linear Programming (ILP) system the additional preemption-related cache misses. We study the effect of cache related timing anomalies, our work is the first to deal with the effect of timing anomalies in CRPD computation. We also present a WCRT computation method that takes advantage of the fact that our computed CRPD does not increase linearly with respect to the preemption count. We evaluated our method by computing the CRPD with realistic benchmarks (e.g. drone control application, robot controller application), under various cache configuration parameters. The experimentation shows that our method is able to compute tight CRPD bound for benchmark tasks.}},
  url = {https://doi.org/10.1145/2597809.2597814},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1310,
  title = {The missing link: dynamic components for ML}},
  author = {Rossberg, Andreas}},
  year = {2006}},
  journal = {Proceedings of the Eleventh ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Despite its powerful module system, ML has not yet evolved for the modern world of dynamic and open modular programming, to which more primitive languages have adapted better so far. We present the design and semantics of a simple yet expressive firstclass component system for ML. It provides dynamic linking in a type-safe and type-flexible manner, and allows selective execution in sandboxes. The system is defined solely by reduction to higherorder modules plus an extension with simple module-level dynamics, which we call packages. To represent components outside processes we employ generic pickling. We give a module calculus formalising the semantics of packages and pickling.}},
  url = {https://doi.org/10.1145/1159803.1159816},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1311,
  title = {A parametric segmentation functor for fully automatic and scalable array content analysis}},
  author = {Cousot, Patrick and Cousot, Radhia and Logozzo, Francesco}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We introduce FunArray, a parametric segmentation abstract domain functor for the fully automatic and scalable analysis of array content properties. The functor enables a natural, painless and efficient lifting of existing abstract domains for scalar variables to the analysis of uniform compound data-structures such as arrays and collections. The analysis automatically and semantically divides arrays into consecutive non-overlapping possibly empty segments. Segments are delimited by sets of bound expressions and abstracted uniformly. All symbolic expressions appearing in a bound set are equal in the concrete. The FunArray can be naturally combined via reduced product with any existing analysis for scalar variables. The analysis is presented as a general framework parameterized by the choices of bound expressions, segment abstractions and the reduction operator. Once the functor has been instantiated with fixed parameters, the analysis is fully automatic.We first prototyped FunArray in Arrayal to adjust and experiment with the abstractions and the algorithms to obtain the appropriate precision/ratio cost. Then we implemented it into Clousot, an abstract interpretation-based static contract checker for .NET. We empirically validated the precision and the performance of the analysis by running it on the main libraries of .NET and on its own code. We were able to infer thousands of non-trivial invariants and verify the implementation with a modest overhead (circa 1\%). To the best of our knowledge this is the first analysis of this kind applied to such a large code base, and proven to scale.}},
  url = {https://doi.org/10.1145/1926385.1926399},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1312,
  title = {Disjointness domains for fine-grained aliasing}},
  author = {Brandauer, Stephan and Clarke, Dave and Wrigstad, Tobias}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Aliasing is crucial for supporting useful implementation patterns, but it makes reasoning about programs difficult. To deal with this problem, numerous type-based aliasing control mechanisms have been proposed, expressing properties such as uniqueness. Uniqueness, however, is black-and-white: either a reference is unique or it can be arbitrarily aliased; and global: excluding aliases throughout the entire system, making code brittle to changing requirements. Disjointness domains, a new approach to alias control, address this problem by enabling more graduations between uniqueness and arbitrary reference sharing. They allow expressing aliasing constraints local to a certain set of variables (either stack variables or fields) for instance that no aliasing occurs between variables within some set of variables but between such sets or the opposite, that aliasing occurs within that set but not between different sets. A hierarchy of disjointness domains controls the flow of references through a program, helping the programmer reason about disjointness and enforce local alias invariants. The resulting system supports fine-grained control of aliasing between both variables and objects, making aliasing explicit to programmers, compilers, and tooling. This paper presents a formal account of disjointness domains along with examples. Disjointness domains provide novel means of expressing may-alias kinds of constraints, which may prove useful in compiler optimisation and verification.}},
  url = {https://doi.org/10.1145/2814270.2814280},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1313,
  title = {Buffer minimization in earliest-deadline first scheduling of dataflow graphs}},
  author = {Bouakaz, Adnan and Talpin, Jean-Pierre}},
  year = {2013}},
  journal = {Proceedings of the 14th ACM SIGPLAN/SIGBED Conference on Languages, Compilers and Tools for Embedded Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Symbolic schedulability analysis of dataflow graphs is the process of synthesizing the timing parameters (i.e. periods, phases, and deadlines) of actors so that the task system is schedulable and achieves a high throughput when using a specific scheduling policy. Furthermore, the resulted schedule must ensure that communication buffers are underflow- and overflow-free. This paper describes a (partitioned) earliest-deadline first symbolic schedulability analysis of dataflow graphs that minimizes the buffering requirements.Our scheduling analysis consists of three major steps. (1) The construction of an abstract affine schedule of the graph that excludes overflow and underflow exceptions and minimizes the buffering requirements assuming some precedences between jobs. (2) Symbolic deadlines adjustment that guarantees precedences without the need for lock-based synchronizations. (3) The concretization of the affine schedule using a symbolic, fast-converging, processor-demand analysis for both uniprocessor and multiprocessor systems. Experimental results show that our technique improves the buffering requirements in many cases.}},
  url = {https://doi.org/10.1145/2491899.2465558},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1314,
  title = {Grain graphs: OpenMP performance analysis made easy}},
  author = {Muddukrishna, Ananya and Jonsson, Peter A. and Podobas, Artur and Brorsson, Mats}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Average programmers struggle to solve performance problems in OpenMP programs with tasks and parallel for-loops. Existing performance analysis tools visualize OpenMP task performance from the runtime system's perspective where task execution is interleaved with other tasks in an unpredictable order. Problems with OpenMP parallel for-loops are similarly difficult to resolve since tools only visualize aggregate thread-level statistics such as load imbalance without zooming into a per-chunk granularity. The runtime system/threads oriented visualization provides poor support for understanding problems with task and chunk execution time, parallelism, and memory hierarchy utilization, forcing average programmers to rely on experts or use tedious trial-and-error tuning methods for performance. We present grain graphs, a new OpenMP performance analysis method that visualizes grains -- computation performed by a task or a parallel for-loop chunk instance -- and highlights problems such as low parallelism, work inflation and poor parallelization benefit at the grain level. We demonstrate that grain graphs can quickly reveal performance problems that are difficult to detect and characterize in fine detail using existing visualizations in standard OpenMP programs, simplifying OpenMP performance analysis. This enables average programmers to make portable optimizations for poor performing OpenMP programs, reducing pressure on experts and removing the need for tedious trial-and-error tuning.}},
  url = {https://doi.org/10.1145/2851141.2851156},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1315,
  title = {A typechecker plugin for units of measure: domain-specific constraint solving in GHC Haskell}},
  author = {Gundry, Adam}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Typed functional programming and units of measure are a natural combination, as F# ably demonstrates. However, encoding statically-checked units in Haskell’s type system leads to inevitable disappointment with the usability of the resulting system. Extending the language itself would produce a much better result, but it would be a lot of work! In this paper, I demonstrate how typechecker plugins in the Glasgow Haskell Compiler allow users to define domain-specific constraint solving behaviour, making it possible to implement units of measure as a type system extension without rebuilding the compiler. This paves the way for a more modular treatment of constraint solving in GHC.}},
  url = {https://doi.org/10.1145/2804302.2804305},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1316,
  title = {Efficient, portable implementation of asynchronous multi-place programs}},
  author = {Bikshandi, Ganesh and Castanos, Jose G. and Kodali, Sreedhar B. and Nandivada, V. Krishna and Peshansky, Igor and Saraswat, Vijay A. and Sur, Sayantan and Varma, Pradeep and Wen, Tong}},
  year = {2009}},
  journal = {Proceedings of the 14th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The X10 programming language is organized around the notion of places (an encapsulation of data and activities operating on the data), partitioned global address space (PGAS), and asynchronous computation and communication.This paper introduces an expressive subset of X10, Flat X10, designed to permit efficient execution across multiple single-threaded places with a simple runtime and without compromising on the productivity of X10. We present the design, implementation and evaluation of a compiler and runtime system for Flat X10. The Flat X10 compiler translates programs into C++ SPMD programs communicating using an active messaging infrastructure. It uses novel techniques to transform explicitly parallel programs into SPMD programs. The runtime system is based on IBM's LAPI (Low-level API) and is easily portable to other libraries such as GASNet and ARMCI.Our implementation realizes performance comparable to hand-written MPI programs for well-known HPC benchmarks such as Random Access, Stream, and FFT, on a Federation-based cluster of Power5 SMPs (with hundreds of processors) and the Blue Gene (with thousands of processors). Submissions based on the work presented in this paper were co-winners of the 2007 and 2008 HPC Challenge Type II Awards.}},
  url = {https://doi.org/10.1145/1504176.1504215},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1317,
  title = {Compact data structure and scalable algorithms for the sparse grid technique}},
  author = {Murarasu, Alin and Weidendorfer, Josef and Buse, Gerrit and Butnaru, Daniel and Pfl\"{u}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The sparse grid discretization technique enables a compressed representation of higher-dimensional functions. In its original form, it relies heavily on recursion and complex data structures, thus being far from well-suited for GPUs. In this paper, we describe optimizations that enable us to implement compression and decompression, the crucial sparse grid algorithms for our application, on Nvidia GPUs. The main idea consists of a bijective mapping between the set of points in a multi-dimensional sparse grid and a set of consecutive natural numbers. The resulting data structure consumes a minimum amount of memory. For a 10-dimensional sparse grid with approximately 127 million points, it consumes up to 30 times less memory than trees or hash tables which are typically used. Compared to a sequential CPU implementation, the speedups achieved on GPU are up to 17 for compression and up to 70 for decompression, respectively. We show that the optimizations are also applicable to multicore CPUs.}},
  url = {https://doi.org/10.1145/1941553.1941559},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1318,
  title = {Contracts for modular discrete controller synthesis}},
  author = {Delaval, Gwena\"{e}},
  year = {2010}},
  journal = {Proceedings of the ACM SIGPLAN/SIGBED 2010 Conference on Languages, Compilers, and Tools for Embedded Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We describe the extension of a reactive programming language with a behavioral contract construct. It is dedicated to the programming of reactive control of applications in embedded systems, and involves principles of the supervisory control of discrete event systems. Our contribution is in a language approach where modular discrete controller synthesis (DCS) is integrated, and it is concretized in the encapsulation of DCS into a compilation process. From transition system specifications of possible behaviors, DCS automatically produces controllers that make the controlled system satisfy the property given as objective. Our language features and compiling technique provide correctness-by-construction in that sense, and enhance reliability and verifiability. Our application domain is adaptive and reconfigurable systems: closed-loop adaptation mechanisms enable flexible execution of functionalities w.r.t. changing resource and environment conditions. Our language can serve programming such adaption controllers. This paper particularly describes the compilation of the language. We present a method for the modular application of discrete controller synthesis on synchronous programs, and its integration in the BZR language. We consider structured programs, as a composition of nodes, and first apply DCS on particular nodes of the program, in order to reduce the complexity of the controller computation; then, we allow the abstraction of parts of the program for this computation; and finally, we show how to recompose the different controllers computed from different abstractions for their correct co-execution with the initial program. Our work is illustrated with examples, and we present quantitative results about its implementation.}},
  url = {https://doi.org/10.1145/1755888.1755898},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1319,
  title = {Principled parsing for indentation-sensitive languages: revisiting landin's offside rule}},
  author = {Adams, Michael D.}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Several popular languages, such as Haskell, Python, and F#, use the indentation and layout of code as part of their syntax. Because context-free grammars cannot express the rules of indentation, parsers for these languages currently use ad hoc techniques to handle layout. These techniques tend to be low-level and operational in nature and forgo the advantages of more declarative specifications like context-free grammars. For example, they are often coded by hand instead of being generated by a parser generator.This paper presents a simple extension to context-free grammars that can express these layout rules, and derives GLR and LR(k) algorithms for parsing these grammars. These grammars are easy to write and can be parsed efficiently. Examples for several languages are presented, as are benchmarks showing the practical efficiency of these algorithms.}},
  url = {https://doi.org/10.1145/2429069.2429129},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1320,
  title = {White box sampling in uncertain data processing enabled by program analysis}},
  author = {Bao, Tao and Zheng, Yunhui and Zhang, Xiangyu}},
  year = {2012}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Sampling is a very important and low-cost approach to uncertain data processing, in which output variations caused by input errors are sampled. Traditional methods tend to treat a program as a blackbox. In this paper, we show that through program analysis, we can expose the internals of sample executions so that the process can become more selective and focused. In particular, we develop a sampling runtime that can selectively sample in input error bounds to expose discontinuity in output functions. It identifies all the program factors that can potentially lead to discontinuity and hash the values of such factors during execution in a cost-effective way. The hash values are used to guide the sampling process. Our results show that the technique is very effective for real-world programs. It can achieve the precision of a high sampling rate with the cost of a lower sampling rate.}},
  url = {https://doi.org/10.1145/2384616.2384681},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1321,
  title = {Directed incremental symbolic execution}},
  author = {Person, Suzette and Yang, Guowei and Rungta, Neha and Khurshid, Sarfraz}},
  year = {2011}},
  journal = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The last few years have seen a resurgence of interest in the use of symbolic execution -- a program analysis technique developed more than three decades ago to analyze program execution paths. Scaling symbolic execution and other path-sensitive analysis techniques to large systems remains challenging despite recent algorithmic and technological advances. An alternative to solving the problem of scalability is to reduce the scope of the analysis. One approach that is widely studied in the context of regression analysis is to analyze the differences between two related program versions. While such an approach is intuitive in theory, finding efficient and precise ways to identify program differences, and characterize their effects on how the program executes has proved challenging in practice.In this paper, we present Directed Incremental Symbolic Execution (DiSE), a novel technique for detecting and characterizing the effects of program changes. The novelty of DiSE is to combine the efficiencies of static analysis techniques to compute program difference information with the precision of symbolic execution to explore program execution paths and generate path conditions affected by the differences. DiSE is a complementary technique to other reduction or bounding techniques developed to improve symbolic execution. Furthermore, DiSE does not require analysis results to be carried forward as the software evolves -- only the source code for two related program versions is required. A case-study of our implementation of DiSE illustrates its effectiveness at detecting and characterizing the effects of program changes.}},
  url = {https://doi.org/10.1145/1993498.1993558},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1322,
  title = {Shape analysis with inductive recursion synthesis}},
  author = {Guo, Bolei and Vachharajani, Neil and August, David I.}},
  year = {2007}},
  journal = {Proceedings of the 28th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Separation logic with recursively defined predicates allows for concise yet precise description of the shapes of data structures. However, most uses of separation logic for program analysis rely on pre-defined recursive predicates, limiting the class of programs analyzable to those that manipulate only a priori data structures. This paper describes a general algorithm based on inductive program synthesis that automatically infers recursive shape invariants, yielding a shape analysis based on separation logic that can be applied to any program.A key strength of separation logic is that it facilitates, via explicit expression of structural separation, local reasoning about heap where the effects of altering one part of a data structure are analyzed in isolation from the rest. The interaction between local reasoning and the global invariants given by recursive predicates is a difficult area, especially in the presence of complex internal sharing in the data structures. Existing approaches, using logic rules specifically designed for the list predicate to unfold and fold linked-lists, again require a priori knowledge about the shapes of the data structures and do not easily generalize to more complex data structures. We introduce a notion of "truncation points" in a recursive predicate, which gives rise to generic algorithms for unfolding and folding arbitrary data structures.}},
  url = {https://doi.org/10.1145/1250734.1250764},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1323,
  title = {Finding reusable data structures}},
  author = {Xu, Guoqing}},
  year = {2012}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A big source of run-time performance problems in large-scale, object-oriented applications is the frequent creation of data structures (by the same allocation site) whose lifetimes are disjoint, and whose shapes and data content are always the same. Constructing these data structures and computing the same data values many times is expensive; significant performance improvements can be achieved by reusing their instances, shapes, and/or data values rather than reconstructing them. This paper presents a run-time technique that can be used to help programmers find allocation sites that create such data structures to improve performance. At the heart of the technique are three reusability definitions and novel summarization approaches that compute summaries for data structures based on these definitions. The computed summaries are used subsequently to find data structures that have disjoint lifetimes, and/or that have the same shapes and content. We have implemented this technique in the Jikes RVM and performed extensive studies on large-scale, real-world programs. We describe our experience using six case studies, in which we have achieved large performance gains by fixing problems reported by our tool.}},
  url = {https://doi.org/10.1145/2384616.2384690},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1324,
  title = {Understanding POWER multiprocessors}},
  author = {Sarkar, Susmit and Sewell, Peter and Alglave, Jade and Maranget, Luc and Williams, Derek}},
  year = {2011}},
  journal = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Exploiting today's multiprocessors requires high-performance and correct concurrent systems code (optimising compilers, language runtimes, OS kernels, etc.), which in turn requires a good understanding of the observable processor behaviour that can be relied on. Unfortunately this critical hardware/software interface is not at all clear for several current multiprocessors.In this paper we characterise the behaviour of IBM POWER multiprocessors, which have a subtle and highly relaxed memory model (ARM multiprocessors have a very similar architecture in this respect). We have conducted extensive experiments on several generations of processors: POWER G5, 5, 6, and 7. Based on these, on published details of the microarchitectures, and on discussions with IBM staff, we give an abstract-machine semantics that abstracts from most of the implementation detail but explains the behaviour of a range of subtle examples. Our semantics is explained in prose but defined in rigorous machine-processed mathematics; we also confirm that it captures the observable processor behaviour, or the architectural intent, for our examples with an executable checker. While not officially sanctioned by the vendor, we believe that this model gives a reasonable basis for reasoning about current POWER multiprocessors.Our work should bring new clarity to concurrent systems programming for these architectures, and is a necessary precondition for any analysis or verification. It should also inform the design of languages such as C and C++, where the language memory model is constrained by what can be efficiently compiled to such multiprocessors.}},
  url = {https://doi.org/10.1145/1993498.1993520},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1325,
  title = {Mixed-mode multicore reliability}},
  author = {Wells, Philip M. and Chakraborty, Koushik and Sohi, Gurindar S.}},
  year = {2009}},
  journal = {Proceedings of the 14th International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Future processors are expected to observe increasing rates of hardware faults. Using Dual-Modular Redundancy (DMR), two cores of a multicore can be loosely coupled to redundantly execute a single software thread, providing very high coverage from many difference sources of faults. This reliability, however, comes at a high price in terms of per-thread IPC and overall system throughput.We make the observation that a user may want to run both applications requiring high reliability, such as financial software, and more fault tolerant applications requiring high performance, such as media or web software, on the same machine at the same time. Yet a traditional DMR system must fully operate in redundant mode whenever any application requires high reliability.This paper proposes a Mixed-Mode Multicore (MMM), which enables most applications, including the system software, to run with high reliability in DMR mode, while applications that need high performance can avoid the penalty of DMR. Though conceptually simple, two key challenges arise: 1) care must be taken to protect reliable applications from any faults occurring to applications running in high performance mode, and 2) the desire to execute additional independent software threads for a performance application complicates the scheduling of computation to cores. After solving these issues, an MMM is shown to improve overall system performance, compared to a traditional DMR system, by approximately 2X when one reliable and one performance application are concurrently executing.}},
  url = {https://doi.org/10.1145/1508244.1508265},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1326,
  title = {The implementation and evaluation of fusion and contraction in array languages}},
  author = {Lewis, E. Christopher and Lin, Calvin and Snyder, Lawrence}},
  year = {1998}},
  journal = {Proceedings of the ACM SIGPLAN 1998 Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Array languages such as Fortran 90, HPF and ZPL have many benefits in simplifying array-based computations and expressing data parallelism. However, they can suffer large performance penalties because they introduce intermediate arrays---both at the source level and during the compilation process---which increase memory usage and pollute the cache. Most compilers address this problem by simply scalarizing the array language and relying on a scalar language compiler to perform loop fusion and array contraction. We instead show that there are advantages to performing a form of loop fusion and array contraction at the array level. This paper describes this approach and explains its advantages. Experimental results show that our scheme typically yields runtime improvements of greater than 20\% and sometimes up to 400\%. In addition, it yields superior memory use when compared against commercial compilers and exhibits comparable memory use when compared with scalar languages. We also explore the interaction between these transformations and communication optimizations.}},
  url = {https://doi.org/10.1145/277650.277663},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1327,
  title = {Copperhead: compiling an embedded data parallel language}},
  author = {Catanzaro, Bryan and Garland, Michael and Keutzer, Kurt}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Modern parallel microprocessors deliver high performance on applications that expose substantial fine-grained data parallelism. Although data parallelism is widely available in many computations, implementing data parallel algorithms in low-level languages is often an unnecessarily difficult task. The characteristics of parallel microprocessors and the limitations of current programming methodologies motivate our design of Copperhead, a high-level data parallel language embedded in Python. The Copperhead programmer describes parallel computations via composition of familiar data parallel primitives supporting both flat and nested data parallel computation on arrays of data. Copperhead programs are expressed in a subset of the widely used Python programming language and interoperate with standard Python modules, including libraries for numeric computation, data visualization, and analysis. In this paper, we discuss the language, compiler, and runtime features that enable Copperhead to efficiently execute data parallel code. We define the restricted subset of Python which Copperhead supports and introduce the program analysis techniques necessary for compiling Copperhead code into efficient low-level implementations. We also outline the runtime support by which Copperhead programs interoperate with standard Python modules. We demonstrate the effectiveness of our techniques with several examples targeting the CUDA platform for parallel programming on GPUs. Copperhead code is concise, on average requiring 3.6 times fewer lines of code than CUDA, and the compiler generates efficient code, yielding 45-100\% of the performance of hand-crafted, well optimized CUDA code.}},
  url = {https://doi.org/10.1145/1941553.1941562},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1328,
  title = {Scalable framework for mapping streaming applications onto multi-GPU systems}},
  author = {Huynh, Huynh Phung and Hagiescu, Andrei and Wong, Weng-Fai and Goh, Rick Siow Mong}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Graphics processing units leverage on a large array of parallel processing cores to boost the performance of a specific streaming computation pattern frequently found in graphics applications. Unfortunately, while many other general purpose applications do exhibit the required streaming behavior, they also possess unfavorable data layout and poor computation-to-communication ratios that penalize any straight-forward execution on the GPU. In this paper we describe an efficient and scalable code generation framework that can map general purpose streaming applications onto a multi-GPU system. This framework spans the entire core and memory hierarchy exposed by the multi-GPU system. Several key features in our framework ensure the scalability required by complex streaming applications. First, we propose an efficient stream graph partitioning algorithm that partitions the complex application to achieve the best performance under a given shared memory constraint. Next, the resulting partitions are mapped to multiple GPUs using an efficient architecture-driven strategy. The mapping balances the workload while considering the communication overhead. Finally, a highly effective pipeline execution is employed for the execution of the partitions on the multi-GPU system. The framework has been implemented as a back-end of the StreamIt programming language compiler. Our comprehensive experiments show its scalability and significant performance speedup compared with a previous state-of-the-art solution.}},
  url = {https://doi.org/10.1145/2145816.2145818},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1329,
  title = {On inter-procedural analysis of programs with lists and data}},
  author = {Bouajjani, Ahmed and Dr\u{a}},
  year = {2011}},
  journal = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We address the problem of automatic synthesis of assertions on sequential programs with singly-linked lists containing data over infinite domains such as integers or reals. Our approach is based on an accurate abstract inter-procedural analysis. Program configurations are represented by graphs where nodes represent list segments without sharing. The data in these list segments are characterized by constraints in abstract domains. We consider a domain where constraints are in a universally quantified fragment of the first-order logic over sequences, as well as a domain constraining the multisets of data in sequences.Our analysis computes the effect of each procedure in a local manner, by considering only the reachable part of the heap from its actual parameters. In order to avoid losses of information, we introduce a mechanism based on unfolding/folding operations allowing to strengthen the analysis in the domain of first-order formulas by the analysis in the multisets domain.The same mechanism is used for strengthening the sound (but incomplete) entailment operator of the domain of first-order formulas. We have implemented our techniques in a prototype tool and we have shown that our approach is powerful enough for automatic (1) generation of non-trivial procedure summaries, (2) pre/post-condition reasoning, and (3) procedure equivalence checking.}},
  url = {https://doi.org/10.1145/1993498.1993566},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1330,
  title = {An integrated proof language for imperative programs}},
  author = {Zee, Karen and Kuncak, Viktor and Rinard, Martin C.}},
  year = {2009}},
  journal = {Proceedings of the 30th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present an integrated proof language for guiding the actions of multiple reasoning systems as they work together to prove complex correctness properties of imperative programs. The language operates in the context of a program verification system that uses multiple reasoning systems to discharge generated proof obligations. It is designed to 1) enable developers to resolve key choice points in complex program correctness proofs, thereby enabling automated reasoning systems to successfully prove the desired correctness properties; 2) allow developers to identify key lemmas for the reasoning systems to prove, thereby guiding the reasoning systems to find an effective proof decomposition; 3) enable multiple reasoning systems to work together productively to prove a single correctness property by providing a mechanism that developers can use to divide the property into lemmas, each of which is suitable for a different reasoning system; and 4) enable developers to identify specific lemmas that the reasoning systems should use when attempting to prove other lemmas or correctness properties, thereby appropriately confining the search space so that the reasoning systems can find a proof in an acceptable amount of time.The language includes a rich set of declarative proof constructs that enables developers to direct the reasoning systems as little or as much as they desire. Because the declarative proof statements are embedded into the program as specialized comments, they also serve as verified documentation and are a natural extension of the assertion mechanism found in most program verification systems.We have implemented our integrated proof language in the context of a program verification system for Java and used the resulting system to verify a collection of linked data structure implementations. Our experience indicates that our proof language makes it possible to successfully prove complex program correctness properties that are otherwise beyond the reach of automated reasoning systems.}},
  url = {https://doi.org/10.1145/1542476.1542514},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1331,
  title = {Algorithms for user interfaces}},
  author = {J\"{a}},
  year = {2009}},
  journal = {Proceedings of the Eighth International Conference on Generative Programming and Component Engineering}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {User interfaces for modern applications must support a rich set of interactive features. It is commonplace to find applications with dependencies between values manipulated by user interface elements, conditionally enabled controls, and script record-ability and playback against different documents. A significant fraction of the application programming effort is devoted to implementing such functionality, and the resulting code is typically not reusable.This paper extends our "property models" approach to programming user interfaces. Property models allow a large part of the functionality of a user interface to be implemented in reusable libraries, reducing application specific code to a set of declarative rules. We describe how, as a by-product of computations that maintain the values of user interface elements, property models obtain accurate information of the currently active dependencies among those elements. This information enables further expanding the class of user interface functionality that we can encode as generic algorithms. In particular, we describe automating the decisions for the enablement of user interface widgets and activation of command widgets. Failing to disable or deactivate widgets correctly is a common source of user-interface defects, which our approach largely removes.We report on the increased reuse, reduced defect rates, and improved user interface design turnarounds in a commercial software development effort as a result of adopting our approach.}},
  url = {https://doi.org/10.1145/1621607.1621630},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1332,
  title = {Meta-programming with names and necessity}},
  author = {Nanevski, Aleksandar}},
  year = {2002}},
  journal = {Proceedings of the Seventh ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Meta-programming languages provide infrastructure to generate and execute object programs at run-time. In a typed setting, they contain a modal type constructor which classifies object code. These code types generally come in two flavors: closed and open. Closed code expressions can be invoked at run-time, but the computations over them are more rigid, and typically produce less efficient residual object programs. Open code provides better inlining and partial evaluation of object programs, but once constructed, expressions of this type cannot in general be evaluated.Recent work in this area has focused on combining the two notions into a sound system. We present a novel way to achieve this. It is based on adding the notion of names from the work on Nominal Logic and FreshML to the λ -calculus of proof terms for the necessity fragment of modal logic S4. The resulting language provides a more fine-grained control over free variables of object programs when compared to the existing languages for meta-programming. In addition, this approach lends itself well to addition of intensional code analysis, i.e. ability of meta programs to inspect and destruct object programs at run-time in a type-safe manner, which we also undertake.}},
  url = {https://doi.org/10.1145/581478.581498},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1333,
  title = {Compositional CompCert}},
  author = {Stewart, Gordon and Beringer, Lennart and Cuellar, Santiago and Appel, Andrew W.}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper reports on the development of Compositional CompCert, the first verified separate compiler for C.Specifying and proving separate compilation for C is made challenging by the coincidence of: compiler optimizations, such as register spilling, that introduce compiler-managed (private) memory regions into function stack frames, and C's stack-allocated addressable local variables, which may leak portions of stack frames to other modules when their addresses are passed as arguments to external function calls. The CompCert compiler, as built/proved by Leroy etal 2006--2014, has proofs of correctness for whole programs, but its simulation relations are too weak to specify or prove separately compiled modules.Our technical contributions that make Compositional CompCert possible include: language-independent linking, a new operational model of multilanguage linking that supports strong semantic contextual equivalences; and structured simulations, a refinement of Beringer etal logical simulation relations that enables expressive module-local invariants on the state communicated between compilation units at runtime. All the results in the paper have been formalized in Coq and are available for download together with the Compositional CompCert compiler.}},
  url = {https://doi.org/10.1145/2676726.2676985},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1334,
  title = {Generating precise and concise procedure summaries}},
  author = {Yorsh, Greta and Yahav, Eran and Chandra, Satish}},
  year = {2008}},
  journal = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a framework for generating procedure summaries that are (a) precise - applying the summary in a given context yields the same result as re-analyzing the procedure in that context, and(b) concise - the summary exploits the commonalitiesin the ways the procedure manipulates abstract values, and does not contain superfluous context information.The use of a precise and concise procedure summary inmodular analyses provides a way to capture infinitely many possible contexts in a finite way; in interprocedural analyses, it provides a compact representation of an explicit input-output summary table without loss of precision.We define a class of abstract domains and transformers for which precise and concise summaries can be efficiently generated using our framework. Our framework is rich enough to encode a wide range of problems, including all IFDS and IDE problems. In addition, we show how the framework is instantiated to provide novel solutions to two hard problems: modular linear constant propagation and modular typestate verification, both in the presence of aliasing. We implemented a prototype of our framework that computes summaries for the typestate domain, and report on preliminary experimental results.}},
  url = {https://doi.org/10.1145/1328438.1328467},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1335,
  title = {Copatterns: programming infinite structures by observations}},
  author = {Abel, Andreas and Pientka, Brigitte and Thibodeau, David and Setzer, Anton}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Inductive datatypes provide mechanisms to define finite data such as finite lists and trees via constructors and allow programmers to analyze and manipulate finite data via pattern matching. In this paper, we develop a dual approach for working with infinite data structures such as streams. Infinite data inhabits coinductive datatypes which denote greatest fixpoints. Unlike finite data which is defined by constructors we define infinite data by observations. Dual to pattern matching, a tool for analyzing finite data, we develop the concept of copattern matching, which allows us to synthesize infinite data. This leads to a symmetric language design where pattern matching on finite and infinite data can be mixed.We present a core language for programming with infinite structures by observations together with its operational semantics based on (co)pattern matching and describe coverage of copatterns. Our language naturally supports both call-by-name and call-by-value interpretations and can be seamlessly integrated into existing languages like Haskell and ML. We prove type soundness for our language and sketch how copatterns open new directions for solving problems in the interaction of coinductive and dependent types.}},
  url = {https://doi.org/10.1145/2429069.2429075},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1336,
  title = {Towards batched linear solvers on accelerated hardware platforms}},
  author = {Haidar, Azzam and Dong, Tingxing and Luszczek, Piotr and Tomov, Stanimire and Dongarra, Jack}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {As hardware evolves, an increasingly effective approach to develop energy efficient, high-performance solvers, is to design them to work on many small and independent problems. Indeed, many applications already need this functionality, especially for GPUs, which are known to be currently about four to five times more energy efficient than multicore CPUs for every floating-point operation. In this paper, we describe the development of the main one-sided factorizations: LU, QR, and Cholesky; that are needed for a set of small dense matrices to work in parallel. We refer to such algorithms as batched factorizations. Our approach is based on representing the algorithms as a sequence of batched BLAS routines for GPU-contained execution. Note that this is similar in functionality to the LAPACK and the hybrid MAGMA algorithms for large-matrix factorizations. But it is different from a straightforward approach, whereby each of GPU's symmetric multiprocessors factorizes a single problem at a time. We illustrate how our performance analysis together with the profiling and tracing tools guided the development of batched factorizations to achieve up to 2-fold speedup and 3-fold better energy efficiency compared to our highly optimized batched CPU implementations based on the MKL library on a two-sockets, Intel Sandy Bridge server. Compared to a batched LU factorization featured in the NVIDIA's CUBLAS library for GPUs, we achieves up to 2.5-fold speedup on the K40 GPU.}},
  url = {https://doi.org/10.1145/2688500.2688534},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1337,
  title = {Delinearization: an efficient way to break multiloop dependence equations}},
  author = {Maslov, Vadim}},
  year = {1992}},
  journal = {Proceedings of the ACM SIGPLAN 1992 Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Exact and efficient data dependence testing is a key to success of loop-parallelizing compiler for computationally intensive programs. A number of algorithms has been created to test array references contained in parameter loops for dependence but most of them are unable to answer the following question correctly: Are references C(i1 + 10j1) and C(i2 + 5), 0 ≤ i1, i2 ≤ 4, 0 ≤ j1,j2 ≤ 9 independent? The technique introduced in this paper recognizes that i1, i2 and j1, j2 make different order contributions to the subscript index, and breaks dependence equation i1 + 10j1 = i2 + 10j2 + 5 into two equations i1 = i2 and 10j1 = 10j2 which then can be solved independently. Since resulting equations contain less variables it is less expensive to solve them. We call this technique delinearization because it is reverse of the linearization much discussed in the literature.In the introduction we demonstrate that linearized references are used not infrequently in scientific FORTRAN and C codes. Then we present a theorem on which delinearization algorithm is based and the algorithm itself. The algorithm is fairly simple and inexpensive. As a byproduct it tests equations it produces for independence as exactly as it is done by GCD-test and Banerjee inequalities combined. The algorithm has been implemented at Moscow State University in a vectorizer named VIC.}},
  url = {https://doi.org/10.1145/143095.143130},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1338,
  title = {Iterative type inference with attribute grammars}},
  author = {Middelkoop, Arie and Dijkstra, Atze and Swierstra, S. Doaitse}},
  year = {2010}},
  journal = {Proceedings of the Ninth International Conference on Generative Programming and Component Engineering}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Type inference is the process of constructing a typing derivation while gradually discovering type information. During this process, inference algorithms typically make subtle decisions based on the derivation constructed so far.Because a typing derivation is a decorated tree we aim to use attribute grammars as the main implementation tool. Unfortunately, we can neither express iteration, nor express decisions based on intermediate derivations in such grammars.We present the language ruler-front, a conservative extension to ordered attribute grammars, that deals with the aforementioned problems. We show why this extension is suitable for the description of constraint-based inference algorithms.}},
  url = {https://doi.org/10.1145/1868294.1868302},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1339,
  title = {Verification of semantic commutativity conditions and inverse operations on linked data structures}},
  author = {Kim, Deokhwan and Rinard, Martin C.}},
  year = {2011}},
  journal = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a new technique for verifying commutativity conditions, which are logical formulas that characterize when operations commute. Because our technique reasons with the abstract state of verified linked data structure implementations, it can verify commuting operations that produce semantically equivalent (but not necessarily identical) data structure states in different execution orders. We have used this technique to verify sound and complete commutativity conditions for all pairs of operations on a collection of linked data structure implementations, including data structures that export a set interface (ListSet and HashSet) as well as data structures that export a map interface (AssociationList, HashTable, and ArrayList). This effort involved the specification and verification of 765 commutativity conditions.Many speculative parallel systems need to undo the effects of speculatively executed operations. Inverse operations, which undo these effects, are often more efficient than alternate approaches (such as saving and restoring data structure state). We present a new technique for verifying such inverse operations. We have specified and verified, for all of our linked data structure implementations, an inverse operation for every operation that changes the data structure state.Together, the commutativity conditions and inverse operations provide a key resource that language designers, developers of program analysis systems, and implementors of software systems can draw on to build languages, program analyses, and systems with strong correctness guarantees.}},
  url = {https://doi.org/10.1145/1993498.1993561},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1340,
  title = {AVANCE: an object management system}},
  author = {Bjornerstedt, Anders and Britts, Stefan}},
  year = {1988}},
  journal = {Conference Proceedings on Object-Oriented Programming Systems, Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {AVANCE1 is an integrated application development and run-time system. It provides facilities for programming with shared and persistent objects, transactions and processes. The architecture is designed with decentralization in mind by having a large object identifier space and a remote procedure call interface to objects. Emphasis in this paper is on the programming language PAL and its relation with the underlying virtual machine.}},
  url = {https://doi.org/10.1145/62083.62102},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1341,
  title = {A look at several memory management units, TLB-refill mechanisms, and page table organizations}},
  author = {Jacob, Bruce L. and Mudge, Trevor N.}},
  year = {1998}},
  journal = {Proceedings of the Eighth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Virtual memory is a staple in modem systems, though there is little agreement on how its functionality is to be implemented on either the hardware or software side of the interface. The myriad of design choices and incompatible hardware mechanisms suggests potential performance problems, especially since increasing numbers of systems (even embedded systems) are using memory management. A comparative study of the implementation choices in virtual memory should therefore aid system-level designers.This paper compares several virtual memory designs, including combinations of hierarchical and inverted page tables on hardware-managed and software-managed translation lookaside buffers (TLBs). The simulations show that systems are fairly sensitive to TLB size; that interrupts already account for a large portion of memory-management overhead and can become a significant factor as processors execute more concurrent instructions; and that if one includes the cache misses inflicted on applications by the VM system, the total VM overhead is roughly twice what was thought (10--20\% rather than 5--10\%).}},
  url = {https://doi.org/10.1145/291069.291065},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1342,
  title = {Synthesizing racy tests}},
  author = {Samak, Malavika and Ramanathan, Murali Krishna and Jagannathan, Suresh}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Subtle concurrency errors in multithreaded libraries that arise because of incorrect or inadequate synchronization are often difficult to pinpoint precisely using only static techniques. On the other hand, the effectiveness of dynamic race detectors is critically dependent on multithreaded test suites whose execution can be used to identify and trigger races. Usually, such multithreaded tests need to invoke a specific combination of methods with objects involved in the invocations being shared appropriately to expose a race. Without a priori knowledge of the race, construction of such tests can be challenging. In this paper, we present a lightweight and scalable technique for synthesizing precisely these kinds of tests. Given a multithreaded library and a sequential test suite, we describe a fully automated analysis that examines sequential execution traces, and produces as its output a concurrent client program that drives shared objects via library method calls to states conducive for triggering a race. Experimental results on a variety of well-tested Java libraries yield 101 synthesized multithreaded tests in less than four minutes. Analyzing the execution of these tests using an off-the-shelf race detector reveals 187 harmful races, including several previously unreported ones. Our implementation, named NARADA, and the results of our experiments are available at http://www.csa.iisc.ernet.in/~sss/tools/narada.}},
  url = {https://doi.org/10.1145/2737924.2737998},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1343,
  title = {Functional modelling of musical harmony: an experience report}},
  author = {Magalh\~{a}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Music theory has been essential in composing and performing music for centuries. Within Western tonal music, from the early Baroque on to modern-day jazz and pop music, the function of chords within a chord sequence can be explained by harmony theory. Although Western tonal harmony theory is a thoroughly studied area, formalising this theory is a hard problem.We present a formalisation of the rules of tonal harmony as a Haskell (generalized) algebraic datatype. Given a sequence of chord labels, the harmonic function of a chord in its tonal context is automatically derived. For this, we use several advanced functional programming techniques, such as type-level computations, datatype-generic programming, and error-correcting parsers. As a detailed example, we show how our model can be used to improve content-based retrieval of jazz songs.We explain why Haskell is the perfect match for these tasks, and compare our implementation to an earlier solution in Java. We also point out shortcomings of the language and libraries that limit our work, and discuss future developments which may ameliorate our solution.}},
  url = {https://doi.org/10.1145/2034773.2034797},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1344,
  title = {Algorithm-based fault tolerance for dense matrix factorizations}},
  author = {Du, Peng and Bouteiller, Aurelien and Bosilca, George and Herault, Thomas and Dongarra, Jack}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Dense matrix factorizations, such as LU, Cholesky and QR, are widely used for scientific applications that require solving systems of linear equations, eigenvalues and linear least squares problems. Such computations are normally carried out on supercomputers, whose ever-growing scale induces a fast decline of the Mean Time To Failure (MTTF). This paper proposes a new hybrid approach, based on Algorithm-Based Fault Tolerance (ABFT), to help matrix factorizations algorithms survive fail-stop failures. We consider extreme conditions, such as the absence of any reliable component and the possibility of loosing both data and checksum from a single failure. We will present a generic solution for protecting the right factor, where the updates are applied, of all above mentioned factorizations. For the left factor, where the panel has been applied, we propose a scalable checkpointing algorithm. This algorithm features high degree of checkpointing parallelism and cooperatively utilizes the checksum storage leftover from the right factor protection. The fault-tolerant algorithms derived from this hybrid solution is applicable to a wide range of dense matrix factorizations, with minor modifications. Theoretical analysis shows that the fault tolerance overhead sharply decreases with the scaling in the number of computing units and the problem size. Experimental results of LU and QR factorization on the Kraken (Cray XT5) supercomputer validate the theoretical evaluation and confirm negligible overhead, with- and without-errors.}},
  url = {https://doi.org/10.1145/2145816.2145845},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1345,
  title = {Combining syntactic and semantic bidirectionalization}},
  author = {Voigtl\"{a}},
  year = {2010}},
  journal = {Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Matsuda et al. [2007, ICFP] and Voigtl\"{a}},
  url = {https://doi.org/10.1145/1863543.1863571},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1346,
  title = {Remix: online detection and repair of cache contention for the JVM}},
  author = {Eizenberg, Ariel and Hu, Shiliang and Pokam, Gilles and Devietti, Joseph}},
  year = {2016}},
  journal = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {As ever more computation shifts onto multicore architectures, it is increasingly critical to find effective ways of dealing with multithreaded performance bugs like true and false sharing. Previous approaches to fixing false sharing in unmanaged languages have employed highly-invasive runtime program modifications. We observe that managed language runtimes, with garbage collection and JIT code compilation, present unique opportunities to repair such bugs directly, mirroring the techniques used in manual repairs. We present Remix, a modified version of the Oracle HotSpot JVM which can detect cache contention bugs and repair false sharing at runtime. Remix's detection mechanism leverages recent performance counter improvements on Intel platforms, which allow for precise, unobtrusive monitoring of cache contention at the hardware level. Remix can detect and repair known false sharing issues in the LMAX Disruptor high-performance inter-thread messaging library and the Spring Reactor event-processing framework, automatically providing 1.5-2x speedups over unoptimized code and matching the performance of hand-optimization. Remix also finds a new false sharing bug in SPECjvm2008, and uncovers a true sharing bug in the HotSpot JVM that, when fixed, improves the performance of three NAS Parallel Benchmarks by 7-25x. Remix incurs no statistically-significant performance overhead on other benchmarks that do not exhibit cache contention, making Remix practical for always-on use.}},
  url = {https://doi.org/10.1145/2908080.2908090},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1347,
  title = {Spiral in scala: towards the systematic construction of generators for performance libraries}},
  author = {Ofenbeck, Georg and Rompf, Tiark and Stojanov, Alen and Odersky, Martin and P\"{u}},
  year = {2013}},
  journal = {Proceedings of the 12th International Conference on Generative Programming: Concepts \&amp; Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Program generators for high performance libraries are an appealing solution to the recurring problem of porting and optimizing code with every new processor generation, but only few such generators exist to date. This is due to not only the difficulty of the design, but also of the actual implementation, which often results in an ad-hoc collection of standalone programs and scripts that are hard to extend, maintain, or reuse. In this paper we ask whether and which programming language concepts and features are needed to enable a more systematic construction of such generators. The systematic approach we advocate extrapolates from existing generators: a) describing the problem and algorithmic knowledge using one, or several, domain-specific languages (DSLs), b) expressing optimizations and choices as rewrite rules on DSL programs, c) designing data structures that can be configured to control the type of code that is generated and the data representation used, and d) using autotuning to select the best-performing alternative. As a case study, we implement a small, but representative subset of Spiral in Scala using the Lightweight Modular Staging (LMS) framework. The first main contribution of this paper is the realization of c) using type classes to abstract over staging decisions, i.e. which pieces of a computation are performed immediately and for which pieces code is generated. Specifically, we abstract over different complex data representations jointly with different code representations including generating loops versus unrolled code with scalar replacement - a crucial and usually tedious performance transformation. The second main contribution is to provide full support for a) and d) within the LMS framework: we extend LMS to support translation between different DSLs and autotuning through search.}},
  url = {https://doi.org/10.1145/2517208.2517228},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1348,
  title = {Modular verification of preemptive OS kernels}},
  author = {Gotsman, Alexey and Yang, Hongseok}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Most major OS kernels today run on multiprocessor systems and are preemptive: it is possible for a process running in the kernel mode to get descheduled. Existing modular techniques for verifying concurrent code are not directly applicable in this setting: they rely on scheduling being implemented correctly, and in a preemptive kernel, the correctness of the scheduler is interdependent with the correctness of the code it schedules. This interdependency is even stronger in mainstream kernels, such as Linux, FreeBSD or XNU, where the scheduler and processes interact in complex ways.We propose the first logic that is able to decompose the verification of preemptive multiprocessor kernel code into verifying the scheduler and the rest of the kernel separately, even in the presence of complex interdependencies between the two components. The logic hides the manipulation of control by the scheduler when reasoning about preemptable code and soundly inherits proof rules from concurrent separation logic to verify it thread-modularly. This is achieved by establishing a novel form of refinement between an operational semantics of the real machine and an axiomatic semantics of OS processes, where the latter assumes an abstract machine with each process executing on a separate virtual CPU. The refinement is local in the sense that the logic focuses only on the relevant state of the kernel while verifying the scheduler. We illustrate the power of our logic by verifying an example scheduler, modelled on the one from Linux 2.6.11.}},
  url = {https://doi.org/10.1145/2034773.2034827},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1349,
  title = {Advanced automata minimization}},
  author = {Mayr, Richard and Clemente, Lorenzo}},
  year = {2013}},
  journal = {Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present an efficient algorithm to reduce the size of nondeterministic Buchi word automata, while retaining their language. Additionally, we describe methods to solve PSPACE-complete automata problems like universality, equivalence and inclusion for much larger instances (1-3 orders of magnitude) than before. This can be used to scale up applications of automata in formal verification tools and decision procedures for logical theories.The algorithm is based on new transition pruning techniques. These use criteria based on combinations of backward and forward trace inclusions. Since these relations are themselves PSPACE-complete, we describe methods to compute good approximations of them in polynomial time.Extensive experiments show that the average-case complexity of our algorithm scales quadratically. The size reduction of the automata depends very much on the class of instances, but our algorithm consistently outperforms all previous techniques by a wide margin. We tested our algorithm on Buchi automata derived from LTL-formulae, many classes of random automata and automata derived from mutual exclusion protocols, and compared its performance to the well-known automata tool GOAL.}},
  url = {https://doi.org/10.1145/2429069.2429079},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1350,
  title = {CGCExplorer: a semi-automated search procedure for provably correct concurrent collectors}},
  author = {Vechev, Martin T. and Yahav, Eran and Bacon, David F. and Rinetzky, Noam}},
  year = {2007}},
  journal = {Proceedings of the 28th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Concurrent garbage collectors are notoriously hard to design, implement, and verify. We present a framework for the automatic exploration of a space of concurrent mark-and-sweep collectors. In our framework, the designer specifies a set of "building blocks" from which algorithms can be constructed. These blocks reflect the designer's insights about the coordination between the collector and the mutator. Given a set of building blocks, our framework automatically explores a space of algorithms, using model checking with abstraction to verify algorithms in the space.We capture the intuition behind some common mark-and-sweep algorithms using a set of building blocks. We utilize our framework to automatically explore a space of more than 1,600,000 algorithms built from these blocks, and derive over 100 correct fine-grained algorithms with various space, synchronization, and precision tradeoffs.}},
  url = {https://doi.org/10.1145/1250734.1250787},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1351,
  title = {A Coalgebraic Decision Procedure for NetKAT}},
  author = {Foster, Nate and Kozen, Dexter and Milano, Mae and Silva, Alexandra and Thompson, Laure}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {NetKAT is a domain-specific language and logic for specifying and verifying network packet-processing functions. It consists of Kleene algebra with tests (KAT) augmented with primitives for testing and modifying packet headers and encoding network topologies. Previous work developed the design of the language and its standard semantics, proved the soundness and completeness of the logic, defined a PSPACE algorithm for deciding equivalence, and presented several practical applications.This paper develops the coalgebraic theory of NetKAT, including a specialized version of the Brzozowski derivative, and presents a new efficient algorithm for deciding the equational theory using bisimulation. The coalgebraic structure admits an efficient sparse representation that results in a significant reduction in the size of the state space. We discuss the details of our implementation and optimizations that exploit NetKAT's equational axioms and coalgebraic structure to yield significantly improved performance. We present results from experiments demonstrating that our tool is competitive with state-of-the-art tools on several benchmarks including all-pairs connectivity, loop-freedom, and translation validation.}},
  url = {https://doi.org/10.1145/2676726.2677011},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1352,
  title = {Instrumentation and sampling strategies for cooperative concurrency bug isolation}},
  author = {Jin, Guoliang and Thakur, Aditya and Liblit, Ben and Lu, Shan}},
  year = {2010}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Fixing concurrency bugs (or "crugs") is critical in modern software systems. Static analyses to find crugs such as data races and atomicity violations scale poorly, while dynamic approaches incur high run-time overheads. Crugs manifest only under specific execution interleavings that may not arise during in-house testing, thereby demanding a lightweight program monitoring technique that can be used post-deployment.We present Cooperative Crug Isolation (CCI), a low-overhead instrumentation framework to diagnose production-run failures caused by crugs. CCI tracks specific thread interleavings at run-time, and uses statistical models to identify strong failure predictors among these. We offer a varied suite of predicates that represent different trade-offs between complexity and fault isolation capability. We also develop variant random sampling strategies that suit different types of predicates and help keep the run-time overhead low. Experiments with 9 real-world bugs in 6 non-trivial C applications show that these schemes span a wide spectrum of performance and diagnosis capabilities, each suitable for different usage scenarios.}},
  url = {https://doi.org/10.1145/1869459.1869481},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1353,
  title = {ActorSpace: an open distributed programming paradigm}},
  author = {Agha, Gul and Callsen, Christian J.}},
  year = {1993}},
  journal = {Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a new programming paradigm called ActorSpace. ActorSpace provides a new communication model based on destination patterns. An actorSpace is a computationally passive container of actors which acts as a context for matching patterns. Patterns are matched against listed attributes of actors and actorSpaces that are visible in the actorSpace. Both visibility and attributes are dynamic. Messages may be sent to one or all members of a group defined by a pattern. The paradigm provides powerful support for component-based construction of massively parallel and distributed applications. In particular, it supports open interfaces to servers and pattern-directed access to software repositories.}},
  url = {https://doi.org/10.1145/155332.155335},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1354,
  title = {Continuations and concurrency}},
  author = {Hieb, R. and Dybvig, R. Kent}},
  year = {1990}},
  journal = {Proceedings of the Second ACM SIGPLAN Symposium on Principles \&amp; Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Continuations have proven to be useful for implementing a variety of control structures, including exception handling facilities and breadth-first searching algorithms. However, traditional continuations are not useful in the presence of concurrency, because the notion of the rest of the computation represented by a continuation does not in general make sense. This paper presents a new type of continuation, called a process continuation, that may be used to control tree-structured concurrency. Just as a traditional continuation represents the rest of a computation from a given point in the computation, a process continuation represents the rest of a subcomputation, or process, from a given point in the subcomputation. Process continuations allow nonlocal exits to arbitrary points in the process tree and allow the capture of a subtree of a computation as a composable continuation for later use. Even in the absence of multiple processes, the precise control achievable with process continuations makes them more useful than traditional continuations.}},
  url = {https://doi.org/10.1145/99163.99178},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1355,
  title = {The design and implementation of a certifying compiler}},
  author = {Necula, George C. and Lee, Peter}},
  year = {1998}},
  journal = {SIGPLAN Not.}},
  tipo = {Article}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents the design and implementation of a compiler that translates programs written in a type-safe subset of the C programming language into highly optimized DEC Alpha assembly language programs, and a certifier that automatically checks the type safety and memory safety of any assembly language program produced by the compiler. The result of the certifier is either a formal proof of type safety or a counterexample pointing to a potential violation of the type system by the target program. The ensemble of the compiler and the certifier is called a certifying compiler.Several advantages of certifying compilation over previous approaches can be claimed. The notion of a certifying compiler is significantly easier to employ than a formal compiler verification, in part because it is generally easier to verify the correctness of the result of a computation than to prove the correctness of the computation itself. Also, the approach can be applied even to highly optimizing compilers, as demonstrated by the fact that our compiler generates target code, for a range of realistic C programs, which is competitive with both the cc and gcc compilers with all optimizations enabled. The certifier also drastically improves the effectiveness of compiler testing because, for each test case, it statically signals compilation errors that might otherwise require many executions to detect. Finally, this approach is a practical way to produce the safety proofs for a Proof-Carrying Code system, and thus may be useful in a system for safe mobile code.}},
  url = {https://doi.org/10.1145/277652.277752},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1356,
  title = {Verifying systems rules using rule-directed symbolic execution}},
  author = {Cui, Heming and Hu, Gang and Wu, Jingyue and Yang, Junfeng}},
  year = {2013}},
  journal = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Systems code must obey many rules, such as "opened files must be closed." One approach to verifying rules is static analysis, but this technique cannot infer precise runtime effects of code, often emitting many false positives. An alternative is symbolic execution, a technique that verifies program paths over all inputs up to a bounded size. However, when applied to verify rules, existing symbolic execution systems often blindly explore many redundant program paths while missing relevant ones that may contain bugs.Our key insight is that only a small portion of paths are relevant to rules, and the rest (majority) of paths are irrelevant and do not need to be verified. Based on this insight, we create WOODPECKER, a new symbolic execution system for effectively checking rules on systems programs. It provides a set of builtin checkers for common rules, and an interface for users to easily check new rules. It directs symbolic execution toward the program paths relevant to a checked rule, and soundly prunes redundant paths, exponentially speeding up symbolic execution. It is designed to be heuristic-agnostic, enabling users to leverage existing powerful search heuristics.Evaluation on 136 systems programs totaling 545K lines of code, including some of the most widely used programs, shows that, with a time limit of typically just one hour for each verification run, WOODPECKER effectively verifies 28.7\% of the program and rule combinations over bounded input, whereas an existing symbolic execution system KLEE verifies only 8.5\%. For the remaining combinations, WOODPECKER verifies 4.6 times as many relevant paths as KLEE. With a longer time limit, WOODPECKER verifies much more paths than KLEE, e.g., 17 times as many with a fourhour limit. WOODPECKER detects 113 rule violations, including 10 serious data loss errors with 2 most serious ones already confirmed by the corresponding developers.}},
  url = {https://doi.org/10.1145/2451116.2451152},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1357,
  title = {An optimizing compiler for lexically scoped LISP}},
  author = {Brooks, Rodney A. and Gabriel, Richard P. and Steele, Guy L.}},
  year = {1982}},
  journal = {Proceedings of the 1982 SIGPLAN Symposium on Compiler Construction}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We are developing an optimizing compiler for a dialect of the LISP language. The current target architecture is the S-I, a multiprocessing supercomputer designed at Lawrence Livermore National Laboratory. While LISP is usually thought of as a language primarily for symbolic processing and list manipulation, this compiler is also intended to compete with the S-1 PASCAL and FORTRAN compilers for quality of compiled numerical code. The S-1 is designed for extremely high-speed signal processing as well as for symbolic computation; it provides primitive operations on vectors of floating-point and complex numbers. The LISP compiler is designed to exploit the architecture heavily.The compiler is structurally and conceptually similar to the BLISS-11 compiler and the compilers produced by PQCC. In particular, the TNBIND technique has been borrowed and extended.}},
  url = {https://doi.org/10.1145/800230.807000},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1358,
  title = {Algebraic fusion of functions with an accumulating parameter and its improvement}},
  author = {Katsumata, Shin-ya and Nishimura, Susumu}},
  year = {2006}},
  journal = {Proceedings of the Eleventh ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a unifying solution to the problem of fusion of functions, where both the producer function and the consumer function have one accumulating parameter. The key idea in this development is to formulate the producer function as a function which computes over a monoid of data contexts. Upon this formulation, we develop a fusion method called algebraic fusion based on the elementary theory of universal algebra and monoids. The producer function is fused with a monoid homomorphism that is derived from the definition of the consumer function, and is turned into a higher-order function f that computes over the monoid of endofunctions.We then introduce a general concept called improvement, in order to reduce the cost of computing over the monoid of endofunctions (i. e., function closures). An improvement of the function f via a monoid homomorphism h is a function g that satisfies f =h ºg. This provides a principled way of finding a first-order function representing a solution to the fusion problem. It also presents a clean and unifying account for varying fusion methods that have been proposed so far. Furthermore, we show that our method extends to support partial and infinite data structures, by means of an appropriate monoid structure.}},
  url = {https://doi.org/10.1145/1159803.1159835},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1359,
  title = {Register promotion by sparse partial redundancy elimination of loads and stores}},
  author = {Lo, Raymond and Chow, Fred and Kennedy, Robert and Liu, Shin-Ming and Tu, Peng}},
  year = {1998}},
  journal = {Proceedings of the ACM SIGPLAN 1998 Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {An algorithm for register promotion is presented based on the observation that the circumstances for promoting a memory location's value to register coincide with situations where the program exhibits partial redundancy between accesses to the memory location. The recent SSAPRE algorithm for eliminating partial redundancy using a sparse SSA representation forms the foundation for the present algorithm to eliminate redundancy among memory accesses, enabling us to achieve both computational and live range optimality in our register promotion results. We discuss how to effect speculative code motion in the SSAPRE framework. We present two different algorithms for performing speculative code motion: the conservative speculation algorithm used in the absence of profile data, and the the profile-driven speculation algorithm used when profile data are available. We define the static single use (SSU) form and develop the dual of the SSAPRE algorithm, called SSUPRE, to perform the partial redundancy elimination of stores. We provide measurement data on the SPECint95 benchmark suite to demonstrate the effectiveness of our register promotion approach in removing loads and stores. We also study the relative performance of the different speculative code motion strategies when applied to scalar loads and stores.}},
  url = {https://doi.org/10.1145/277650.277659},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1360,
  title = {Checking Concurrent Data Structures Under the C/C++11 Memory Model}},
  author = {Ou, Peizhao and Demsky, Brian}},
  year = {2017}},
  journal = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Concurrent data structures often provide better performance on multi-core processors but are significantly more difficult to design and test than their sequential counterparts. The C/C++11 standard introduced a weak memory model with support for low-level atomic operations such as compare and swap (CAS). While low-level atomic operations can significantly improve the performance of concurrent data structures, they introduce non-intuitive behaviors that can increase the difficulty of developing code.In this paper, we develop a correctness model for concurrent data structures that make use of atomic operations. Based on this correctness model, we present CDSSPEC, a specification checker for concurrent data structures under the C/C++11 memory model. We have evaluated CDSSPEC on 10 concurrent data structures, among which CDSSPEC detected 3 known bugs and 93\% of the injected bugs.}},
  url = {https://doi.org/10.1145/3018743.3018749},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1361,
  title = {Compositional may-must program analysis: unleashing the power of alternation}},
  author = {Godefroid, Patrice and Nori, Aditya V. and Rajamani, Sriram K. and Tetali, Sai Deep}},
  year = {2010}},
  journal = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Program analysis tools typically compute two types of information: (1) may information that is true of all program executions and is used to prove the absence of bugs in the program, and (2) must information that is true of some program executions and is used to prove the existence of bugs in the program. In this paper, we propose a new algorithm, dubbed SMASH, which computes both may and must information compositionally . At each procedure boundary, may and must information is represented and stored as may and must summaries, respectively. Those summaries are computed in a demand driven manner and possibly using summaries of the opposite type. We have implemented SMASH using predicate abstraction (as in SLAM) for the may part and using dynamic test generation (as in DART) for the must part. Results of experiments with 69 Microsoft Windows 7 device drivers show that SMASH can significantly outperform may-only, must-only and non-compositional may-must algorithms. Indeed, our empirical results indicate that most complex code fragments in large programs are actually often either easy to prove irrelevant to the specific property of interest using may analysis or easy to traverse using directed testing. The fine-grained coupling and alternation of may (universal) and must (existential) summaries allows SMASH to easily navigate through these code fragments while traditional may-only, must-only or non-compositional may-must algorithms are stuck in their specific analyses.}},
  url = {https://doi.org/10.1145/1706299.1706307},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1362,
  title = {Family-based deductive verification of software product lines}},
  author = {Th\"{u}},
  year = {2012}},
  journal = {Proceedings of the 11th International Conference on Generative Programming and Component Engineering}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A software product line is a set of similar software products that share a common code base. While software product lines can be implemented efficiently using feature-oriented programming, verifying each product individually does not scale, especially if human effort is required (e.g., as in interactive theorem proving). We present a family-based approach of deductive verification to prove the correctness of a software product line efficiently. We illustrate and evaluate our approach for software product lines written in a feature-oriented dialect of Java and specified using the Java Modeling Language. We show that the theorem prover KeY can be used off-the-shelf for this task, without any modifications. Compared to the individual verification of each product, our approach reduces the verification time needed for our case study by more than 85\%.}},
  url = {https://doi.org/10.1145/2371401.2371404},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1363,
  title = {The lazy happens-before relation: better partial-order reduction for systematic concurrency testing}},
  author = {Thomson, Paul and Donaldson, Alastair F.}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present the lazy happens-before relation (lazy HBR), which ignores mutex-induced edges to provide a more precise notion of state equivalence compared with the traditional happens-before relation. We demonstrate experimentally that the lazy HBR has the potential to provide greater schedule reduction during systematic concurrency testing with respect to a set of 79 Java benchmarks.}},
  url = {https://doi.org/10.1145/2688500.2688533},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1364,
  title = {Better science through art}},
  author = {Gabriel, Richard P. and Sullivan, Kevin J.}},
  year = {2010}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {How do artists and scientists work? The same.}},
  url = {https://doi.org/10.1145/1869459.1869533},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1365,
  title = {Uniqueness and reference immutability for safe parallelism}},
  author = {Gordon, Colin S. and Parkinson, Matthew J. and Parsons, Jared and Bromfield, Aleks and Duffy, Joe}},
  year = {2012}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A key challenge for concurrent programming is that side-effects (memory operations) in one thread can affect the behavior of another thread. In this paper, we present a type system to restrict the updates to memory to prevent these unintended side-effects. We provide a novel combination of immutable and unique (isolated) types that ensures safe parallelism (race freedom and deterministic execution). The type system includes support for polymorphism over type qualifiers, and can easily create cycles of immutable objects. Key to the system's flexibility is the ability to recover immutable or externally unique references after violating uniqueness without any explicit alias tracking. Our type system models a prototype extension to C# that is in active use by a Microsoft team. We describe their experiences building large systems with this extension. We prove the soundness of the type system by an embedding into a program logic.}},
  url = {https://doi.org/10.1145/2384616.2384619},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1366,
  title = {Compiler verification meets cross-language linking via data abstraction}},
  author = {Wang, Peng and Cuellar, Santiago and Chlipala, Adam}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many real programs are written in multiple different programming languages, and supporting this pattern creates challenges for formal compiler verification. We describe our Coq verification of a compiler for a high-level language, such that the compiler correctness theorem allows us to derive partial-correctness Hoare-logic theorems for programs built by linking the assembly code output by our compiler and assembly code produced by other means. Our compiler supports such tricky features as storable cross-language function pointers, without giving up the usual benefits of being able to verify different compiler phases (including, in our case, two classic optimizations) independently. The key technical innovation is a mixed operational and axiomatic semantics for the source language, with a built-in notion of abstract data types, such that compiled code interfaces with other languages only through axiomatically specified methods that mutate encapsulated private data, represented in whatever formats are most natural for those languages.}},
  url = {https://doi.org/10.1145/2660193.2660201},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1367,
  title = {OOPAL: integrating array programming in object-oriented programming}},
  author = {Mougin, Philippe and Ducasse, St\'{e}},
  year = {2003}},
  journal = {Proceedings of the 18th Annual ACM SIGPLAN Conference on Object-Oriented Programing, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Array programming shines in its ability to express computations at a high-level of abstraction, allowing one to manipulate and query whole sets of data at once. This paper presents the OPA model that enhances object-oriented programming with array programming features. The goal of OPA is to determine a minimum set of modifications that must be made to the traditional object model in order to take advantage of the possibilities of array programming. It is based on a minimal extension of method invocation and the definition of a kernel of methods implementing fundamental array programming operations. The OPA model presents a generalization of traditional message passing in the sense that a message can be send to an entire set of objects. The model is validated in FS, a new scripting language.}},
  url = {https://doi.org/10.1145/949305.949312},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1368,
  title = {Runtime pointer disambiguation}},
  author = {Alves, P\'{e}},
  year = {2015}},
  journal = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {To optimize code effectively, compilers must deal with memory dependencies. However, the state-of-the-art heuristics available in the literature to track memory dependencies are inherently imprecise and computationally expensive. Consequently, the most advanced code transformations that compilers have today are ineffective when applied on real-world programs. The goal of this paper is to solve this conundrum through dynamic disambiguation of pointers. We provide different ways to determine at runtime when two memory locations can overlap. We then produce two versions of a code region: one that is aliasing-free - hence, easy to optimize - and another that is not. Our checks let us safely branch to the optimizable region. We have applied these ideas on Polly-LLVM, a loop optimizer built on top of the LLVM compilation infrastructure. Our experiments indicate that our method is precise, effective and useful: we can disambiguate every pair of pointer in the loop intensive Polybench benchmark suite. The result of this precision is code quality: the binaries we generate are 10\% faster than those that Polly-LLVM produces without our optimization, at the -O3 optimization level of LLVM.}},
  url = {https://doi.org/10.1145/2814270.2814285},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1369,
  title = {Elaborating intersection and union types}},
  author = {Dunfield, Jana}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Designing and implementing typed programming languages is hard. Every new type system feature requires extending the metatheory and implementation, which are often complicated and fragile. To ease this process, we would like to provide general mechanisms that subsume many different features.In modern type systems, parametric polymorphism is fundamental, but intersection polymorphism has gained little traction in programming languages. Most practical intersection type systems have supported only refinement intersections, which increase the expressiveness of types (more precise properties can be checked) without altering the expressiveness of terms; refinement intersections can simply be erased during compilation. In contrast, unrestricted intersections increase the expressiveness of terms, and can be used to encode diverse language features, promising an economy of both theory and implementation.We describe a foundation for compiling unrestricted intersection and union types: an elaboration type system that generates ordinary λ-calculus terms. The key feature is a Forsythe-like merge construct. With this construct, not all reductions of the source program preserve types; however, we prove that ordinary call-by-value evaluation of the elaborated program corresponds to a type-preserving evaluation of the source program.We also describe a prototype implementation and applications of unrestricted intersections and unions: records, operator overloading, and simulating dynamic typing.}},
  url = {https://doi.org/10.1145/2364527.2364534},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1370,
  title = {Effective fine-grain synchronization for automatically parallelized programs using optimistic synchronization primitives}},
  author = {Rinard, Martin}},
  year = {1997}},
  journal = {Proceedings of the Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {As shared-memory multiprocessors become the dominant commodity source of computation, parallelizing compilers must support mainstream computations that manipulate irregular, pointer-based data structures such as lists, trees and graphs, Our experience with a parallelizing compiler for this class of applications shows that their synchronization requirements differ significantly from those of traditional parallel computations. Instead of coarse-grain barrier synchronization, irregular computations require synchronization primitives that support efficient fine-grain atomic operations.The standard implementation mechanism for atomic operations uses mutual exclusion locks. But the overhead of acquiring and releasing locks can reduce the performance. Locks can also consume significant amourtts of memory. Optimistic synchronization primitives such as load linked/stor conditional are an attractive alternative. They require no additional memory and eliminate the use of heavyweight blocking synchronization constructs.This paper presents our experience using optimistic synchronization to implement fine-grain atomic operations in the context of a parallelizing compiler for irregular object-based programs. We have implemented two versions of the compiler. One version generates code that uses mutual exclusion locks to make operations execute atomically. The other version uses optimistic synchronization. This paper presents the first published algorithm that enables compilers to automatically generate optimistically synchronized parallel code. The presented experimental results indicate that optimistic synchronization is clearly the superior choice for our set of applications. Our results show that it can significantly reduce the memory consumption and improve the overall performance.}},
  url = {https://doi.org/10.1145/263764.263781},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1371,
  title = {River trail: a path to parallelism in JavaScript}},
  author = {Herhut, Stephan and Hudson, Richard L. and Shpeisman, Tatiana and Sreeram, Jaswanth}},
  year = {2013}},
  journal = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {JavaScript is the most popular language on the web and is a crucial component of HTML5 applications and services that run on consumer platforms ranging from desktops to phones. However, despite ample amount of hardware parallelism available to web applications on such platforms, JavaScript web applications remain predominantly sequential. Common parallel programming solutions accepted by other programming languages failed to transfer themselves to JavaScript due to differences in programming models, the additional requirements of the web and different developer expectations.In this paper we present River Trail - a parallel programming model and API for JavaScript that provides safe, portable, programmer-friendly, deterministic parallelism to JavaScript applications. River Trail allows web applications to effectively utilize multiple cores, vector instructions, and GPUs on client platforms while allowing the web developer to remain within the environment of JavaScript. We describe the implementation of the River Trail compiler and runtime and present experimental results that show the impact of River Trail on performance and scalability for a variety of realistic HTML5 applications. Our experiments show that River Trail has a dramatic positive impact on overall performance and responsiveness of computationally intense JavaScript based applications achieving up to 33.6 times speedup for kernels and up to 11.8 times speedup for realistic web applications compared to sequential JavaScript. Moreover, River Trail enables new interactive web usages that are simply not even possible with standard sequential JavaScript.}},
  url = {https://doi.org/10.1145/2509136.2509516},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1372,
  title = {Automated error diagnosis using abductive inference}},
  author = {Dillig, Isil and Dillig, Thomas and Aiken, Alex}},
  year = {2012}},
  journal = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {When program verification tools fail to verify a program, either the program is buggy or the report is a false alarm. In this situation, the burden is on the user to manually classify the report, but this task is time-consuming, error-prone, and does not utilize facts already proven by the analysis. We present a new technique for assisting users in classifying error reports. Our technique computes small, relevant queries presented to a user that capture exactly the information the analysis is missing to either discharge or validate the error. Our insight is that identifying these missing facts is an instance of the abductive inference problem in logic, and we present a new algorithm for computing the smallest and most general abductions in this setting. We perform the first user study to rigorously evaluate the accuracy and effort involved in manual classification of error reports. Our study demonstrates that our new technique is very useful for improving both the speed and accuracy of error report classification. Specifically, our approach improves classification accuracy from 33\% to 90\% and reduces the time programmers take to classify error reports from approximately 5 minutes to under 1 minute.}},
  url = {https://doi.org/10.1145/2254064.2254087},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1373,
  title = {Decidable logics combining heap structures and data}},
  author = {Madhusudan, P. and Parlato, Gennaro and Qiu, Xiaokang}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We define a new logic, STRAND, that allows reasoning with heap-manipulating programs using deductive verification and SMT solvers. STRAND logic ("STRucture ANd Data" logic) formulas express constraints involving heap structures and the data they contain; they are defined over a class of pointer-structures R defined using MSO-defined relations over trees, and are of the form ∃→x∀→y (→x,→) x" , where "φ" is a monadic second-order logic (MSO) formulawith additional quantification that combines structural constraints as well as data-constraints, but where the data-constraints are only allowed to refer to "→x" and "→y"The salient aspects of the logic are: (a) the logic is powerful, allowing existential and universal quantification over the nodes, and complex combinations of data and structural constraints; (b) checking Hoare-triples for linear blocks of statements with pre-conditions and post-conditions expressed as Boolean combinations of existential and universal STRAND formulas reduces to satisfiability of a STRAND formula; (c) there are powerful decidable fragments of STRAND, one semantically defined and one syntactically defined, where the decision procedure works by combining the theory of MSO over trees and the quantifier-free theory of the underlying data-logic. We demonstrate the effectiveness and practicality of the logic by checking verification conditions generated in proving properties of several heap-manipulating programs, using a tool that combines an MSO decision procedure over trees (MONA) with an SMT solver for integer constraints (Z3).}},
  url = {https://doi.org/10.1145/1926385.1926455},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1374,
  title = {The logical approach to stack typing}},
  author = {Ahmed, Amal and Walker, David}},
  year = {2003}},
  journal = {Proceedings of the 2003 ACM SIGPLAN International Workshop on Types in Languages Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We develop a logic for reasoning about adjacency and separation of memory blocks, as well as aliasing of pointers. We provide a memory model for our logic and present a sound set of natural deduction-style inference rules. We deploy the logic in a simple type system for a stack-based assembly language. The connectives for the logic provide a flexible yet concise mechanism for controlling allocation, deallocation and access to both heap-allocated and stack-allocated data.}},
  url = {https://doi.org/10.1145/604174.604185},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1375,
  title = {Functors are Type Refinement Systems}},
  author = {Melli\`{e}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The standard reading of type theory through the lens of category theory is based on the idea of viewing a type system as a category of well-typed terms. We propose a basic revision of this reading: rather than interpreting type systems as categories, we describe them as functors from a category of typing derivations to a category of underlying terms. Then, turning this around, we explain how in fact any functor gives rise to a generalized type system, with an abstract notion of typing judgment, typing derivations and typing rules. This leads to a purely categorical reformulation of various natural classes of type systems as natural classes of functors.The main purpose of this paper is to describe the general framework (which can also be seen as providing a categorical analysis of refinement types), and to present a few applications. As a larger case study, we revisit Reynolds' paper on ``The Meaning of Types'' (2000), showing how the paper's main results may be reconstructed along these lines.}},
  url = {https://doi.org/10.1145/2676726.2676970},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1376,
  title = {Determinacy in static analysis for jQuery}},
  author = {Andreasen, Esben and M\o{}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages \&amp; Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Static analysis for JavaScript can potentially help programmers find errors early during development. Although much progress has been made on analysis techniques, a major obstacle is the prevalence of libraries, in particular jQuery, which apply programming patterns that have detrimental consequences on the analysis precision and performance. Previous work on dynamic determinacy analysis has demonstrated how information about program expressions that always resolve to a fixed value in some call context may lead to significant scalability improvements of static analysis for such code. We present a static dataflow analysis for JavaScript that infers and exploits determinacy information on-the-fly, to enable analysis of some of the most complex parts of jQuery. The analysis combines selective context and path sensitivity, constant propagation, and branch pruning, based on a systematic investigation of the main causes of analysis imprecision when using a more basic analysis.The techniques are implemented in the TAJS analysis tool and evaluated on a collection of small programs that use jQuery. Our results show that the proposed analysis techniques boost both precision and performance, specifically for inferring type information and call graphs.}},
  url = {https://doi.org/10.1145/2660193.2660214},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1377,
  title = {TeJaS: retrofitting type systems for JavaScript}},
  author = {Lerner, Benjamin S. and Politz, Joe Gibbs and Guha, Arjun and Krishnamurthi, Shriram}},
  year = {2013}},
  journal = {Proceedings of the 9th Symposium on Dynamic Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {JavaScript programs vary widely in functionality, complexity, and use, and analyses of these programs must accommodate such variations. Type-based analyses are typically the simplest such analyses, but due to the language's subtle idioms and many application-specific needs---such as ensuring general-purpose type correctness, security properties, or proper library usage---we have found that a single type system does not suffice for all purposes. However, these varied uses still share many reusable common elements.In this paper we present TeJaS, a framework for building type systems for JavaScript. TeJaS has been engineered modularly to encourage experimentation. Its initial type environment is reified, to admit easy modeling of the various execution contexts of JavaScript programs, and its type language and typing rules are extensible, to enable variations of the type system to be constructed easily.The paper presents the base TeJaS type system, which performs traditional type-checking for JavaScript. Because JavaScript demands complex types, we explain several design decisions to improve user ergonomics. We then describe TeJaS's modular structure, and illustrate it by reconstructing the essence of a very different type system for JavaScript. Systems built from TeJaS have been applied to several real-world, third-party JavaScript programs.}},
  url = {https://doi.org/10.1145/2508168.2508170},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1378,
  title = {Machine-verified network controllers}},
  author = {Guha, Arjun and Reitblatt, Mark and Foster, Nate}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In many areas of computing, techniques ranging from testing to formal modeling to full-blown verification have been successfully used to help programmers build reliable systems. But although networks are critical infrastructure, they have largely resisted analysis using formal techniques. Software-defined networking (SDN) is a new network architecture that has the potential to provide a foundation for network reasoning, by standardizing the interfaces used to express network programs and giving them a precise semantics.This paper describes the design and implementation of the first machine-verified SDN controller. Starting from the foundations, we develop a detailed operational model for OpenFlow (the most popular SDN platform) and formalize it in the Coq proof assistant. We then use this model to develop a verified compiler and run-time system for a high-level network programming language. We identify bugs in existing languages and tools built without formal foundations, and prove that these bugs are absent from our system. Finally, we describe our prototype implementation and our experiences using it to build practical applications.}},
  url = {https://doi.org/10.1145/2491956.2462178},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1379,
  title = {Finding effective compilation sequences}},
  author = {Almagor, L. and Cooper, Keith D. and Grosul, Alexander and Harvey, Timothy J. and Reeves, Steven W. and Subramanian, Devika and Torczon, Linda and Waterman, Todd}},
  year = {2004}},
  journal = {Proceedings of the 2004 ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Most modern compilers operate by applying a fixed, program-independent sequence of optimizations to all programs. Compiler writers choose a single "compilation sequence", or perhaps a couple of compilation sequences. In choosing a sequence, they may consider performance of benchmarks or other important codes. These sequences are intended as general-purpose tools, accessible through command-line flags such as -O2 and -O3.Specific compilation sequences make a significant difference in the quality of the generated code, whether compiling for speed, for space, or for other metrics. A single universal compilation sequence does not produce the best results over all programs [8, 10, 29, 32]. Finding an optimal program-specific compilation sequence is difficult because the space of potential sequences is huge and the interactions between optimizations are poorly understood. Moreover, there is no systematic exploration of the costs and benefits of searching for good (i.e., within a certain percentage of optimal) program-specific compilation sequences.In this paper, we perform a large experimental study of the space of compilation sequences over a set of known benchmarks, using our prototype adaptive compiler. Our goal is to characterize these spaces and to determine if it is cost-effective to construct custom compilation sequences. We report on five exhaustive enumerations which demonstrate that 80\% of the local minima in the space are within 5 to 10\% of the optimal solution. We describe three algorithms tailored to search such spaces and report on experiments that use these algorithms to find good compilation sequences. These experiments suggest that properties observed in the enumerations hold for larger search spaces and larger programs. Our findings indicate that for the cost of 200 to 4,550 compilations, we can find custom sequences that are 15 to 25\% better than the human-designed fixed-sequence originally used in our compiler.}},
  url = {https://doi.org/10.1145/997163.997196},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1380,
  title = {Call-by-value is dual to call-by-name}},
  author = {Wadler, Philip}},
  year = {2003}},
  journal = {Proceedings of the Eighth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The rules of classical logic may be formulated in pairs corresponding to De Morgan duals: rules about \&amp; are dual to rules about V. A line of work, including that of Filinski (1989), Griffin (1990), Parigot (1992), Danos, Joinet, and Schellinx (1995), Selinger (1998,2001), and Curien and Herbelin (2000), has led to the startling conclusion that call-by-value is the de Morgan dual of call-by-name.This paper presents a dual calculus that corresponds to the classical sequent calculus of Gentzen (1935) in the same way that the lambda calculus of Church (1932,1940) corresponds to the intuitionistic natural deduction of Gentzen (1935). The paper includes crisp formulations of call-by-value and call-by-name that are obviously dual; no similar formulations appear in the literature. The paper gives a CPS translation and its inverse, and shows that the translation is both sound and complete, strengthening a result in Curien and Herbelin (2000).}},
  url = {https://doi.org/10.1145/944705.944723},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1381,
  title = {Ribbons: a partially shared memory programming model}},
  author = {Hoffman, Kevin J. and Metzger, Harrison and Eugster, Patrick}},
  year = {2011}},
  journal = {Proceedings of the 2011 ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The need for programs to execute subcomponents in isolation from each other or with lower privileges is prevalent among today's systems. We introduce ribbons: a shared memory programming model that allows for more implicit sharing of memory than processes but is more restrictive than threads. Ribbons structure the heap into protection domains. Privileges between these protection domains are carefully controlled in order to confine computation. We propose RibbonJ, a backwards-compatible extension of Java, to easily create or port programs to use the ribbons model. We study the progress and isolation properties of a subset of the language. Building on JikesRVM we implement ribbons by leveraging existing memory protection mechanisms in modern hardware and operating systems, avoiding the overhead of inline security checks and read or write barriers. We evaluate efficiency via microbenchmarks and the DaCapo suite, observing minor overhead. Additionally, we refactor Apache Tomcat to use ribbons for application isolation, discuss the refactoring's design and complexity, and evaluate performance using the SPECweb2009 benchmark.}},
  url = {https://doi.org/10.1145/2048066.2048091},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1382,
  title = {Dynamic feedback: an effective technique for adaptive computing}},
  author = {Diniz, Pedro C. and Rinard, Martin C.}},
  year = {1997}},
  journal = {Proceedings of the ACM SIGPLAN 1997 Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents dynamic feedback, a technique that enables computations to adapt dynamically to different execution environments. A compiler that uses dynamic feedback produces several different versions of the same source code; each version uses a different optimization policy. The generated code alternately performs sampling phases and production phases. Each sampling phase measures the overhead of each version in the current environment. Each production phase uses the version with the least overhead in the previous sampling phase. The computation periodically resamples to adjust dynamically to changes in the environment.We have implemented dynamic feedback in the context of a parallelizing compiler for object-based programs. The generated code uses dynamic feedback to automatically choose the best synchronization optimization policy. Our experimental results show that the synchronization optimization policy has a significant impact on the overall performance of the computation, that the best policy varies from program to program, that the compiler is unable to statically choose the best policy, and that dynamic feedback enables the generated code to exhibit performance that is comparable to that of code that has been manually tuned to use the best policy. We have also performed a theoretical analysis which provides, under certain assumptions, a guaranteed optimality bound for dynamic feedback relative to a hypothetical (and unrealizable) optimal algorithm that uses the best policy at every point during the execution.}},
  url = {https://doi.org/10.1145/258915.258923},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1383,
  title = {Proving that programs eventually do something good}},
  author = {Cook, Byron and Gotsman, Alexey and Podelski, Andreas and Rybalchenko, Andrey and Vardi, Moshe Y.}},
  year = {2007}},
  journal = {Proceedings of the 34th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In recent years we have seen great progress made in the area of automatic source-level static analysis tools. However, most of today's program verification tools are limited to properties that guarantee the absence of bad events (safety properties). Until now no formal software analysis tool has provided fully automatic support for proving properties that ensure that good events eventually happen (liveness properties). In this paper we present such a tool, which handles liveness properties of large systems written in C. Liveness properties are described in an extension of the specification language used in the SDV system. We have used the tool to automatically prove critical liveness properties of Windows device drivers and found several previously unknown liveness bugs.}},
  url = {https://doi.org/10.1145/1190216.1190257},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1384,
  title = {Propositions as sessions}},
  author = {Wadler, Philip}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Continuing a line of work by Abramsky (1994), by Bellin and Scott (1994), and by Caires and Pfenning (2010), among others, this paper presents CP, a calculus in which propositions of classical linear logic correspond to session types. Continuing a line of work by Honda (1993), by Honda, Kubo, and Vasconcelos (1998), and by Gay and Vasconcelos (2010), among others, this paper presents GV, a linear functional language with session types, and presents a translation from GV into CP. The translation formalises for the first time a connection between a standard presentation of session types and linear logic, and shows how a modification to the standard presentation yield a language free from deadlock, where deadlock freedom follows from the correspondence to linear logic.}},
  url = {https://doi.org/10.1145/2364527.2364568},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1385,
  title = {Gradual typing for generics}},
  author = {Ina, Lintaro and Igarashi, Atsushi}},
  year = {2011}},
  journal = {Proceedings of the 2011 ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Gradual typing is a framework to combine static and dynamic typing in a single programming language. In this paper, we develop a gradual type system for class-based object-oriented languages with generics. We introduce a special type to denote dynamically typed parts of a program; unlike dynamic types introduced to C# 4.0, however, our type system allows for more seamless integration of dynamically and statically typed code.We formalize a gradual type system for Featherweight GJ with a semantics given by a translation that inserts explicit run-time checks. The type system guarantees that statically typed parts of a program do not go wrong, even if it includes dynamically typed parts. We also describe a basic implementation scheme for Java and report preliminary performance evaluation.}},
  url = {https://doi.org/10.1145/2048066.2048114},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1386,
  title = {HLIO: mixing static and dynamic typing for information-flow control in Haskell}},
  author = {Buiras, Pablo and Vytiniotis, Dimitrios and Russo, Alejandro}},
  year = {2015}},
  journal = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Information-Flow Control (IFC) is a well-established approach for allowing untrusted code to manipulate sensitive data without disclosing it. IFC is typically enforced via type systems and static analyses or via dynamic execution monitors. The LIO Haskell library, originating in operating systems research, implements a purely dynamic monitor of the sensitivity level of a computation, particularly suitable when data sensitivity levels are only known at runtime. In this paper, we show how to give programmers the flexibility of deferring IFC checks to runtime (as in LIO), while also providing static guarantees---and the absence of runtime checks---for parts of their programs that can be statically verified (unlike LIO). We present the design and implementation of our approach, HLIO (Hybrid LIO), as an embedding in Haskell that uses a novel technique for deferring IFC checks based on singleton types and constraint polymorphism. We formalize HLIO, prove non-interference, and show how interesting IFC examples can be programmed. Although our motivation is IFC, our technique for deferring constraints goes well beyond and offers a methodology for programmer-controlled hybrid type checking in Haskell.}},
  url = {https://doi.org/10.1145/2784731.2784758},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1387,
  title = {Automatically generating the dynamic semantics of gradually typed languages}},
  author = {Cimini, Matteo and Siek, Jeremy G.}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Many language designers have adopted gradual typing. However, there remains open questions regarding how to gradualize languages. Cimini and Siek (2016) created a methodology and algorithm to automatically generate the type system of a gradually typed language from a fully static version of the language. In this paper, we address the next challenge of how to automatically generate the dynamic semantics of gradually typed languages. Such languages typically use an intermediate language with explicit casts. Our first result is a methodology for generating the syntax, type system, and dynamic semantics of the intermediate language with casts. Next, we present an algorithm that formalizes and automates the methodology, given a language definition as input. We show that our approach is general enough to automatically gradualize several languages, including features such as polymorphism, recursive types and exceptions. We prove that our algorithm produces languages that satisfy the key correctness criteria of gradual typing. Finally, we implement the algorithm, generating complete specifications of gradually typed languages in lambda-Prolog, including executable interpreters.}},
  url = {https://doi.org/10.1145/3009837.3009863},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1388,
  title = {Lazy functional state threads}},
  author = {Launchbury, John and Peyton Jones, Simon L.}},
  year = {1994}},
  journal = {Proceedings of the ACM SIGPLAN 1994 Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Some algorithms make critical internal use of updatable state, even though their external specification is purely functional. Based on earlier work on monads, we present a way of securely encapsulating stateful computations that manipulate multiple, named, mutable objects, in the context of a non-strict, purely-functional language.The security of the encapsulation is assured by the type system, using parametricity. Intriguingly, this parametricity requires the provision of a (single) constant with a rank-2 polymorphic type.}},
  url = {https://doi.org/10.1145/178243.178246},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1389,
  title = {Model checking transactional memories}},
  author = {Guerraoui, Rachid and Henzinger, Thomas A. and Jobstmann, Barbara and Singh, Vasu}},
  year = {2008}},
  journal = {Proceedings of the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Model checking software transactional memories (STMs) is difficult because of the unbounded number, length, and delay of concurrent transactions and the unbounded size of the memory. We show that, under certain conditions, the verification problem can be reduced to a finite-state problem, and we illustrate the use of the method by proving the correctness of several STMs, including two-phase locking, DSTM, TL2, and optimistic concurrency control. The safety properties we consider include strict serializability and opacity; the liveness properties include obstruction freedom, livelock freedom, and wait freedom.Our main contribution lies in the structure of the proofs, which are largely automated and not restricted to the STMs mentioned above. In a first step we show that every STM that enjoys certain structural properties either violates a safety or liveness requirement on some program with two threads and two shared variables, or satisfies the requirement on all programs. In the second step we use a model checker to prove the requirement for the STM applied to a most general program with two threads and two variables. In the safety case, the model checker constructs a simulation relation between two carefully constructed finite-state transition systems, one representing the given STM applied to a most general program, and the other representing a most liberal safe STM applied to the same program. In the liveness case, the model checker analyzes fairness conditions on the given STM transition system.}},
  url = {https://doi.org/10.1145/1375581.1375626},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1390,
  title = {Dependently typed programming with singletons}},
  author = {Eisenberg, Richard A. and Weirich, Stephanie}},
  year = {2012}},
  journal = {Proceedings of the 2012 Haskell Symposium}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Haskell programmers have been experimenting with dependent types for at least a decade, using clever encodings that push the limits of the Haskell type system. However, the cleverness of these encodings is also their main drawback. Although the ideas are inspired by dependently typed programs, the code looks significantly different. As a result, GHC implementors have responded with extensions to Haskell's type system, such as GADTs, type families, and datatype promotion. However, there remains a significant difference between programming in Haskell and in full-spectrum dependently typed languages. Haskell enforces a phase separation between runtime values and compile-time types. Therefore, singleton types are necessary to express the dependency between values and types. These singleton types introduce overhead and redundancy for the programmer.This paper presents the singletons library, which generates the boilerplate code necessary for dependently typed programming using GHC. To compare with full-spectrum languages, we present an extended example based on an Agda interface for safe database access. The paper concludes with a detailed discussion on the current capabilities of GHC for dependently typed programming and suggestions for future extensions to better support this style of programming.}},
  url = {https://doi.org/10.1145/2364506.2364522},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1391,
  title = {Adaptive input-aware compilation for graphics engines}},
  author = {Samadi, Mehrzad and Hormati, Amir and Mehrara, Mojtaba and Lee, Janghaeng and Mahlke, Scott}},
  year = {2012}},
  journal = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {While graphics processing units (GPUs) provide low-cost and efficient platforms for accelerating high performance computations, the tedious process of performance tuning required to optimize applications is an obstacle to wider adoption of GPUs. In addition to the programmability challenges posed by GPU's complex memory hierarchy and parallelism model, a well-known application design problem is target portability across different GPUs. However, even for a single GPU target, changing a program's input characteristics can make an already-optimized implementation of a program perform poorly. In this work, we propose Adaptic, an adaptive input-aware compilation system to tackle this important, yet overlooked, input portability problem. Using this system, programmers develop their applications in a high-level streaming language and let Adaptic undertake the difficult task of input portable optimizations and code generation. Several input-aware optimizations are introduced to make efficient use of the memory hierarchy and customize thread composition. At runtime, a properly optimized version of the application is executed based on the actual program input. We perform a head-to-head comparison between the Adaptic generated and hand-optimized CUDA programs. The results show that Adaptic is capable of generating codes that can perform on par with their hand-optimized counterparts over certain input ranges and outperform them when the input falls out of the hand-optimized programs' "comfort zone". Furthermore, we show that input-aware results are sustainable across different GPU targets making it possible to write and optimize applications once and run them anywhere.}},
  url = {https://doi.org/10.1145/2254064.2254067},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1392,
  title = {Gradual refinement types}},
  author = {Lehmann, Nico and Tanter, \'{E}},
  year = {2017}},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Refinement types are an effective language-based verification technique. However, as any expressive typing discipline, its strength is its weakness, imposing sometimes undesired rigidity. Guided by abstract interpretation, we extend the gradual typing agenda and develop the notion of gradual refinement types, allowing smooth evolution and interoperability between simple types and logically-refined types. In doing so, we address two challenges unexplored in the gradual typing literature: dealing with imprecise logical information, and with dependent function types. The first challenge leads to a crucial notion of locality for refinement formulas, and the second yields novel operators related to type- and term-level substitution, identifying new opportunity for runtime errors in gradual dependently-typed languages. The gradual language we present is type safe, type sound, and satisfies the refined criteria for gradually-typed languages of Siek et al. We also explain how to extend our approach to richer refinement logics, anticipating key challenges to consider.}},
  url = {https://doi.org/10.1145/3009837.3009856},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1393,
  title = {Stratified type inference for generalized algebraic data types}},
  author = {Pottier, Fran\c{c}},
  year = {2006}},
  journal = {Conference Record of the 33rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Stratified type inference for generalized algebraic data types.}},
  url = {https://doi.org/10.1145/1111037.1111058},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1394,
  title = {Oh Lord, please don't let contracts be misunderstood (functional pearl)}},
  author = {Dimoulas, Christos and New, Max S. and Findler, Robert Bruce and Felleisen, Matthias}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Contracts feel misunderstood, especially those with a higher-order soul. While software engineers appreciate contracts as tools for articulating the interface between components, functional programmers desperately search for their types and meaning, completely forgetting about their pragmatics. This gem presents a novel analysis of contract systems. Applied to the higher-order kind, this analysis reveals their large and clearly unappreciated software engineering potential. Three sample applications illustrate where this kind of exploration may lead.}},
  url = {https://doi.org/10.1145/2951913.2951930},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1395,
  title = {Relational inductive shape analysis}},
  author = {Chang, Bor-Yuh Evan and Rival, Xavier}},
  year = {2008}},
  journal = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Shape analyses are concerned with precise abstractions of the heap to capture detailed structural properties. To do so, they need to build and decompose summaries of disjoint memory regions. Unfortunately, many data structure invariants require relations be tracked across disjoint regions, such as intricate numerical data invariants or structural invariants concerning back and cross pointers. In this paper, we identify issues inherent to analyzing relational structures and design an abstract domain that is parameterized both by an abstract domain for pure data properties and by user-supplied specifications of the data structure invariants to check. Particularly, it supports hybrid invariants about shape and data and features a generic mechanism for materializing summaries at the beginning, middle, or end of inductive structures. Around this domain, we build a shape analysis whose interesting components include a pre-analysis on the user-supplied specifications that guides the abstract interpretation and a widening operator over the combined shape and data domain. We then demonstrate our techniques on the proof of preservation of the red-black tree invariants during insertion.}},
  url = {https://doi.org/10.1145/1328438.1328469},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1396,
  title = {Dependent classes}},
  author = {Gasiunas, Vaidas and Mezini, Mira and Ostermann, Klaus}},
  year = {2007}},
  journal = {Proceedings of the 22nd Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Virtual classes allow nested classes to be refined in subclasses. In this way nested classes can be seen as dependent abstractions of the objects of the enclosing classes. Expressing dependency via nesting, however, has two limitations: Abstractions that depend on more than one object cannot be modeled and a class must know all classes that depend on its objects. This paper presents dependent classes, a generalization of virtual classes that expresses similar semantics by parameterization rather than by nesting. This increases expressivity of class variations as well as the flexibility of their modularization. Besides, dependent classes complement multimethods in scenarios where multi-dispatched abstractions rather than multi-dispatched methods are needed. They can also be used to express more precise signatures of multimethods and even extend their dispatch semantics. We present a formal semantics of dependent classes and a machine-checked type soundness proof in Isabelle/HOL [29], the first of this kind for a language with virtual classes and path-dependent types.}},
  url = {https://doi.org/10.1145/1297027.1297038},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1397,
  title = {Engineering formal metatheory}},
  author = {Aydemir, Brian and Chargu\'{e}},
  year = {2008}},
  journal = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Machine-checked proofs of properties of programming languages have become acritical need, both for increased confidence in large and complex designsand as a foundation for technologies such as proof-carrying code. However, constructing these proofs remains a black art, involving many choices in the formulation of definitions and theorems that make a huge cumulative difference in the difficulty of carrying out large formal developments. There presentation and manipulation of terms with variable binding is a key issue.We propose a novel style for formalizing metatheory, combining locally nameless representation of terms and cofinite quantification of free variable names in inductivedefinitions of relations on terms (typing, reduction, ...). The key technical insight is that our use of cofinite quantification obviates the need for reasoning about equivariance (the fact that free names can be renamed in derivations); in particular, the structural induction principles of relations defined using cofinite quantification are strong enough for metatheoretic reasoning, and need not be explicitly strengthened. Strong inversion principles follow (automatically, in Coq) from the induction principles. Although many of the underlying ingredients of our technique have been used before, their combination here yields a significant improvement over other methodologies using first-order representations, leading to developments that are faithful to informal practice, yet require noexternal tool support and little infrastructure within the proof assistant.We have carried out several large developments in this style using the Coq proof assistant and have made them publicly available. Our developments include type soundness for System F sub; and core ML (with references, exceptions, datatypes, recursion, and patterns) and subject reduction for the Calculus of Constructions. Not only do these developments demonstrate the comprehensiveness of our approach; they have also been optimized for clarity and robustness, making them good templates for future extension.}},
  url = {https://doi.org/10.1145/1328438.1328443},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1398,
  title = {Synthesizing software verifiers from proof rules}},
  author = {Grebenshchikov, Sergey and Lopes, Nuno P. and Popeea, Corneliu and Rybalchenko, Andrey}},
  year = {2012}},
  journal = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Automatically generated tools can significantly improve programmer productivity. For example, parsers and dataflow analyzers can be automatically generated from declarative specifications in the form of grammars, which tremendously simplifies the task of implementing a compiler. In this paper, we present a method for the automatic synthesis of software verification tools. Our synthesis procedure takes as input a description of the employed proof rule, e.g., program safety checking via inductive invariants, and produces a tool that automatically discovers the auxiliary assertions required by the proof rule, e.g., inductive loop invariants and procedure summaries. We rely on a (standard) representation of proof rules using recursive equations over the auxiliary assertions. The discovery of auxiliary assertions, i.e., solving the equations, is based on an iterative process that extrapolates solutions obtained for finitary unrollings of equations. We show how our method synthesizes automatic safety and liveness verifiers for programs with procedures, multi-threaded programs, and functional programs. Our experimental comparison of the resulting verifiers with existing state-of-the-art verification tools confirms the practicality of the approach.}},
  url = {https://doi.org/10.1145/2254064.2254112},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1399,
  title = {A formal foundation for dynamic delta-oriented software product lines}},
  author = {Damiani, Ferruccio and Padovani, Luca and Schaefer, Ina}},
  year = {2012}},
  journal = {Proceedings of the 11th International Conference on Generative Programming and Component Engineering}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Delta-oriented programming (DOP) is a flexible approach for implementing software product lines (SPLs). DOP SPLs are implemented by a code base (a set of delta modules encapsulating changes to object-oriented programs) and a product line declaration (providing the connection of the delta modules with the product features). In this paper, we extend DOP by the capability to switch the implemented product configuration at runtime and present a formal foundation for dynamic DOP. A dynamic DOP SPL is a DOP SPL with a dynamic reconfiguration graph that specifies how to switch between different feature configurations. Dynamic DOP supports (unanticipated) software evolution such that at runtime, the product line declaration, the code base and the dynamic reconfiguration graph can be changed in any (unanticipated) way that preserves the currently running product. The type system of our dynamic DOP core calculus ensures that the dynamic reconfigurations lead to type safe products and do not cause runtime type errors.}},
  url = {https://doi.org/10.1145/2371401.2371403},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1400,
  title = {Dynamic multirole session types}},
  author = {Deni\'{e}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Multiparty session types enforce structured safe communications between several participants, as long as their number is fixed when the session starts. In order to handle common distributed interaction patterns such as peer-to-peer protocols or cloud algorithms, we propose a new role-based multiparty session type theory where roles are defined as classes of local behaviours that an arbitrary number of participants can dynamically join and leave. We offer programmers a polling operation that gives access to the current set of a role's participants in order to fork processes. Our type system with universal types for polling can handle this dynamism and retain type safety. A multiparty locking mechanism is introduced to provide communication safety, but also to ensure a stronger progress property for joining participants that has never been guaranteed in previous systems. Finally, we present some implementation mechanisms used in our prototype extension of ML.}},
  url = {https://doi.org/10.1145/1926385.1926435},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1401,
  title = {Common Compiler Optimisations are Invalid in the C11 Memory Model and what we can do about it}},
  author = {Vafeiadis, Viktor and Balabonski, Thibaut and Chakraborty, Soham and Morisset, Robin and Zappa Nardelli, Francesco}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We show that the weak memory model introduced by the 2011 C and C++ standards does not permit many common source-to-source program transformations (such as expression linearisation and "roach motel" reorderings) that modern compilers perform and that are deemed to be correct. As such it cannot be used to define the semantics of intermediate languages of compilers, as, for instance, LLVM aimed to. We consider a number of possible local fixes, some strengthening and some weakening the model. We evaluate the proposed fixes by determining which program transformations are valid with respect to each of the patched models. We provide formal Coq proofs of their correctness or counterexamples as appropriate.}},
  url = {https://doi.org/10.1145/2676726.2676995},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1402,
  title = {Compositional dynamic test generation}},
  author = {Godefroid, Patrice}},
  year = {2007}},
  journal = {Proceedings of the 34th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Dynamic test generation is a form of dynamic program analysis that attempts to compute test inputs to drive a program along a specific program path. Directed Automated Random Testing, or DART for short, blends dynamic test generation with model checking techniques with the goal of systematically executing all feasible program paths of a program while detecting various types of errors using run-time checking tools (like Purify, for instance). Unfortunately, systematically executing all feasible program paths does not scale to large, realistic programs.This paper addresses this major limitation and proposes to perform dynamic test generation compositionally, by adapting known techniques for interprocedural static analysis. Specifically, we introduce a new algorithm, dubbed SMART for Systematic Modular Automated Random Testing, that extends DART by testing functions in isolation, encoding test results as function summaries expressed using input preconditions and output postconditions, and then re-using those summaries when testing higher-level functions. We show that, for a fixed reasoning capability, our compositional approach to dynamic test generation (SMART) is both sound and complete compared to monolithic dynamic test generation (DART). In other words, SMART can perform dynamic test generation compositionally without any reduction in program path coverage. We also show that, given a bound on the maximum number of feasible paths in individual program functions, the number of program executions explored by SMART is linear in that bound, while the number of program executions explored by DART can be exponential in that bound. We present examples of C programs and preliminary experimental results that illustrate and validate empirically these properties.}},
  url = {https://doi.org/10.1145/1190216.1190226},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1403,
  title = {Coarse-grained transactions}},
  author = {Koskinen, Eric and Parkinson, Matthew and Herlihy, Maurice}},
  year = {2010}},
  journal = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Traditional transactional memory systems suffer from overly conservative conflict detection, yielding so-called false conflicts, because they are based on fine-grained, low-level read/write conflicts. In response, the recent trend has been toward integrating various abstract data-type libraries using ad-hoc methods of high-level conflict detection. These proposals have led to improved performance but a lack of a unified theory has led to confusion in the literature.We clarify these recent proposals by defining a generalization of transactional memory in which a transaction consists of coarse-grained (abstract data-type) operations rather than simple memory read/write operations. We provide semantics for both pessimistic (e.g. transactional boosting) and optimistic (e.g. traditional TMs and recent alternatives) execution. We show that both are included in the standard atomic semantics, yet find that the choice imposes different requirements on the coarse-grained operations: pessimistic requires operations be left-movers, optimistic requires right-movers. Finally, we discuss how the semantics applies to numerous TM implementation details discussed widely in the literature.}},
  url = {https://doi.org/10.1145/1706299.1706304},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1404,
  title = {ILP-based Instruction Scheduling for IA-64}},
  author = {K\"{a}},
  year = {2001}},
  journal = {Proceedings of the ACM SIGPLAN Workshop on Languages, Compilers and Tools for Embedded Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The IA-64 architecture has been designed as a synthesis of VLIW and superscalar design principles. It incorporates typical functionality known from embedded processors as multiply/accumulate units and SIMD operations for 3D graphics operations. In this paper we present an ILP formulation for the problem of instruction scheduling for IA-64. In order to obtain a feasible schedule it is necessary to model the data dependences, resource constraints as well as additional encoding restrictions—the bundling mechanism. These different aspects represent subproblems that are closely coupled which gives the motivation for a modeling based on integer linear programming. The presented approach is divided in to two phases which allows us to compute mostly optimal solutions with acceptable computation time.}},
  url = {https://doi.org/10.1145/384197.384217},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1405,
  title = {Inferable object-oriented typed assembly language}},
  author = {Tate, Ross and Chen, Juan and Hawblitzel, Chris}},
  year = {2010}},
  journal = {Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A certifying compiler preserves type information through compilation to assembly language programs, producing typed assembly language (TAL) programs that can be verified for safety independently so that the compiler does not need to be trusted. There are two challenges for adopting certifying compilation in practice. First, requiring every compiler transformation and optimization to preserve types is a large burden on compilers, especially when adopting certifying compilation into existing optimizing non-certifying compilers. Second, type annotations significantly increase the size of assembly language programs.This paper proposes an alternative to traditional certifying compilers. It presents iTalX, the first inferable TAL type system that supports existential types, arrays, interfaces, and stacks. We have proved our inference algorithm is complete, meaning if an assembly language program is typeable with iTalX then our algorithm will infer an iTalX typing for that program. Furthermore, our algorithm is guaranteed to terminate even if the assembly language program is untypeable. We demonstrate that it is practical to infer such an expressive TAL by showing a prototype implementation of type inference for code compiled by Bartok, an optimizing C# compiler. Our prototype implementation infers complete type annotations for 98\% of functions in a suite of realistic C# benchmarks. The type-inference time is about 8\% of the compilation time. We needed to change only 2.5\% of the compiler code, mostly adding new code for defining types and for writing types to object files. Most transformations are untouched. Type-annotation size is only 17\% of the size of pure code and data, reducing type annotations in our previous certifying compiler [4] by 60\%. The compiler needs to preserve only essential type information such as method signatures, object-layout information, and types for static data and external labels. Even non-certifying compilers have most of this information available.}},
  url = {https://doi.org/10.1145/1806596.1806644},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1406,
  title = {StreamScan: fast scan algorithms for GPUs without global barrier synchronization}},
  author = {Yan, Shengen and Long, Guoping and Zhang, Yunquan}},
  year = {2013}},
  journal = {Proceedings of the 18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Scan (also known as prefix sum) is a very useful primitive for various important parallel algorithms, such as sort, BFS, SpMV, compaction and so on. Current state of the art of GPU based scan implementation consists of three consecutive Reduce-Scan-Scan phases. This approach requires at least two global barriers and 3N (N is the problem size) global memory accesses. In this paper we propose StreamScan, a novel approach to implement scan on GPUs with only one computation phase. The main idea is to restrict synchronization to only adjacent workgroups, and thereby eliminating global barrier synchronization completely. The new approach requires only 2N global memory accesses and just one kernel invocation. On top of this we propose two important op-timizations to further boost performance speedups, namely thread grouping to eliminate unnecessary local barriers, and register optimization to expand the on chip problem size. We designed an auto-tuning framework to search the parameter space automatically to generate highly optimized codes for both AMD and Nvidia GPUs. We implemented our technique with OpenCL. Compared with previous fast scan implementations, experimental results not only show promising performance speedups, but also reveal dramatic different optimization tradeoffs between Nvidia and AMD GPU platforms.}},
  url = {https://doi.org/10.1145/2442516.2442539},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1407,
  title = {The design and implementation of typed scheme}},
  author = {Tobin-Hochstadt, Sam and Felleisen, Matthias}},
  year = {2008}},
  journal = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {When scripts in untyped languages grow into large programs, maintaining them becomes difficult. A lack of types in typical scripting languages means that programmers must (re)discover critical pieces of design information every time they wish to change a program. This analysis step both slows down the maintenance process and may even introduce mistakes due to the violation of undiscovered invariants.This paper presents Typed Scheme, an explicitly typed extension of an untyped scripting language. Its type system is based on the novel notion of occurrence typing, which we formalize and mechanically prove sound. The implementation of Typed Scheme additionally borrows elements from a range of approaches, including recursive types, true unions and subtyping, plus polymorphism combined with a modicum of local inference. Initial experiments with the implementation suggest that Typed Scheme naturally accommodates the programming style of the underlying scripting language, at least for the first few thousand lines of ported code.}},
  url = {https://doi.org/10.1145/1328438.1328486},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1408,
  title = {An error-tolerant type system for variational lambda calculus}},
  author = {Chen, Sheng and Erwig, Martin and Walkingshaw, Eric}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Conditional compilation and software product line technologies make it possible to generate a huge number of different programs from a single software project. Typing each of these programs individually is usually impossible due to the sheer number of possible variants. Our previous work has addressed this problem with a type system for variational lambda calculus (VLC), an extension of lambda calculus with basic constructs for introducing and organizing variation. Although our type inference algorithm is more efficient than the brute-force strategy of inferring the types of each variant individually, it is less robust since type inference will fail for the entire variational expression if any one variant contains a type error. In this work, we extend our type system to operate on VLC expressions containing type errors. This extension directly supports locating ill-typed variants and the incremental development of variational programs. It also has many subtle implications for the unification of variational types. We show that our extended type system possesses a principal typing property and that the underlying unification problem is unitary. Our unification algorithm computes partial unifiers that lead to result types that (1) contain errors in as few variants as possible and (2) are most general. Finally, we perform an empirical evaluation to determine the overhead of this extension compared to our previous work, to demonstrate the improvements over the brute-force approach, and to explore the effects of various error distributions on the inference process.}},
  url = {https://doi.org/10.1145/2364527.2364535},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1409,
  title = {The essence of compiling with traces}},
  author = {Guo, Shu-yu and Palsberg, Jens}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The technique of trace-based just-in-time compilation was introduced by Bala et al. and was further developed by Gal et al. It currently enjoys success in Mozilla Firefox's JavaScript engine. A trace-based JIT compiler leverages run-time profiling to optimize frequently-executed paths while enabling the optimized code to ``bail out'' to the original code when the path has been invalidated. This optimization strategy differs from those of other JIT compilers and opens the question of which trace optimizations are sound. In this paper we present a framework for reasoning about the soundness of trace optimizations, and we show that some traditional optimization techniques are sound when used in a trace compiler while others are unsound. The converse is also true: some trace optimizations are sound when used in a traditional compiler while others are unsound. So, traditional and trace optimizations form incomparable sets. Our setting is an imperative calculus for which tracing is explicitly spelled out in the semantics. We define optimization soundness via a notion of bisimulation, and we show that sound optimizations lead to confluence and determinacy of stores.}},
  url = {https://doi.org/10.1145/1926385.1926450},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1410,
  title = {Inferring aliasing and encapsulation properties for java}},
  author = {Ma, Kin-Keung and Foster, Jeffrey S.}},
  year = {2007}},
  journal = {Proceedings of the 22nd Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {There are many proposals for language techniques to control aliasing and encapsulation in object oriented programs, typically based on notions of object ownership and pointer uniqueness. Most of these systems require extensive manual annotations, and thus there is little experience with these properties in large, existing Java code bases. To remedy this situation, we present Uno, a novel static analysis for automatically inferring ownership, uniqueness, and other aliasing and encapsulation properties in Java. Our analysis requires no annotations, and combines an intraprocedural points-to analysis with an interprocedural, demand-driven predicate resolution algorithm. We have applied Uno to a variety of Java applications and found that some aliasing properties, such as temporarily lending a reference to a method, are common, while others, in particular field and argument ownership, are relatively uncommon. As a result, we believe that Uno can be a valuable tool for discovering and understanding aliasing and encapsulation in Java programs.}},
  url = {https://doi.org/10.1145/1297027.1297059},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1411,
  title = {Static analysis for probabilistic programs: inferring whole program properties from finitely many paths}},
  author = {Sankaranarayanan, Sriram and Chakarov, Aleksandar and Gulwani, Sumit}},
  year = {2013}},
  journal = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We propose an approach for the static analysis of probabilistic programs that sense, manipulate, and control based on uncertain data. Examples include programs used in risk analysis, medical decision making and cyber-physical systems. Correctness properties of such programs take the form of queries that seek the probabilities of assertions over program variables. We present a static analysis approach that provides guaranteed interval bounds on the values (assertion probabilities) of such queries. First, we observe that for probabilistic programs, it is possible to conclude facts about the behavior of the entire program by choosing a finite, adequate set of its paths. We provide strategies for choosing such a set of paths and verifying its adequacy. The queries are evaluated over each path by a combination of symbolic execution and probabilistic volume-bound computations. Each path yields interval bounds that can be summed up with a "coverage" bound to yield an interval that encloses the probability of assertion for the program as a whole. We demonstrate promising results on a suite of benchmarks from many different sources including robotic manipulators and medical decision making programs.}},
  url = {https://doi.org/10.1145/2491956.2462179},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1412,
  title = {Sponge: portable stream programming on graphics engines}},
  author = {Hormati, Amir H. and Samadi, Mehrzad and Woh, Mark and Mudge, Trevor and Mahlke, Scott}},
  year = {2011}},
  journal = {Proceedings of the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Graphics processing units (GPUs) provide a low cost platform for accelerating high performance computations. The introduction of new programming languages, such as CUDA and OpenCL, makes GPU programming attractive to a wide variety of programmers. However, programming GPUs is still a cumbersome task for two primary reasons: tedious performance optimizations and lack of portability. First, optimizing an algorithm for a specific GPU is a time-consuming task that requires a thorough understanding of both the algorithm and the underlying hardware. Unoptimized CUDA programs typically only achieve a small fraction of the peak GPU performance. Second, GPU code lacks efficient portability as code written for one GPU can be inefficient when executed on another. Moving code from one GPU to another while maintaining the desired performance is a non-trivial task often requiring significant modifications to account for the hardware differences. In this work, we propose Sponge, a compilation framework for GPUs using synchronous data flow streaming languages. Sponge is capable of performing a wide variety of optimizations to generate efficient code for graphics engines. Sponge alleviates the problems associated with current GPU programming methods by providing portability across different generations of GPUs and CPUs, and a better abstraction of the hardware details, such as the memory hierarchy and threading model. Using streaming, we provide a write-once software paradigm and rely on the compiler to automatically create optimized CUDA code for a wide variety of GPU targets. Sponge's compiler optimizations improve the performance of the baseline CUDA implementations by an average of 3.2x.}},
  url = {https://doi.org/10.1145/1950365.1950409},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1413,
  title = {Trampolined style}},
  author = {Ganz, Steven E. and Friedman, Daniel P. and Wand, Mitchell}},
  year = {1999}},
  journal = {Proceedings of the Fourth ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {A trampolined program is organized as a single loop in which computations are scheduled and their execution allowed to proceed in discrete steps. Writing programs in trampolined style supports primitives for multithreading without language support for continuations. Various forms of trampolining allow for different degrees of interaction between threads. We present two architectures based on an only mildly intrusive trampolined style. Concurrency can be supported at multiple levels of granularity by performing the trampolining transformation multiple times.}},
  url = {https://doi.org/10.1145/317636.317779},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1414,
  title = {Demand-driven alias analysis for C}},
  author = {Zheng, Xin and Rugina, Radu}},
  year = {2008}},
  journal = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents a demand-driven, flow-insensitive analysisalgorithm for answering may-alias queries. We formulate thecomputation of alias queries as a CFL-reachability problem, and use this formulation to derive a demand-driven analysis algorithm. The analysis uses a worklist algorithm that gradually explores the program structure and stops as soon as enough evidence is gathered to answer the query. Unlike existing techniques, our approach does not require building or intersecting points-to sets.Experiments show that our technique is effective at answering alias queries accurately and efficiently in a demand-driven fashion. For a set of alias queries from the SPEC2000 benchmarks, an implementation of our analysis is able to accurately answer 96\% of the queries in 0.5 milliseconds per query on average, using only 65 KB of memory. Compared to a demand-driven points-to analysis that constructs and intersects points-to sets on the fly, our alias analysis can achieve better accuracy while running more than 30 times faster. The low run-time cost and low memory demands of the analysis make it a very good candidate not only for compilers, but also for interactive tools, such as program understanding tools or integrated development environments (IDEs).}},
  url = {https://doi.org/10.1145/1328438.1328464},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1415,
  title = {Complete and decidable type inference for GADTs}},
  author = {Schrijvers, Tom and Peyton Jones, Simon and Sulzmann, Martin and Vytiniotis, Dimitrios}},
  year = {2009}},
  journal = {Proceedings of the 14th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {GADTs have proven to be an invaluable language extension, for ensuring data invariants and program correctness among others. Unfortunately, they pose a tough problem for type inference: we lose the principal-type property, which is necessary for modular type inference.We present a novel and simplified type inference approach for local type assumptions from GADT pattern matches. Our approach is complete and decidable, while more liberal than previous such approaches.}},
  url = {https://doi.org/10.1145/1596550.1596599},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1416,
  title = {Featherweight X10: a core calculus for async-finish parallelism}},
  author = {Lee, Jonathan K. and Palsberg, Jens}},
  year = {2010}},
  journal = {Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a core calculus with two of X10's key constructs for parallelism, namely async and finish. Our calculus forms a convenient basis for type systems and static analyses for languages with async-finish parallelism, and for tractable proofs of correctness. For example, we give a short proof of the deadlock-freedom theorem of Saraswat and Jagadeesan. Our main contribution is a type system that solves the open problem of context-sensitive may-happen-in-parallel analysis for languages with async-finish parallelism. We prove the correctness of our type system and we report experimental results of performing type inference on 13,000 lines of X10 code. Our analysis runs in polynomial time, takes a total of 28 seconds on our benchmarks, and produces a low number of false positives, which suggests that our analysis is a good basis for other analyses such as race detectors.}},
  url = {https://doi.org/10.1145/1693453.1693459},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1417,
  title = {Design and implementation of sparse global analyses for C-like languages}},
  author = {Oh, Hakjoo and Heo, Kihong and Lee, Wonchan and Lee, Woosuk and Yi, Kwangkeun}},
  year = {2012}},
  journal = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {In this article we present a general method for achieving global static analyzers that are precise, sound, yet also scalable. Our method generalizes the sparse analysis techniques on top of the abstract interpretation framework to support relational as well as non-relational semantics properties for C-like languages. We first use the abstract interpretation framework to have a global static analyzer whose scalability is unattended. Upon this underlying sound static analyzer, we add our generalized sparse analysis techniques to improve its scalability while preserving the precision of the underlying analysis. Our framework determines what to prove to guarantee that the resulting sparse version should preserve the precision of the underlying analyzer.We formally present our framework; we present that existing sparse analyses are all restricted instances of our framework; we show more semantically elaborate design examples of sparse non-relational and relational static analyses; we present their implemen- tation results that scale to analyze up to one million lines of C programs. We also show a set of implementation techniques that turn out to be critical to economically support the sparse analysis process.}},
  url = {https://doi.org/10.1145/2254064.2254092},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1418,
  title = {Lazy evaluation and delimited control}},
  author = {Garcia, Ronald and Lumsdaine, Andrew and Sabry, Amr}},
  year = {2009}},
  journal = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The call-by-need lambda calculus provides an equational framework for reasoning syntactically about lazy evaluation. This paper examines its operational characteristics.By a series of reasoning steps, we systematically unpack the standard-order reduction relation of the calculus and discover a novel abstract machine definition which, like the calculus, goes "under lambdas." We prove that machine evaluation is equivalent to standard-order evaluation.Unlike traditional abstract machines, delimited control plays a significant role in the machine's behavior. In particular, the machine replaces the manipulation of a heap using store-based effects with disciplined management of the evaluation stack using control-based effects. In short, state is replaced with control.To further articulate this observation, we present a simulation of call-by-need in a call-by-value language using delimited control operations.}},
  url = {https://doi.org/10.1145/1480881.1480903},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1419,
  title = {Focusing and higher-order abstract syntax}},
  author = {Zeilberger, Noam}},
  year = {2008}},
  journal = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Focusing is a proof-search strategy, originating in linear logic, that elegantly eliminates inessential nondeterminism, with one byproduct being a correspondence between focusing proofs and programs with explicit evaluation order. Higher-order abstract syntax (HOAS) is a technique for representing higher-order programming language constructs (e.g., λ's) by higher-order terms at the"meta-level", thereby avoiding some of the bureaucratic headaches of first-order representations (e.g., capture-avoiding substitution).This paper begins with a fresh, judgmental analysis of focusing for intuitionistic logic (with a full suite of propositional connectives), recasting the "derived rules" of focusing as iterated inductive definitions. This leads to a uniform presentation, allowing concise, modular proofs of the identity and cut principles. Then we show how this formulation of focusing induces, through the Curry-Howard isomorphism, a new kind of higher-order encoding of abstract syntax: functions are encoded by maps from patterns to expressions. Dually, values are encoded as patterns together with explicit substitutions. This gives us pattern-matching "for free", and lets us reason about a rich type system with minimal syntactic overhead. We describe how to translate the language and proof of type safety almost directly into Coq using HOAS, and finally, show how the system's modular design pays off in enabling a very simple extension with recursion and recursive types.}},
  url = {https://doi.org/10.1145/1328438.1328482},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1420,
  title = {Towards the Essence of Hygiene}},
  author = {Adams, Michael D.}},
  year = {2015}},
  journal = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Hygiene is an essential aspect of Scheme's macro system that prevents unintended variable capture. However, previous work on hygiene has focused on algorithmic implementation rather than precise, mathematical definition of what constitutes hygiene. This is in stark contrast with lexical scope, alpha-equivalence and capture-avoiding substitution, which also deal with preventing unintended variable capture but have widely applicable and well-understood mathematical definitions.This paper presents such a precise, mathematical definition of hygiene. It reviews various kinds of hygiene violation and presents examples of how they occur. From these examples, we develop a practical algorithm for hygienic macro expansion. We then present algorithm-independent, mathematical criteria for whether a macro expansion algorithm is hygienic. This characterization corresponds closely to existing hygiene algorithms and sheds light on aspects of hygiene that are usually overlooked in informal definitions.}},
  url = {https://doi.org/10.1145/2676726.2677013},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1421,
  title = {CSX: an extended compression format for spmv on shared memory systems}},
  author = {Kourtis, Kornilios and Karakasis, Vasileios and Goumas, Georgios and Koziris, Nectarios}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The Sparse Matrix-Vector multiplication (SpMV) kernel scales poorly on shared memory systems with multiple processing units due to the streaming nature of its data access pattern. Previous research has demonstrated that an effective strategy to improve the kernel's performance is to drastically reduce the data volume involved in the computations. Since the storage formats for sparse matrices include metadata describing the structure of non-zero elements within the matrix, we propose a generalized approach to compress metadata by exploiting substructures within the matrix. We call the proposed storage format Compressed Sparse eXtended (CSX). In our implementation we employ runtime code generation to construct specialized SpMV routines for each matrix. Experimental evaluation on two shared memory systems for 15 sparse matrices demonstrates significant performance gains as the number of participating cores increases. Regarding the cost of CSX construction, we propose several strategies which trade performance for preprocessing cost making CSX applicable both to online and offline preprocessing.}},
  url = {https://doi.org/10.1145/1941553.1941587},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1422,
  title = {Establishing object invariants with delayed types}},
  author = {Fahndrich, Manuel and Xia, Songtao}},
  year = {2007}},
  journal = {Proceedings of the 22nd Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Mainstream object-oriented languages such as C# and Java provide an initialization model for objects that does not guarantee programmer controlled initialization of fields. Instead, all fields are initialized to default values (0 for scalars and null for non-scalars) on allocation. This is in stark contrast to functional languages, where all parts of an allocation are initialized to programmer-provided values. These choices have a direct impact on two main issues: 1) the prevalence of null in object oriented languages (and its general absence in functional languages), and 2) the ability to initialize circular data structures. This paper explores connections between these differing approaches and proposes a fresh look at initialization. Delayed types are introduced to express and formalize prevalent initialization patterns in object-oriented languages.}},
  url = {https://doi.org/10.1145/1297027.1297052},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1423,
  title = {Verified just-in-time compiler on x86}},
  author = {Myreen, Magnus O.}},
  year = {2010}},
  journal = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents a method for creating formally correct just-in-time (JIT) compilers. The tractability of our approach is demonstrated through, what we believe is the first, verification of a JIT compiler with respect to a realistic semantics of self-modifying x86 machine code. Our semantics includes a model of the instruction cache. Two versions of the verified JIT compiler are presented: one generates all of the machine code at once, the other one is incremental i.e. produces code on-demand. All proofs have been performed inside the HOL4 theorem prover.}},
  url = {https://doi.org/10.1145/1706299.1706313},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1424,
  title = {A theory of platform-dependent low-level software}},
  author = {Nita, Marius and Grossman, Dan and Chambers, Craig}},
  year = {2008}},
  journal = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {The C language definition leaves the sizes and layouts of types partially unspecified. When a C program makes assumptions about type layout, its semantics is defined only on platforms (C compilers and the underlying hardware) on which those assumptions hold. Previous work on formalizing C-like languages has ignored this issue, either by assuming that programs do not make such assumptions or by assuming that all valid programs target only one platform. In the latter case, the platform's choices are hard-wired in the language semantics.In this paper, we present a practically-motivated model for a C-like language in which the memory layouts of types are left largely unspecified. The dynamic semantics is parameterized by a platform's layout policy and makes manifest the consequence of platform-dependent (i.e., unspecified) steps. A type-and-effect system produces a layout constraint: a logic formula encoding layout conditions under which the program is memory-safe. We prove that if a program type-checks, it is memory-safe on all platforms satisfying its constraint.Based on our theory, we have implemented a tool that discovers unportable layout assumptions in C programs. Our approach should generalize to other kinds of platform-dependent assumptions.}},
  url = {https://doi.org/10.1145/1328438.1328465},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1425,
  title = {A simple semantics for Haskell overloading}},
  author = {Morris, J. Garrett}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM SIGPLAN Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {As originally proposed, type classes provide overloading and ad-hoc definition, but can still be understood (and implemented) in terms of strictly parametric calculi. This is not true of subsequent extensions of type classes. Functional dependencies and equality constraints allow the satisfiability of predicates to refine typing; this means that the interpretations of equivalent qualified types may not be interconvertible. Overlapping instances and instance chains allow predicates to be satisfied without determining the implementations of their associated class methods, introducing truly non-parametric behavior. We propose a new approach to the semantics of type classes, interpreting polymorphic expressions by the behavior of each of their ground instances, but without requiring that those behaviors be parametrically determined. We argue that this approach both matches the intuitive meanings of qualified types and accurately models the behavior of programs.}},
  url = {https://doi.org/10.1145/2633357.2633364},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1426,
  title = {Fast: a transducer-based language for tree manipulation}},
  author = {D'Antoni, Loris and Veanes, Margus and Livshits, Benjamin and Molnar, David}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Tree automata and tree transducers are used in a wide range of applications in software engineering, from XML processing to language type-checking. While these formalisms are of immense practical use, they can only model finite alphabets, and since many real-world applications operate over infinite domains such as integers, this is often a limitation. To overcome this problem we augment tree automata and transducers with symbolic alphabets represented as parametric theories. Admitting infinite alphabets makes these models more general and succinct than their classical counterparts. Despite this, we show how the main operations, such as composition and language equivalence, remain computable given a decision procedure for the alphabet theory.We introduce a high-level language called Fast that acts as a front-end for the above formalisms. Fast supports symbolic alphabets through tight integration with state-of-the-art satisfiability modulo theory (SMT) solvers. We demonstrate our techniques on practical case studies, covering a wide range of applications.}},
  url = {https://doi.org/10.1145/2594291.2594309},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1427,
  title = {Promoting functions to type families in Haskell}},
  author = {Eisenberg, Richard A. and Stolarek, Jan}},
  year = {2014}},
  journal = {Proceedings of the 2014 ACM SIGPLAN Symposium on Haskell}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Haskell, as implemented in the Glasgow Haskell Compiler (GHC), is enriched with many extensions that support type-level programming, such as promoted datatypes, kind polymorphism, and type families. Yet, the expressiveness of the type-level language remains limited. It is missing many features present at the term level, including case expressions, anonymous functions, partially-applied functions, and let expressions. In this paper, we present an algorithm - with a proof of correctness - to encode these term-level constructs at the type level. Our approach is automated and capable of promoting a wide array of functions to type families. We also highlight and discuss those term-level features that are not promotable. In so doing, we offer a critique on GHC's existing type system, showing what it is already capable of and where it may want improvement.We believe that delineating the mismatch between GHC's term level and its type level is a key step toward supporting dependently typed programming.}},
  url = {https://doi.org/10.1145/2633357.2633361},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1428,
  title = {Towards a software product line of trie-based collections}},
  author = {Steindorfer, Michael J. and Vinju, Jurgen J.}},
  year = {2016}},
  journal = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Collection data structures in standard libraries of programming languages are designed to excel for the average case by carefully balancing memory footprint and runtime performance. These implicit design decisions and hard-coded trade-offs do constrain users from using an optimal variant for a given problem. Although a wide range of specialized collections is available for the Java Virtual Machine (JVM), they introduce yet another dependency and complicate user adoption by requiring specific Application Program Interfaces (APIs) incompatible with the standard library. A product line for collection data structures would relieve library designers from optimizing for the general case. Furthermore, a product line allows evolving the potentially large code base of a collection family efficiently. The challenge is to find a small core framework for collection data structures which covers all variations without exhaustively listing them, while supporting good performance at the same time. We claim that the concept of Array Mapped Tries (AMTs) embodies a high degree of commonality in the sub-domain of immutable collection data structures. AMTs are flexible enough to cover most of the variability, while minimizing code bloat in the generator and the generated code. We implemented a Data Structure Code Generator (DSCG) that emits immutable collections based on an AMT skeleton foundation. The generated data structures outperform competitive hand-optimized implementations, and the generator still allows for customization towards specific workloads.}},
  url = {https://doi.org/10.1145/2993236.2993251},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1429,
  title = {Higher-order multi-parameter tree transducers and recursion schemes for program verification}},
  author = {Kobayashi, Naoki and Tabuchi, Naoshi and Unno, Hiroshi}},
  year = {2010}},
  journal = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We introduce higher-order, multi-parameter, tree transducers (HMTTs, for short), which are kinds of higher-order tree transducers that take input trees and output a (possibly infinite) tree. We study the problem of checking whether the tree generated by a given HMTT conforms to a given output specification, provided that the input trees conform to input specifications (where both input/output specifications are regular tree languages). HMTTs subsume higher-order recursion schemes and ordinary tree transducers, so that their verification has a number of potential applications to verification of functional programs using recursive data structures, including resource usage verification, string analysis, and exact type-checking of XML-processing programs.We propose a sound but incomplete verification algorithm for the HMTT verification problem: the algorithm reduces the verification problem to a model-checking problem for higher-order recursion schemes extended with finite data domains, and then uses (an extension of)Kobayashi's algorithm for model-checking recursion schemes. While the algorithm is incomplete (indeed, as we show in the paper, the verification problem is undecidable in general), it is sound and complete for a subclass of HMTTs called linear HMTTs . We have applied our HMTT verification algorithm to various program verification problems and obtained promising results.}},
  url = {https://doi.org/10.1145/1706299.1706355},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1430,
  title = {Smooth interpretation}},
  author = {Chaudhuri, Swarat and Solar-Lezama, Armando}},
  year = {2010}},
  journal = {Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present smooth interpretation, a method to systematically approximate numerical imperative programs by smooth mathematical functions. This approximation facilitates the use of numerical search techniques like gradient descent for program analysis and synthesis. The method extends to programs the notion of Gaussian smoothing, a popular signal-processing technique that filters out noise and discontinuities from a signal by taking its convolution with a Gaussian function.In our setting, Gaussian smoothing executes a program according to a probabilistic semantics; the execution of program P on an input x after Gaussian smoothing can be summarized as follows: (1) Apply a Gaussian perturbation to x -- the perturbed input is a random variable following a normal distribution with mean x. (2) Compute and return the expected output of P on this perturbed input. Computing the expectation explicitly would require the execution of P on all possible inputs, but smooth interpretation bypasses this requirement by using a form of symbolic execution to approximate the effect of Gaussian smoothing on P. The result is an efficient but approximate implementation of Gaussian smoothing of programs.Smooth interpretation has the effect of attenuating features of a program that impede numerical searches of its input space -- for example, discontinuities resulting from conditional branches are replaced by continuous transitions. We apply smooth interpretation to the problem of synthesizing values of numerical control parameters in embedded control applications. This problem is naturally formulated as one of numerical optimization: the goal is to find parameter values that minimize the error between the resulting program and a programmer-provided behavioral specification. Solving this problem by directly applying numerical optimization techniques is often impractical due to the discontinuities in the error function. By eliminating these discontinuities, smooth interpretation makes it possible to search the parameter space efficiently by means of simple gradient descent. Our experiments demonstrate the value of this strategy in synthesizing parameters for several challenging programs, including models of an automated gear shift and a PID controller.}},
  url = {https://doi.org/10.1145/1806596.1806629},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1431,
  title = {EigenCFA: accelerating flow analysis with GPUs}},
  author = {Prabhu, Tarun and Ramalingam, Shreyas and Might, Matthew and Hall, Mary}},
  year = {2011}},
  journal = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We describe, implement and benchmark EigenCFA, an algorithm for accelerating higher-order control-flow analysis (specifically, 0CFA) with a GPU. Ultimately, our program transformations, reductions and optimizations achieve a factor of 72 speedup over an optimized CPU implementation.We began our investigation with the view that GPUs accelerate high-arithmetic, data-parallel computations with a poor tolerance for branching. Taking that perspective to its limit, we reduced Shivers's abstract-interpretive 0CFA to an algorithm synthesized from linear-algebra operations. Central to this reduction were "abstract" Church encodings, and encodings of the syntax tree and abstract domains as vectors and matrices.A straightforward (dense-matrix) implementation of EigenCFA performed slower than a fast CPU implementation. Ultimately, sparse-matrix data structures and operations turned out to be the critical accelerants. Because control-flow graphs are sparse in practice (up to 96\% empty), our control-flow matrices are also sparse, giving the sparse matrix operations an overwhelming space and speed advantage.We also achieved speedups by carefully permitting data races. The monotonicity of 0CFA makes it sound to perform analysis operations in parallel, possibly using stale or even partially-updated data.}},
  url = {https://doi.org/10.1145/1926385.1926445},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1432,
  title = {Binders unbound}},
  author = {Weirich, Stephanie and Yorgey, Brent A. and Sheard, Tim}},
  year = {2011}},
  journal = {Proceedings of the 16th ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Implementors of compilers, program refactorers, theorem provers, proof checkers, and other systems that manipulate syntax know that dealing with name binding is difficult to do well. Operations such as α-equivalence and capture-avoiding substitution seem simple, yet subtle bugs often go undetected. Furthermore, their implementations are tedious, requiring "boilerplate" code that must be updated whenever the object language definition changes.Many researchers have therefore sought to specify binding syntax declaratively, so that tools can correctly handle the details behind the scenes. This idea has been the inspiration for many new systems (such as Beluga, Delphin, FreshML, FreshOCaml, Cαml, FreshLib, and Ott) but there is still room for improvement in expressivity, simplicity and convenience.In this paper, we present a new domain-specific language, Unbound, for specifying binding structure. Our language is particularly expressive - it supports multiple atom types, pattern binders, type annotations, recursive binders, and nested binding (necessary for telescopes, a feature found in dependently-typed languages). However, our specification language is also simple, consisting of just five basic combinators. We provide a formal semantics for this language derived from a locally nameless representation and prove that it satisfies a number of desirable properties.We also present an implementation of our binding specification language as a GHC Haskell library implementing an embedded domain specific language (EDSL). By using Haskell type constructors to represent binding combinators, we implement the EDSL succinctly using datatype-generic programming. Our implementation supports a number of features necessary for practical programming, including flexibility in the treatment of user-defined types, best-effort name preservation (for error messages), and integration with Haskell's monad transformer library.}},
  url = {https://doi.org/10.1145/2034773.2034818},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1433,
  title = {Selective context-sensitivity guided by impact pre-analysis}},
  author = {Oh, Hakjoo and Lee, Wonchan and Heo, Kihong and Yang, Hongseok and Yi, Kwangkeun}},
  year = {2014}},
  journal = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We present a method for selectively applying context-sensitivity during interprocedural program analysis. Our method applies context-sensitivity only when and where doing so is likely to improve the precision that matters for resolving given queries. The idea is to use a pre-analysis to estimate the impact of context-sensitivity on the main analysis's precision, and to use this information to find out when and where the main analysis should turn on or off its context-sensitivity. We formalize this approach and prove that the analysis always benefits from the pre-analysis-guided context-sensitivity. We implemented this selective method for an existing industrial-strength interval analyzer for full C. The method reduced the number of (false) alarms by 24.4\%, while increasing the analysis cost by 27.8\% on average.The use of the selective method is not limited to context-sensitivity. We demonstrate this generality by following the same principle and developing a selective relational analysis.}},
  url = {https://doi.org/10.1145/2594291.2594318},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1434,
  title = {Synthesizing parallel graph programs via automated planning}},
  author = {Prountzos, Dimitrios and Manevich, Roman and Pingali, Keshav}},
  year = {2015}},
  journal = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {We describe a system that uses automated planning to synthesize correct and efficient parallel graph programs from high-level algorithmic specifications. Automated planning allows us to use constraints to declaratively encode program transformations such as scheduling, implementation selection, and insertion of synchronization. Each plan emitted by the planner satisfies all constraints simultaneously, and corresponds to a composition of these transformations. In this way, we obtain an integrated compilation approach for a very challenging problem domain. We have used this system to synthesize parallel programs for four graph problems: triangle counting, maximal independent set computation, preflow-push maxflow, and connected components. Experiments on a variety of inputs show that the synthesized implementations perform competitively with hand-written, highly-tuned code.}},
  url = {https://doi.org/10.1145/2737924.2737953},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1435,
  title = {A Smalltalk system for algebraic manipulation}},
  author = {Abdali, S. Kamal and Cherry, Guy W. and Soiffer, Neil}},
  year = {1986}},
  journal = {Conference Proceedings on Object-Oriented Programming Systems, Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper describes the design of an algebra system Views implemented in Smalltalk. Views contains facilities for dynamic creation and manipulation of computational domains, for viewing these domains as various categories such as groups, rings, or fields, and for expressing algorithms generically at the level of categories. The design of Views has resulted in the addition of some new abstractions to Smalltalk that are quite useful in their own right. Parameterized classes provide a means for run-time creation of new classes that exhibit generally very similar behavior, differing only in minor ways that can be described by different instantiations of certain parameters. Categories allow the abstraction of the common behavior of classes that derives from the class objects and operations satisfying certain laws independently of the implementation of those objects and operations. Views allow the run-time association of classes with categories (and of categories with other categories), facilitating the use of code written for categories with quite different interpretations of operations. Together, categories and views provide an additional mechanism for code sharing that is richer than both single and multiple inheritance. The paper gives algebraic as well as non-algebraic examples of the above-mentioned features.}},
  url = {https://doi.org/10.1145/28697.28724},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1436,
  title = {NDetermin: inferring nondeterministic sequential specifications for parallelism correctness}},
  author = {Burnim, Jacob and Elmas, Tayfun and Necula, George and Sen, Koushik}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Nondeterministic Sequential (NDSeq) specifications have been proposed as a means for separating the testing, debugging, and verifying of a program's parallelism correctness and its sequential functional correctness. In this work, we present a technique that, given a few representative executions of a parallel program, combines dynamic data flow analysis and Minimum-Cost Boolean Satisfiability (MinCostSAT) solving for automatically inferring a likely NDSeq specification for the parallel program. For a number of Java benchmarks, our tool NDetermin infers equivalent or stronger NDSeq specifications than those previously written manually.}},
  url = {https://doi.org/10.1145/2145816.2145879},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1437,
  title = {JavaScript instrumentation for browser security}},
  author = {Yu, Dachuan and Chander, Ajay and Islam, Nayeem and Serikov, Igor}},
  year = {2007}},
  journal = {Proceedings of the 34th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {It is well recognized that JavaScript can be exploited to launch browser-based security attacks. We propose to battle such attacks using program instrumentation. Untrusted JavaScript code goes through a rewriting process which identifies relevant operations, modifies questionable behaviors, and prompts the user (a web page viewer) for decisions on how to proceed when appropriate. Our solution is parametric with respect to the security policy-the policy is implemented separately from the rewriting, and the same rewriting process is carried out regardless of which policy is in use. Be-sides providing a rigorous account of the correctness of our solution, we also discuss practical issues including policy management and prototype experiments. A useful by-product of our work is an operational semantics of a core subset of JavaScript, where code embedded in (HTML) documents may generate further document pieces (with new code embedded) at runtime, yielding a form of self-modifying code.}},
  url = {https://doi.org/10.1145/1190216.1190252},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1438,
  title = {Parallel algorithms for image histogramming and connected components with an experimental study (extended abstract)}},
  author = {Bader, David A. and J\'{a}},
  year = {1995}},
  journal = {Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper presents efficient and portable implementations of two useful primitives in image processing algorithms, histogramming and connected components. Our general framework is a single-address space, distributed memory programming model. We use efficient techniques for distributing and coalescing data as well as efficient combinations of task and data parallelism. Our connected components algorithm uses a novel approach for parallel merging which performs drastically limited updating during iterative steps, and concludes with a total consistency update at the final step. The algorithms have been coded in Split-C and run on a variety of platforms. Our experimental results are consistent with the theoretical analysis and provide the best known execution times for these two primitives, even when compared with machine-specific implementations. More efficient implementations of Split-C will likely result in even faster execution times.}},
  url = {https://doi.org/10.1145/209936.209950},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1439,
  title = {Elixir: a system for synthesizing concurrent graph programs}},
  author = {Prountzos, Dimitrios and Manevich, Roman and Pingali, Keshav}},
  year = {2012}},
  journal = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Algorithms in new application areas like machine learning and network analysis use "irregular" data structures such as graphs, trees and sets. Writing efficient parallel code in these problem domains is very challenging because it requires the programmer to make many choices: a given problem can usually be solved by several algorithms, each algorithm may have many implementations, and the best choice of algorithm and implementation can depend not only on the characteristics of the parallel platform but also on properties of the input data such as the structure of the graph. One solution is to permit the application programmer to experiment with different algorithms and implementations without writing every variant from scratch. Auto-tuning to find the best variant is a more ambitious solution. These solutions require a system for automatically producing efficient parallel implementations from high-level specifications. Elixir, the system described in this paper, is the first step towards this ambitious goal. Application programmers write specifications that consist of an operator, which describes the computations to be performed, and a schedule for performing these computations. Elixir uses sophisticated inference techniques to produce efficient parallel code from such specifications.We used Elixir to automatically generate many parallel implementations for three irregular problems: breadth-first search, single source shortest path, and betweenness-centrality computation. Our experiments show that the best generated variants can be competitive with handwritten code for these problems from other research groups; for some inputs, they even outperform the handwritten versions.}},
  url = {https://doi.org/10.1145/2384616.2384644},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1440,
  title = {Verification of software barriers}},
  author = {Malkis, Alexander and Banerjee, Anindya}},
  year = {2012}},
  journal = {Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {This paper describes frontiers in verification of the software barrier synchronization primitive. So far most software barrier algorithms have not been mechanically verified. We show preliminary results in automatically proving the correctness of the major software barriers.}},
  url = {https://doi.org/10.1145/2145816.2145871},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1441,
  title = {Context-free session types}},
  author = {Thiemann, Peter and Vasconcelos, Vasco T.}},
  year = {2016}},
  journal = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Session types describe structured communication on heterogeneously typed channels at a high level. Their tail-recursive structure imposes a protocol that can be described by a regular language. The types of transmitted values are drawn from the underlying functional language, abstracting from the details of serializing values of structured data types. Context-free session types extend session types by allowing nested protocols that are not restricted to tail recursion. Nested protocols correspond to deterministic context-free languages. Such protocols are interesting in their own right, but they are particularly suited to describe the low-level serialization of tree-structured data in a type-safe way. We establish the metatheory of context-free session types, prove that they properly generalize standard (two-party) session types, and take first steps towards type checking by showing that type equivalence is decidable.}},
  url = {https://doi.org/10.1145/2951913.2951926},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1442,
  title = {The ruby intermediate language}},
  author = {Furr, Michael and An, Jong-hoon (David) and Foster, Jeffrey S. and Hicks, Michael}},
  year = {2009}},
  journal = {Proceedings of the 5th Symposium on Dynamic Languages}},
  tipo = {Conference Paper}},
  publisher = {Association for Computing Machinery}},
  abstract = {Ruby is a popular, dynamic scripting language that aims to "feel natural to programmers" and give users the "freedom to choose" among many different ways of doing the same thing. While this arguably makes programming in Ruby easier, it makes it hard to build analysis and transformation tools that operate on Ruby source code. In this paper, we present the Ruby Intermediate Language (RIL), a Ruby front-end and intermediate representation that addresses these. RIL includes an extensible GLR parser for Ruby, and an automatic translation into an easy-to-analyze intermediate form. This translation eliminates redundant language constructs, unravels the often subtle ordering among side effecting operations, and makes implicit interpreter operations explicit. We also describe several additional useful features of RIL, such as a dynamic instrumentation library for profiling source code and a dataflow analysis engine. We demonstrate the usefulness of RIL by presenting a static and dynamic analysis to eliminate null pointer errors in Ruby programs. We hope that RIL's features will enable others to more easily build analysis tools for Ruby, and that our design will inspire the of similar frameworks for other dynamic languages.}},
  url = {https://doi.org/10.1145/1640134.1640148},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ACM.bib}
}

@article{ref_dup1443,
  title = {An Automatic Grading System for a High School-level Computational Thinking Course}},
  author = {Sirazum Munira Tisha; Rufino A. Oregon; Gerald Baumgartner; Fernando Alegre; Juana Moreno}},
  year = {2022}},
  journal = {2022 IEEE/ACM 4th International Workshop on Software Engineering Education for the Next Generation (SEENG)}},
  tipo = {Conference Paper}},
  publisher = {IEEE}},
  abstract = {Automatic grading systems help lessen the load of manual grading. Most existent autograders are based on unit testing, which focuses on the correctness of the code, but has limited scope for judging code quality. Moreover, it is cumbersome to implement unit testing for evaluating graphical output code. We propose an autograder that can effectively judge the code quality of the visual output codes ...Show More}},
  url = {https://ieeexplore.ieee.org/document/9814929/},
  shared_files = {Data/resultados_ACM.bib, Data/resultados_ieee.bib}
}

@article{ref_dup1444,
  title = {Machines with imagination}},
  author = {G.A. Morton}},
  year = {2000}},
  journal = {Proceedings of the IEEE}},
  tipo = {Journal Article}},
  publisher = {IEEE}},
  abstract = {First Page of the ArticleShow More}},
  url = {https://ieeexplore.ieee.org/document/824005/},
  shared_files = {Data/resultados_ieee.bib, Data/resultados_ieee.bib}
}

@article{ref_dup1445,
  title = {Massively Parallel Switch-Level Simulation: A Feasibility Study}},
  author = {S.A. Kravitz; R.E. Bryant; R.A. Rutenbar}},
  year = {1989}},
  journal = {26th ACM/IEEE Design Automation Conference}},
  tipo = {Conference Paper}},
  publisher = {IEEE}},
  abstract = {This work addresses the feasibility of mapping the COSMOS switch-level simulator onto a computer with thousands of simple processors. COSMOS preprocesses transistor networks into Boolean behavioral models, capturing the switch-level behavior of a circuit in a set of Boolean formulas. We describe a class of massively parallel computers and a mapping of COSMOS onto these computers. We discuss the fa...Show More}},
  url = {https://ieeexplore.ieee.org/document/1586360/},
  shared_files = {Data/resultados_ieee.bib, Data/resultados_ieee.bib}
}

@article{ref_dup1446,
  title = {Series Foreword}},
  author = {Warren Sack}},
  year = {2019}},
  journal = {The Software Arts}},
  tipo = {Book Chapter}},
  publisher = {MIT Press}},
  abstract = {Software is deeply woven into contemporary life—economically, culturally, creatively, politically—in manners both obvious and nearly invisible. Yet while much is written about how software is used, and the activities that it supports and shapes, thinking about software itself has remained largely technical for much of its history. Increasingly, however, artists, scientists, engineers, hacker...Show More}},
  url = {https://ieeexplore.ieee.org/document/8675833/},
  shared_files = {Data/resultados_ieee.bib, Data/resultados_ieee.bib, Data/resultados_ieee.bib}
}

@article{ref_dup1447,
  title = {Index}},
  author = {6G Visions for a Sustainable and People-centric Future: From Communications to Services, the CONASENSE Perspective Year: 2023 | Book Chapter | Publisher: River Publishers}},
  year = {2023}},
  journal = {6G Visions for a Sustainable and People-centric Future: From Communications to Services, the CONASENSE Perspective}},
  tipo = {Book Chapter}},
  publisher = {River Publishers}},
  abstract = {6G is currently under definition, often being addressed from a plain telecommunications perspective as an evolutionary paradigm that represents an extension of 5G. Having as its horizon 2030, 6G initiatives are being deployed across the globe, to further ignite the development of 6G services. In its philosophy core, 6G embodies the “human in the loop” principle. The research effort a...Show More}},
  url = {https://ieeexplore.ieee.org/document/10183907/},
  shared_files = {Data/resultados_ieee.bib, Data/resultados_ieee.bib}
}

@article{ref_dup1448,
  title = {About the Editors}},
  author = {6G Visions for a Sustainable and People-centric Future: From Communications to Services, the CONASENSE Perspective Year: 2023 | Book Chapter | Publisher: River Publishers}},
  year = {2023}},
  journal = {6G Visions for a Sustainable and People-centric Future: From Communications to Services, the CONASENSE Perspective}},
  tipo = {Book Chapter}},
  publisher = {River Publishers}},
  abstract = {6G is currently under definition, often being addressed from a plain telecommunications perspective as an evolutionary paradigm that represents an extension of 5G. Having as its horizon 2030, 6G initiatives are being deployed across the globe, to further ignite the development of 6G services. In its philosophy core, 6G embodies the “human in the loop” principle. The research effort a...Show More}},
  url = {https://ieeexplore.ieee.org/document/10183903/},
  shared_files = {Data/resultados_ieee.bib, Data/resultados_ieee.bib}
}

@article{ref_dup1449,
  title = {Direct and indirect instruction in educational robotics: a comparative study of task performance per cognitive level and student perception}},
  author = {Branko Anđić, Mirjana Maričić, Filiz Mumcu, Theodosia Prodromou, Janika Leoste, Musa Saimon and Zsolt Lavicza}},
  year = {2024}},
  journal = {Smart Learning Environments}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Educational Robotics (ER) has emerged as one of the tools to improve STEM learning in primary education if students are properly instructed. However, there is a lack of studies that guide teachers on which typ...}},
  url = {https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-024-00298-6},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1450,
  title = {A data value metric for quantifying information content and utility}},
  author = {Morteza Noshad, Jerome Choi, Yuming Sun, Alfred Hero III and Ivo D. Dinov}},
  year = {2021}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Data-driven innovation is propelled by recent scientific advances, rapid technological progress, substantial reductions of manufacturing costs, and significant demands for effective decision support systems. T...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00446-6},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1451,
  title = {Optimal instance subset selection from big data using genetic algorithm and open source framework}},
  author = {Junhai Zhai and Dandan Song}},
  year = {2022}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Data is accumulating at an incredible rate, and the era of big data has arrived. Big data brings great challenges to traditional machine learning algorithms, it is difficult for learning tasks in big data scen...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00640-0},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1452,
  title = {The effect of embedded structures on cognitive load for novice learners during block-based code comprehension}},
  author = {Xiaoxin Hao, Zhiyi Xu, Mingyue Guo, Yuzheng Hu and Fengji Geng}},
  year = {2023}},
  journal = {International Journal of STEM Education}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Coding has become an integral part of STEM education. However, novice learners face difficulties in processing codes within embedded structures (also termed nested structures). This study aimed to investigate ...}},
  url = {https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-023-00432-9},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1453,
  title = {Introducing Mplots: scaling time series recurrence plots to massive datasets}},
  author = {Maryam Shahcheraghi, Ryan Mercer, João Manuel de Almeida Rodrigues, Audrey Der, Hugo Filipe Silveira Gamboa, Zachary Zimmerman, Kerry Mauck and Eamonn Keogh}},
  year = {2024}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Time series similarity matrices (informally, recurrence plots or dot-plots), are useful tools for time series data mining. They can be used to guide data exploration, and various useful features can be derived...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00954-1},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1454,
  title = {A chemical-reaction-optimization-based neuro-fuzzy hybrid network for stock closing price prediction}},
  author = {Sarat Chandra Nayak and Bijan Bihari Misra}},
  year = {2019}},
  journal = {Financial Innovation}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Accurate prediction of stock market behavior is a challenging issue for financial forecasting. Artificial neural networks, such as multilayer perceptron have been established as better approximation and classi...}},
  url = {https://www.springeropen.com//jfin-swufe.springeropen.com/articles/10.1186/s40854-019-0153-1},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1455,
  title = {Learning analytics in virtual laboratories: a systematic literature review of empirical research}},
  author = {Ramy Elmoazen, Mohammed Saqr, Mohammad Khalil and Barbara Wasson}},
  year = {2023}},
  journal = {Smart Learning Environments}},
  tipo = {Review}},
  publisher = {Unknown}},
  abstract = {Remote learning has advanced from the theoretical to the practical sciences with the advent of virtual labs. Although virtual labs allow students to conduct their experiments remotely, it is a challenge to eva...}},
  url = {https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-023-00244-y},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1456,
  title = {Beyond the horizon: immersive developments for animal ecology research}},
  author = {Ying Zhang, Karsten Klein, Falk Schreiber and Kamran Safi}},
  year = {2023}},
  journal = {Visual Computing for Industry, Biomedicine, and Art}},
  tipo = {Perspective}},
  publisher = {Unknown}},
  abstract = {More diverse data on animal ecology are now available. This “data deluge” presents challenges for both biologists and computer scientists; however, it also creates opportunities to improve analysis and answer ...}},
  url = {https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-023-00138-3},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1457,
  title = {Crowd cloud routing protocol based on opportunistic computing for wireless sensor networks}},
  author = {Shengli Mao}},
  year = {2016}},
  journal = {EURASIP Journal on Embedded Systems}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {We proposed a crowd cloud routing protocol based on opportunistic computing to improve the data transmission efficiency, reliability, and reduce routing overhead in wireless sensor networks. Based on the analy...}},
  url = {https://www.springeropen.com//jes-eurasipjournals.springeropen.com/articles/10.1186/s13639-016-0063-5},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1458,
  title = {A survey of methods supporting cyber situational awareness in the context of smart cities}},
  author = {Nataliia Neshenko, Christelle Nader, Elias Bou-Harb and Borko Furht}},
  year = {2020}},
  journal = {Journal of Big Data}},
  tipo = {Survey Paper}},
  publisher = {Unknown}},
  abstract = {A modern urban infrastructure no longer operates in isolation, but instead, leverages the latest technologies to collect, process, and distribute aggregated knowledge in order to improve the quality of the pro...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00363-0},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1459,
  title = {HyGraph: a subgraph isomorphism algorithm for efficiently querying big graph databases}},
  author = {Merve Asiler, Adnan Yazıcı and Roy George}},
  year = {2022}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {The big graph database provides strong modeling capabilities and efficient querying for complex applications. Subgraph isomorphism which finds exact matches of a query graph in the database efficiently, is a c...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00589-0},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1460,
  title = {A systematic review of the prediction of consumer preference using EEG measures and machine-learning in neuromarketing research}},
  author = {Adam Byrne, Emma Bonfiglio, Colin Rigby and Nicky Edelstyn}},
  year = {2022}},
  journal = {Brain Informatics}},
  tipo = {Review}},
  publisher = {Unknown}},
  abstract = {The present paper discusses the findings of a systematic review of EEG measures in neuromarketing, identifying which EEG measures are the most robust predictor of customer preference in neuromarketing. The rev...}},
  url = {https://www.springeropen.com//braininformatics.springeropen.com/articles/10.1186/s40708-022-00175-3},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1461,
  title = {Molecular docking assisted exploration on solubilization of poorly soluble drug remdesivir in sulfobutyl ether-tycyclodextrin}},
  author = {Yumeng Zhang, Zhouming Zhao, Kai Wang, Kangjie Lyu, Cai Yao, Lin Li, Xia Shen, Tengfei Liu, Xiaodi Guo, Haiyan Li, Wenshou Wang and Tsai-Ta Lai}},
  year = {2022}},
  journal = {AAPS Open}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {To study structure-specific solubilization effect of Sulfobutyl ether-β-cyclodextrin (SBE-β-CD) on Remdesivir (RDV) and to understand the experimental clathration with the aid of quantum mechanics (QM), molecu...}},
  url = {https://www.springeropen.com//aapsopen.springeropen.com/articles/10.1186/s41120-022-00054-5},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1462,
  title = {Cloud-based parallel power flow calculation using resilient distributed datasets and directed acyclic graph}},
  author = {Dewen WANG, Fangfang ZHOU and Jiangman LI}},
  year = {2018}},
  journal = {Journal of Modern Power Systems and Clean Energy}},
  tipo = {OriginalPaper}},
  publisher = {Unknown}},
  abstract = {With the integration of distributed generation and the construction of cross-regional long-distance power grids, power systems become larger and more complex. They require faster computing speed and better sca...}},
  url = {https://link.springer.com/article/10.1007/s40565-018-0406-4},
  shared_files = {Data/resultados_ieee.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1463,
  title = {Spoofing keystroke dynamics authentication through synthetic typing pattern extracted from screen-recorded video}},
  author = {Chrisando Ryan Pardomuan Siahaan and Andry Chowanda}},
  year = {2022}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {As the inter-connectivity in cyberspace continues to increase exponentially, privacy and security have become two of the most concerning issues that need to be tackled in today’s state of technology. Therefore...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00662-8},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1464,
  title = {Machine learning based customer churn prediction in home appliance rental business}},
  author = {Youngjung Suh}},
  year = {2023}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Customer churn is a major issue for large enterprises. In particular, in the rental business sector, companies are looking for ways to retain their customers because they are their main source of revenue. The ...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00721-8},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1465,
  title = {Mechanisms of Intermittent State Transitions in a Coupled Heterogeneous Oscillator Model of Epilepsy}},
  author = {Marc Goodfellow and Paul Glendinning}},
  year = {2013}},
  journal = {The Journal of Mathematical Neuroscience}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {We investigate the dynamic mechanisms underlying intermittent state transitions in a recently proposed neural mass model of epilepsy. A low dimensional model is constructed, which preserves two key features of...}},
  url = {https://www.springeropen.com//mathematical-neuroscience.springeropen.com/articles/10.1186/2190-8567-3-17},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1466,
  title = {Recent trends in miRNA therapeutics and the application of plant miRNA for prevention and treatment of human diseases}},
  author = {Atiyabanu N. Saiyed, Abhay R. Vasavada and S. R. Kaid Johar}},
  year = {2022}},
  journal = {Future Journal of Pharmaceutical Sciences}},
  tipo = {Review}},
  publisher = {Unknown}},
  abstract = {Researchers now have a new avenue to investigate when it comes to miRNA-based therapeutics. miRNAs have the potential to be valuable biomarkers for disease detection. Variations in miRNA levels may be able to ...}},
  url = {https://www.springeropen.com//fjps.springeropen.com/articles/10.1186/s43094-022-00413-9},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1467,
  title = {Investigating coordinated account creation using burst detection and network analysis}},
  author = {Daniele Bellutta and Kathleen M. Carley}},
  year = {2023}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Democracies around the world face the threat of manipulation of their electorates via coordinated online influence campaigns. Researchers have responded by developing valuable methods for finding automated acc...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00695-7},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1468,
  title = {Interactions of multiple rhythms in a biophysical network of neurons}},
  author = {Alexandros Gelastopoulos and Nancy J. Kopell}},
  year = {2020}},
  journal = {The Journal of Mathematical Neuroscience}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Neural oscillations, including rhythms in the beta1 band (12–20 Hz), are important in various cognitive functions. Often neural networks receive rhythmic input at frequencies different from their natural frequ...}},
  url = {https://www.springeropen.com//mathematical-neuroscience.springeropen.com/articles/10.1186/s13408-020-00096-7},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1469,
  title = {Evaluating embodied conversational agents in multimodal interfaces}},
  author = {Benjamin Weiss, Ina Wechsung, Christine Kühnel and Sebastian Möller}},
  year = {2015}},
  journal = {Computational Cognitive Science}},
  tipo = {Review}},
  publisher = {Unknown}},
  abstract = {Based on cross-disciplinary approaches to Embodied Conversational Agents, evaluation methods for such human-computer interfaces are structured and presented. An introductory systematisation of evaluation topic...}},
  url = {https://www.springeropen.com//computationalcognitivescience.springeropen.com/articles/10.1186/s40469-015-0006-9},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1470,
  title = {Wildfire management decisions outweigh mechanical treatment as the keystone to forest landscape adaptation}},
  author = {Tucker J. Furniss, Nicholas Povak, Paul F. Hessburg, R. Brion Salter, Zhuoran Duan and Mark Wigmosta}},
  year = {2024}},
  journal = {Fire Ecology}},
  tipo = {Original research}},
  publisher = {Unknown}},
  abstract = {Modern land management faces unprecedented uncertainty regarding future climates, novel disturbance regimes, and unanticipated ecological feedbacks. Mitigating this uncertainty requires a cohesive landscape ma...}},
  url = {https://www.springeropen.com//fireecology.springeropen.com/articles/10.1186/s42408-024-00339-y},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1471,
  title = {Distribution network design under demand uncertainty using genetic algorithm and Monte Carlo simulation approach: a case study in pharmaceutical industry}},
  author = {Arman Izadi and Ali Mohammad Kimiagari}},
  year = {2014}},
  journal = {Journal of Industrial Engineering International}},
  tipo = {Original research}},
  publisher = {Unknown}},
  abstract = {Distribution network design as a strategic decision has long-term effect on tactical and operational supply chain management. In this research, the location–allocation problem is studied under demand uncertain...}},
  url = {https://link.springer.com/article/10.1007/s40092-014-0050-1},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1472,
  title = {d2o: a distributed data object for parallel high-performance computing in Python}},
  author = {Theo Steininger, Maksim Greiner, Frederik Beaujean and Torsten Enßlin}},
  year = {2016}},
  journal = {Journal of Big Data}},
  tipo = {Methodology}},
  publisher = {Unknown}},
  abstract = {We introduce d2o, a Python module for cluster-distributed multi-dimensional numerical arrays. It acts as a layer of abstraction between the algorithm code and the data-distribution logic. The main goal is to achi...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-016-0052-5},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1473,
  title = {Computer aided technology based on graph sample and aggregate attention network optimized for soccer teaching and training}},
  author = {Guanghui Yang and Xinyuan Feng}},
  year = {2024}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Football is the most popular game in the world and has significant influence on various aspects including politics, economy and culture. The experience of the football developed nation has shown that the stead...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00893-x},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1474,
  title = {Defining user spectra to classify Ethereum users based on their behavior}},
  author = {Gianluca Bonifazi, Enrico Corradini, Domenico Ursino and Luca Virgili}},
  year = {2022}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {In this paper, we define the concept of user spectrum and adopt it to classify Ethereum users based on their behavior.}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00586-3},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1475,
  title = {Digital social innovation based on Big Data Analytics for health and well-being of society}},
  author = {Kornelia Batko}},
  year = {2023}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Any nation's health policy aims to properly care for its citizens and the society's quality of life. Since in the healthcare system, the population's health is the essential component of national wealth, healt...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00846-w},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1476,
  title = {Mathematical Frameworks for Oscillatory Network Dynamics in Neuroscience}},
  author = {Peter Ashwin, Stephen Coombes and Rachel Nicks}},
  year = {2016}},
  journal = {The Journal of Mathematical Neuroscience}},
  tipo = {Review}},
  publisher = {Unknown}},
  abstract = {The tools of weakly coupled phase oscillator theory have had a profound impact on the neuroscience community, providing insight into a variety of network behaviours ranging from central pattern generation to s...}},
  url = {https://www.springeropen.com//mathematical-neuroscience.springeropen.com/articles/10.1186/s13408-015-0033-6},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1477,
  title = {Enriching feature engineering for short text samples by language time series analysis}},
  author = {Yichen Tang, Kelly Blincoe and Andreas W. Kempa-Liehr}},
  year = {2020}},
  journal = {EPJ Data Science}},
  tipo = {Regular article}},
  publisher = {Unknown}},
  abstract = {In this case study, we are extending feature engineering approaches for short text samples by integrating techniques which have been introduced in the context of time series classification and signal processin...}},
  url = {https://www.springeropen.com//epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-020-00244-9},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1478,
  title = {Time-Predictable Computer Architecture}},
  author = {Martin Schoeberl}},
  year = {2009}},
  journal = {EURASIP Journal on Embedded Systems}},
  tipo = {Research Article}},
  publisher = {Unknown}},
  abstract = {Today's general-purpose processors are optimized for maximum throughput. Real-time systems need a processor with both a reasonable and a known worst-case execution time (WCET). Features such as pipelines with ...}},
  url = {https://www.springeropen.com//jes-eurasipjournals.springeropen.com/articles/10.1155/2009/758480},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1479,
  title = {Cores of Cooperative Games in Information Theory}},
  author = {Mokshay Madiman}},
  year = {2008}},
  journal = {EURASIP Journal on Wireless Communications and Networking}},
  tipo = {Research Article}},
  publisher = {Unknown}},
  abstract = {Cores of cooperative games are ubiquitous in information theory and arise most frequently in the characterization of fundamental limits in various scenarios involving multiple users. Examples include classical...}},
  url = {https://www.springeropen.com//jwcn-eurasipjournals.springeropen.com/articles/10.1155/2008/318704},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1480,
  title = {A comparative analysis of design and analysis methods for steel connections: contrasting American and European perspectives}},
  author = {Muhammad Tayyab Naqash, Ayed Eid Alluqmani and Qazi Umar Farooq}},
  year = {2023}},
  journal = {Journal of Umm Al-Qura University for Engineering and Architecture}},
  tipo = {Original Article}},
  publisher = {Unknown}},
  abstract = {Load transmission from one element to another is achieved using steel connections, making them an integral part of any structural design. This article examines the differences between the American Codes (AISC)...}},
  url = {https://link.springer.com/article/10.1007/s43995-023-00037-x},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1481,
  title = {Bayesian reference analysis for exponential power regression models}},
  author = {Marco AR Ferreira and Esther Salazar}},
  year = {2014}},
  journal = {Journal of Statistical Distributions and Applications}},
  tipo = {Methodology}},
  publisher = {Unknown}},
  abstract = {We develop Bayesian reference analyses for linear regression models when the errors follow an exponential power distribution. Specifically, we obtain explicit expressions for reference priors for all the six p...}},
  url = {https://www.springeropen.com//jsdajournal.springeropen.com/articles/10.1186/2195-5832-1-12},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1482,
  title = {Effects of water domains on seismic wavefields: A simulation case study at Taal volcano, Philippines}},
  author = {Yuta Maeda and Hiroyuki Kumagai}},
  year = {2013}},
  journal = {Earth, Planets and Space}},
  tipo = {Article}},
  publisher = {Unknown}},
  abstract = {We examined the effects of water domains on seismic wavefields at the Taal volcano, Philippines, where there is a summit crater lake on an active volcanic island surrounded by a caldera lake. We conducted forw...}},
  url = {https://www.springeropen.com//earth-planets-space.springeropen.com/articles/10.5047/eps.2012.07.004},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1483,
  title = {Optimal control analysis in a reaction-diffusion SIRC model with cross-immune class}},
  author = {Pan Zhou, Jianpeng Wang, Zhidong Teng, Yanling Zheng and Kai Wang}},
  year = {2024}},
  journal = {Advances in Continuous and Discrete Models}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {In this paper, we propose a reaction-diffusion SIRC model with cross-immunization. The model includes the implementation of vaccination measures for susceptible, and treatment and quarantine measures for infec...}},
  url = {https://www.springeropen.com//advancesincontinuousanddiscretemodels.springeropen.com/articles/10.1186/s13662-024-03850-3},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1484,
  title = {Bibliometric review of visual computing in the construction industry}},
  author = {Heng-Wei Wang, Zhen-Zhong Hu and Jia-Rui Lin}},
  year = {2020}},
  journal = {Visual Computing for Industry, Biomedicine, and Art}},
  tipo = {Review}},
  publisher = {Unknown}},
  abstract = {In the construction area, visuals such as drawings, photos, videos, and 3D models, play a significant role in the design, build and maintenance of a facility, bringing efficiency to generate, transfer, and sto...}},
  url = {https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-020-00050-0},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1485,
  title = {Is data envelopment analysis a suitable tool for performance measurement and benchmarking in non-production contexts?}},
  author = {Victoria Wojcik, Harald Dyckhoff and Marcel Clermont}},
  year = {2018}},
  journal = {Business Research}},
  tipo = {Original Research}},
  publisher = {Unknown}},
  abstract = {After 40 years of research with thousands of application-oriented scientific papers, empirical evidence that data envelopment analysis (DEA) has really improved the practice of performance measurement and benc...}},
  url = {https://link.springer.com/article/10.1007/s40685-018-0077-z},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1486,
  title = {Mimicking biological synapses with a-HfSiOx-based memristor: implications for artificial intelligence and memory applications}},
  author = {Muhammad Ismail, Maria Rasheed, Chandreswar Mahata, Myounggon Kang and Sungjun Kim}},
  year = {2023}},
  journal = {Nano Convergence}},
  tipo = {Full Paper}},
  publisher = {Unknown}},
  abstract = {Memristors, owing to their uncomplicated structure and resemblance to biological synapses, are predicted to see increased usage in the domain of artificial intelligence. Additionally, to augment the capacity f...}},
  url = {https://www.springeropen.com//nanoconvergencejournal.springeropen.com/articles/10.1186/s40580-023-00380-8},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1487,
  title = {Chain and silk: alternative futures of blockchain governance in Kyrgyzstan}},
  author = {Saikal Anvar kyzy, Gregory J. Dunn and John A. Sweeney}},
  year = {2022}},
  journal = {European Journal of Futures Research}},
  tipo = {Research Article}},
  publisher = {Unknown}},
  abstract = {Few technologies have been mired in hype more than blockchain, which is the underlying peer-to-peer network protocol for cryptocurrencies such as Bitcoin. Given the technology’s emphasis on purported “immutabl...}},
  url = {https://www.springeropen.com//eujournalfuturesresearch.springeropen.com/articles/10.1186/s40309-022-00192-9},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1488,
  title = {Integrative radiogenomics for virtual biopsy and treatment monitoring in ovarian cancer}},
  author = {Paula Martin-Gonzalez, Mireia Crispin-Ortuzar, Leonardo Rundo, Maria Delgado-Ortet, Marika Reinius, Lucian Beer, Ramona Woitek, Stephan Ursprung, Helen Addley, James D. Brenton, Florian Markowetz and Evis Sala}},
  year = {2020}},
  journal = {Insights into Imaging}},
  tipo = {Critical Review}},
  publisher = {Unknown}},
  abstract = {Ovarian cancer survival rates have not changed in the last 20 years. The majority of cases are High-grade serous ovarian carcinomas (HGSOCs), which are typically diagnosed at an advanced stage with multiple me...}},
  url = {https://www.springeropen.com//insightsimaging.springeropen.com/articles/10.1186/s13244-020-00895-2},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1489,
  title = {Unveiling the evolution of generative AI (GAI): a comprehensive and investigative analysis toward LLM models (2021–2024) and beyond}},
  author = {Zarif Bin Akhtar}},
  year = {2024}},
  journal = {Journal of Electrical Systems and Information Technology}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {This comprehensive exploration of recent breakthroughs in artificial intelligence (AI) traversed the realms of language models, computer vision, and generative models, unraveling the intricacies of cutting-edg...}},
  url = {https://www.springeropen.com//jesit.springeropen.com/articles/10.1186/s43067-024-00145-1},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1490,
  title = {A narrative perspective of island detection methods under the lens of cyber-attack in data-driven smart grid}},
  author = {Apoorva Shukla, Soham Dutta, Sourav Kumar Sahu and Pradip Kumar Sadhu}},
  year = {2023}},
  journal = {Journal of Electrical Systems and Information Technology}},
  tipo = {Review}},
  publisher = {Unknown}},
  abstract = {As criminals and hackers are developing new methods to interfere with the operation of the power grid, the nature of grid vulnerabilities and threats is continuously evolving. The growing interest in transitio...}},
  url = {https://www.springeropen.com//jesit.springeropen.com/articles/10.1186/s43067-023-00083-4},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1491,
  title = {Modeling temporal aspects of sensor data for MongoDB NoSQL database}},
  author = {Nadeem Qaisar Mehmood, Rosario Culmone and Leonardo Mostarda}},
  year = {2017}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Proliferation of structural, semi-structural and no-structural data, has challenged the scalability, flexibility and processability of the traditional relational database management systems (RDBMS). The next g...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-017-0068-5},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1492,
  title = {The state of metaverse research: a bibliometric visual analysis based on CiteSpace}},
  author = {Huike Li and Bo Li}},
  year = {2024}},
  journal = {Journal of Big Data}},
  tipo = {Brief Report}},
  publisher = {Unknown}},
  abstract = {To understand the current status of research in the field of metaverse, and to analyze the research progress and evolutionary trends in this field.}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-024-00877-x},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1493,
  title = {High-resolution DOA estimation with Meridian prior}},
  author = {Guanghui Zhao, Jie Lin, Fangfang Shen, Guangming Shi, Qingyu Hou and Zicheng Liu}},
  year = {2013}},
  journal = {EURASIP Journal on Advances in Signal Processing}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Based on the assumption that only a few point sources exist in the spatial spectrum, the direction-of-arrival (DOA) estimation problem can be formulated as a problem of sparse representation of signal with res...}},
  url = {https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1186/1687-6180-2013-91},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1494,
  title = {Advanced recurrent network-based hybrid acoustic models for low resource speech recognition}},
  author = {Jian Kang, Wei-Qiang Zhang, Wei-Wei Liu, Jia Liu and Michael T. Johnson}},
  year = {2018}},
  journal = {EURASIP Journal on Audio, Speech, and Music Processing}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Recurrent neural networks (RNNs) have shown an ability to model temporal dependencies. However, the problem of exploding or vanishing gradients has limited their application. In recent years, long short-term m...}},
  url = {https://www.springeropen.com//asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-018-0128-6},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1495,
  title = {Spectral methods for imputation of missing air quality data}},
  author = {Shai Moshenberg, Uri Lerner and Barak Fishbain}},
  year = {2015}},
  journal = {Environmental Systems Research}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Air quality is well recognized as a contributing factor for various physical phenomena and as a public health risk factor. Consequently, there is a need for an accurate way to measure the level of exposure to...}},
  url = {https://www.springeropen.com//environmentalsystemsresearch.springeropen.com/articles/10.1186/s40068-015-0052-z},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1496,
  title = {A method for evaluating discoverability and navigability of recommendation algorithms}},
  author = {Daniel Lamprecht, Markus Strohmaier and Denis Helic}},
  year = {2017}},
  journal = {Computational Social Networks}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Recommendations are increasingly used to support and enable discovery, browsing, and exploration of items. This is especially true for entertainment platforms such as Netflix or YouTube, where frequently, no c...}},
  url = {https://www.springeropen.com//computationalsocialnetworks.springeropen.com/articles/10.1186/s40649-017-0045-3},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1497,
  title = {Network robustness improvement via long-range links}},
  author = {Vincenza Carchiolo, Marco Grassia, Alessandro Longheu, Michele Malgeri and Giuseppe Mangioni}},
  year = {2019}},
  journal = {Computational Social Networks}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Many systems are today modelled as complex networks, since this representation has been proven being an effective approach for understanding and controlling many real-world phenomena. A significant area of int...}},
  url = {https://www.springeropen.com//computationalsocialnetworks.springeropen.com/articles/10.1186/s40649-019-0073-2},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1498,
  title = {Beyond just “flattening the curve”: Optimal control of epidemics with purely non-pharmaceutical interventions}},
  author = {Markus Kantner and Thomas Koprucki}},
  year = {2020}},
  journal = {Journal of Mathematics in Industry}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {When effective medical treatment and vaccination are not available, non-pharmaceutical interventions such as social distancing, home quarantine and far-reaching shutdown of public life are the only available s...}},
  url = {https://www.springeropen.com//mathematicsinindustry.springeropen.com/articles/10.1186/s13362-020-00091-3},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1499,
  title = {Predictability analysis of the Pound’s Brexit exchange rates based on Google Trends data}},
  author = {Amaryllis Mavragani, Konstantinos Gkillas and Konstantinos P. Tsagarakis}},
  year = {2020}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {During the last decade, the use of online search traffic data is becoming popular in examining, analyzing, and predicting human behavior, with Google Trends being a popular tool in monitoring and analyzing the...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00337-2},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1500,
  title = {Move-it: a paperclip-shaped sensing and actuating system for sticky notes}},
  author = {Kentaro Yasu, Kathrin Probst, Maki Sugimoto, Michael Haller and Masahiko Inami}},
  year = {2014}},
  journal = {ROBOMECH Journal}},
  tipo = {Research Article}},
  publisher = {Unknown}},
  abstract = {We developed a paperclip-shaped motion-actuating system, named “Move-it”. With this system, users can add motions to sticky notes just by clipping it. This system is constructed by using clip-shaped devices th...}},
  url = {https://www.springeropen.com//robomechjournal.springeropen.com/articles/10.1186/s40648-014-0012-9},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1501,
  title = {On the qualitative evaluation of the variable-order coupled boundary value problems with a fractional delay}},
  author = {Hasanen A. Hammad, Hassen Aydi and Mohra Zayed}},
  year = {2023}},
  journal = {Journal of Inequalities and Applications}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {The logical progression from the constant order differential equations is the field of variable-order differential equations. Such equations can frequently give a more succinct description of problems in the r...}},
  url = {https://www.springeropen.com//journalofinequalitiesandapplications.springeropen.com/articles/10.1186/s13660-023-03018-9},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1502,
  title = {A SafeML extension for a unified risk assessment to diverse service robots}},
  author = {Takao Miyoshi, Yoshihiro Nakabo, Hidetoshi Fukui, Makoto Yashiro, Iko Miyazawa, Takeshi Sakamoto, Noriaki Ando, Toru Kuga, Atsushi Kitamura, Kenichi Ohara and Tetsuya Kimura}},
  year = {2023}},
  journal = {ROBOMECH Journal}},
  tipo = {Research Article}},
  publisher = {Unknown}},
  abstract = {Risk assessment is one of the important processes in the social implementation of robots. In risk assessment and safety design of systems with various stakeholders, modeling and visualization of related elemen...}},
  url = {https://www.springeropen.com//robomechjournal.springeropen.com/articles/10.1186/s40648-023-00245-z},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1503,
  title = {Manually operated microtube automatic capper/decapper system for clinical laboratory and biological laboratory personnel}},
  author = {Makoto Jinno, Ryosuke Nonoyama, Yasuteru Sakurai, Rokusuke Yoshikawa, Takaaki Kinoshita and Jiro Yasuda}},
  year = {2024}},
  journal = {ROBOMECH Journal}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Polymerase chain reaction (PCR) is an effective method for diagnosing infectious diseases and has been the primary method throughout the novel coronavirus disease (COVID-19) pandemic. PCR tests (from specimen ...}},
  url = {https://www.springeropen.com//robomechjournal.springeropen.com/articles/10.1186/s40648-024-00281-3},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1504,
  title = {Free trade as domestic, economic, and strategic issues: a big data analytics approach}},
  author = {Moch Faisal Karim, Reza Rahutomo, Ida Bagus Kerthyayana Manuaba, Kartika Purwandari, Tirta Nugraha Mursitama and Bens Pardamean}},
  year = {2023}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {This article examines the engagement of domestic actors in public conversation surrounding free trade negotiations with a focus on the framing of these negotiations as economic, strategic or domestic issues. T...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00722-7},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1505,
  title = {A new framework to assess the impact of new IT-based technologies on the success of quality management system}},
  author = {Yiying Cao and Farah Qasim Ahmed Alyousuf}},
  year = {2025}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Today, the ever-increasing growth of technology and data has undeniably affected various industries. Due to the importance of the factor of comprehensive quality management (QM), big data, and information tech...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-025-01061-5},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1506,
  title = {Clus-DTI: improving decision-tree classification with a clustering-based decision-tree induction algorithm}},
  author = {Rodrigo C. Barros, Márcio P. Basgalupp, André C. P. L. F. de Carvalho and Marcos G. Quiles}},
  year = {2012}},
  journal = {Journal of the Brazilian Computer Society}},
  tipo = {Original Paper}},
  publisher = {Unknown}},
  abstract = {Decision-tree induction is a well-known technique for assigning objects to categories in a white-box fashion. Most decision-tree induction algorithms rely on a sub-optimal greedy top-down recursive strategy fo...}},
  url = {https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/s13173-012-0075-5},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1507,
  title = {Cognition-based variable admittance control for active compliance in flexible manipulation of heavy objects with a power-assist robotic system}},
  author = {S. M. Mizanoor Rahman and Ryojun Ikeura}},
  year = {2018}},
  journal = {Robotics and Biomimetics}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {In the first step, a 1-DOF power-assist robotic system (PARS) is developed for lifting lightweight objects. Dynamics for human–robot co-manipulation of objects is derived that considers human cognition (weight...}},
  url = {https://www.springeropen.com//jrobio.springeropen.com/articles/10.1186/s40638-018-0090-x},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1508,
  title = {Approximate equiangular tight frames for compressed sensing and CDMA applications}},
  author = {Evaggelia Tsiligianni, Lisimachos P. Kondi and Aggelos K. Katsaggelos}},
  year = {2017}},
  journal = {EURASIP Journal on Advances in Signal Processing}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Performance guarantees for recovery algorithms employed in sparse representations, and compressed sensing highlights the importance of incoherence. Optimal bounds of incoherence are attained by equiangular uni...}},
  url = {https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-017-0501-0},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1509,
  title = {Development and usability testing of a Web-based decision aid for families of patients receiving prolonged mechanical ventilation}},
  author = {Christopher E Cox, Nicholas G Wysham, Brenda Walton, Derek Jones, Brian Cass, Maria Tobin, Mattias Jonsson, Jeremy M Kahn, Douglas B White, Catherine L Hough, Carmen L Lewis and Shannon S Carson}},
  year = {2015}},
  journal = {Annals of Intensive Care}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Web-based decision aids are increasingly important in medical research and clinical care. However, few have been studied in an intensive care unit setting. The objectives of this study were to develop a Web-ba...}},
  url = {https://www.springeropen.com//annalsofintensivecare.springeropen.com/articles/10.1186/s13613-015-0045-0},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1510,
  title = {Does narrative drive dynamic attention to a prolonged stimulus?}},
  author = {Stephen J. Hinde, Tim J. Smith and Iain D. Gilchrist}},
  year = {2018}},
  journal = {Cognitive Research: Principles and Implications}},
  tipo = {Original article}},
  publisher = {Unknown}},
  abstract = {Attention in the “real world” fluctuates over time, but these fluctuations are hard to examine using a timed trial-based experimental paradigm. Here we use film to study attention. To achieve short-term engage...}},
  url = {https://www.springeropen.com//cognitiveresearchjournal.springeropen.com/articles/10.1186/s41235-018-0140-5},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1511,
  title = {Orthogonality is superiority in piecewise-polynomial signal segmentation and denoising}},
  author = {Michaela Novosadová, Pavel Rajmic and Michal Šorel}},
  year = {2019}},
  journal = {EURASIP Journal on Advances in Signal Processing}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Segmentation and denoising of signals often rely on the polynomial model which assumes that every segment is a polynomial of a certain degree and that the segments are modeled independently of each other. Segm...}},
  url = {https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-018-0598-9},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1512,
  title = {Assessing the economic and environmental impacts of urban food systems for public school canteens: case study of Great Lyon region}},
  author = {Laura Palacios-Argüello, Jesus Gonzalez-Feliu, Natacha Gondran and Fabien Badeig}},
  year = {2018}},
  journal = {European Transport Research Review}},
  tipo = {Original Paper}},
  publisher = {Unknown}},
  abstract = {Urban logistics is a subject that interests both city planners and researchers. Although many works are found in non-food distribution, food-based logistics is less studied in an urban context, and sustainabil...}},
  url = {https://www.springeropen.com//etrr.springeropen.com/articles/10.1186/s12544-018-0306-8},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1513,
  title = {Chewing-gum stimulation did not reduce the absorbed dose to salivary glands during radioiodine treatment of thyroid cancer as inferred from pre-therapy 124I PET/CT imaging}},
  author = {Walter Jentzen, Marion Richter, James Nagarajah, Thorsten Dirk Poeppel, Wolfgang Brandau, Colin Dawes, Andreas Bockisch and Ina Binse}},
  year = {2014}},
  journal = {EJNMMI Physics}},
  tipo = {Original research}},
  publisher = {Unknown}},
  abstract = {The goal of this prospective study was to estimate the absorbed (radiation) doses to salivary glands in radioiodine therapy of thyroid cancer under chewing-gum stimulation using 124I positron emission tomography ...}},
  url = {https://www.springeropen.com//ejnmmiphys.springeropen.com/articles/10.1186/s40658-014-0100-1},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1514,
  title = {Genomics of model organisms}},
  author = {},
  year = {2009}},
  journal = {Genomic Medicine}},
  tipo = {Abstracts}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://www.springeropen.com//thehugojournal.springeropen.com/articles/10.1007/s11568-009-9094-5},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1515,
  title = {Developing insights from the collective voice of target users in Twitter}},
  author = {Kang-Pyo Lee and Suyong Song}},
  year = {2022}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {This study develops a pragmatic scheme that facilitates insight development from the collective voice of target users in Twitter, which has not been considered in the existing literature. While relying on a wi...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00611-5},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1516,
  title = {Detection and prevention of SQLI attacks and developing compressive framework using machine learning and hybrid techniques}},
  author = {Wubetu Barud Demilie and Fitsum Gizachew Deriba}},
  year = {2022}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {A web application is a software system that provides an interface to its users through a web browser on any operating system (OS). Despite their growing popularity, web application security threats have become...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00678-0},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1517,
  title = {Time series modeling of road traffic accidents in Amhara Region}},
  author = {Kidane Alemtsega Getahun}},
  year = {2021}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Road traffic accidents (RTA) are commonly encountered incidents that can cause injuries, death, and property damage to members of society. Ethiopia is one of the highest incident rates of road traffic accident...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00493-z},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1518,
  title = {Accuracy improvements for cold-start recommendation problem using indirect relations in social networks}},
  author = {Fu Jie Tey, Tin-Yu Wu, Chiao-Ling Lin and Jiann-Liang Chen}},
  year = {2021}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Recent advances in Internet applications have facilitated information spreading and, thanks to a wide variety of mobile devices and the burgeoning 5G networks, users easily and quickly gain access to informati...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00484-0},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1519,
  title = {The impact of ensemble learning on surgical tools classification during laparoscopic cholecystectomy}},
  author = {Jaafar Jaafari, Samira Douzi, Khadija Douzi and Badr Hssina}},
  year = {2022}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Laparoscopic surgery also know as minimally invasive surgery (MIS), is a type of surgical procedure that allows a surgeon to examine the organs inside of the abdomen without having to make large incisions in t...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00602-6},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1520,
  title = {Watch and learn: event-domain term extraction from social networks}},
  author = {Nicholas Mamo, Joel Azzopardi and Colin Layfield}},
  year = {2024}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Event tracking algorithms detect and track, but they do not understand what happens in events. Term extraction research has studied the concepts of general domains-computer science, medicine, law-but not What ...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-024-01040-2},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1521,
  title = {A new feature popularity framework for detecting cyberattacks using popular features}},
  author = {Richard Zuech, John Hancock and Taghi M. Khoshgoftaar}},
  year = {2022}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {We propose a novel feature popularity framework, and introduce this new framework to the cybersecurity domain. Feature popularity has not yet been used in machine learning or data mining, and we implement it w...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00661-9},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1522,
  title = {Examining the impact of cross-domain learning on crime prediction}},
  author = {Fateha Khanam Bappee, Amilcar Soares, Lucas May Petry and Stan Matwin}},
  year = {2021}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Nowadays, urban data such as demographics, infrastructure, and criminal records are becoming more accessible to researchers. This has led to improvements in quantitative crime research for predicting future cr...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00489-9},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1523,
  title = {An integrated model for evaluation of big data challenges and analytical methods in recommender systems}},
  author = {Adeleh Asemi, Asefeh Asemi, Andrea Ko and Ali Alibeigi}},
  year = {2022}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {The study aimed to present an integrated model for evaluation of big data (BD) challenges and analytical methods in recommender systems (RSs). The proposed model used fuzzy multi-criteria decision making (MCDM...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00560-z},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1524,
  title = {A robust machine learning approach to SDG data segmentation}},
  author = {Kassim S. Mwitondi, Isaac Munyakazi and Barnabas N. Gatsheni}},
  year = {2020}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {In the light of the recent technological advances in computing and data explosion, the complex interactions of the Sustainable Development Goals (SDG) present both a challenge and an opportunity to researchers...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00373-y},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1525,
  title = {Interpretable machine learning model to predict the acute occurrence of delirium in elderly patients in the intensive care units: a retrospective cohort study}},
  author = {Xin Hu, Jun Luo, Hong Liang, Jingwei Yue, Yeqing Qi and Hui Liu}},
  year = {2025}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Delirium is a severe complication in critical elderly patients. This study aimed to develop interpretable machine-learning (ML) models to predict acute delirium and identify risk factors for medical interventi...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-025-01107-8},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1526,
  title = {Bayesian hierarchical analyses for entrepreneurial intention of students}},
  author = {Mesfin Mulu Ayalew}},
  year = {2020}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {In recent years, entrepreneurship has become an important issue due to national economic development and the contribution of society. Data with a hierarchical structure received more attention and occur freque...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00293-x},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1527,
  title = {On hierarchical clustering-based approach for RDDBS design}},
  author = {Hassan I. Abdalla, Ali A. Amer and Sri Devi Ravana}},
  year = {2023}},
  journal = {Journal of Big Data}},
  tipo = {Methodology}},
  publisher = {Unknown}},
  abstract = {Distributed database system (DDBS) design is still an open challenge even after decades of research, especially in a dynamic network setting. Hence, to meet the demands of high-speed data gathering and for the...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00849-7},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1528,
  title = {Privacy preserved incremental record linkage}},
  author = {Shahidul Islam Khan, Abir Bin Ayub Khan and Abu Sayed Md Latiful Hoque}},
  year = {2022}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Using an incremental approach to solve the record linkage problem is a relatively new research area. In incremental record linkage, every inserted record is compared with some existing clusters of records base...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00655-7},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1529,
  title = {Evaluating classifier performance with highly imbalanced Big Data}},
  author = {John T. Hancock, Taghi M. Khoshgoftaar and Justin M. Johnson}},
  year = {2023}},
  journal = {Journal of Big Data}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {Using the wrong metrics to gauge classification of highly imbalanced Big Data may hide important information in experimental results. However, we find that analysis of metrics for performance evaluation and wh...}},
  url = {https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00724-5},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1530,
  title = {The first lidar observations of the nighttime sodium layer at low latitudes Gadanki (13.5°N, 79.2°E), India}},
  author = {Y. Bhavani Kumar, P. Vishnu Prasanth, D. Narayana Rao, M. Sundara Murthy and M. Krishnaiah}},
  year = {2007}},
  journal = {Earth, Planets and Space}},
  tipo = {Article}},
  publisher = {Unknown}},
  abstract = {We report on the first lidar observations of the nighttime mesospheric sodium layer from Gadanki (13.5°N, 79.2°E) site in India. The lidar measurements of upper atmospheric sodium made on 6 nights between the ...}},
  url = {https://www.springeropen.com//earth-planets-space.springeropen.com/articles/10.1186/BF03352722},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1531,
  title = {Scoring and classifying regions via multimodal transportation networks}},
  author = {Aaron Bramson, Megumi Hori, Bingran Zha and Hirohisa Inamoto}},
  year = {2019}},
  journal = {Applied Network Science}},
  tipo = {Research}},
  publisher = {Unknown}},
  abstract = {In order to better understand the role of transportation convenience in location preferences, as well as to uncover transportation system patterns that span multiple modes of transportation, we analyze 500 loc...}},
  url = {https://www.springeropen.com//appliednetsci.springeropen.com/articles/10.1007/s41109-019-0191-7},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1532,
  title = {A linked stress release model for historical Japanese earthquakes: coupling among major seismic regions}},
  author = {Chunsheng Lu, David Harte and Mark Bebbington}},
  year = {2014}},
  journal = {Earth, Planets and Space}},
  tipo = {Article}},
  publisher = {Unknown}},
  abstract = {A linked stress release model is proposed for the analysis of spatial interaction of earthquake occurrences through stress transfer within a large area of the Earth’s crust. As an example, the model is used fo...}},
  url = {https://www.springeropen.com//earth-planets-space.springeropen.com/articles/10.1186/BF03351562},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1533,
  title = {LDPC Code Design for Nonuniform Power-Line Channels}},
  author = {Ali Sanaei and Masoud Ardakani}},
  year = {2007}},
  journal = {EURASIP Journal on Advances in Signal Processing}},
  tipo = {Research Article}},
  publisher = {Unknown}},
  abstract = {We investigate low-density parity-check code design for discrete multitone channels over power lines. Discrete multitone channels are well modeled as nonuniform channels, that is, different bits experience var...}},
  url = {https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/76146},
  shared_files = {Data/resultados_springer_open.bib, Data/resultados_springer_open.bib}
}

@article{ref_dup1534,
  title = {Visual Programming Environments and Computational Thinking Performance of Fifth- and Sixth-Grade Students}},
  author = {Sheng-Yi Wu, Yu-Sheng Su}},
  year = {2021}},
  journal = {Journal of Educational Computing Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Currently, many countries actively cultivate students to develop computational thinking ability. Many visual programming environments (VPEs) and physical robot courses have been integrated into computational thinking learning in the elementary education ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0735633120988807},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1535,
  title = {Computational Design Research in Architecture: The Legacy of the Hochschule für Gestaltung, Ulm}},
  author = {Isabel Clara Neves, João Rocha, José Pinto Duarte}},
  year = {2014}},
  journal = {International Journal of Architectural Computing}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The use of computational processes in architecture is a widespread practice which draws on a set of theories of computer science developed in the 60s and 70s. With the advent of computers, many of these methodologies were developed in research centres in ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1260/1478-0771.12.1.1},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1536,
  title = {Thinking Forward: Conrad Wolfram on the Computational Knowledge Economy}},
  author = {Daniel Araya}},
  year = {2013}},
  journal = {E-Learning and Digital Media}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.2304/elea.2013.10.3.324},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1537,
  title = {Effects of a Programming Course Using the GAME Model on Undergraduates’ Self-Efficacy and Basic Programming Concepts}},
  author = {Chun-Yen Tsai, Yun-An Chen, Fu-Pei Hsieh, Min-Hsiung Chuang, Chien-Liang Lin}},
  year = {2023}},
  journal = {Journal of Educational Computing Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In higher education, it is challenging to cultivate non-computer science majors’ programming concepts. This study used the GAME model (gamification, assessment, modeling, and enquiry) in a programming education course to enhance undergraduates’ self-...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/07356331231206071},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1538,
  title = {Southern creative robotics – A collaborative framework to implement computational design, fabrication and robotics}},
  author = {Goncalo Castro Henriques, Andres Martin Passaro, Rodrigo Garcia Alvarado, [...], View all}},
  year = {2024}},
  journal = {International Journal of Architectural Computing}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Scientific knowledge, ideally neutral and impartial, is inevitably shaped by geographical, economic, and cultural contexts. This research contends that overcoming the constraints of human and economic scarcity, inertia, and limited funding access demands ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/14780771241260855},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1539,
  title = {English Learners in STEM Subjects: Contemporary Views on STEM Subjects and Language With English Learners}},
  author = {Okhee Lee, Amy Stephens}},
  year = {2020}},
  journal = {Educational Researcher}},
  tipo = {Review article}},
  publisher = {Unknown}},
  abstract = {With the release of the consensus report English Learners in STEM Subjects: Transforming Classrooms, Schools, and Lives, we highlight foundational constructs and perspectives associated with STEM subjects and language with English learners (ELs) that ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3102/0013189X20923708},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1540,
  title = {Modding Europa Universalis IV: An informal gaming practice transposed into a formal learning setting}},
  author = {Rhett Loban}},
  year = {2021}},
  journal = {E-Learning and Digital Media}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This article explores the use of modding as a formal tool for learning history. The article examines data from a formal analysis of Europa Universalis IV (EUIV), a survey of 331 EUIV forum participants and a case study of 18 university participants. ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/20427530211022964},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1541,
  title = {Computational Grounded Theory: A Methodological Framework}},
  author = {Laura K. Nelson}},
  year = {2017}},
  journal = {Sociological Methods & Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This article proposes a three-step methodological framework called computational grounded theory, which combines expert human knowledge and hermeneutic skills with the processing power and pattern recognition of computers, producing a more ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0049124117729703},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1542,
  title = {Editorial}},
  author = {Steve Charlton, Karen Ryder, Jacqui Taylor}},
  year = {2016}},
  journal = {Psychology Learning & Teaching}},
  tipo = {Editorial}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1475725716673004},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1543,
  title = {A contemporary course on the introduction to computational fluid dynamics}},
  author = {Steven A E Miller}},
  year = {2019}},
  journal = {International Journal of Mechanical Engineering Education}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The University of Florida Department of Mechanical and Aerospace Engineering recently created a new senior technical elective in the field of computational fluid dynamics. The main objectives of the class are learning the process of computational fluid ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0306419019838880},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1544,
  title = {Making Computational Social Science Effective: Epistemology, Methodology, and Technology}},
  author = {Steven Bankes, Robert Lempert, Steven Popper}},
  year = {2002}},
  journal = {Social Science Computer Review}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {There has been significant recent interest in Agent Based Modeling in many social sciences including economics, sociology, anthropology, political science, and game theory. This article describes three problems that need to be addressed in order for such ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/089443902237317},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1545,
  title = {Contests}},
  author = {},
  year = {2024}},
  journal = {Gifted Child Today}},
  tipo = {Other}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/10762175241289858},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1546,
  title = {Integrating Petri Nets and Flux Balance Methods in Computational Biology Models: a Methodological and Computational Practice}},
  author = {Pernice Simone, Follia Laura, Balbo Gianfranco, Milanesi Luciano, Sartini Giulia, Totis Niccoló, [...], View all}},
  year = {2019}},
  journal = {Fundamenta Informaticae}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Computational Biology is a fast-growing field that is enriched by different data-driven methodological approaches and by findings and applications in a broad range of biological areas. Fundamental to these approaches are the mathematical and computational ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/FI-2020-1888},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1547,
  title = {Modern developments in computational aeroelasticity}},
  author = {O O Bendiksen}},
  year = {2004}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A survey of the main challenges in computational aeroelasticity is presented and recent ideas and developments are discussed. Advances over the past 25 years have to a large extent been paced by the required developments in computational fluid ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1243/0954410041872861},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1548,
  title = {Performance Metrics Based on Computational Action}},
  author = {Robert W. Numrich}},
  year = {2004}},
  journal = {The International Journal of High Performance Computing Applications}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {We propose a new performance metric based on computational action. We examine work as it evolves in time and compute computational action as the integral of the work function over time. We compare the action generated at less than full power with the ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1094342004048538},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1549,
  title = {From Blocks to Text: Bridging Programming Misconceptions}},
  author = {Monika Mladenović, Žana Žanko, Goran Zaharija}},
  year = {2024}},
  journal = {Journal of Educational Computing Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The use of a pedagogical approach mediated transfer with the bridging method has been successful in facilitating the transitions from block-based to text-based programming languages. Nevertheless, there is a lack of research addressing the impact of this ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/07356331241240047},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1550,
  title = {Computational chaos control based on small perturbations for complex spectra simulation}},
  author = {Jesús Manuel Rodríguez-Núñez, Aned de León, Martín E Molinar-Tabares, Mario Flores-Acosta, SJ Castillo}},
  year = {2022}},
  journal = {SIMULATION}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In this paper, we propose to use a computational method of chaos control to simulate complex experimental spectra. This computational chaos control technique is based on the Ott–Grebogi–York (OGY) method. We chose the logistic map as the base mathematical ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/00375497221098417},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1551,
  title = {Towards a framework for computational persuasion with applications in behaviour change}},
  author = {Anthony Hunter}},
  year = {2018}},
  journal = {Argument & Computation}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Persuasion is an activity that involves one party trying to induce another party to believe something or to do something. It is an important and multifaceted human facility. Obviously, sales and marketing is heavily dependent on persuasion. But many other ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/AAC-170032},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1552,
  title = {AI-driven computational creativity in fashion design: a review}},
  author = {Jennifer Xiaopei Wu, Li Li}},
  year = {2025}},
  journal = {Textile Research Journal}},
  tipo = {Review article}},
  publisher = {Unknown}},
  abstract = {The emergence of text-to-image generative AI tools has garnered significant attention, particularly in creative design applications. This review article explores the field of AI-driven computational creativity, which has witnessed advancements in ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/00405175241279976},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1553,
  title = {Computational basis of decision-making impairment in multiple sclerosis}},
  author = {Rodrigo S Fernández, Lucia Crivelli, María E Pedreira, Ricardo F Allegri, Jorge Correale}},
  year = {2021}},
  journal = {Multiple Sclerosis Journal}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Background: Multiple sclerosis (MS) is commonly associated with decision-making, neurocognitive impairments, and mood and motivational symptoms. However, their relationship may be obscured by traditional scoring methods. Objectives: To study the ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/13524585211059308},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1554,
  title = {Computational physiology of uterine smooth muscle}},
  author = {Joseph R Dunford, E Josiah Lutton, Jolene Atia, Andrew M Blanks, Hugo A van den Berg}},
  year = {2019}},
  journal = {Science Progress}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Pregnancy can be accompanied by serious health risks to mother and child, such as pre-eclampsia, premature birth and postpartum haemorrhage. Understanding of the normal physiology of uterine function is essential to an improved management of such risks. ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0036850419850431},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1555,
  title = {Negotiating human engagement and the fixity of computational design: Toward a performative design space for the differently-abled bodymind}},
  author = {Sean Ahlquist}},
  year = {2020}},
  journal = {International Journal of Architectural Computing}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Computational design affords agency: the ability to orchestrate the material, spatial, and technical architectural system. In this specific case, it occurs through enhanced, authored means to facilitate making and performance—typically driven by concerns ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1478077120919850},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1556,
  title = {Computational design, engineering and manufacturing of a material-efficient 3D printed lattice structure}},
  author = {Roberto Naboni, Anja Kunic, Luca Breseghello}},
  year = {2020}},
  journal = {International Journal of Architectural Computing}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Building with additive manufacturing is an increasingly relevant research topic in the field of Construction 4.0, where designers are seeking higher levels of automation, complexity and precision compared to conventional construction methods. As an answer ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1478077120947990},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1557,
  title = {Computational Systems Biology in Cancer: Modeling Methods and Applications}},
  author = {Wayne Materi, David S. Wishart}},
  year = {2007}},
  journal = {Gene Regulation and Systems Biology}},
  tipo = {Review article}},
  publisher = {Unknown}},
  abstract = {In recent years it has become clear that carcinogenesis is a complex process, both at the molecular and cellular levels. Understanding the origins, growth and spread of cancer, therefore requires an integrated or system-wide approach. Computational ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/117762500700100010},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1558,
  title = {Data Journalism Teaching, Fast and Slow}},
  author = {Paul Bradshaw}},
  year = {2018}},
  journal = {Asia Pacific Media Educator}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This commentary draws on a decade’s experience of teaching data journalism within a variety of contexts to describe the lessons learned regarding different pedagogical techniques and choices about the aspects of data journalism to teach. What emerges is a ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1326365X18769395},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1559,
  title = {Computational opposition analysis using word embeddings: A method for strategising resonant informal argument}},
  author = {Cameron Shackell, Laurianne Sitbon}},
  year = {2019}},
  journal = {Argument & Computation}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In informal argument, an essential step is to ask what will “resonate” with a particular audience and hence persuade. Marketers, for example, may recommend a certain colour for a new soda can because it “pops” on Instagram; politicians may “fine-tune” ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/AAC-190467},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1560,
  title = {Opendda: a Novel High-Performance Computational Framework for the Discrete Dipole Approximation}},
  author = {James Mc Donald, Aaron Golden, S. Gerard Jennings}},
  year = {2009}},
  journal = {The International Journal of High Performance Computing Applications}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This work presents a highly optimized computational framework for the Discrete Dipole Approximation, a numerical method for calculating the optical properties associated with a target of arbitrary geometry that is widely used in atmospheric, astrophysical ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1094342008097914},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1561,
  title = {PY-RATE ADVENTURES: A 2D Platform Serious Game for Learning the Basic Concepts of Programming With Python}},
  author = {Grigorios Sideris, Stelios Xinogalos}},
  year = {2019}},
  journal = {Simulation & Gaming}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Background.Learning programming is a cognitively demanding field of study accompanied with various difficulties. Although there is a high demand in the market for programmers, software analysts and engineers, a high dropout rate is recorded in relevant ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1046878119872797},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1562,
  title = {On the Computational Complexity of Optimal Multisplitting}},
  author = {Tapio Elomaa, Juho Rousu}},
  year = {2001}},
  journal = {Fundamenta Informaticae}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The need to partition or discretize numeric value ranges arises in machine learning and data mining algorithms. This subtask is a potential time-consumption bottleneck, since the number of candidate partitions is exponential in the number of possible cut ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/FUN-2001-471-204},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1563,
  title = {Scientific and Technological Thinking}},
  author = {Michael E. Gorman}},
  year = {2006}},
  journal = {Review of General Psychology}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Studies of scientific and technological thinking can be organized via a taxonomy of methodological approaches that reveal areas for further study. Dunbar provides such a taxonomy, using biological methods like in vivo and in vitro as examples. In vitro ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1037/1089-2680.10.2.113},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1564,
  title = {The Deficit Profile of Elementary Students With Computational Difficulties Versus Word Problem-Solving Difficulties}},
  author = {Xin Lin MA, Peng Peng PhD, Hongjing Luo BA}},
  year = {2019}},
  journal = {Learning Disability Quarterly}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The purpose of the study was to compare the deficit profiles of two important types of mathematics difficulties. Three cognitive measures (working memory, processing speed, and reasoning), two mathematics measures (numerical facts retrieval and ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0731948719865499},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1565,
  title = {Computational soundness of symbolic zero-knowledge proofs}},
  author = {Michael Backes, Dominique Unruh}},
  year = {2010}},
  journal = {Journal of Computer Security}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The abstraction of cryptographic operations by term algebras, called Dolev–Yao models, is essential in almost all tool-supported methods for proving security protocols. Recently significant progress was made in proving that Dolev–Yao models offering the ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/JCS-2009-0392},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1566,
  title = {Thinking about thinking: A coordinate-based meta-analysis of neuroimaging studies of metacognitive judgements}},
  author = {Anthony G. Vaccaro, Stephen M. Fleming}},
  year = {2018}},
  journal = {Brain and Neuroscience Advances}},
  tipo = {Review article}},
  publisher = {Unknown}},
  abstract = {Metacognition supports reflection upon and control of other cognitive processes. Despite metacognition occupying a central role in human psychology, its neural substrates remain underdetermined, partly due to study-specific differences in task domain and ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/2398212818810591},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1567,
  title = {Computational modeling of observational learning inspired by the cortical underpinnings of human primates}},
  author = {Emmanouil Hourdakis, Panos Trahanias}},
  year = {2012}},
  journal = {Adaptive Behavior}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Recent neuroscientific evidence in human and non-human primates indicates that the regions that become active during motor execution and motor observation overlap extensively in the cerebral cortex. This suggests that to observe an action, these primates ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1059712312445902},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1568,
  title = {Practical and Fundamental Developments in the Computational Modelling of Fluid Flows}},
  author = {J C R Hunt MA, PhD, FIMA, FRMetSoc, FRS}},
  year = {1995}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science}},
  tipo = {Review article}},
  publisher = {Unknown}},
  abstract = {The reasons for the recent growth of computational fluid dynamics (CFD) for industrial and environmental applications are briefly explained, and thence why the users and managers of CFD systems should understand the main underlying principles, the ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1243/PIME_PROC_1995_209_159_02},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1569,
  title = {Computational Intelligence techniques for Web personalization}},
  author = {G. Castellano, A.M. Fanelli, M.A. Torsello}},
  year = {2008}},
  journal = {Web Intelligence and Agent Systems}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Computational Intelligence (CI) paradigms reveal to be potential tools to face under the Web uncertainty. In particular, CI techniques may be properly exploited to handle Web usage data and develop Web-based applications tailored on users preferences. The ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/WIA-2008-0140},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1570,
  title = {Between Innovation and Standardization: Best Practices and Inclusive Guidelines in Computational Communication Science}},
  author = {Annie Waldherr, Matthew Weber, Shangyuan Wu, Mario Haim, [...], View all}},
  year = {2024}},
  journal = {Journalism & Mass Communication Quarterly}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Computational communication science is transitioning from an emerging to an established field within communication research, creating a need for proper guidelines and methodological standards. This forum gathers experienced computational communication ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/10776990241301676},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1571,
  title = {Development of educational-use computational fluid dynamics programming environment and workshop}},
  author = {Akito Nakano, John N Bohn, Akira Wakita}},
  year = {2016}},
  journal = {International Journal of Architectural Computing}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In the current computer-aided architectural design education, students do not necessarily need to be taught to use more digital tools, but need to be introduced to the possibilities of designing their own digital tools. Designing the original tools with ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1478077116638924},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1572,
  title = {Estimation of wall shear stress in bypass grafts with computational fluid dynamics method}},
  author = {L. Goubergrits, K. Affeld, E. Wellnhofer, R. Zurbrügg, T. Holmer}},
  year = {2001}},
  journal = {The International Journal of Artificial Organs}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Coronary artery bypass graft (CABG) operation for coronary artery disease with different types of grafts has a large clinical application world wide. Immediately after this operation patients are usually relieved of their chest pain and have improved ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/039139880102400306},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1573,
  title = {An Integrated Approach to Testing Dynamic, Multilevel Theory: Using Computational Models to Connect Theory, Model, and Data}},
  author = {Timothy Ballard, Hector Palada, Mark Griffin, Andrew Neal}},
  year = {2019}},
  journal = {Organizational Research Methods}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Some of the most influential theories in organizational sciences explicitly describe a dynamic, multilevel process. Yet the inherent complexity of such theories makes them difficult to test. These theories often describe multiple subprocesses that ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1094428119881209},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1574,
  title = {Engaging Young Students in Effective Robotics Education: An Embodied Learning-Based Computer Programming Approach}},
  author = {Xinli Zhang, Yuchen Chen, Danqing Li, Lailin Hu, Gwo-Jen Hwang, Yun-Fang Tu}},
  year = {2023}},
  journal = {Journal of Educational Computing Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Robotics education has received widespread attention in K-12 education. Studies have pointed out that in robotics courses, learners face challenges in learning abstract content, such as constructing a robot with a good structure and writing programs to ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/07356331231213548},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1575,
  title = {Computational Fluid Dynamics Investigation of Chronic Aortic Dissection Hemodynamics Versus Normal Aorta}},
  author = {Christof Karmonik PhD, Matthias Müller-Eschner MD, Sasan Partovi MD, Philipp Geisbüsch MD, [...], View all}},
  year = {2013}},
  journal = {Vascular and Endovascular Surgery}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Objectives: To evaluate hemodynamic changes during aneurysmal dilatation in chronic type B aortic dissections compared to hemodynamic parameters in the healthy aorta with the use of computational fluid dynamics (CFD). Methods: True lumen (TL)/false lumen (FL)...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1538574413503561},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1576,
  title = {A computational framework for aesthetic preferences in architecture using computer vision and artificial neural networks}},
  author = {Victor Sardenberg, Igor Guatelli, Mirco Becker}},
  year = {2024}},
  journal = {International Journal of Architectural Computing}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This paper introduces a computational aesthetics framework utilizing computer vision (CV) and artificial neural networks (ANN) to predict the aesthetic preferences of groups of people for architecture. It relies on part-to-whole theories from aesthetics ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/14780771241279350},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1577,
  title = {Emotional Experience in the Computational Belief–Desire Theory of Emotion}},
  author = {Rainer Reisenzein}},
  year = {2009}},
  journal = {Emotion Review}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Based on the belief that computational modeling (thinking in terms of representation and computations) can help to clarify controversial issues in emotion theory, this article examines emotional experience from the perspective of the Computational Belief–...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1754073909103589},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1578,
  title = {An Investigation of High School Students’ Errors in Introductory Programming: A Data-Driven Approach}},
  author = {Yizhou Qian, James Lehman}},
  year = {2019}},
  journal = {Journal of Educational Computing Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This study implemented a data-driven approach to identify Chinese high school students’ common errors in a Java-based introductory programming course using the data in an automated assessment tool called the Mulberry. Students’ error-related behaviors ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0735633119887508},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1579,
  title = {Robustness to algorithmic singularities and sensitivity in computational kinematics}},
  author = {L Gracia, J Angeles}},
  year = {2011}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A robust approach to computational kinematics intended to cope with algorithmic singularities is introduced in this article. The approach is based on the reduction of the original system of equations to a subsystem of bivariate equations, as opposed to ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1243/09544062JMES2464},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1580,
  title = {Development of a Computational Simulation Model for Conflict Management in Team Building}},
  author = {W. M. Wang, S. L. Ting}},
  year = {2011}},
  journal = {International Journal of Engineering Business Management}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Conflict management is one of the most important issues in leveraging organizational competitiveness. However, traditional social scientists built theories or models in this area which were mostly expressed in words and diagrams are insufficient. Social ...}},
  url = {https://journals.sagepub.com/doi/abs/10.5772/50932},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1581,
  title = {Optimal design of an annular thrust air bearing using parametric computational fluid dynamics model and genetic algorithms}},
  author = {Qiang Gao, Lihua Lu, Wanqun Chen, Guanglin Wang}},
  year = {2017}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part J: Journal of Engineering Tribology}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The performance of air bearing is highly influenced by the geometrical parameters of its restrictor. This study aims to maximize the load-carrying capacity and stiffness of air bearing, and minimize its volume flow rate by optimizing the geometrical ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1350650117743684},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1582,
  title = {Translating Informal Theories Into Formal Theories: The Case of the Dynamic Computational Model of the Integrated Model of Work Motivation}},
  author = {Jeffrey B. Vancouver, Mo Wang, Xiaofei Li}},
  year = {2018}},
  journal = {Organizational Research Methods}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Theories are the core of any science, but many imprecisely stated theories in organizational and management science are hampering progress in the field. Computational modeling of existing theories can help address the issue. Computational models are a ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1094428118780308},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1583,
  title = {Accurate calculation of aerodynamic coefficients of parafoil airdrop system based on computational fluid dynamic}},
  author = {Wannan Wu, Qinglin Sun, Shuzhen Luo, Mingwei Sun, Zengqiang Chen, Hao Sun}},
  year = {2018}},
  journal = {International Journal of Advanced Robotic Systems}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Accurate calculation of canopy aerodynamic parameters is a prerequisite for precise modeling of a parafoil airdrop system. This investigation analyses the aerodynamic performance of the canopy in airdrop testing combining the leading-edge incision and the ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1729881418766190},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1584,
  title = {“What if We Explore…” Promoting Engaged Learning and Collaboration with MOUNTAIN RESCUE}},
  author = {Denise M. Bressler, M. Shane Tutwiler, Amanda Siebert-Evenstone, Leonard A. Annetta, Jason A. Chen}},
  year = {2022}},
  journal = {Simulation & Gaming}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Background According to the Committee on STEM Education, K-12 science students need access to learning experiences that promote collaboration and engagement. To fill that void, we need to develop activities that stimulate engaged learning and scaffold ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/10468781221120690},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1585,
  title = {Transformation of Psychology: From Thinking/Thought to Metathinking/Metathought}},
  author = {Ilya E. Garber, Steven E. Wallis}},
  year = {2015}},
  journal = {Review of General Psychology}},
  tipo = {Other}},
  publisher = {Unknown}},
  abstract = {The prefix meta is popular in psychology as well as in other sciences oriented on the investigation of psychology. In the first case the usage is quite simple: every term is “the sum” of meta and usual psychological concept. In the second case, the ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1037/gpr0000050},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1586,
  title = {Digital/computational phenotyping: What are the differences in the science and the ethics?}},
  author = {Federica Lucivero, Nina Hallowell}},
  year = {2021}},
  journal = {Big Data & Society}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The concept of ‘digital phenotyping’ was originally developed by researchers in the mental health field, but it has travelled to other disciplines and areas. This commentary draws upon our experiences of working in two scientific projects that are based ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/20539517211062885},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1587,
  title = {Analysis of power losses in an industrial planetary speed reducer: Measurements and computational fluid dynamics calculations}},
  author = {Franco Concli, Edoardo Conrado, Carlo Gorla}},
  year = {2013}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part J: Journal of Engineering Tribology}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In this work, experimental and theoretical analyses of the power losses in an industrial planetary speed reducer were performed in order to define the weight of the different types of power losses in the global efficiency as well as to understand the ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1350650113496980},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1588,
  title = {Thermal and hygric assessment of an inside-insulated brick wall: 2D critical experiment and computational analysis}},
  author = {Václav Kočí, Jan Kočí, Jiří Maděra, Zbyšek Pavlík, Xianglin Gu, Weiping Zhang, Robert Černý}},
  year = {2018}},
  journal = {Journal of Building Physics}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A combined experimental-computational approach is used for the analysis of hygrothermal performance of a brick wall provided with interior thermal insulation system. A 2D laboratory experiment is performed to determine temperature and moisture fields in a ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1744259117750495},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1589,
  title = {Computational Algorithms for a Damage-coupled Cyclic Viscoplasticity Material Model}},
  author = {A.H. Zhao, C.L. Chow}},
  year = {2008}},
  journal = {International Journal of Damage Mechanics}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The primary objective of this investigation is to develop efficient and robust computational schemes for a damage-coupled cyclic thermoviscoplasticity model for solder materials. Three constitutive integration algorithms, Euler, modified Euler, and semi-...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1056789508090748},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1590,
  title = {Learner Engagement in a Business Simulation Game: Impact on Higher-Order Thinking Skills}},
  author = {Yueh-Min Huang, Lusia M. Silitonga, Astrid T. Murti, Ting-Ting Wu}},
  year = {2022}},
  journal = {Journal of Educational Computing Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Higher-order thinking skills (HOTS) are reliable predictors of success in school and the workplace. A typical technique for encouraging higher-order thinking is to use instructional design interventions that engage learners in simple cognitive activities. ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/07356331221106918},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1591,
  title = {Numerical simulation of lignocellulosic biomass gasification in concentric tube entrained flow gasifier through computational fluid dynamics}},
  author = {Ghulamullah Maitlo, Imran Nazir Unar, Rasool Bux Mahar, Khan Mohammad Brohi}},
  year = {2019}},
  journal = {Energy Exploration & Exploitation}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Thermochemical conversion of biomass is an encouraging way for the production of syngas. In the present research, four different biomass materials were used for gasification which includes rice husk, cotton stalks, sugarcane bagasse, and sawdust. These ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0144598719839760},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1592,
  title = {Computational fluid dynamics simulation and geometric design of hydraulic turbine draft tube}},
  author = {JB Sosa, G Urquiza, JC García, LL Castro}},
  year = {2015}},
  journal = {Advances in Mechanical Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Any hydraulic reaction turbine is installed with a draft tube that impacts widely the entire turbine performance, on which its functions are as follows: drive the flux in appropriate manner after it releases its energy to the runner; recover the suction ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1687814015606307},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1593,
  title = {Spillover as Movement Agenda Setting: Using Computational and Network Techniques for Improved Rare Event Identification}},
  author = {Thomas Elliott, Misty Ring-Ramirez, Jennifer Earl}},
  year = {2021}},
  journal = {Social Science Computer Review}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The increasing availability of data, along with sophisticated computational methods for analyzing them, presents researchers with new opportunities and challenges. In this article, we address both by describing computational and network methods that can ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0894439320951766},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1594,
  title = {Research on Immersive Environments and 21st Century Skills: an Introduction to the Special Issue}},
  author = {P. G. Schrader Ph.D., Kimberly A. Lawless Ph.D.}},
  year = {2011}},
  journal = {Journal of Educational Computing Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.2190/EC.44.4.a},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1595,
  title = {The sound of disinformation: TikTok, computational propaganda, and the invasion of Ukraine}},
  author = {Marcus Bösch, Tom Divon}},
  year = {2024}},
  journal = {New Media & Society}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {TikTok has emerged as a powerful platform for the dissemination of mis- and disinformation about the war in Ukraine. During the initial three months after the Russian invasion in February 2022, videos under the hashtag #Ukraine garnered 36.9 billion views,...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/14614448241251804},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1596,
  title = {Computational Design of Targeted Inhibitors of Polo-Like Kinase 1 (Plk1)}},
  author = {Krupa S. Jani, D.S. Dalafave}},
  year = {2012}},
  journal = {Bioinformatics and Biology Insights}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Computational design of small molecule putative inhibitors of Polo-like kinase 1 (Plk1) is presented. Plk1, which regulates the cell cycle, is often over expressed in cancers. Down regulation of Plk1 has been shown to inhibit tumor progression. Most ...}},
  url = {https://journals.sagepub.com/doi/abs/10.4137/BBI.S8971},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1597,
  title = {Developing an artificial intelligence model for English grammar correction: A computational linguistics approach}},
  author = {Han Jiang}},
  year = {2025}},
  journal = {Journal of Computational Methods in Sciences and Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This article aimed to explore the development of an artificial intelligence model for English grammar correction based on computational linguistics methods. Traditional grammar correction systems suffer from problems such as complex rules, sparse data, ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/14727978251318799},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1598,
  title = {Assessing the Development of Digital Scientific Literacy With a Computational Evidence-Based Reasoning Tool}},
  author = {Nancy Holincheck, Terrie M. Galanti, James Trefil}},
  year = {2022}},
  journal = {Journal of Educational Computing Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The evolving digital world requires scientifically literate citizens who are able to critically evaluate Internet sources of varying credibility. Instruction on evidence evaluation in postsecondary education often focuses on peer-review as a singular ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/07356331221081484},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1599,
  title = {Data science literacy: Toward a philosophy of accessible and adaptable data science skill development in public administration programs}},
  author = {Michael Overton, Stephen Kleinschmit}},
  year = {2021}},
  journal = {Teaching Public Administration}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Public administration is struggling to contend with a substantial shift in practice fueled by the accelerating adoption of information technology. New skills, competencies and pedagogies are required by the field to help overcome the data-skills gap. As a ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/01447394211004990},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1600,
  title = {Computational Modelling of Culture and Affect}},
  author = {Ruth Aylett, Ana Paiva}},
  year = {2012}},
  journal = {Emotion Review}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This article discusses work on implementing emotional and cultural models into synthetic graphical characters. An architecture, FAtiMA, implemented first in the antibullying application FearNot! and then extended as FAtiMA-PSI in the cultural-sensitivity ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1754073912439766},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1601,
  title = {The simulation of the emission of iron fumes caused by shielded metal arc welding using a computational fluid dynamics method}},
  author = {Fatemeh Paridokht, Shiva Soury, Sara Karimi Zeverdegani}},
  year = {2022}},
  journal = {Toxicology and Industrial Health}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Computational fluid dynamics (CFD) is an indispensable simulation tool for predicting the emission of pollutants in the work environment. Welding is one of the most common industrial processes that might expose the operators and surrounding workers to ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/07482337221144143},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1602,
  title = {Reactive computational fluid dynamics modelling of methane–hydrogen admixtures in internal combustion engines: Part I – RANS}},
  author = {Jann Koch, Christian Schürch, Yuri M Wright, Konstantinos Boulouchos}},
  year = {2020}},
  journal = {International Journal of Engine Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The effects of hydrogen addition to internal combustion engines operated by natural gas/methane has been widely demonstrated experimentally in the literature. Already small hydrogen contents in the fuel show promising benefits with respect to increased ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1468087420916380},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1603,
  title = {Contribution of inflow artery to observed flow in a vascular access: A computational fluid dynamic modeling study of an arteriovenous fistula circuit}},
  author = {Jeffrey Krampf, Ramesh Agarwal, Surendra Shenoy}},
  year = {2020}},
  journal = {The Journal of Vascular Access}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Introduction: The volume of blood flowing through the vascular access is an important parameter necessary to provide adequate dialysis for a functional arteriovenous fistula. Higher blood flows are seen in arteriovenous access that receive inflow from ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1129729820944069},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1604,
  title = {Computational Theories of Vision}},
  author = {M. J. Morgan}},
  year = {1984}},
  journal = {The Quarterly Journal of Experimental Psychology Section A}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This exciting and original book describes the work of David Marr and his colleagues at MIT on the computational theory of vision, particularly the early stages of shape analysis. Although much of the theorising is frankly speculative and not certain to ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1080/14640748408401509},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1605,
  title = {Computational study on aerodynamically coupled piezoelectric harvesters}},
  author = {Bruno A Roccia, Marcos L Verstraete, Luis R Ceballos, Balakumar Balachandran, Sergio Preidikman}},
  year = {2020}},
  journal = {Journal of Intelligent Material Systems and Structures}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In this work, the authors present a two-dimensional computational model for predicting the aeroelastic response as well as the output power of vertically arranged harvesters by taking into account all aerodynamic interactions. The piezo-aeroelastic ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1045389X20930093},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1606,
  title = {Analytic Versus Computational Cognitive Models: Agent-Based Modeling as a Tool in Cognitive Sciences}},
  author = {Jens Koed Madsen, Richard Bailey, Ernesto Carrella, Philipp Koralus}},
  year = {2019}},
  journal = {Current Directions in Psychological Science}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Computational cognitive models typically focus on individual behavior in isolation. Models frequently employ closed-form solutions in which a state of the system can be computed if all parameters and functions are known. However, closed-form models are ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0963721419834547},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1607,
  title = {Computational fluid dynamics applied to the ventilation of small-animal laboratory cages}},
  author = {Ira Katz, Kateryna Voronetska, Mickaël Libardi, Matthieu Chalopin, Patricia Privat, [...], View all}},
  year = {2020}},
  journal = {Laboratory Animals}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Several studies based on in vivo or in vitro models have found promising results for the noble gas argon in neuroprotection against ischaemic pathologies. The development of argon as a medicinal product includes the requirement for toxicity testing ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0023677220937718},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1608,
  title = {Genome-Wide Search for Tyrosine Phosphatases in the Human Genome Through Computational Approaches Leads to the Discovery of Few New Domain Architectures}},
  author = {Teerna Bhattacharyya, Ramanathan Sowdhamini}},
  year = {2019}},
  journal = {Evolutionary Bioinformatics}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Reversible phosphorylation maintained by protein kinases and phosphatases is an integral part of intracellular signalling, and phosphorylation on tyrosine is extensively utilised in higher eukaryotes. Tyrosine phosphatases are enzymes that not only ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1176934319840289},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1609,
  title = {Computational aeroacoustics of aerofoil leading edge noise using the volume penalization-based immersed boundary methods}},
  author = {Wei Ying, Ryu Fattah, Sinforiano Cantos, Siyang Zhong, Tatiana Kozubskaya}},
  year = {2022}},
  journal = {International Journal of Aeroacoustics}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Broadband noise due to the turbulence-aerofoil interaction, which is also called the leading edge noise, is one of the major noise sources of aircraft (including the engine). To study the noise properties numerically is a popular approach with the ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1475472X221079557},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1610,
  title = {Design optimization for overhead ventilation duct system for a train using computational fluid dynamics and design of experiment}},
  author = {Joon-Hyung Kim, Joo-Hyun Rho}},
  year = {2016}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part E: Journal of Process Mechanical Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The overhead multiple outlets ventilation duct system of 18 m long is used to maintain the specified indoor thermal comfort environment for each railway passenger car. Therefore, the flow uniformity of the overhead ventilation duct system is very ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0954408916646403},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1611,
  title = {Performance improvement of a radial organic Rankine cycle turbine by means of automated computational fluid dynamic design}},
  author = {John Harinck, David Pasquale, Rene Pecnik, Jos van Buijtenen, Piero Colonna}},
  year = {2013}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {There is a growing interest in organic Rankine cycle turbogenerators because of their ability to efficiently utilize external heat sources at low-to-medium temperature in the small-to-medium power range. High-temperature organic Rankine cycle turbines ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0957650913499565},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1612,
  title = {Toward improving student motivation and performance in introductory programming learning by Scratch: The role of achievement emotions}},
  author = {Fu-Hsiang Wen, Tienhua Wu, Wei-Chih Hsu}},
  year = {2023}},
  journal = {Science Progress}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This study investigated the differences in programming novices’ intrinsic motivation and performance within a Scratch-based programming learning environment using a pretest–posttest intervention design. Specifically, this study aimed to examine what and ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/00368504231205985},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1613,
  title = {Computational comparison and experimental performance analysis on heat pipes using concentrating solar parabolic trough collector}},
  author = {P Ravindra Kumar, Nelakuditi N Babu, K Lakshmi Prasad}},
  year = {2022}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The heat pipes in solar applications uses are important and it is significantly growing nowadays. Capable of meeting the world’s challenges with a threat to the climate; a shortage of conventional energy sources, usually fossil fuels, high electricity ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/09576509211070118},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1614,
  title = {The Hyper-Modular Associative Mind: A Computational Analysis of Associative Responses of Persons with Asperger Syndrome}},
  author = {Yoed N Kenett, Rinat Gold, Miriam Faust}},
  year = {2015}},
  journal = {Language and Speech}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Rigidity of thought is considered a main characteristic of persons with Asperger syndrome (AS). This rigidity may explain the poor comprehension of unusual semantic relations, frequently exhibited by persons with AS. Research indicates that such ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0023830915589397},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1615,
  title = {Trends in application of advancing computational approaches in GPCR ligand discovery}},
  author = {Siyu Zhu, Meixian Wu, Ziwei Huang, Jing An}},
  year = {2021}},
  journal = {Experimental Biology and Medicine}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {G protein-coupled receptors (GPCRs) comprise the most important superfamily of protein targets in current ligand discovery and drug development. GPCRs are integral membrane proteins that play key roles in various cellular signaling processes. Therefore, ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1535370221993422},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1616,
  title = {Computational Comparison Between a Classic Bifurcated Endograft and a Customized Model With “Dog Bone”–Shaped Limbs}},
  author = {Efstratios Georgakarakos MD, MSc, PhD, Antonios Xenakis MEng, PhD, George S. Georgiadis MD, PhD}},
  year = {2019}},
  journal = {Journal of Endovascular Therapy}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Purpose: To use computational simulations to compare the hemodynamic characteristics of a classic bifurcated stent-graft to an equally long endograft design with “dog bone”–shaped limbs (DB), which have large diameter proximal and distal ends and ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1526602819834713},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1617,
  title = {Publication Bias in Methodological Computational Research}},
  author = {Anne-Laure Boulesteix, Veronika Stierle, Alexander Hapfelmeier}},
  year = {2015}},
  journal = {Cancer Informatics}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The problem of publication bias has long been discussed in research fields such as medicine. There is a consensus that publication bias is a reality and that solutions should be found to reduce it. In methodological computational research, including ...}},
  url = {https://journals.sagepub.com/doi/abs/10.4137/CIN.S30747},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1618,
  title = {Computational studies of ‘whiplashg’ injuries}},
  author = {C R Gentle, W Z Golinski, F Heitplatz}},
  year = {2001}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The term ‘whiplash’ was initially used to describe injuries to the neck caused by the head being forced backwards during a rear-end collision in cars without head restraints. The addition of head restraints in the 1970s was expected to solve this ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1243/0954411011533742},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1619,
  title = {A Computational Assay to Design an Epitope-Based Peptide Vaccine against Saint Louis Encephalitis Virus}},
  author = {Anayet Hasan Md., Mehjabeen Hossain, Jibran Alam Md.}},
  year = {2013}},
  journal = {Bioinformatics and Biology Insights}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Saint Louis encephalitis virus, a member of the flaviviridae subgroup, is a culex mosquito-borne pathogen. Despite severe epidemic outbreaks on several occasions, not much progress has been made with regard to an epitope-based vaccine designed for Saint ...}},
  url = {https://journals.sagepub.com/doi/abs/10.4137/BBI.S13402},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1620,
  title = {A computational model for measuring discourse complexity}},
  author = {Kun Sun, Wenxin Xiong}},
  year = {2019}},
  journal = {Discourse Studies}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In past studies, the few quantitative approaches to discourse structure were mostly confined to the presentation of the frequency of discourse relations. However, quantitative approaches should take into account both hierarchical and relational layers in ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1461445619866985},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1621,
  title = {Numerical analysis of textured piston compression ring conjunction using two-dimensional-computational fluid dynamics and Reynolds methods}},
  author = {Chengwei Wen, Xianghui Meng, Wenxiang Li}},
  year = {2018}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part J: Journal of Engineering Tribology}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The Reynolds equation, in which some items have been omitted, is a simplified form of the Navier–Stokes equations. When surface texturing exists, it may unreasonably reveal the tribological effects in some cases. In this paper, both the two-dimensional ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1350650118755248},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1622,
  title = {Improving performance using computational compression through memoization: A case study using a railway power consumption simulator}},
  author = {Alejandro Calderón, Alberto García, Félix García-Carballeira, Jesús Carretero, Javier Fernández}},
  year = {2016}},
  journal = {The International Journal of High Performance Computing Applications}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The objective of data compression is to avoid redundancy in order to reduce the size of the data to be stored or transmitted. In some scenarios, data compression may help to increase global performance by reducing the amount of data at a competitive cost ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1094342016637813},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1623,
  title = {Computational Evaluation of Elasto-Viscoplastic Deformation and Strength of Rubber Blended Semi-crystalline Polymer}},
  author = {Makoto Uchida, Naoya Tada, Yoshihiro Tomita}},
  year = {2009}},
  journal = {International Journal of Damage Mechanics}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In order to clarify the elasto-viscoplastic deformation behavior and strength of rubber blended semi-crystalline polymer, micro- to mesoscopic mechanical behavior was modeled by using large deformation finite element homogenization method. In this model, ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1056789509103646},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1624,
  title = {New directions in computational, combinational and structural creativity}},
  author = {Bruce Garvey, Liuqing Chen, Feng Shi, Ji Han, Peter RN Childs}},
  year = {2018}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This paper examines how new and creative relationships in datasets, not easily revealed by conventional information retrieval methods and technologies, can be identified using a mix of established and new methods. The authors present how the integration ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0954406218769919},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1625,
  title = {Understanding underspecification: A comparison of two computational implementations}},
  author = {Pavel Logačev, Shravan Vasishth}},
  year = {2016}},
  journal = {Quarterly Journal of Experimental Psychology}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Swets et al. (2008. Underspecification of syntactic ambiguities: Evidence from self-paced reading. Memory and Cognition, 36(1), 201–216) presented evidence that the so-called ambiguity advantage [Traxler et al. (1998). Adjunct attachment is not a form of ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1080/17470218.2015.1134602},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1626,
  title = {An analytical formulation of thermodynamic properties of dry and metastable steam suitable for computational fluid dynamics modelling of steam turbine flows}},
  author = {Jan Hrubý, Jaroslav Pátek, Michal Duška}},
  year = {2013}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Application of computational fluid dynamics to real steam flows including non-equilibrium condensing flows requires an accurate and, at the same time, computationally inexpensive formulation of thermodynamic properties of steam. The present formulation ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0957650913509088},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1627,
  title = {A Network is a Network is a Network: Reflections on the Computational and the Societies of Control}},
  author = {David M. Berry, Alexander R. Galloway}},
  year = {2015}},
  journal = {Theory, Culture & Society}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In this wide-ranging conversation, Berry and Galloway explore the implications of undertaking media theoretical work for critiquing the digital in a time when networks proliferate and, as Galloway claims, we need to ‘forget Deleuze’. Through the lens of ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0263276415590237},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1628,
  title = {Comparison of the design maps of TPMS based bone scaffolds using a computational modeling framework simultaneously considering various conditions}},
  author = {Yongtao Lu, Yi Huo, Jia’ao Zou, Yanchen Li, Zhuoyue Yang, Hanxing Zhu, Chengwei Wu}},
  year = {2022}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In recent years, the triply periodic minimal surface (TPMS)-based scaffolds have been served as one of the crucial types of structures for biological replacements, the energy absorber, etc. Meanwhile, the development of additive manufacturing (AM) has ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/09544119221102704},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1629,
  title = {Software Project Management and Quality Engineering Practices for Complex, Coupled Multiphysics, Massively Parallel Computational Simulations: Lessons Learned From ASCI}},
  author = {D. E. Post, R. P. Kendall}},
  year = {2004}},
  journal = {The International Journal of High Performance Computing Applications}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Many institutions are now developing large-scale, complex, coupled multiphysics computational simulations for massively parallel platforms for the simulation of the performance of nuclear weapons and certification of the stockpile, and for research in ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1094342004048534},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1630,
  title = {Normal fluid stresses are prevalent in rotary ventricular assist devices: A computational fluid dynamics analysis}},
  author = {Dominica PY Khoo, Andrew N Cookson, Harinderjit S Gill, Katharine H Fraser}},
  year = {2018}},
  journal = {The International Journal of Artificial Organs}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Despite the evolution of ventricular assist devices, ventricular assist device patients still suffer from complications due to the damage to blood by fluid dynamic stress. Since rotary ventricular assist devices are assumed to exert mainly shear stress, ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0391398818792757},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1631,
  title = {Media Ontology and Transcendental Instrumentality}},
  author = {Luciana Parisi}},
  year = {2019}},
  journal = {Theory, Culture & Society}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This article takes inspiration from Kittler’s claim that philosophy has neglected the means used for its production. Kittler’s argument for media ontology will be compared to the post-Kantian project of re-inventing philosophy through the medium of ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0263276419843582},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1632,
  title = {Autopoiesis and Digital Design Theory: CAD Systems as Cognitive Instruments}},
  author = {Eduardo Lyon}},
  year = {2005}},
  journal = {International Journal of Architectural Computing}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In contrast to traditional models of design process fundamentally defined by the abstract manipulation of objects, this study recognizes that the resources available for rethinking architecture are to be found in a reformulation of its theory and ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1260/147807705775377366},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1633,
  title = {Methodological Cyborg as Black Feminist Technology: Constructing the Social Self Using Computational Digital Autoethnography and Social Media}},
  author = {Nicole Marie Brown}},
  year = {2018}},
  journal = {Cultural Studies ↔ Critical Methodologies}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This article reimagines the quantified self within the context of Black feminist technologies. Bringing computation and autoethnographic methods together using a methodology I call computational digital autoethnography, I harvest my social media data to ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1532708617750178},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1634,
  title = {Effects of direct water injection and injector configurations on performance and emission characteristics of a gasoline direct injection engine: A computational fluid dynamics analysis}},
  author = {Ankit A Raut, J M Mallikarjuna}},
  year = {2019}},
  journal = {International Journal of Engine Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In-cylinder water injection is a promising approach for reducing NOx and soot emissions from internal combustion engines. It allows one to use a higher compression ratio by reducing engine knock; hence, higher fuel economy and power output can be ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1468087419890418},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1635,
  title = {Computational fluid dynamics analysis of the combustion process and emission characteristics for a direct-injection-premixed charge compression ignition engine}},
  author = {Hiroshi Kawanabe, Takuji Ishiyama}},
  year = {2014}},
  journal = {International Journal of Engine Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The direct-injection-premixed charge compression ignition–based combustion process with a high exhaust gas recirculation ratio and early injection timing is simulated using a Reynolds-averaged Navier–Stokes–based commercial computational fluid dynamics ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1468087413498868},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1636,
  title = {A simple computational model of the right coronary artery on the beating heart – effects of the temporal change of curvature and torsion on the blood flow}},
  author = {Hiroyuki Hayashi, Takami Yamaguchi}},
  year = {2002}},
  journal = {Biorheology}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A computational fluid dynamics study was conducted using a simplified model of the right coronary artery, which deforms with contraction of the heart. The right coronary artery was modeled using an ordinary helix, whose torsion and curvature changed in ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0006355X2002039003004016},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1637,
  title = {Understanding climate-induced migration through computational modeling: A critical overview with guidance for future efforts}},
  author = {Charlotte Till, Jamie Haverkamp, Devin White, Budhendra Bhaduri}},
  year = {2016}},
  journal = {The Journal of Defense Modeling and Simulation}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Climate change has the potential to displace large populations in many parts of the developed and developing world. Understanding why, how, and when environmental migrants decide to move is critical to successful strategic planning within organizations ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1548512916679038},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1638,
  title = {Development and application of a computational fluid dynamics methodology to predict fuel–air mixing and sources of soot formation in gasoline direct injection engines}},
  author = {Tommaso Lucchini, Gianluca D’ Errico, Angelo Onorati, Giovanni Bonandrini, Luca Venturoli, Rita Di Gioia}},
  year = {2013}},
  journal = {International Journal of Engine Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A detailed understanding of the air–fuel mixing process in gasoline direct injection engines is necessary to avoid soot formation that might result from charge inhomogeneities or liquid fuel impingement on the cylinder walls. Within this context, the use ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1468087413500297},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1639,
  title = {Using a Deep Learning-Based Visual Computational Model to Identify Cognitive Strategies in Matrix Reasoning}},
  author = {Zhimou Wang, Yaohui Liu, Peida Zhan}},
  year = {2024}},
  journal = {Journal of Educational and Behavioral Statistics}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Constructive matching and response elimination strategies are two primarily used cognitive strategies in Raven’s Advanced Progressive Matrices (APM), a valid measurement instrument of general intelligence. Identifying strategies is necessary for ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3102/10769986241268907},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1640,
  title = {Computational Identification of Novel Genes: Current and Future Perspectives}},
  author = {Steffen Klasberg, Tristan Bitard-Feildel, Ludovic Mallet}},
  year = {2016}},
  journal = {Bioinformatics and Biology Insights}},
  tipo = {Review article}},
  publisher = {Unknown}},
  abstract = {While it has long been thought that all genomic novelties are derived from the existing material, many genes lacking homology to known genes were found in recent genome projects. Some of these novel genes were proposed to have evolved de novo, ie, out of ...}},
  url = {https://journals.sagepub.com/doi/abs/10.4137/BBI.S39950},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1641,
  title = {Teaching higher order thinking skills to gifted students: A meta-analysis}},
  author = {C Owen Lo, Li-Chuan Feng}},
  year = {2020}},
  journal = {Gifted Education International}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The current study examined the effects of higher order thinking skills (HOTS) interventions with gifted students in Taiwan. A total of 25 studies published between 1997 and 2017 were included. Twenty-nine effect sizes were extracted for the 25 studies. ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0261429420917854},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1642,
  title = {Three-dimensional computational fluid dynamics model of a tubular-shaped ambient air-breathing proton exchange membrane fuel cell}},
  author = {M A R S Al-Baghdadi}},
  year = {2008}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The development of physically representative models that allow reliable simulation of the processes under realistic conditions is essential for the development and optimization of fuel cells, the introduction of cheaper materials and fabrication ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1243/09576509JPE611},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1643,
  title = {Effects of Computational Domain on Numerical Simulation of Building Fires}},
  author = {Xiaocui Zhang, Manjiang Yang, Jian Wang, Yaping He}},
  year = {2010}},
  journal = {Journal of Fire Protection Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Computational fluid dynamics (CFD) modeling (or field modeling) is becoming the main method for numerical simulation of building fires. Among many factors that influence the validity and accuracy of CFD simulation results, the computational domain is ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1042391510367349},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1644,
  title = {A computational and experimental study of heating and cooling cycles during thermal sterilization of liquid foods in pouches using CFD}},
  author = {A G Ghani, M M Farid, X D Chen}},
  year = {2003}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part E: Journal of Process Mechanical Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In this study, a theoretical analysis of a heating and cooling cycle during sterilization of a three-dimensional pouch filled with carrot-orange soup was presented and analysed. Transient temperature, the shape of the slowest heating zone (SHZ) ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1243/09544080360562936},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1645,
  title = {Computational reproducibility of scientific workflows at extreme scales}},
  author = {Line Pouchard, Sterling Baldwin, Todd Elsethagen, Shantenu Jha, Bibi Raju, Eric Stephan, [...], View all}},
  year = {2019}},
  journal = {The International Journal of High Performance Computing Applications}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {We propose an approach for improved reproducibility that includes capturing and relating provenance characteristics and performance metrics. We discuss two use cases: scientific reproducibility of results in the Energy Exascale Earth System Model (E3SM—...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1094342019839124},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1646,
  title = {Computational Issues in the Statistical Design and Analysis of Experimental Games}},
  author = {Mahmoud A. EI-Gamal, Richard D. McKelvey, Thomas R. Palfrey}},
  year = {1993}},
  journal = {The International Journal of Supercomputing Applications}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {One goal of experimental economics is to provide data to identify models that best describe the behavior of experimental subjects and, more generally, human economic behavior. We discuss here what we think are the three main steps required to make ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/109434209300700302},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1647,
  title = {Can the dual-route cascaded computational model of reading offer a valid account of the masked onset priming effect?}},
  author = {Petroula Mousikou, Max Coltheart, Matthew Finkbeiner, Steven Saunders}},
  year = {2010}},
  journal = {Quarterly Journal of Experimental Psychology}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The masked onset priming effect (MOPE) refers to the empirical finding that target naming is faster when the target (SIB) is preceded by a briefly presented masked prime that starts with the same letter/phoneme (suf) than when it does not (mof; Kinoshita, ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1080/17470210903156586},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1648,
  title = {Cyber-physical-social-thinking modeling and computing for geological information service system}},
  author = {Yueqin Zhu, Yongjie Tan, Ruixin Li, Xiong Luo}},
  year = {2016}},
  journal = {International Journal of Distributed Sensor Networks}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The serious geological hazards occurred frequently in the last few years. They have inflicted heavy casualties and property losses. Hence, it is necessary to design a geological information service system to analyze and evaluate geological hazards. With ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1550147716666666},
  shared_files = {Data/resultados_ieee.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1649,
  title = {Impingement of a propeller-slipstream on a leading edge with a flow-permeable insert: A computational aeroacoustic study}},
  author = {Francesco Avallone, Damiano Casalino, Daniele Ragni}},
  year = {2018}},
  journal = {International Journal of Aeroacoustics}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This manuscript describes an aeroacoustic computational study on the impingement of a tractor-propeller slipstream on the leading edge of a pylon. Both the flow and acoustic fields are studied for two pylon leading edges: a solid and a flow-permeable one. ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1475472X18788961},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1650,
  title = {Predicting language choice in a digital medium: A computational approach to analyzing WhatsApp code-switching in Hong Kong}},
  author = {Wilkinson Daniel Wong Gonzales}},
  year = {2025}},
  journal = {International Journal of Bilingualism}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Aims and objectives: This paper attempts to develop a predictive computational model of Cantonese–English code-switching (CS) in Hong Kong, informed by language-internal and “language-external” (e.g., social) factors. I analyze this bilingual practice with ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/13670069251325036},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1651,
  title = {Emotional states recognition, implementing a low computational complexity strategy}},
  author = {Adrian Rodriguez Aguiñaga, Miguel Angel Lopez Ramirez}},
  year = {2016}},
  journal = {Health Informatics Journal}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This article describes a methodology to recognize emotional states through an electroencephalography signals analysis, developed with the premise of reducing the computational burden that is associated with it, implementing a strategy that reduces the ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1460458216661862},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1652,
  title = {Special Issue on Computational Models for Life Sciences}},
  author = {Tuan Pham, Xiaobo Zhou}},
  year = {2008}},
  journal = {International Journal of Hybrid Intelligent Systems}},
  tipo = {Editorial}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/HIS-2008-5401},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1653,
  title = {Delineation of Demographic Regions with GIS and Computational Intelligence}},
  author = {Thomas Hatzichristos}},
  year = {2004}},
  journal = {Environment and Planning B: Planning and Design}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This paper presents a methodology for the creation of homogeneous demographic regions with geographical information systems (GIS) and computational intelligence. The proposed method is unsupervised fuzzy classification performed by neural networks using ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1068/b1296},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1654,
  title = {Programming learning and teaching of pre-service computer science teachers: Challenges, concerns, and solutions}},
  author = {Seyfullah Gökoğlu, Servet Kilic}},
  year = {2022}},
  journal = {E-Learning and Digital Media}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This study investigates pre-service computer science (CS) teachers’ perspectives on the factors affecting their programming abilities, concerns about their future professional lives, and pedagogical suggestions for effective programming teaching. The ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/20427530221117331},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1655,
  title = {Navigating industry 4.0: Crafting a responsive curriculum for library and information science}},
  author = {Yanga Livi, Oghenere Gabriel Salubi}},
  year = {2024}},
  journal = {Business Information Review}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This paper presents a nascent conceptual framework (The LISI4C framework) for designing a Library and Information Science (LIS) curriculum tailored to meet the demands of Industry 4.0 (4IR) in particularly South Africa, with global applicability. The ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/02663821241289820},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1656,
  title = {Computational Vision Model to Assess Work-Zone Conspicuity}},
  author = {Jay E. Barton, James A. Misener, Theodore E. Cohn}},
  year = {2002}},
  journal = {Transportation Research Record}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A computational means to assess the conspicuity of highway features was developed, verified, and then applied to a sample construction work zone scene. This work was conceived as a balance between modeling the complex phenomena within the human visual ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3141/1801-09},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1657,
  title = {The Importance of Computational Skill for Answering Items in a Mathematics Problem Solving Test: Implications for Construct Validity}},
  author = {Robert A. Forsyth, Timothy N. Ansley}},
  year = {1982}},
  journal = {Educational and Psychological Measurement}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The primary purpose of this study was to investigate the importance of computational skill for answering items in the Quantitative Thinking subtest (Test Q) of the Iowa Tests of Educational Development (ITED). Nine matched pairs of schools participated in ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0013164482421032},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1658,
  title = {Predicting boiling heat transfer using computational fluid dynamics}},
  author = {J G Hawley, M Wilson, N A F Campbell, G P Hammond, M J Leathard}},
  year = {2004}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A study has been undertaken to assess the capability of incorporating different empirical approaches in a computational ftuid dynamics (CFD) environment for predicting boiling heat transfer. The application is for internal combustion (IC) engine ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1243/095440704774061165},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1659,
  title = {A computational model of argumentation in agreement negotiation processes}},
  author = {Mare Koit, Haldur Õim}},
  year = {2015}},
  journal = {Argument & Computation}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The paper describes a computational model that we have implemented in an experimental dialogue system (DS). Communication in a natural language between two participants A and B is considered, where A has a communicative goal that his/her partner B will ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1080/19462166.2014.915233},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1660,
  title = {Netsolve: a Network-Enabled Server for Solving Computational Science Problems}},
  author = {Henri Casanova, Jack Dongarra}},
  year = {1997}},
  journal = {The International Journal of Supercomputer Applications and High Performance Computing}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This paper presents a new system, called NetSolve, that allows users to access computational resources, such as hardware and software, distributed across the network. The development of NetSolve was motivated by the need for an easy-to-use, efficient ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/109434209701100304},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1661,
  title = {A mixed-fidelity computational model of aero engine for inlet distortion}},
  author = {Jin Guo, Jun Hu, Baofeng Tu, Zhiqiang Wang}},
  year = {2019}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This paper presents a mixed-fidelity model to investigate the effect of complex inlet distortion on aero gas turbine engines. The approach is developed by integrating a three-dimensional body force model of multistage compressors and a classical two-...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0954410019841798},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1662,
  title = {Big Data, Digital Media, and Computational Social Science: Possibilities and Perils}},
  author = {Dhavan V. Shah, Joseph N. Cappella, W. Russell Neuman}},
  year = {2015}},
  journal = {The ANNALS of the American Academy of Political and Social Science}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0002716215572084},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1663,
  title = {What’s the story? A computational analysis of narrative competence in autism}},
  author = {Michelle Lee, Gary E Martin, Abigail Hogan, Deanna Hano, Peter C Gordon, Molly Losh}},
  year = {2017}},
  journal = {Autism}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Individuals with autism spectrum disorder demonstrate narrative (i.e. storytelling) difficulties which can significantly impact their ability to form and maintain social relationships. However, existing research has not comprehensively documented these ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1362361316677957},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1664,
  title = {Performance comparison of a structured bed reactor with and without a chimney tray on the gas-flow maldistribution: A computational fluid dynamics study}},
  author = {Hajer Troudi, Moncef Ghiss, Mohamed Ellejmi, Zoubeir Tourki}},
  year = {2019}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part E: Journal of Process Mechanical Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In the present paper, two configurations of structured reactors (with and without) chimney tray placed below the packed bed have been investigated to study their effect on maldistribution factor and pressure drop characteristics. A simulation result based ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0954408919889417},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1665,
  title = {A computational and experimental study of the scavenging flow in the transfer duct of a motored two-stroke cycle engine}},
  author = {J. P. Creaven, R. G. Kenny, R Fleck, G Cunningham}},
  year = {2001}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This study was carried out to assess the ability of a computational fluid dynamics (CFD) code to predict the scavenging flow in the transfer duct of a two-stroke cycle engine. A two-stroke cycle engine was modified to allow laser Doppler ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1243/0954407011528581},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1666,
  title = {Analyzing Market-Based Resource Allocation Strategies for the Computational Grid}},
  author = {Rich Wolski, James S. Plank, John Brevik, Todd Bryan}},
  year = {2001}},
  journal = {The International Journal of High Performance Computing Applications}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In this paper, the authors investigate G-commerce—computational economies for controlling resource allocation in computational Grid settings. They define hypothetical resource consumers (representing users and Grid-aware applications) and resource ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/109434200101500305},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1667,
  title = {Understanding ferrovanadium smelting through computational thermodynamics modelling}},
  author = {D. R. Swinbourne, T. Richardson, F. Cabalteja}},
  year = {2016}},
  journal = {Mineral Processing and Extractive Metallurgy}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Ferrovanadium is essential for the production of many alloy steels. It is made by the aluminothermic reduction of vanadium oxides, together with scrap steel and burnt lime as a flux at very high temperatures. In this work, the theory of aluminothermic ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1179/1743285515Y.0000000019},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1668,
  title = {Customized project charter for computational scientific software products}},
  author = {Shlomo Mark, Yotam Lurie}},
  year = {2018}},
  journal = {Journal of Computational Methods in Science and Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Different domains of software applications require that the project management’s development process be customized to its specific conditions and needs. One of the most widely used operation research is the development of a scientific software product. ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/JCM-180778},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1669,
  title = {Hydraulic turbines—basic principles and state-of-the-art computational fluid dynamics applications}},
  author = {P Drtina, M Sallaberger}},
  year = {1999}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The present paper discusses the basic principles of hydraulic turbines, with special emphasis on the use of computational fluid dynamics (CFD) as a tool which is being increasingly applied to gain insight into the complex three-dimensional (3D) ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1243/0954406991522202},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1670,
  title = {Applying Automated Originality Scoring to the Verbal Form of Torrance Tests of Creative Thinking}},
  author = {Selcuk Acar, Kelly Berthiaume, Katalin Grajzel, Denis Dumas, Charles “Tedd” Flemister, Peter Organisciak}},
  year = {2021}},
  journal = {Gifted Child Quarterly}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In this study, we applied different text-mining methods to the originality scoring of the Unusual Uses Test (UUT) and Just Suppose Test (JST) from the Torrance Tests of Creative Thinking (TTCT)–Verbal. Responses from 102 and 123 participants who completed ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/00169862211061874},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1671,
  title = {Computational fluid dynamics and combustion modelling of HIsarna incinerator}},
  author = {R. Sripriya, T. Peeters, K. Meijer, C. Zeilstra, D. van der Plas}},
  year = {2016}},
  journal = {Ironmaking & Steelmaking}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {HIsarna technology combines the cyclone converter furnace (CCF) technology owned by Tata Steel and the HIsmelt technology owned by RioTinto. The CCF is mainly a prereduction vessel that prereduces and melts the iron ore particles, while the final ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1179/1743281215Y.0000000031},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1672,
  title = {Comparison between two computational fluid dynamics methods for gust response predictions}},
  author = {Zhenlong Wu, Yuan Gao, Xiaoming He, Weizhe Fu, Jianqiang Shi, Zhibo Zhang, Ruitao Zhou}},
  year = {2023}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Based on the open-source computational fluid dynamics (CFD) platform, OpenFOAM, two numerical simulation methods for gusty inflow characterization and gust response prediction are implemented by solving the fundamental incompressible unsteady Reynolds ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/09544100231163202},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1673,
  title = {Thermal performance of three concentrating collectors with bifacial photovoltaic cells part I – Experimental and computational fluid dynamics study}},
  author = {Miguel Lança, João Gomes, Diogo Cabral}},
  year = {2023}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Bifacial photovoltaic cells can produce electricity from incoming solar radiation on both sides. These cells have a strong potential to reduce electricity generation costs and may play an important role in the energy system of the future. However, today, ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/09576509231197881},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1674,
  title = {Computational pregnant occupant model, ‘Expecting’, for crash simulations}},
  author = {B Serpil Acar, D van Lopik}},
  year = {2009}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A computational model of the pregnant occupant, which is capable of simulating the dynamic response to acceleration impacts, is introduced. The occupant model represents a 5th percentile female at around the 38th week of pregnancy. A finite ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1243/09544070JAUTO1072},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1675,
  title = {Task-parallel in situ temporal compression of large-scale computational fluid dynamics data}},
  author = {Heather Pacella, Alec Dunton, Alireza Doostan, Gianluca Iaccarino}},
  year = {2022}},
  journal = {The International Journal of High Performance Computing Applications}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Present day computational fluid dynamics (CFD) simulations generate considerable amounts of data, sometimes on the order of TB/s. Often, a significant fraction of this data is discarded because current storage systems are unable to keep pace. To address ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/10943420221085000},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1676,
  title = {Multi-objective shape optimization of submarine hull using genetic algorithm integrated with computational fluid dynamics}},
  author = {K. L. Vasudev, R. Sharma, S. K. Bhattacharyya}},
  year = {2017}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part M: Journal of Engineering for the Maritime Environment}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A multi-objective optimization framework is developed for design of submarine hull shape. Internal volume of the vehicle and its hydrodynamic drag are optimized by seamlessly integrating non-dominated sorting genetic algorithm and Reynolds averaged Navier–...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1475090217714649},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1677,
  title = {Performance Analysis and Application of Three Different Computational Methods for Solar Heating System with Seasonal Water Tank Heat Storage}},
  author = {Dongliang Sun, Jinliang Xu, Peng Ding}},
  year = {2013}},
  journal = {Advances in Mechanical Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {We analyze and compare three different computational methods for a solar heating system with seasonal water tank heat storage (SHS-SWTHS). These methods are accurate numerical method, temperature stratification method, and uniform temperature method. The ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1155/2013/857941},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1678,
  title = {Computational aspects of statistical confidentiality the CASC-project}},
  author = {Anco Hundepool}},
  year = {2001}},
  journal = {Statistical Journal of the United Nations Economic Commission for Europe}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In this paper we will give an overview of the CASC (Computational Aspects of Statistical Confidentiality) project (The CASC-project (see http://neon.vb.cbs.nl/casc) is partly funded under the 5th Research, Technological Development and Demonstration (RTD) ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/SJU-2001-18405},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1679,
  title = {Computational Modelling and Similarity Reduction of Equations for Transient Fluid Flow and Heat Transfer with Variable Properties}},
  author = {R. J. Moitsheki, O. D. Makinde}},
  year = {2013}},
  journal = {Advances in Mechanical Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {We consider a system of coupled partial differential equations describing transient fluid flow and heat transfer with variable flow properties. Classical Lie point symmetry analysis of this system resulted in admitted large Lie algebras for some special ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1155/2013/983962},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1680,
  title = {Validation of a comprehensive computational fluid dynamics methodology to predict the direct injection process of gasoline sprays using Spray G experimental data}},
  author = {Davide Paredi, Tommaso Lucchini, Gianluca D’Errico, Angelo Onorati, Lyle Pickett, Joshua Lacey}},
  year = {2019}},
  journal = {International Journal of Engine Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A detailed prediction of injection and air–fuel mixing is fundamental in modern direct injection, spark-ignition engines to guarantee a stable and efficient combustion process and to minimize pollutant formation. Within this context, computational fluid ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1468087419868020},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1681,
  title = {Hybrid and Non-Hybrid Block-Based Programming Languages in an Introductory College Computer-Science Course}},
  author = {Wen-Chin Hsu, Julie Gainsburg}},
  year = {2021}},
  journal = {Journal of Educational Computing Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Block-based programming languages (BBLs) have been proposed as a way to prepare students for learning to program in more sophisticated, text-based languages, such as Java. Hybrid BBLs add the ability to view and edit the block commands in auto-generated, ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0735633120985108},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1682,
  title = {A Computational Investigation of Engine Heat Transfer with Ducted Fuel Injection}},
  author = {Ramazan Şener, Christopher W Nilsen, Drummond E Biles, Charles J Mueller}},
  year = {2023}},
  journal = {International Journal of Engine Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Ducted fuel injection (DFI) is an innovative method that curtails or prevents soot formation in direct-injection compression-ignition engines. DFI uses a simple duct, positioned outside each injector hole, facilitating the fuel/charge gas mixing before ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/14680874221149321},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1683,
  title = {Assessment of Wind Atlas Analysis and Application Program and computational fluid dynamics estimates for power production on a Jeju Island wind farm}},
  author = {Beomcheol Ju, Jihyun Jeong, Kyungnam Ko}},
  year = {2016}},
  journal = {Wind Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The accuracy of power production predictions made using a linear model, Wind Atlas Analysis and Application Program, and the computational fluid dynamics model, Meteodyn WT, was assessed for a Seongsan wind farm on Jeju Island, South Korea. The actual ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0309524X15624346},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1684,
  title = {A Review of Tools and Techniques for Data-Enabled Formative Assessment}},
  author = {Rob Nyland}},
  year = {2017}},
  journal = {Journal of Educational Technology Systems}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The purpose of this literature review is to understand the current state of research on tools that collect data for the purpose of formative assessment. We were interested in identifying the types of data collected by these tools, how these data were ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0047239517748936},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1685,
  title = {Examining the Acquisition of Phonological Word Forms with Computational Experiments}},
  author = {Michael S Vitevitch, Holly L Storkel}},
  year = {2012}},
  journal = {Language and Speech}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {It has been hypothesized that known words in the lexicon strengthen newly formed representations of novel words, resulting in words with dense neighborhoods being learned more quickly than words with sparse neighborhoods. Tests of this hypothesis in a ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0023830912460513},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1686,
  title = {Imbalance wall functions with density and material property variation effects applied to engine heat transfer computational fluid dynamics simulations}},
  author = {Mika A Nuutinen, Ossi T Kaario, Ville A Vuorinen, Paul N Nwosu, Martti J Larmi}},
  year = {2013}},
  journal = {International Journal of Engine Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Heat transfer is a significant factor affecting internal combustion engine efficiency, emissions and performance. This study concentrates on model development for convective heat transfer and near-wall turbulent flow. The solution of complete fluid ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1468087413481779},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1687,
  title = {How Do Drivers Respond to Silent Automation Failures? Driving Simulator Study and Comparison of Computational Driver Braking Models}},
  author = {Giulio Bianchi Piccinini, Esko Lehtonen, Fabio Forcolin, Johan Engström, Deike Albers, [...], View all}},
  year = {2019}},
  journal = {Human Factors}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Objective This paper aims to describe and test novel computational driver models, predicting drivers’ brake reaction times (BRTs) to different levels of lead vehicle braking, during driving with cruise control (CC) and during silent failures of adaptive ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0018720819875347},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1688,
  title = {Computational study of hole shape effect on film cooling performance}},
  author = {J Yao, Y F Yao}},
  year = {2011}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Film cooling effectiveness has been studied by using a computational approach based on solving the Reynolds-averaged Navier–Stokes equations. A wind tunnel test configuration is considered with a total of four cooling hole geometries as a cylindrical hole,...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0957650911399013},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1689,
  title = {Computational GOMSL Modeling towards Understanding Cognitive Strategy in Dual-Task Performance with Automation}},
  author = {Sang-Hwan Kim, David B. Kaber, Carlene M. Perry}},
  year = {2007}},
  journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting}},
  tipo = {Review article}},
  publisher = {Unknown}},
  abstract = {The objective of this study was to assess the use of a computational cognitive model for describing human performance with an adaptively automated system, with and without advance cueing of control mode transitions. A dual-task piloting simulation was ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/154193120705101206},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1690,
  title = {A computational approach to modeling chaff clouds consisting of fractal particles}},
  author = {Ioannis Xenidis, Jason P Dauby, Maung H Myat, David W Scholfield}},
  year = {2011}},
  journal = {The Journal of Defense Modeling and Simulation}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A chaff cloud is an electronic countermeasure to radio frequency emitters. The cloud immerses a protected entity in a multitude of false targets by reradiating incident electromagnetic energy from millions of thin aluminized fibers, foil strips, or ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1548512911430967},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1691,
  title = {Three-Mode Factor Analysis Via a Modification of Tucker's Computational Method—III}},
  author = {Thomas J. Zenisek}},
  year = {1978}},
  journal = {Educational and Psychological Measurement}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A FORTRAN G/H computer program was derived for an IBM series 360/370 computer system (or compatible systems such as Amdol or Honeywell) that provides at least 1536-K of core storage. This large amount of core storage is usually provided via a VS-1 or VS-2 ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/001316447803800318},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1692,
  title = {Study on the combustion and emission characteristics of a diesel engine with multi-injection modes based on experimental investigation and computational fluid dynamics modelling}},
  author = {X-Y Shi, X-Q Qiao, J-M Ni, Y-Y Zheng, N-Y Ye}},
  year = {2010}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In this paper, experiments were carried out on a direct-injection diesel engine using a common-rail system, in order to study the effects of multi-injection modes on the combustion characteristics and pollutant emissions. A soot model was proposed ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1243/09544070JAUTO1434},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1693,
  title = {Breakup dynamics in a pressure-swirl injector for urea-water solution applications: A computational study}},
  author = {Javier Marco-Gimeno, Katherine J. Asztalos, Chi Young Moon, Christopher F. Powell, [...], View all}},
  year = {2024}},
  journal = {International Journal of Engine Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The co-optimization of in-cylinder combustion and after-treatment technology has become a major aspect in engine design and development, with the goal of meeting the increasingly restrictive emission regulations in the transportation industry. Selective ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/14680874241286206},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1694,
  title = {Feasibility documents as critical structuring objects: An approach to the study of documents in digital research production}},
  author = {Urszula Pawlicka-Deger}},
  year = {2022}},
  journal = {Convergence}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Documents have been increasingly recognised as important objects of investigation in Science and Technology Studies (STS); however, so far, much less attention has been given to the study of documents produced in Digital Humanities. The author proposes ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/13548565221111073},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1695,
  title = {Computational Approaches for Modeling the Multiphysics in Pultrusion Process}},
  author = {P. Carlone, I. Baran, J. H. Hattel, G. S. Palazzo}},
  year = {2013}},
  journal = {Advances in Mechanical Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Pultrusion is a continuous manufacturing process used to produce high strength composite profiles with constant cross section. The mutual interactions between heat transfer, resin flow and cure reaction, variation in the material properties, and stress/...}},
  url = {https://journals.sagepub.com/doi/abs/10.1155/2013/301875},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1696,
  title = {Integrating Autopoiesis and Behavior: An Exploration in Computational Chemo-ethology}},
  author = {Matthew D. Egbert, Ezequiel Di Paolo}},
  year = {2009}},
  journal = {Adaptive Behavior}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {It has been argued that the difference between an autonomous entity and an agent is in the ability of the latter to perform behaviors supplemental to processes of self-maintenance (autopoiesis). Theories have been proposed concerning how such behaviors ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1059712309343821},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1697,
  title = {Assessing the Efficacy of Dynamic Adaptive Planning of Infrastructure: Results from Computational Experiments}},
  author = {Jan H Kwakkel, Warren E Walker, Vincent A W J Marchau}},
  year = {2012}},
  journal = {Environment and Planning B: Planning and Design}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In this paper we assess the efficacy of a dynamic adaptive planning (DAP) approach for guiding the long-term development of infrastructure. The efficacy of the approach is tested on the specific case of airport strategic planning. Utilizing a fast and ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1068/b37151},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1698,
  title = {Modeling Mendel's Laws on Inheritance in Computational Biology and Medical Sciences}},
  author = {Gurmukh Singh Ph.D., Khalid Siddiqui Ph.D., Mankiran Singh, Satpal Singh Ph.D.}},
  year = {2010}},
  journal = {Journal of Educational Technology Systems}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The current research article is based on a simple and practical way of employing the computational power of widely available, versatile software MS Excel 2007 to perform interactive computer simulations for undergraduate/graduate students in biology, ...}},
  url = {https://journals.sagepub.com/doi/abs/10.2190/ET.39.1.d},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1699,
  title = {Selectorecombinative Genetic Algorithm to Relax Computational Complexity of Discrete Network Design Problem}},
  author = {Kyunghwi Jeon, Jong Sung Lee, Satish Ukkusuri, S. Travis Waller}},
  year = {2006}},
  journal = {Transportation Research Record}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A new approach is proposed for relaxing the computational complexity of the user equilibrium discrete network design problem (UE-DNDP), under deterministic traffic conditions, with a solution search procedure based on the selectorecombinative genetic ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0361198106196400111},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1700,
  title = {The implementation of an automated method for solution term-tracking as a basis for symbolic computational dynamics}},
  author = {D I M Forehand, M P Cartmell}},
  year = {2011}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This article proposes that additional mathematical information, inherent and implicit within mathematical models of physical dynamic systems, can be extracted and visualized in a physically meaningful and useful manner as an adjunct to standard analytical ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1243/09544062JMES2473},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1701,
  title = {Computational modeling for climate change: Simulating and visualizing a resilient landscape architecture design approach}},
  author = {Aidan Ackerman, Jonathan Cave, Chien-Yu Lin, Kyle Stillwell}},
  year = {2019}},
  journal = {International Journal of Architectural Computing}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Coastlines are changing, wildfires are raging, cities are getting hotter, and spatial designers are charged with the task of designing to mitigate these unknowns. This research examines computational digital workflows to understand and alleviate the ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1478077119849659},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1702,
  title = {Chapter VIII Computational Technics}},
  author = {Nicholas A. Fattu}},
  year = {1948}},
  journal = {Review of Educational Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.3102/00346543018005485},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1703,
  title = {The Effects of Using Calculators to Reduce the Computational Burden on a Standardized Test of Mathematics Problem Solving}},
  author = {Timothy N. Ansley, Kevin F. Spratt, Robert A. Forsyth}},
  year = {1989}},
  journal = {Educational and Psychological Measurement}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Problem solving is a key skill in mathematics education. Thus, it is important to be able to assess students' proficiencies as problem solvers. The Quantitative Thinking Subtest (Test Q) of the Iowa Tests of Educational Development is a test of problem ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0013164489491031},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1704,
  title = {Computational Fluid Dynamics Simulation of a-v Fistulas: From MRI and Ultrasound Scans to Numeric Evaluation of Hemodynamics}},
  author = {Anders Koustrup Niemann MD, Samuel Thrysoe, Jens Vinge Nygaard, John Michael Hasenkam, [...], View all}},
  year = {2011}},
  journal = {The Journal of Vascular Access}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Purpose A-v anastomosis entails dramatic changes in hemodynamic conditions, which may lead to major alterations to the vessels involved; primarily dilatations and devastating stenoses. Wall shear stress is thought to play a key role in the remodeling of ...}},
  url = {https://journals.sagepub.com/doi/abs/10.5301/JVA.2011.8440},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1705,
  title = {A Computational Study of a Period of Infant Object-Concept Development}},
  author = {Slava Prazdny}},
  year = {1980}},
  journal = {Perception}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Piaget has distinguished a number of distinct stages in the development of the concept of an enduring external object during infancy. I present a theory of a class of behaviours at one of these stages embodied in a working computer program. The behaviour ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1068/p090125},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1706,
  title = {Learning Science and Engineering From Videos and Games: A Randomized Trial of PBS KIDS The Cat in the Hat Knows a Lot About That!}},
  author = {Megan Silander, Todd Grindal, Sarah Nixon Gerard, Tiffany Salone}},
  year = {2025}},
  journal = {Educational Researcher}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Preschool-age children receive little formal instruction regarding science and engineering concepts. Digital media interventions have been effective in supporting young children’s literacy and mathematics skills, but there is little evidence of their ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3102/0013189X251327187},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1707,
  title = {The Use of Ubiquitous Computing/Computational Neuroscience for Distributed Battlefield Management}},
  author = {Michael D. McNeese}},
  year = {2001}},
  journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Military missions are increasingly contingent upon the ‘emergent qualities’ of distributed cognition. Cognition is situated and shared across multiple agents, objects, and environments. The total information surround is evolutionary, chaotic, and presents ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/154193120104500444},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1708,
  title = {Computational studies of air-assisted spray impingement on a flat plate}},
  author = {D-L Chang, C-f F Lee}},
  year = {2003}},
  journal = {International Journal of Engine Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Fuel impingement and film formation are critical issues for the goal of reducing hydrocarbon emissions from automotive gasoline direct injection engines. This computational study compares fuel impingement from a simulated air-assisted spray with ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1243/146808703322743921},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1709,
  title = {Urine-Based Biomarkers for Alzheimer’s Disease Identified Through Coupling Computational and Experimental Methods}},
  author = {Fang Yao, Xiaoyu Hong, Shuiming Li, Yan Zhang, Qing Zhao, Wei Du, Yong Wang, Jiazuan Ni}},
  year = {2018}},
  journal = {Journal of Alzheimer’s Disease}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Alzheimer’s disease (AD) is a chronic neurodegenerative disorder contributing to nearly 70% of dementia cases. However, no diagnostic protein biomarkers are available in urine. In this study, we combined computational and experimental methods to identify ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/JAD-180261},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1710,
  title = {A Generalized Computational Fluid Dynamics Approach for Journal Bearing Performance Prediction}},
  author = {P G Tucker BEng, DPhil, P S Keogh BSc, PhD}},
  year = {1995}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part J: Journal of Engineering Tribology}},
  tipo = {Review article}},
  publisher = {Unknown}},
  abstract = {The use of computational fluid dynamics (CFD) techniques enables performance predictions of bearing designs to be made when the usual operating assumptions of the Reynolds equation Jail to hold. This paper addresses the application of a full three-...}},
  url = {https://journals.sagepub.com/doi/abs/10.1243/PIME_PROC_1995_209_412_02},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1711,
  title = {The Seer Technique: A Non-Computational Procedure for Quickly Estimating Readability Level}},
  author = {Harry Singer}},
  year = {1975}},
  journal = {Journal of Reading Behavior}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A sample-methods scales technique, analogous to Thomdike's procedure for rating handwriting, was used to determine readability levels of eight randomly arrayed paragraphs selected from children's literature covering the range from grades 1 through 6. ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1080/10862967509547144},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1712,
  title = {Computational methodology to support functional vision assessment in premature infants: A viability study}},
  author = {Ricardo Pires Maciel, Bruna Samantha Marchi, Henrique da Silva da Silveira, [...], View all}},
  year = {2024}},
  journal = {NeuroRehabilitation}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {BACKGROUND: Premature newborns have a higher risk of abnormal visual development and visual impairment. OBJECTIVE: To develop a computational methodology to help assess functional vision in premature infants by tracking iris distances. METHODS: This ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/NRE-230193},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1713,
  title = {Computational Design of Copper Ligands with Controlled Metal Chelating, Pharmacokinetics, and Redox Properties for Alzheimer’s Disease}},
  author = {Diego Chaparro, Areli Flores-Gaspar, Jorge Alí-Torres}},
  year = {2020}},
  journal = {Journal of Alzheimer’s Disease}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Background: Redox active metal cations, such as Cu2 +, have been related to induce amyloid plaques formation and oxidative stress, which are two of the key events in the development of Alzheimer’s disease (AD) and others metal promoted neurodegenerative ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/JAD-200911},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1714,
  title = {Modeling, stability analysis and computational aspects of nonlinear fuzzy PID controllers}},
  author = {N.K. Arun, B.M. Mohan}},
  year = {2016}},
  journal = {Journal of Intelligent & Fuzzy Systems}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This paper presents mathematical models of the simplest fuzzy PID controller of Mamdani type. This controller is called the “simplest” as it employs minimum number of fuzzy sets (two on each input variable and four on output variable) while satisfying the ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/JIFS-152626},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1715,
  title = {Investigating political news coverage patterns in Bangladesh during 2013-2022 using computational methods}},
  author = {Md. Sayeed Al-Zaman, Mridha Md. Shiblee Noman}},
  year = {2024}},
  journal = {Newspaper Research Journal}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The political landscape in Bangladesh is complex and multifaceted, with limited exploration in relation to the rapidly growing news industry. This study aims to address this gap by analyzing 15,801 news articles, utilizing trend analysis and topic ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/07395329231221515},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1716,
  title = {Being-towards the social: Mood and orientation to location-based social media, computational things and applications}},
  author = {Leighton Evans}},
  year = {2014}},
  journal = {New Media & Society}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Through an investigation of patterns of use of the location-based social network Foursquare derived from an extensive ethnographic survey of users, this paper focuses on the orientation of users towards location-based social media and mobile computational ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1461444813518183},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1717,
  title = {Two computational formulae for similarity measures on intuitionistic fuzzy sets based on intuitionistic fuzzy equivalencies}},
  author = {Xingxing He, Yingfang Li, Limin Du, Keyun Qin}},
  year = {2019}},
  journal = {Journal of Intelligent & Fuzzy Systems}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The measure of the similarity between intuitionistic fuzzy sets (IFSs) is an important topic in IFSs theory. In this paper, we propose two computational formulae for similarity measures on IFSs based on a quaternary function called intuitionistic fuzzy ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/JIFS-181739},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1718,
  title = {A new approach for association rules mining using computational and artificial intelligence}},
  author = {Fahed Yoseph, Markku Heikkilä}},
  year = {2020}},
  journal = {Journal of Intelligent & Fuzzy Systems}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Market Intelligence is knowledge extracted from numerous data sources, both internal and external, to provide a holistic view of the market and to support decision-making. Association Rules Mining provides powerful data mining techniques for identifying ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/JIFS-200707},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1719,
  title = {Experimental and computational investigation on comparison of micro-scale open rotor and shrouded rotor hovering in ground effect}},
  author = {Han Han, Changle Xiang, Bin Xu, Yong Yu}},
  year = {2020}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Various investigations on open rotor (OR) hovering in-ground effect (IGE) are carried out, but few papers report shrouded rotor (SR) hovering IGE. This paper compares aerodynamic performance and flowfield characteristics of OR and SR hovering IGE by both ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0954410020949292},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1720,
  title = {A Tutorial on Fuzzy Rule-Based Expert Systems (FRBES) Models. 2: Models and Computational Techniques}},
  author = {Nader Vadiee, Mohammad Jamshidi}},
  year = {1993}},
  journal = {Journal of Intelligent & Fuzzy Systems}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Fuzzy rule-based expert systems (FRBES) models, described by a set of fuzzy conditional statements in the canonical form of (If x is Ak then yk is Bk for k = 1, 2, … , r) or a system of conjunctive or disjunctive fuzzy relational equations in the form of (...}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/IFS-1993-1306},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1721,
  title = {Explicit Instruction and Next Generation Science Standards Aligned Classrooms: A Fit or a Split?}},
  author = {William J. Therrien, Sarah K. Benson, Charles A. Hughes, Jared R. Morris}},
  year = {2017}},
  journal = {Learning Disabilities Research & Practice}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The purpose of this article is to discuss the use of explicit instruction in the curriculum area of science where non–explicit approaches (e.g., discovery learning) are often used. While there has been a relative paucity of research on explicit ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1111/ldrp.12137},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1722,
  title = {Understanding Cognitive Strategy With Adaptive Automation in Dual-Task Performance Using Computational Cognitive Models}},
  author = {David B. Kaber, Sang-Hwan Kim}},
  year = {2011}},
  journal = {Journal of Cognitive Engineering and Decision Making}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The objectives of this study were to investigate the effects of advance auditory cuing of control mode changes in an adaptively automated system on human performance and to explain cognitive behaviors at mode changes by using a computational cognitive ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1555343411416442},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1723,
  title = {Effects of presentation modality and duration on children’s strategy use: A study in computational estimation}},
  author = {Svenja Hammerstein, Sebastian Poloczek, Patrick Lösche, Patrick Lemaire, Gerhard Büttner}},
  year = {2021}},
  journal = {Quarterly Journal of Experimental Psychology}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Two experiments were run to determine how presentation modality and duration influence children’s arithmetic performance and strategy selection. Third and fourth graders were asked to find estimates for two-digit addition problems (e.g., 52 + 39). ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/17470218211053309},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1724,
  title = {Reconstruction of a Deformed Tumor Based on Fiducial Marker Registration: A Computational Feasibility Study}},
  author = {Ye Han MS, Emily Oakley BS, Gal Shafirstein DSc, Yoed Rabin DSc, Levent Burak Kara PhD}},
  year = {2018}},
  journal = {Technology in Cancer Research & Treatment}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Interstitial photodynamic therapy has shown promising results in the treatment of locally advanced head and neck cancer. In this therapy, systemic administration of a light-sensitive drug is followed by insertion of multiple laser fibers to illuminate the ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1533034618766792},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1725,
  title = {USING COMPUTATIONAL METHODS TO PERFORM COUNTERFACTUAL ANALYSES OF FORMAL THEORIES}},
  author = {Andrew D. Martin, Kevin M. Quinn}},
  year = {1996}},
  journal = {Rationality and Society}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Recently there has been an increase in the number of researchers who use rational choice models to explain single cases and rare events. Because of the small number of cases under study, these researchers must rely either explicitly or implicitly on ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/104346396008003004},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1726,
  title = {Experimental and computational fluid dynamics modeling of a novel tray humidifier column in humidification dehumidification desalination; evaluation of hydrodynamic and heat transfer characteristics}},
  author = {Taleb Zarei, Reza Hamidi Jahromi, Arash Mohammadi Karachi}},
  year = {2020}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part E: Journal of Process Mechanical Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In this article, a novel tray humidifier column for humidification dehumidification desalination was proposed. The performance of the humidifier column has been investigated with experimental and computational fluid dynamics simulations. The hydrodynamics ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0954408920916580},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1727,
  title = {Calculating Critique: Thinking Outside the Methods Matching Game}},
  author = {J. Samuel Barkin, Laura Sjoberg}},
  year = {2015}},
  journal = {Millennium}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This article suggests to quantitative methodologists that the tools that they use (and often others they do not) are more broadly applicable than is often assumed; to reflexivist researchers that there are many more tools available to their research than ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0305829815576819},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1728,
  title = {Decoding Emotional (In)Congruency: A Computational Approach Toward Ad Placement on YouTube}},
  author = {Taylor Jing Wen, Ching-Hua Chuan, Wanhsiu Sunny Tsai, Jing Yang}},
  year = {2022}},
  journal = {Journal of Interactive Marketing}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This study illuminates the varied emotional mechanisms underlying consumer response to ads paired with emotionally congruent versus incongruent content in different placement positions. This work expands the media planning literature that has narrowly ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/10949968221095546},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1729,
  title = {Beyond Algorithmic Bias: A Socio-Computational Interrogation of the Google Search by Image Algorithm}},
  author = {Orestis Papakyriakopoulos, Arwa M. Mboya}},
  year = {2022}},
  journal = {Social Science Computer Review}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {We perform a socio-computational interrogation of the google search by image algorithm, a main component of the google search engine. We audit the algorithm by presenting it with more than 40 thousands faces of all ages and more than four races and ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/08944393211073169},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1730,
  title = {Human Way-Finding in an Urban Environment: A Performance Analysis of a Computational Process Model}},
  author = {S Gopal, T R Smith}},
  year = {1990}},
  journal = {Environment and Planning A: Economy and Space}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In this paper the characteristics and performance are described of a computational process model (CPM) of human way-finding that is based on psychological models of cognition and experimental data on human way-finding. The model comprises two modules, one ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1068/a220169},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1731,
  title = {Myside Bias, Rational Thinking, and Intelligence}},
  author = {Keith E. Stanovich, Richard F. West, Maggie E. Toplak}},
  year = {2013}},
  journal = {Current Directions in Psychological Science}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Myside bias occurs when people evaluate evidence, generate evidence, and test hypotheses in a manner biased toward their own prior opinions and attitudes. Research across a wide variety of myside bias paradigms has revealed a somewhat surprising finding ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0963721413480174},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1732,
  title = {Building Empathic Agents? Comment on “Computational Modelling of Culture and Affect” by Aylett and Paiva}},
  author = {Toyoaki Nishida}},
  year = {2012}},
  journal = {Emotion Review}},
  tipo = {Article commentary}},
  publisher = {Unknown}},
  abstract = {This comment discusses work by Aylett and Paiva (2012) which describes a synthetic approach to building a virtual world inhabited by synthetic characters where the user can experience subjective culture, that is, the experience of social reality, and ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1754073912439777},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1733,
  title = {Thermal Phenomena in Fiber-reinforced Thermoplastic Tape Winding Process: Computational Simulations and Experimental Validations}},
  author = {Yves M.P. Toso, Paolo Ermanni, Dimos Poulikakos}},
  year = {2004}},
  journal = {Journal of Composite Materials}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This paper presents experimentally validated three-dimensional transient simulations of the thermal phenomena of the tape winding process, as well as a method to determine separately the heat transfer between the hot gas originating from a torch and the ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0021998304038651},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1734,
  title = {A computational procedure for prediction of ballasted track profile degradation under railway traffic loading}},
  author = {K Nguyen, D I Villalmanzo, J M Goicolea, F Gabaldon}},
  year = {2015}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part F: Journal of Rail and Rapid Transit}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A computational procedure is developed in the present paper, allowing the prediction of the ballasted track profile degradation under railway traffic loading. In this procedure, an integration of the short-term and long-term mechanical processes of track ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0954409715615374},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1735,
  title = {Black-Scholes Options Pricing Formula: Confluence of Financial Economics, Mathematics and Computational Science}},
  author = {Vipul K. Singh}},
  year = {2014}},
  journal = {Asia-Pacific Journal of Management Research and Innovation}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This research article aims to describe how the confluence of financial economics, mathematics and computational technologies can prove to be so effective in the practice of financial engineering. As per modern demands, financial derivatives constitute a ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/2319510X14529492},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1736,
  title = {Managers’ Technology-Mediated Communication Competence: A Theoretical Framework}},
  author = {Jonna Koponen, Marianna Turunen, Anne Laajalahti, Saara Julkunen, Brian H. Spitzberg}},
  year = {2025}},
  journal = {Journal of Business and Technical Communication}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Management practices are changing globally due to rapid workplace digitalization. The COVID-19 pandemic has created new demands for management and affected how information and communication technology and communication channels are used in everyday work. ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/10506519251326558},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1737,
  title = {The Half-Empty/Full Glass in Mental Health: A Reference-Dependent Computational Model of Evaluation in Psychopathology}},
  author = {Francesco Rigoli, Cristina Martinelli, Giovanni Pezzulo}},
  year = {2021}},
  journal = {Clinical Psychological Science}},
  tipo = {Review article}},
  publisher = {Unknown}},
  abstract = {Evaluation (the process attributing value to outcomes) underlies “hot” aspects of cognition, such as emotion, affect, and motivation. In several psychopathologies, such as depression and addiction, impairments in evaluation are critical. Contemporary ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/2167702621998344},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1738,
  title = {Small (Poly)Unsaturated Oxygen Containing Ions and Molecules: A Brief Assessment of Their Thermochemistry Based on Computational Chemistry}},
  author = {John L. Holmes, Karl J. Jobst, Johan K. Terlouw}},
  year = {2009}},
  journal = {European Journal of Mass Spectrometry}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The CBS-QB3, CBS-APNO and Gaussian-3 model chemistries have been used to determine the ionic and neutral heats of formation and the adiabatic ionization energies (IEa) derived therefrom, for the ca 30 principal isomers of the C3H2O•+ and the C4H4O•+ ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1255/ejms.959},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1739,
  title = {Rayleigh’s quotient–based damage detection algorithm: Theoretical concepts, computational techniques, and field implementation strategies}},
  author = {Wilfried Njomo Wandji}},
  year = {2017}},
  journal = {Structural Health Monitoring}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This article proposes a Rayleigh’s quotient–based damage detection algorithm. It aims at efficiently revealing nascent structural changes on a given structure with the capability to differentiate between an actual damage and a change in operational ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1475921717691018},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1740,
  title = {The Validation of Computational Prediction Techniques}},
  author = {Andrew P. Worth, Martin D. Barratt, J. Brian Houston}},
  year = {1998}},
  journal = {Alternatives to Laboratory Animals}},
  tipo = {Other}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/026119299802600208},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1741,
  title = {Computational analysis and experimental validation of the friction-stir welding behaviour of Ti—6Al—4V}},
  author = {M Grujicic, G Arakere, B Pandurangan, A Hariharan, B A Cheeseman, C-F Yen, C Fountzoulas}},
  year = {2011}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part B: Journal of Engineering Manufacture}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A fully coupled thermomechanical finite element analysis of the friction-stir welding (FSW) process developed in the authors' previous work is combined with the basic physical metallurgy of Ti—6Al—4V to predict/assess the structural response of FSW ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/09544054JEM2013},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1742,
  title = {Big Data-Driven Public Policy Decisions: Transformation Toward Smart Governance}},
  author = {Md Altab Hossin, Jie Du, Lei Mu, Isaac Owusu Asante}},
  year = {2023}},
  journal = {Sage Open}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Big data analytics (BDA) enhances knowledge and decision-making. Despite its importance, the connection between technical progress and political change is neglected in the administrative process. Most studies focus on e-government, e-governance, and how ...Importance: Big data analytics (BDA) enhances knowledge and decision-making. Despite its importance, the connection between technical progress and political change is neglected in the administrative process. Most studies focus on e-...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/21582440231215123},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1743,
  title = {Computational Fluid Dynamics and Artificial Organs}},
  author = {},
  year = {2001}},
  journal = {The International Journal of Artificial Organs}},
  tipo = {Abstract}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/039139880102400826},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1744,
  title = {Computational modeling of shear deformation and failure of nanoscale hydrated calcium silicate hydrate in cement paste: Calcium silicate hydrate Jennite}},
  author = {John S Rivas Murillo, Ahmed Mohamed, Wayne Hodo, Ram V Mohan, A Rajendran, R Valisetty}},
  year = {2015}},
  journal = {International Journal of Damage Mechanics}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Calcium silicate hydrate Jennite is a molecular structure commonly accepted as a representation of the complex calcium silicate hydrate gel formed during the hydration of typical Portland cement. In this paper, the behavior of nanoscale calcium silicate ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1056789515580184},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1745,
  title = {Measuring Parents’ Perceptions of Programming Education in P-12 Schools: Scale Development and Validation}},
  author = {Siu-Cheung Kong, Robert Kwok-Yiu Li, Ron Chi-Wai Kwok}},
  year = {2018}},
  journal = {Journal of Educational Computing Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Schools around the globe increasingly realized the importance of technology and its application in the education system. To guarantee a successful educational innovation, schools seek out different parties for valuable opinions. Among them, parents are ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0735633118783182},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1746,
  title = {Computational fluid dynamics and experimental study of inter-stage seal clearance on submersible well pump}},
  author = {Xiongfa Gao, Weidong Shi, Ling Zhou, Desheng Zhang, Qihua Zhang}},
  year = {2016}},
  journal = {Advances in Mechanical Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In this article, a typical submersible well pump was investigated to study the effects of inter-stage leakage on the inner flow field and external characteristics. The whole flow field of the model pump with different seal clearances was simulated by ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1687814016632680},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1747,
  title = {Symbolic and Sub-Symbolic Representations in Computational Models of Human Cognition: What Can be Learned from Biology?}},
  author = {Troy D. Kelley}},
  year = {2003}},
  journal = {Theory & Psychology}},
  tipo = {Other}},
  publisher = {Unknown}},
  abstract = {The debate over symbolic versus sub-symbolic representations of human cognition has been continuing for thirty years, with little indication of a resolution. The argument is this: Does the human cognitive system use symbols as a representation of ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0959354303136005},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1748,
  title = {An empirical basis for the use of design patterns by architects in parametric design}},
  author = {Rongrong Yu, John S Gero}},
  year = {2016}},
  journal = {International Journal of Architectural Computing}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This article presents the results from exploring the impact of using a parametric design tool on designers’ behavior in terms of using design patterns in the early conceptual development stage of designing. It is based on an empirical cognitive study in ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1478077116663351},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1749,
  title = {Effects of nozzle hole size and rail pressure on diesel spray and mixture characteristics under similar injection rate profile – experimental, computational and analytical studies under non-evaporating spray condition}},
  author = {Safiullah, Keiya Nishida, Youichi Ogata, Tetsuya Oda, Katsuyuki Ohsawa}},
  year = {2021}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In the present work, effects of nozzle hole size and rail pressure under non-evaporating spray condition are demonstrated. Three single hole injectors with the bore size of 0.101, 0.122, and 0.133 mm are experimented with injection pressures of 140, 45, ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/09544070211022099},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1750,
  title = {Utilization of synthetic system intelligence as a new industrial asset}},
  author = {Imre Horváth}},
  year = {2023}},
  journal = {Journal of Integrated Design and Process Science}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {System knowledge and reasoning mechanisms are essential means for intellectualization of cyber-physical systems (CPSs). As enablers of system intelligence, they make such systems able to solve application problems and to maintain their efficient ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/JID-220024},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1751,
  title = {A computational fluid dynamics investigation of subsonic wing designs for unmanned aerial vehicle application}},
  author = {Z Siddiqi, JW Lee}},
  year = {2019}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The wing of an unmanned aerial vehicle, RQ-7 Shadow, is modified to study the changes in the aerodynamics of the wing. The main focus is to investigate the effects of changing the components of wing design when the aircraft climbs and accelerates. These ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0954410019852553},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1752,
  title = {Computational Biomechanics to Simulate the Femoropopliteal Intersection During Knee Flexion: A Preliminary Study}},
  author = {Nicolas Diehm MD, Sangmun Sin MSc, Hanno Hoppe MD3; Iris Baumgartner, MD, Philippe Büchler PhD}},
  year = {2011}},
  journal = {Journal of Endovascular Therapy}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Purpose To assess if finite element (FE) models can be used to predict deformation of the femoropopliteal segment during knee flexion. Methods Magnetic resonance angiography (MRA) images were acquiredonthe lower limbs of 8 healthy volunteers (5 men; mean age ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1583/10-3337.1},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1753,
  title = {Computational study on the effect of thermal deformation of myocardium on lesion formation during radiofrequency ablation}},
  author = {Pei Xin Niu, Xiang Xiang Wang, Jing Jin Shen, Xiao Xiao Jin, Zhen Yu Zhou}},
  year = {2025}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part H}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Radiofrequency (RF) catheter ablation treats cardiac diseases by inducing thermal lesion of cardiac tissues through radiofrequency energy operating at around 500 kHz. The electromagnetic wavelength is significantly longer than the size of the ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/09544119251321131},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1754,
  title = {Measuring posture change to detect emotional conditions for workers: A scoping review}},
  author = {Jihye Do, Ingyu Yoo}},
  year = {2022}},
  journal = {WORK}},
  tipo = {Review article}},
  publisher = {Unknown}},
  abstract = {BACKGROUND: The emotional management of workers can not only increase the efficiency of work, but also contribute to the improvement of the productivity of a company. OBJECTIVE: This scoping review surveyed the literature to identify the relationship between ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/WOR-210496},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1755,
  title = {How does thinking relate to tool making?}},
  author = {Lambros Malafouris}},
  year = {2020}},
  journal = {Adaptive Behavior}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {How the boundaries of the mind should be drawn with respect to action and the material world is a core research question that cognitive archaeology shares with contemporary cognitive sciences. The study of hominin technical thinking, as in the case of ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1059712320950539},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1756,
  title = {CAD Scripting and Visual Programming Languages for Implementing Computational Design Concepts: A Comparison from a Pedagogical Point of View}},
  author = {Gabriela Celani, Carlos Eduardo Verzola Vaz}},
  year = {2012}},
  journal = {International Journal of Architectural Computing}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This paper compares the use of scripting languages and visual programming languages for teaching computational design concepts to novice and advanced architecture students. Both systems are described and discussed in terms of the representation methods ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1260/1478-0771.10.1.121},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1757,
  title = {Computational model to design plastic multi-layer films for food packaging to assure a shelf life at the best cost}},
  author = {María del Pilar Noriega, Omar Estrada, Iván López}},
  year = {2013}},
  journal = {Journal of Plastic Film & Sheeting}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A computational model to design plastic food packaging is proposed. The model minimizes the cost of the multi-layer structure satisfying the specific product requirements, using a heuristic optimization algorithm. The product requirements are defined by ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/8756087913484920},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1758,
  title = {Global Thinking, or the Utility of Trivia}},
  author = {John S. Harris}},
  year = {2001}},
  journal = {Journal of Technical Writing and Communication}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The constant emphasis on specialization produces university graduates who do not or cannot look at problems broadly. As a result, engineers, scientists and executives—indeed graduates in all fields—including the supposedly broad-based humanities—often ...}},
  url = {https://journals.sagepub.com/doi/abs/10.2190/J570-9MKA-7BHN-ULN9},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1759,
  title = {Preservation of femoral and tibial coronal alignment to improve biomechanical effects of medial unicompartment knee arthroplasty: Computational study}},
  author = {Kyoung-Tak Kang, Juhyun Son, Sae Kwang Kwon, Oh-Ryong Kwon, Yong-Gon Koh}},
  year = {2018}},
  journal = {Bio-Medical Materials and Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {BACKGROUND: Medial unicompartmental knee arthroplasty (UKA) could be concerned with wear of the cartilage or the wear in the polyethylene (PE) insert. Mechanical alignment determines the biomechanical effect in the long term. However, previous ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/BME-181015},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1760,
  title = {Toward Just-in-Time Data Communications over Shared Networks and Computational Resources on Massive Client Environment}},
  author = {Hirochika Asai, Yusuke Doi, Ryokichi Onishi}},
  year = {2021}},
  journal = {Transportation Research Record}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Intelligent driving has been benefiting from advances in automotive big data analysis and on-demand data communications, where efficient vehicle-to-cloud communication is a key technology able to collect a huge amount of data from vehicles. However, as ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/03611981211006731},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1761,
  title = {Advances in Validation of Computational Mechanics Models}},
  author = {Erwin Hack, George Lampeas}},
  year = {2016}},
  journal = {The Journal of Strain Analysis for Engineering Design}},
  tipo = {Editorial}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0309324715625824},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1762,
  title = {Frequency and time domain analysis of surface acoustic wave propagation on a piezoelectric gallium arsenide substrate: A computational insight}},
  author = {Claudio Maruccio, Marco Scigliuzzo, Silvia Rizzato, Pasquale Scarlino, Giuseppe Quaranta, [...], View all}},
  year = {2018}},
  journal = {Journal of Intelligent Material Systems and Structures}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A computational study of the electromechanical response of micro-structure engineered two port surface acoustic wave delay lines on gallium arsenide is presented. The influence on the results of geometrical, material, and mesh parameters is also ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1045389X18803461},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1763,
  title = {Imagery, Regressive Thinking, and Verbal Performance in Internal Monologue}},
  author = {Robert Hogenraad, Etienne Orianne}},
  year = {1985}},
  journal = {Imagination, Cognition and Personality}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In order to analyze a free-associative discourse–internal monologues obtained by the method of thinking aloud–, two analyses are offered, descriptive and deductive. In the descriptive analysis, an Imagery Dictionary and a Regressive Imagery Dictionary, ...}},
  url = {https://journals.sagepub.com/doi/abs/10.2190/8DB8-ELNU-FCDY-ENMR},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1764,
  title = {On Human Beings, Computers and Representational-Computational vs Hermeneutic-Dialogical Approaches to Human Cognition and Communication}},
  author = {Ragnar Rommetveit}},
  year = {1998}},
  journal = {Culture & Psychology}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Current representational-computational approaches to human cognition and communication, it is argued, represent natural-scientifically coached ramifications of an analyticrationalist philosophical tradition concerned with formal features D of 'pure' and ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1354067X9800400204},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1765,
  title = {Epistemological aspects of computational simulations and their approach through educational simulations in high school}},
  author = {María Eugenia Seoane, Ileana M Greca, Irene Arriassecq}},
  year = {2020}},
  journal = {SIMULATION}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Computer simulations are currently used in diverse scientific disciplines as well as in science teaching. The simulations proposed for physics teaching are designed for specific purposes and allow studying natural phenomena through exploration and/or ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0037549720930084},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1766,
  title = {Computational Modeling of Signaling Pathways Mediating Cell Cycle Checkpoint Control and Apoptotic Responses to Ionizing Radiation-Induced DNA Damage}},
  author = {Yuchao Zhao, In Chio Lou, Rory B. Conolly}},
  year = {2011}},
  journal = {Dose-Response}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The shape of dose response of ionizing radiation (IR) induced cancer at low dose region, either linear non-threshold or J-shaped, has been a debate for a long time. This dose response relationship can be influenced by built-in capabilities of cells that ...}},
  url = {https://journals.sagepub.com/doi/abs/10.2203/dose-response.11-021.Zhao},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1767,
  title = {The Lens Model: Computational Procedures and Applications}},
  author = {Don Beal, John S. Gillis, Tom Stewart}},
  year = {1978}},
  journal = {Perceptual and Motor Skills}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This paper is a general introduction to the lens model, written primarily for individuals in a variety of applied areas (Clinical Psychology, Nursing, Public Administration, Clinical Pharmacology, and Education). First the background issues of Social ...}},
  url = {https://journals.sagepub.com/doi/abs/10.2466/pms.1978.46.1.3},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1768,
  title = {Personalized Computational Models of Tissue-Rearrangement in the Scalp Predict the Mechanical Stress Signature of Rotation Flaps}},
  author = {Taeksang Lee PhD, Sergey Y. Turin MD, Casey Stowers BS, Arun K. Gosain MD, Adrian Buganza Tepole PhD}},
  year = {2020}},
  journal = {The Cleft Palate Craniofacial Journal}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Objective: To elucidate the mechanics of scalp rotation flaps through 3D imaging and computational modeling. Excessive tension near a wound or sutured region can delay wound healing or trigger complications. Measuring tension in the operating room is ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1055665620954094},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1769,
  title = {Improvement of Portable Concrete Barrier Design Using Computational Mechanics}},
  author = {Guido Bonin, Giuseppe Cantisani, Giuseppe Loprencipe, Alessandro Ranzo}},
  year = {2006}},
  journal = {Transportation Research Record}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Concrete safety barriers have been employed broadly in Italy since the 1980s, particularly on highways and freeways. Safety barrier homologation and design standards have not yet precisely determined specific fields of application or modality of ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0361198106198400101},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1770,
  title = {Using computational fluid dynamics to evaluate a novel venous cannula (Smart canula ®) for use in cardiopulmonary bypass operating procedures}},
  author = {D. Jegger, S. Sundaram, K. Shah, I. Mallabiabarrena, G. Mucciolo, L.K. von Segesser}},
  year = {2007}},
  journal = {Perfusion}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Peripheral access cardiopulmonary bypass (CPB) is initiated with percutaneous cannulae (CTRL) and venous drainage is often impeded due to smaller vessel and cannula size. A new cannula (Smartcanula ®, SC) was developed which can change shape in situ and, ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0267659107083657},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1771,
  title = {Visualizing Large-Scale Data in Educational, Behavioral, Psychometrical and Social Sciences: Utilities and Design Patterns of the SEER Computational and Graphical Statistics}},
  author = {Christopher WT Chiu, Peter Pashley, Marilyn Seastrom, Peggy Carr}},
  year = {2005}},
  journal = {Information Visualization}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This paper introduces a graphical method SEE Repeated-measure data (SEER) to visually analyze data commonly collected in large-scale surveys, market research, biostatistics, and educational and psychological measurement. Many researchers in these ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1057/palgrave.ivs.9500105},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1772,
  title = {Promising Practices for Supporting Students with Disabilities through Writing in Science}},
  author = {Lauren W. Collins, Lori Fulton}},
  year = {2017}},
  journal = {TEACHING Exceptional Children}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0040059916670629},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1773,
  title = {Computational methods for the detection of facial palsy}},
  author = {Noura Afifi, Joachim Diederich, Tamer Shanableh}},
  year = {2006}},
  journal = {Journal of Telemedicine and Telecare}},
  tipo = {Other}},
  publisher = {Unknown}},
  abstract = {We are developing a telemedicine application which offers automated diagnosis of facial (Bell's) palsy through a Web service. We used a test data set of 43 images of facial palsy patients and 44 normal people to develop the automatic recognition ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1258/135763306779380129},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1774,
  title = {Optimization of a high-speed direct-injection diesel engine at low-load operation using computational fluid dynamics with detailed chemistry and a multi-objective genetic algorithm}},
  author = {H-W Ge, Y Shi, R D Reitz, W Willems}},
  year = {2010}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A passenger car high-speed direct-injection diesel engine operating at low-load conditions in the modulated kinetic combustion mode was optimized using a multi-dimensional computational fluid dynamics code and a multi-objective genetic algorithm. ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1243/09544070JAUTO1351},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1775,
  title = {Restructuring Teacher Education Programs for Higher-Order Thinking Skills}},
  author = {David S. Martin}},
  year = {1989}},
  journal = {Journal of Teacher Education}},
  tipo = {Other}},
  publisher = {Unknown}},
  abstract = {The rising emphasis in schools on the explicit teaching of higher-level cogni tive skills has important implications for preservice teacher education programs. Action on this topic by teacher educators is important now for establishing a strong ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/002248718904000301},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1776,
  title = {Computational intelligence identifies alkaline phosphatase (ALP), alpha-fetoprotein (AFP), and hemoglobin levels as most predictive survival factors for hepatocellular carcinoma}},
  author = {Davide Chicco, Luca Oneto}},
  year = {2021}},
  journal = {Health Informatics Journal}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Liver cancer kills approximately 800 thousand people annually worldwide, and its most common subtype is hepatocellular carcinoma (HCC), which usually affects people with cirrhosis. Predicting survival of patients with HCC remains an important challenge, ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1460458220984205},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1777,
  title = {Combining internal and external evaluations within a multilevel evaluation framework: Computational text analysis of lessons from the Asian Development Bank}},
  author = {Nihit Goyal, Michael Howlett}},
  year = {2019}},
  journal = {Evaluation}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Although the literature on evaluation has theorized about the distinction between internal and external evaluation, hardly any research has compared them empirically. This article examines whether the lessons of internal evaluations differed from those of ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1356389019827035},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1778,
  title = {Outcome-based Education: An Outline}},
  author = {N. J. Rao}},
  year = {2020}},
  journal = {Higher Education for the Future}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Things we can do because of learning are called outcomes of learning. Outcome based education (OBE) was propounded by William Spady in the 90s to bring the focus of formal education to what the students learn rather than what they were taught. OBE is a ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/2347631119886418},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1779,
  title = {Practising difference across geography: A transdisciplinary and Deleuzian approach to intradisciplinary thinking}},
  author = {Heather J Miles}},
  year = {2023}},
  journal = {Environment and Planning F}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The quality of intradisciplinary thinking during research design is crucial not only for alleviating multifaceted problems – such as global environmental change – but potentially to avoid worsening them. This article builds on the emphasis placed on ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/26349825231200607},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1780,
  title = {Behavior of sandwich structures with 3D-printed auxetic and non-auxetic cores under low velocity impact: Experimental and computational analysis}},
  author = {Marwa Allouch, Hana Mellouli, Hanen Mallek, Mondher Wali, Fakhreddine Dammak}},
  year = {2024}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In recent years, impact-resistant structures are highly sought after in various fields such automotive and aerospace applications as they proved notable performances, garnering significant success. The aim of this study is to assess the behavior of the 3D-...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/09544062241277310},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1781,
  title = {Semi-analytical and computational investigation of different fibrous structures affecting the performance of fibrous media}},
  author = {Liang Zhang, JiaWei Zhou, Bo Zhang, Wei Gong}},
  year = {2019}},
  journal = {Science Progress}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Three arrangement types of fibrous media have been established including “Mesh” (layered distribution), “Para” (unidirectional distribution), and “Nurbs” (random distribution), with fiber diameters ranging from 5 to 7 µm and solid volume fractions ranging ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0036850419874231},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1782,
  title = {Revealing What's Important to the User: Value-Focused Thinking}},
  author = {Janet E. Miller, Chris McGee}},
  year = {2005}},
  journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting}},
  tipo = {Review article}},
  publisher = {Unknown}},
  abstract = {Despite the importance of software interfaces, trade-offs must be made during development. Value Focused Thinking (VFT) provides an objective methodology that is well suited for handling multi-objective problems such as interface design issues. ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/154193120504900513},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1783,
  title = {Computational investigations of mechanical failures of internal plate fixation}},
  author = {G Chen, B Schmutz, M Wullschleger, M J Pearcy, M A Schuetz}},
  year = {2009}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This paper investigated the biomechanics of two clinical cases of bone fracture treatments. Both fractures were treated with the same locking compression plate but with different numbers of screws as well as different plate materials. The fracture ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1243/09544119JEIM670},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1784,
  title = {A Computational Approach to Visual Recognition of Arm Movements}},
  author = {Lucia Vaina, Youcef Bennour}},
  year = {1985}},
  journal = {Perceptual and Motor Skills}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A representation for the visual recognition of skilled arm movements is proposed that lies within Mart and Vaina's (1982) three-dimensional model representation for shape movements. Algorithms for segmenting arm movements into pieces are proposed. It is ...}},
  url = {https://journals.sagepub.com/doi/abs/10.2466/pms.1985.60.1.203},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1785,
  title = {What Happens After Nomination? Evaluating the Probability of Gifted Identification With the Torrance Test of Creative Thinking}},
  author = {Lindsay Ellis Lee, Anne N. Rinn, Karen E. Rambo-Hernandez}},
  year = {2024}},
  journal = {Gifted Child Quarterly}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The Torrance Test of Creative Thinking (TTCT) is the most widely used norm-referenced creativity test used in gifted identification. Although commonly used for identifying talent, little is known about how creativity tests, like the TTCT-Figural, ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/00169862231222886},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1786,
  title = {A computational fluid dynamics study of the influence of sleeper shape and ballast depth on ballast flight during passage of a simplified train}},
  author = {Lee Pardoe, William Powrie, Zhiwei Hu}},
  year = {2024}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part F: Journal of Rail and Rapid Transit}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The paper assesses the effect on the air flow regime underneath a simplified high-speed train of changing the ballast depth and the sleeper shape, with regard to its potential for causing ballast flight or pickup. The study was carried out numerically ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/09544097231226148},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1787,
  title = {Politics Go “Viral”: A Computational Text Analysis of the Public Attribution and Attitude Regarding the COVID-19 Crisis and Governmental Responses on Twitter}},
  author = {Weilu Zhang MA, Lingshu Hu PhD, Jihye Park MA}},
  year = {2022}},
  journal = {Social Science Computer Review}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The U.S. confronts an unprecedented public health crisis, the COVID-19 pandemic, in the presidential election year in 2020. In such a compound situation, a real-time dynamic examination of how the general public ascribe the crisis responsibilities taking ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/08944393211053743},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1788,
  title = {Computational Fluid Dynamics without Complex Mathematics: The Advantages for Thermofluids Education}},
  author = {J G L Aston BSc, MSc, PhD}},
  year = {1991}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {With the vastly increased capabilities of computers within the past decade the teaching of thermodynamics and fluid dynamics must evolve to encompass more treatment of numerical methods than before. Accepting that CFD will occupy a significant portion of ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1243/PIME_PROC_1991_205_004_02},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1789,
  title = {Exploring digital literacy in the era of digital civilization: A framework for college students in China}},
  author = {Danning Wu}},
  year = {2024}},
  journal = {Information Services and Use}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Cultivating the digital literacy of all citizens, especially for college students, would contribute to the construction of a ‘Learning Society’ where everybody loves learning and would offer a powerful impetus for building a modern country. First, this ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/ISU-230199},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1790,
  title = {Experimental and computational study comparing conventional diesel injectors and diverging group hole nozzle injectors in a high temperature pressure vessel and a heavy-duty diesel engine}},
  author = {Chaitanya Kavuri, Chad Koci, Jon Anders, Kenth Svensson, Russ Fitzgerald, Glen Martin, [...], View all}},
  year = {2022}},
  journal = {International Journal of Engine Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Group hole nozzles (GHN) are injector tips with clusters of small holes used in place of a single large hole. In this work, GHN with two holes per cluster and a diverging angle of 15° between the holes were studied using experiments and computational ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/14680874221083371},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1791,
  title = {Computational Investigation of the Effective Thermal Conductivity of Interpenetrating Network Composites}},
  author = {Hongtao Zhang, Yu Zeng, Hongyan Zhang, Fangfang Guo}},
  year = {2009}},
  journal = {Journal of Composite Materials}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In this article, the effective thermal conductivity of composites with a network structure as reinforcement was calculated using finite element method (FEM), and a tetrakaidecahedron configuration was employed to represent the network structure. A ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0021998309351606},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1792,
  title = {Thinking About the Mind}},
  author = {Arnold Goldberg}},
  year = {2003}},
  journal = {Journal of the American Psychoanalytic Association}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/00030651030510030901},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1793,
  title = {(Re)Conceptualizing STEM Learning and Teaching for Young Children With Disabilities}},
  author = {Hsiu-Wen Yang PhD, Megan Vinh PhD, Elica Sharifnia PhD, Jessica Amsbary PhD, [...], View all}},
  year = {2024}},
  journal = {Topics in Early Childhood Special Education}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Centering equity in our work eliminates disparities and promotes the learning and development of ALL children. Although there has been increasing focus on supporting science, technology, engineering, and mathematics (STEM) in early childhood, many ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/02711214241288209},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1794,
  title = {Book Review: Field Analysis: Experimental and Computational Methods}},
  author = {P. Hammond}},
  year = {1967}},
  journal = {International Journal of Electrical Engineering & Education}},
  tipo = {Book review}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/002072096700500255},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1795,
  title = {Computational investigation of the effect of up-armouring on the reduction in occupant injury or fatality in a prototypical high-mobility multi-purpose wheeled vehicle subjected to mine blast}},
  author = {M Grujicic, G Arakere, W C Bell, I Haque}},
  year = {2009}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A comprehensive finite-element-based computational investigation of the ability of different measures (e.g. up-armouring, seat cushion, or seat belt restraint system) to protect the occupant(s) of a prototypical high-mobility multi-purpose wheeled ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1243/09544070JAUTO1170},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1796,
  title = {A benchmark strain gradient elasticity solution in two-dimensions for verifying computational approaches by means of the finite element method}},
  author = {Navid Shekarchizadeh, Bilen Emek Abali, Alberto Maria Bersani}},
  year = {2022}},
  journal = {Mathematics and Mechanics of Solids}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In elasticity, microstructure-related deviations may be modeled by strain gradient elasticity. For so-called metamaterials, different implementations are possible for solving strain gradient elasticity problems numerically. Analytical solutions of simple ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/10812865221114336},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1797,
  title = {Aligning English Language Proficiency Standards With Content Standards: Shared Opportunity and Responsibility Across English Learner Education and Content Areas}},
  author = {Okhee Lee}},
  year = {2019}},
  journal = {Educational Researcher}},
  tipo = {Review article}},
  publisher = {Unknown}},
  abstract = {The Every Student Succeeds Act of 2015 mandates that English language proficiency (ELP) standards align with content standards. As the fast-growing population of English learners (ELs) is expected to meet college- and career-ready content standards, the ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3102/0013189X19872497},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1798,
  title = {Invited Review: Development of a one-dimensional computational fluid dynamics modeling approach to predict cycle-to-cycle variability in spark-ignition engines based on physical understanding acquired from large-eddy simulation}},
  author = {Stéphane Richard, Alessio Dulbecco, Christian Angelberger, Karine Truffin}},
  year = {2014}},
  journal = {International Journal of Engine Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In order to satisfy emission standards and CO2 targets, spark-ignition engines are designed to operate with high dilution rates, compression ratios and boost levels, thus increasing the propensity for unstable combustion. Therefore it is important to ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1468087414560592},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1799,
  title = {Voice Matters for Viksit Bharat: Dr. Ambedkar’s Visions Realized Through NEP 2020}},
  author = {Deepak Kumar Behera, Satyaki Paul}},
  year = {2025}},
  journal = {The Oriental Anthropologist}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Babasaheb Dr. Bhimrao Ramji Ambedkar was a true educationist which is evident through his eruditeness and pursuit of higher education in the early twentieth century. This was a time when receiving an array of degrees from abroad was unheard of by most ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0972558X251317483},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1800,
  title = {Computational investigation of ammonia-hydrogen peroxide blends in HCCI engine mode}},
  author = {Omar Shafiq, Efstathios-Al. Tingas}},
  year = {2022}},
  journal = {International Journal of Engine Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The potential use of hydrogen peroxide as an ignition promoter to enable the use of ammonia in compression ignition engines is explored in the current study. A simplified numerical HCCI engine model within the Chemkin Pro suite is employed. The numerical ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/14680874221117686},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1801,
  title = {Singapore teachers’ attitudes towards the use of information and communication technologies in physical education}},
  author = {Nien Xiang Tou, Ying Hwa Kee, Koon Teck Koh, Martin Camiré, Jia Yi Chow}},
  year = {2019}},
  journal = {European Physical Education Review}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The purpose of the present study was to examine and compare Singaporean physical education teachers’ attitudes towards information and communication technologies in physical education across different demographic groups that included gender, age, teaching ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1356336X19869734},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1802,
  title = {A combined experimental and computational investigation of an inducer’s characteristics with gas-water two-phase flow}},
  author = {Jie Chen, Yong Wang, Houlin Liu, Lilin Lv, Yanhong Mao, Linglin Jiang}},
  year = {2024}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The objective of the paper is to investigate the internal flow characteristics inside the inducer under the gas-water condition using a combination of experiment and numerical simulations. Gas-water mixing is achieved by an annular gas mixing device and ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/09576509241240012},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1803,
  title = {Rethinking the MS brain: Synaptic loss and computational modelling of brain networks}},
  author = {Massimiliano Di Filippo, Andrea Mancini}},
  year = {2022}},
  journal = {Multiple Sclerosis Journal}},
  tipo = {Editorial}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/13524585221124307},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1804,
  title = {The History of the Theory of Structures: From Arch Analysis to Computational Mechanics}},
  author = {Karl-Eugen Kurrer}},
  year = {2008}},
  journal = {International Journal of Space Structures}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.1260/026635108786261018},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1805,
  title = {Autonomous Flight at Low Altitude Using Light Sensors and Little Computational Power}},
  author = {Jean-Christophe Zufferey, Antoine Beyeler, Dario Floreano}},
  year = {2010}},
  journal = {International Journal of Micro Air Vehicles}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The ability to fly at low altitude while actively avoiding collisions with the terrain and objects such as trees and buildings is a great challenge for small unmanned aircraft. This paper builds on top of a control strategy called optiPilot whereby a ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1260/1756-8293.2.2.107},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1806,
  title = {Book Review: Introduction of Computational Communication by Zhang Lun, Wang Chengjun, and Xu Xiaoke}},
  author = {Dani Fadillah, Luo Zhenglin, Bai Long}},
  year = {2022}},
  journal = {Global Media and China}},
  tipo = {Book review}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/20594364221081327},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1807,
  title = {Computational parasites and hydropower: A political ecology of Bitcoin mining on the Columbia River}},
  author = {Nick Lally, Kelly Kay, Jim Thatcher}},
  year = {2019}},
  journal = {Environment and Planning E: Nature and Space}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Over the past three years, the dams of Chelan County, Washington, its watershed and fish, the electrical grid and the laborers who maintain it, and cleared land with warehouses filled with computers, have all been enrolled as part of the decentralized ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/2514848619867608},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1808,
  title = {Computational modeling of carbon nanofibers reinforced composites: A comparative study}},
  author = {Pei-Liang Bian, Hai Qing}},
  year = {2021}},
  journal = {Journal of Composite Materials}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The carbon nanotubes/nanofibers reinforced composites (CNRC) show great mechanical properties. There are several methods to simulate the mechanical properties of composites. Among the modeling techniques, embedded region (ER) shows the possibility for ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0021998320987893},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1809,
  title = {Antibacterial Profiling of Zanthoxylum armatum Extracts: A Comprehensive Computational and Experimental Study}},
  author = {Mamuna Mukhtar, Haris Ahmed Khan, Shumaila Naz}},
  year = {2024}},
  journal = {Natural Product Communications}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Objectives The current study was conducted to evaluate the antibacterial potential of leaf and fruit extracts of Zanthoxylum armatum against two pathogenic bacterial isolates, Staphylococcus aureus and Staphylococcus epidermidis. Methods Twelve commercially ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1934578X241237911},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1810,
  title = {Computational intelligence: The experts speak, by David B. Fogel and Charles J. Robinson, eds.}},
  author = {Vladik Kreinovich}},
  year = {2004}},
  journal = {Journal of Intelligent & Fuzzy Systems}},
  tipo = {Book review}},
  publisher = {Unknown}},
  abstract = {},
  url = {https://journals.sagepub.com/doi/abs/10.3233/IFS-2004-00228},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1811,
  title = {Revised Structure by Computational Methods for a Coumarin Isolated from Zanthoxylum Rhoifolium (Rutaceae)}},
  author = {Augusto Rivera, Jaime Rios-Motta}},
  year = {2008}},
  journal = {Natural Product Communications}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {On the basis of theoretical methods using DFT/B3LYP/6-31G* geometries and HF/6-311+G** GIAO NMR predictions, the 13C NMR spectroscopic data of a coumarin from Zanthoxylum rhoifolium was reassigned. The structure was revised to be 4-methoxy-3-(3-methylbut-...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1934578X0800300306},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1812,
  title = {Computational Model of Recognition-Primed Decisions (RPD): Improving Realism in Computer-Generated Forces (CGF)}},
  author = {Robert J. B. Hutton, Walter Warwick Ph.D., Terry Stanard Ph.D, Patricia L. McDermott Ph.D, Stacey McIlwaine Ph.D}},
  year = {2001}},
  journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Modeling an intelligent adversary has provided great challenges to simulated training realism. Traditional approaches to modeling have relied on rule-based and analytical decision-making models in an attempt to optimize the decision making of an ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/154193120104502607},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1813,
  title = {Dynamic Bertrand and Cournot competition: Asymptotic and computational analysis of product differentiation}},
  author = {Andrew Ledvina, Ronnie Sircar}},
  year = {2012}},
  journal = {Risk and Decision Analysis}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {We study continuous time oligopolies in which a small number of firms producing similar goods compete with one another by setting prices or quantities. We study a deterministic version of the problem using an asymptotic expansion of the relevant HJB ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/RDA-2011-0057},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1814,
  title = {Embracing Computational Approaches to Social Media Research: Implications for Theory and Praxis in Sport Communication}},
  author = {Brandon C. Boatwright}},
  year = {2023}},
  journal = {Communication & Sport}},
  tipo = {Editorial}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/21674795231196429},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1815,
  title = {Parametric investigation of a large two-stroke marine high-pressure direct injection engine by using computational fluid dynamics method}},
  author = {Renyou Yang, Gerasimos Theotokatos, Dracos Vassalos}},
  year = {2020}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part M: Journal of Engineering for the Maritime Environment}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This study aims at the parametric investigation of the gas injection system settings of a large marine two-stroke dual fuel engine by using a developed and customized CFD method in the ANSYS Fluent software. The investigated engine injection system ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1475090219895639},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1816,
  title = {A new multiscale computational method for electromechanically coupled analysis of heterogeneous piezoelectric composites}},
  author = {Jun Lv, Hongwu Zhang, Xiaowei Gao, Yi Huang}},
  year = {2014}},
  journal = {Journal of Intelligent Material Systems and Structures}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This article is concerned with the electromechanically coupled multiscale behaviors of the heterogeneous piezoelectric materials, which consist of periodic or non-periodic distributed microstructures. A multiscale framework based on the extended ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1045389X14529030},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1817,
  title = {Experimentally quantified and computational anisotropic damage rules for flax fabric composites}},
  author = {SM Panamoottil, R Das, K Jayaraman}},
  year = {2016}},
  journal = {International Journal of Damage Mechanics}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Continuum damage mechanics models have been used for predicting failure in composites for several years now. However, their application to natural fibre composites is quite recent. In this work, an approach for evaluating damage processes in natural fibre ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1056789516663614},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1818,
  title = {Artificial intelligence in the educational stages from kindergarten to university: A systematic review of Arab studies from 2010 to 2023}},
  author = {Basant A Alakabawy}},
  year = {2024}},
  journal = {E-Learning and Digital Media}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Artificial Intelligence (AI) is increasingly being used at different stages of teaching and learning, from kindergarten to university education, to enhance the learning and development of learners. Research has proven that AI can perform multiple roles ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/20427530241276140},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1819,
  title = {TetraScript: A Responsive Pavilion, from Generative Design to Automation}},
  author = {Gonçalo Castro Henriques}},
  year = {2012}},
  journal = {International Journal of Architectural Computing}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This research is part of a broader investigation into the use of digital technologies in the Architecture, Engineering and Construction (AEC) sector. The intention is to improve the ability of buildings to respond to context by proposing a skylight system ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1260/1478-0771.10.1.87},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1820,
  title = {Implementation and Computational Issues for Combined Models of Location, Destination, Mode, and Route Choice}},
  author = {D E Boyce, K S Chon, Y J Lee, K T Lin, L J LeBlanc}},
  year = {1983}},
  journal = {Environment and Planning A: Economy and Space}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A unified approach to deriving models of urban location, destination, mode, and route choice is illustrated, and an algorithm based on Evans's approach and the Lagrange multiplier procedure is proposed. By examining derivatives of the Lagrangian function, ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1068/a151219},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1821,
  title = {Discursive framing and organizational venues: mechanisms of artificial intelligence policy adoption}},
  author = {Frans af Malmborg, Jarle Trondal}},
  year = {2021}},
  journal = {International Review of Administrative Sciences}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The purpose of this article is twofold: to theoretically assess ideational and organizational explanatory factors in the adoption of artificial intelligence policies; and to examine the extent to which the European Union has managed to facilitate a ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/00208523211007533},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1822,
  title = {Attitudes Matter: Examining How Teaching Strategies for Attitudinal Change Help Adults Value Reflection and Calibrate Their Reflective Thinking}},
  author = {Kevin M. Roessger}},
  year = {2023}},
  journal = {Adult Education Quarterly}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Developing adults’ reflective thinking habits is an aim of adult education, but the best way to do it has been overlooked. Common strategies communicate the skills and knowledge needed to reflect while providing practice opportunities. Yet research ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/07417136231165007},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1823,
  title = {Rheolytic effects of left main mid-shaft/distal stenting: a computational flow dynamic analysis}},
  author = {Gianluca Rigatelli, Marco Zuin, Fabio Dell’Avvocata, Thach Nguyen}},
  year = {2018}},
  journal = {Therapeutic Advances in Cardiovascular Disease}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Background The aim of this study was to evaluate the rheolytic effects of stenting a mid-shaft/distal left main coronary artery (LMCA) lesion with and without ostial coverage. Stenting of the LMCA has emerged as a valid alternative in place of traditional ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1753944718765734},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1824,
  title = {An interview with Professor Raj Reddy on Web Intelligence (WI) and Computational Social Science (CSS)}},
  author = {Ning Zhong, Jiming Liu, Yong Shi, Yiyu Yao}},
  year = {2018}},
  journal = {Web Intelligence}},
  tipo = {Other}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/WEB-180388},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1825,
  title = {Automating board-game based learning. A comprehensive study to assess reliability and accuracy of AI in game evaluation}},
  author = {Andrea Tinterri, Federica Pelizzari, Marilena di Padova, Francesco Palladino, Giordano Vignoli, Anna Dipace}},
  year = {2024}},
  journal = {Intelligenza Artificiale}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Game-Based Learning (GBL) and its subset, Board Game-Based Learning (bGBL), are dynamic pedagogical approaches leveraging the immersive power of games to enrich the learning experience. bGBL is distinguished by its tactile and social dimensions, fostering ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/IA-240030},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1826,
  title = {Meso–macro-scale computational analysis of boron nitride nanotube-reinforced aluminium and epoxy nanocomposites: A case study on crack propagation}},
  author = {Srikant Padmanabhan, Ankit Gupta, Gaurav Arora, Himanshu Pathak, Ramesh G Burela, Anant S Bhatnagar}},
  year = {2020}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part L: Journal of Materials: Design and Applications}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In this paper, an outline of mean-field homogenization and fracture toughness of boron nitride nanotube-reinforced aluminium and epoxy composites have been presented. The meso-scale material modelling has been achieved using Mori-Tanaka and double ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1464420720961426},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1827,
  title = {Reviews: Handbook of Clinical and Experimental Neuropsychology, High-Level Motion Processing: Computational, Neurobiological, and Psychophysical Perspectives}},
  author = {E Forde, Adriane E Seiffert}},
  year = {1999}},
  journal = {Perception}},
  tipo = {Book review}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.1068/p2808rvw},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1828,
  title = {Language Views for Scientific Sensemaking Matter: A Synthesis of Research on Multilingual Students’ Experiences with Science Practices Through a Translanguaging Lens}},
  author = {María González-Howard, Sage Andersen, Karina Méndez Pérez, Enrique Suárez}},
  year = {2023}},
  journal = {Educational Researcher}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This synthesis examines recent science education research on multilingual students’ experiences with language-rich science practices. Adopting a translanguaging lens, we explore how researchers’ language conceptualizations impact the science practices ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3102/0013189X231206172},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1829,
  title = {Mock circulatory loops used for testing cardiac assist devices: A review of computational and experimental models}},
  author = {Femke Cappon, Tingting Wu, Theodore Papaioannou, Xinli Du, Po-Lin Hsu, Ashraf W Khir}},
  year = {2021}},
  journal = {The International Journal of Artificial Organs}},
  tipo = {Review article}},
  publisher = {Unknown}},
  abstract = {Heart failure is a major health risk, and with limited availability of donor organs, there is an increasing need for developing cardiac assist devices (CADs). Mock circulatory loops (MCL) are an important in-vitro test platform for CAD’s performance ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/03913988211045405},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1830,
  title = {Relationships among tasks, collaborative inquiry processes, inquiry resolutions, and knowledge outcomes in adolescents during guided discovery-based game design in school}},
  author = {Rebecca B. Reynolds}},
  year = {2016}},
  journal = {Journal of Information Science}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This research study investigates US middle school students’ collaborative information-seeking, sense-making and knowledge-building practices in a guided discovery-based programme of game design learning in which students and their teachers participate in ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0165551515614537},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1831,
  title = {The idea of the university as a heterotopia: The ethics and politics of thinking in the age of informational capitalism}},
  author = {Bregham Dalgliesh}},
  year = {2023}},
  journal = {Thesis Eleven}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Drawing on struggles within academe between faculty that promote critical education and advocates of New Public Management (NPM) who endorse instrumental learning, I reimagine the university as a counter-space that positions it as a counter-power to ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/07255136231169061},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1832,
  title = {Thermo-Electro-Mechanical Behavior of Ferroelectric Materials Part I: A Computational Micromechanical Model Versus Experimental Results}},
  author = {Lisa D. Mauck, Christopher S. Lynch}},
  year = {2003}},
  journal = {Journal of Intelligent Material Systems and Structures}},
  tipo = {Other}},
  publisher = {Unknown}},
  abstract = {A micro-electro-mechanical model of the behavior of piezoelectric ceramics including thermal effects is presented and compared to experimental data. Results include analytical and numerical investigations of the behavior of piezoelectric ceramics. The ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/104538903038023},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1833,
  title = {Redshift or Adduct Stabilization—A Computational Study of Hydrogen Bonding in Adducts of Protonated Carboxylic Acids}},
  author = {Solveig Gaarn Olesen, Steen Hammerum}},
  year = {2009}},
  journal = {European Journal of Mass Spectrometry}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {It is generally expected that the hydrogen bond strength in a D–H•••A adduct is predicted by the difference between the proton affinities (ΔPA) of D and A, measured by the adduct stabilization, and demonstrated by the infrared (IR) redshift of the D–H ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1255/ejms.970},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1834,
  title = {The Importance of Thinking Multivariately When Setting Subscale Cutoff Scores}},
  author = {Edward Kroc, Oscar L. Olvera Astivia}},
  year = {2021}},
  journal = {Educational and Psychological Measurement}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Setting cutoff scores is one of the most common practices when using scales to aid in classification purposes. This process is usually done univariately where each optimal cutoff value is decided sequentially, subscale by subscale. While it is widely ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/00131644211023569},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1835,
  title = {20 years of quantitative geographical thinking}},
  author = {Michael Batty}},
  year = {2016}},
  journal = {Environment and Planning B: Planning and Design}},
  tipo = {Editorial}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0265813516655408},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1836,
  title = {A Prejudice for the Thinking Classes: Media Exposure, Political Sophistication, and the Anti-Christian Fundamentalist}},
  author = {Louis Bolce, Gerald De Maio}},
  year = {2007}},
  journal = {American Politics Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Research on attitudes toward Christian fundamentalists shows that antagonism toward this group has become a significant factor since the early 1990s in structuring candidate preferences and issue positions. This article explores how information conveyed ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1532673X07309601},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1837,
  title = {Thinking Aloud: Analysing Students' Mathematics Performance}},
  author = {Michael J. Lawson, Donald N. Rice}},
  year = {1987}},
  journal = {School Psychology International}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Psychologists, teachers and researchers have a common interest in understanding how students solve mathematics problems. We want, and need, to understand how solutions to problems are developed so that interactions with both successful and unsuccessful ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/014303438700800404},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1838,
  title = {Interrelationships Among Several Variables Reflecting Quantitative Thinking in Elementary School Children with Particular Emphasis Upon Those Measures Involving Metric and Decimal Skills}},
  author = {Delon Selman, Patti Sell, William B. Michael, John Martois}},
  year = {1976}},
  journal = {Educational and Psychological Measurement}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Intercorrelations among several measures reflecting quantitative thinking tasks were found for subsamples of pupils at each of the first five grade levels in elementary schools in a middle-class suburban community in the greater Los Angeles area. One set ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/001316447603600427},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1839,
  title = {Computational model of an infant brain subjected to periodic motion simplified modelling and bayesian sensitivity analysis}},
  author = {D C Batterbee, N D Sims, W Becker, K Worden, J Rowson}},
  year = {2011}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Non-accidental head injury in infants, or shaken baby syndrome, is a highly controversial and disputed topic. Biomechanical studies often suggest that shaking alone cannot cause the classical symptoms, yet many medical experts believe the contrary. ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0954411911420002},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1840,
  title = {Redefining Filter Bubbles as (Escapable) Socio-Technical Recursion}},
  author = {Huw C Davies}},
  year = {2018}},
  journal = {Sociological Research Online}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Personalisation of media content is not a new phenomenon. Now, however, by configuring our search results and data feeds, algorithms that ‘learn’ from our digital footprint are determining what we see and hear. Pariser calls this the ‘Filter Bubble Effect’...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1360780418763824},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1841,
  title = {The Paradox of “Acting Globally While Thinking Locally”: Discordance in Climate Change Adaption Policy}},
  author = {Daniel A. Mazmanian, John Jurewitz, Hal T. Nelson}},
  year = {2013}},
  journal = {The Journal of Environment & Development}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The paradox motivating this article is why California has acted globally by enacting a comprehensive mitigation policy to reduce the emissions of Greenhouse gases, a true public good since the benefits will be shared across the planet, but has not ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1070496512471947},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1842,
  title = {A Computational Model for The Access to Medical Service in a Basic Prototype of a Healthcare System}},
  author = {Luigia Petre, Usman Sanwal, Gohar Shah, Charmi Panchal, Dwitiya Tyagi, Ion Petre, [...], View all}},
  year = {2019}},
  journal = {Fundamenta Informaticae}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {How robust is a healthcare system? How does a patient navigate the system and what is the cost (e.g., number of medical services required or number of times the medical provider had to be changed to get access to the required medical services) incurred ...}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/FI-2020-1886},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1843,
  title = {Modeling of a dry low nitrogen oxides burner using a three-dimensional computational fluid dynamics simulation}},
  author = {Guodong Sun, Xuejing Duan, Bo Hao, Afshin Davarpanah}},
  year = {2021}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part E}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Nitrogen oxides are considered as one of the greenhouse gases. Among the most significant emission sources for this gas is a natural gas-fired power plant. The United Nations General assembly suggested in 1988 that human activities can negatively impact ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/09544089211035595},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1844,
  title = {Incorporation of shear thickening fluid effects into computational modelling of woven fabrics subjected to impact loading: A review}},
  author = {Dakshitha Weerasinghe, Damith Mohotti, Jeremy Anderson}},
  year = {2019}},
  journal = {International Journal of Protective Structures}},
  tipo = {Review article}},
  publisher = {Unknown}},
  abstract = {Soft armour consisting of multi-layered high-performance fabrics are a popular choice for personal protection. Extensive work done in the last few decades suggests that shear thickening fluids improve the impact resistance of woven fabrics. Shear ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/2041419619889071},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1845,
  title = {Hydrated Copper Ion Chemistry: Guided Ion Beam and Computational Investigation of Cu2+(H2O)n (n = 7–10) Complexes}},
  author = {Andrew F. Sweeney, P.B. Armentrout}},
  year = {2015}},
  journal = {European Journal of Mass Spectrometry}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Cross sections for the threshold collision-induced dissociation of Cu2+(H2O)n, where n = 8–10, are measured using a guided ion beam tandem mass spectrometer. The primary dissociation pathway is found to be loss of a single water molecule followed by the ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1255/ejms.1334},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1846,
  title = {Book Reviews : Fuzzy Thinking: The New Science of Fuzzy Logic Burt Kosko Publisher: Hyperion Year of Publication: 1993 Length: 288 pages Price: $24.95}},
  author = {},
  year = {1994}},
  journal = {Social Science Computer Review}},
  tipo = {Review article}},
  publisher = {Unknown}},
  abstract = {Burt Kosko is a bright, impetuous, and intense scientist who displays all of these traits in his newest book: Fuzzy Thinking: The New Science of Fuzzy Logic. Relying on his diverse background in math, engineering, music, and philosophy Kosko sets out to ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/089443939401200319},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1847,
  title = {A PDP Approach to Localized Fractal Dimension Computation with Segmentation Boundaries}},
  author = {George W. Rogers, Jeffrey L. Solka, Carey E. Priebe}},
  year = {1995}},
  journal = {SIMULATION}},
  tipo = {Other}},
  publisher = {Unknown}},
  abstract = {A parallel distributed processing approach to the computation of localized fractal dimension values in imagery is pre sented. This approach is a further development of the covering method which requires only nearest neighbor commu nication. A major ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/003754979506500104},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1848,
  title = {Computational Fluid Dynamic study on the effect of near gravity material on dense medium cyclone treating coal using Discrete Phase Model and Algebraic Slip mixture multiphase model}},
  author = {Veera AK Aketi, TR Vakamalla, M Narasimha, GE Sreedhar, R Shivakumar, RajanKumar}},
  year = {2016}},
  journal = {The Journal of Computational Multiphase Flows}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In this paper, the effect of near gravity material at desired separation density during the coal washing is studied. It is believed that the Dense Medium Separation of coal particles in the presence of high percentage of near gravity material, results in ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1757482X16677755},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1849,
  title = {Modeling the valve dynamics in a reciprocating compressor based on two-dimensional computational fluid dynamic numerical simulation}},
  author = {Yu Wang, Jianmei Feng, Bo Zhang, Xueyuan Peng}},
  year = {2012}},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part E: Journal of Process Mechanical Engineering}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This article presents a numerical simulation of the thermodynamic process in the cylinder and dynamics of the self-acting valves for an air reciprocating compressor. The finite-volume method was employed to solve the compressible turbulent flow in the ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0954408912465366},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1850,
  title = {Sociology and the Virtual: Interactive Mirrors, Representational Thinking and Intensive Power}},
  author = {Rebecca Coleman}},
  year = {2013}},
  journal = {The Sociological Review}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This article explores the role of images in the workings of contemporary power. It examines one of the central ways in which sociology has approached images as representations and proposes an alternative understanding of images through the concepts of ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1111/1467-954X.12002},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1851,
  title = {How to rate the vehicle cockpit seat usage experience? A computational method based on embodied cognition theory}},
  author = {Mengya Zhu, Cong Sun, Min Zhao, Jingluan Wang, Dengkai Chen}},
  year = {2025}},
  journal = {WORK}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Background Vehicle cockpit seats are an important part of a car's interior. A superior vehicle cockpit seat usage experience plays a vital role in enhancing consumers’ driving experience. Objective The objective of this study is to identify the factors ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/10519815251320272},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1852,
  title = {Evaluation of Contemporary Computational Techniques to Optimize Adsorption Process for Simultaneous Removal of COD and TOC in Wastewater}},
  author = {Areej Alhothali, Hifsa Khurshid, Muhammad Raza Ul Mustafa, Kawthar Mostafa Moria, [...], View all}},
  year = {2022}},
  journal = {Adsorption Science & Technology}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This study was aimed at evaluating the artificial neural network (ANN), genetic algorithm (GA), adaptive neurofuzzy interference (ANFIS), and the response surface methodology (RSM) approaches for modeling and optimizing the simultaneous adsorptive removal ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1155/2022/7874826},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1853,
  title = {Reviews: Vision: A Computational Investigation into the Human Representation and Processing of Visual Information, Hearing Research and Theory, Volume 1, the Perception of Odors}},
  author = {R Watt, G Pick, R Harper}},
  year = {1983}},
  journal = {Perception}},
  tipo = {Book review}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.1068/p120089},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1854,
  title = {Computational technique of thermal comparative examination of Cu and Au nanoparticles suspended in sodium alginate as Sutterby nanofluid via extending PTSC surface}},
  author = {Wasim Jamshed, Rabia Safdar, Zulfiqar Rehman, Maha M. A. Lashin, Mohamed Ehab, [...], View all}},
  year = {2022}},
  journal = {Journal of Applied Biomaterials & Functional Materials}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Current research underscores entropy investigation in an infiltrating mode of Sutterby nanofluid (SNF) stream past a dramatically expanding flat plate that highlights Parabolic Trough Solar Collector (PTSC). Satisfactory likeness factors are utilized to ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/22808000221104004},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1855,
  title = {A novel methodology for surveying children for designing library services: A case study of the Municipal Library of Prague}},
  author = {Jan Stejskal, Petr Hajek, Pavel Cerny}},
  year = {2020}},
  journal = {Journal of Librarianship and Information Science}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Recognizing individual needs and customer preferences is key to succeeding and increasing competitiveness in both the commercial and public sectors. In the public sector, this is one of the ways to increase the efficiency of public funds allocated to ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0961000620948568},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1856,
  title = {Computational Galerkin Finite Element Method for Thermal Hydrogen Energy Utilization of First Grade Viscoelastic Hybrid Nanofluid Flowing Inside PTSC in Solar Powered Ship Applications}},
  author = {Fatimah S Bayones, Wasim Jamshed, SH Elhag, Mohamed Rabea Eid}},
  year = {2022}},
  journal = {Energy & Environment}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Parabolic trough solar collectors (PTSCs) are commonly used in solar thermal implementations to achieve high-temperatures. The current investigation looks at entropy formation and the effect of nano solid particles on a parabolic trough surface collector (...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0958305X221081463},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1857,
  title = {Research news and Comment: Parents’ Thinking About Standardized Tests and Performance Assessments}},
  author = {Lorrie A. Shepard, Carribeth L. Bliem}},
  year = {1995}},
  journal = {Educational Researcher}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.3102/0013189X024008025},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1858,
  title = {Discipline and promote: Building infrastructure and managing algorithms in a “structured journalism” project by professional fact-checking groups}},
  author = {Lucas Graves, CW Anderson}},
  year = {2020}},
  journal = {New Media & Society}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {News organizations have adapted in various ways to a digital media environment dominated by algorithmic gatekeepers such as search engines and social networks. This article dissects a campaign to actively shape that environment led by professional fact-...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1461444819856916},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1859,
  title = {Creative Thinking Abilities of Intellectually Superior Children in the Regular Grades}},
  author = {Robert M. Smith, John T. Neisworth}},
  year = {1966}},
  journal = {Psychological Reports}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This study was designed to determine on which of 18 creativity factors a group of 60 intellectually bright students in regular fifth grade classes would differ from a group of intellectually normal Ss (matched on sex, race, socio-economic position, school,...}},
  url = {https://journals.sagepub.com/doi/abs/10.2466/pr0.1966.18.2.335},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1860,
  title = {Review: Visual Agnosia, Thinking and Seeing: Visual Metacognition in Adults and Children}},
  author = {Paolo Bartolomeo, Bruce Hood}},
  year = {2005}},
  journal = {Perception}},
  tipo = {Book review}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.1068/p3407rvw},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1861,
  title = {Don’t panic: Bringing complexity thinking to UK Government evaluation guidance}},
  author = {Martha Bicket, Dione Hills, Helen Wilkinson, Alexandra Penn}},
  year = {2021}},
  journal = {Evaluation}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Central government guidance seeks to ensure and enhance the quality of practice and decision-making across – and sometimes beyond – government. The Magenta Book, published by HM Treasury, is the key UK Government resource on policy evaluation, setting out ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1356389020980479},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1862,
  title = {Learning: Association or Computation? Introduction to a Special Section}},
  author = {Alan M. Leslie}},
  year = {2001}},
  journal = {Current Directions in Psychological Science}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The fundamental nature of learning is a central problem in psychology. Traditionally, psychologists have assumed that learning must involve the formation of associations. Early last century, the pioneering work of Pavlov on conditioned learning in animals ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1111/1467-8721.00131},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1863,
  title = {Hot topics in Evolutionary Computation}},
  author = {Leonardo Vanneschi, Luca Mussi, Stefano Cagnoni}},
  year = {2011}},
  journal = {Intelligenza Artificiale}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {We introduce the special issue on Evolutionary Computation (EC) reporting a non-exhaustive list of topics which have recently attracted much interest from the EC community, with particular regard to the ones dealt with by the papers included in this issue:...}},
  url = {https://journals.sagepub.com/doi/abs/10.3233/IA-2011-0001},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1864,
  title = {Combine Statistical Thinking With Open Scientific Practice: A Protocol of a Bayesian Research Project}},
  author = {Alexandra Sarafoglou, Anna van der Heijden, Tim Draws, Joran Cornelisse, [...], View all}},
  year = {2022}},
  journal = {Psychology Learning & Teaching}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Current developments in the statistics community suggest that modern statistics education should be structured holistically, that is, by allowing students to work with real data and to answer concrete statistical questions, but also by educating them ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/14757257221077307},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1865,
  title = {Not only data: The necessity of abstract thinking to make sense of digital phenomena}},
  author = {Varun Grover, Kalle Lyytinen}},
  year = {2023}},
  journal = {Journal of Information Technology}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/02683962231156622},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1866,
  title = {New Empiricisms in the Anthropocene: Thinking With Speculative Fiction About Science and Social Inquiry}},
  author = {Elizabeth de Freitas, Sarah E. Truman}},
  year = {2020}},
  journal = {Qualitative Inquiry}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Interest in new empiricisms and transdisciplinary methods has led many social inquirers to engage with 20th-century post-classical physical science. Many of these projects have focused on alternative matter–mind mixtures and in/organic variation, ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1077800420943643},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1867,
  title = {Review: The Architecture of the City, a Scientific Autobiography, Arcades: The History of a Building Type, Computer Graphics and Environmental Planning, the Fractal Geometry of Nature, Professionals and Urban Form, Vision: A Computational Investigation into the Human Representation and Processing of Visual Information}},
  author = {D Kunze, D Hawkes, M Batty, U I Flemming, I Cooper, A Mehta}},
  year = {1983}},
  journal = {Environment and Planning B: Planning and Design}},
  tipo = {Book review}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.1068/b100357},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1868,
  title = {Real design practice, real design computation}},
  author = {Zichu Will Wang}},
  year = {2020}},
  journal = {International Journal of Architectural Computing}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The business model of architecture has been accused of being technologically deficient. Through case studies, this paper investigates alternatives to the status quo of computation in architecture, with a focus on design-oriented service providers. It ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1478077120958165},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1869,
  title = {Scientific Discovery: Computational Explorations of the Creative Process, Gary L. Bradshaw, Pat Langley, Herbert A. Simon and Jan M. Zytkow. 1987. The MIT Press, Cambridge, MA. 340 pages. Name and Subject Index. ISBN: 0-262-12116-6. $9.95 paperback}},
  author = {},
  year = {1988}},
  journal = {Bulletin of Science, Technology & Society}},
  tipo = {Other}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/027046768800800417},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1870,
  title = {Climate change contrarian think tanks in Europe: A network analysis}},
  author = {Núria Almiron, Jose A. Moreno, Justin Farrell}},
  year = {2022}},
  journal = {Public Understanding of Science}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Drawing from network theory and previous findings from US-based analyses, we measure the structure and interconnectedness of climate contrarian think tanks in Europe. This exploratory analysis can illustrate European organizations’ capacity to promote or ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/09636625221137815},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1871,
  title = {From Computer to Compost: Rethinking Our Metaphors for Memory}},
  author = {William L. Randall}},
  year = {2007}},
  journal = {Theory & Psychology}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This paper introduces the compost heap as a metaphor for autobiographical memory. As an alternative to the computer, such a metaphor, it is argued, comes closer to capturing the dynamics of memory across the lifespan and how it feels to us as we age, ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0959354307081619},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1872,
  title = {Thought Beyond Language: Neural Dissociation of Algebra and Natural Language}},
  author = {Martin M. Monti, Lawrence M. Parsons, Daniel N. Osherson}},
  year = {2012}},
  journal = {Psychological Science}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A central question in cognitive science is whether natural language provides combinatorial operations that are essential to diverse domains of thought. In the study reported here, we addressed this issue by examining the role of linguistic mechanisms in ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0956797612437427},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1873,
  title = {Re-Thinking the Concept of Money: Investor's Knowledge about Cryptocurrency as an Investment Alternative and Mode of Payment}},
  author = {Poojha S, Siva Hari Krishnan R, Priyanka Khanzode}},
  year = {2024}},
  journal = {Abhigyan}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.56401/Abhigyan_41.4.2023.2-14},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1874,
  title = {Thought self-leadership and effectiveness in self-management teams}},
  author = {Pedro Marques Quinteiro, Ana Passos, Luís Curral}},
  year = {2014}},
  journal = {Leadership}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {This study empirically examines the multilevel nature of thought self-leadership at work. Furthermore, this study tests the relationship between team level thought self-leadership and team effectiveness (i.e. performance and viability) through collective ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1742715014543579},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1875,
  title = {Avoidant Automatic Thoughts Are Associated With Task Avoidance and Inattention in the Moment: Replication in a Community Sample}},
  author = {Laura E. Knouse, Yueyi Fan, Aditya Narayanan, William D. Ellison}},
  year = {2025}},
  journal = {Journal of Attention Disorders}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Purpose: Avoidant automatic thoughts (AAT) are thoughts that precede or accompany a delay in the starting or ending of a task. In a prior study of college students using Ecological Momentary Assessment (EMA), AATs were frequent daily occurrences and ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/10870547251314924},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1876,
  title = {A benchmark of Spanish language datasets for computationally driven research}},
  author = {Gustavo Candela, María-Dolores Sáez, Pilar Escobar, Manuel Marco-Such}},
  year = {2021}},
  journal = {Journal of Information Science}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {In the domain of Galleries, Libraries, Archives and Museums (GLAM) institutions, creative and innovative tools and methodologies for content delivery and user engagement have recently gained international attention. New methods have been proposed to ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/01655515211060530},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1877,
  title = {Computation of the self-noise of a controlled-diffusion airfoil based on the acoustic analogy}},
  author = {P Martínez-Lera, J Christophe, C Schram}},
  year = {2017}},
  journal = {International Journal of Aeroacoustics}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The self-noise of a controlled-diffusion airfoil is computed with several numerical techniques based on the acoustic analogy and involving different degrees of approximation. The flow solution is obtained through an incompressible large eddy simulation. ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1475472X16680447},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1878,
  title = {Computation of Density for A Linear Combination of –Generalized Normal Random Variables}},
  author = {Ru-Ying Lee, I. R. Goodman}},
  year = {1975}},
  journal = {Calcutta Statistical Association Bulletin}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {A computational procedure is presented for the approximation of the density of a linear combination of univariate -generalized normal random variables. (The -generalized normal random variable generalizes the ordinary normal one by replacing the power two ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0008068319750110},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1879,
  title = {The Energy Use Associated with Neural Computation in the Cerebellum}},
  author = {Clare Howarth, Claire M Peppiatt-Wildman, David Attwell}},
  year = {2009}},
  journal = {Journal of Cerebral Blood Flow & Metabolism}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The brain's energy supply determines its information processing power, and generates functional imaging signals, which are often assumed to reflect principal neuron spiking. Using measured cellular properties, we analysed how energy expenditure relates to ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1038/jcbfm.2009.231},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1880,
  title = {Microsimulation Model Calibration with Approximate Bayesian Computation in R: A Tutorial}},
  author = {Peter Shewmaker, Stavroula A. Chrysanthopoulou, Rowan Iskandar, Derek Lake, Earic Jutkowitz}},
  year = {2022}},
  journal = {Medical Decision Making}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Mathematical health policy models, including microsimulation models (MSMs), are widely used to simulate complex processes and predict outcomes consistent with available data. Calibration is a method to estimate parameter values such that model predictions ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0272989X221085569},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1881,
  title = {Thinking Out-of-the-Box: A Non-Standard Application of Standard Pulse-Oximetry and Standard Near-Infrared Spectroscopy in a COVID-19 Patient}},
  author = {Patrick Schober, Erik J. Lust, Leo M. A. Heunks, Lothar A. Schwarte}},
  year = {2020}},
  journal = {Journal of Intensive Care Medicine}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Purpose: Purpose of this report is to describe the feasibility of lingual pulse oximetry and lingual near-infrared spectroscopy (NIRS) in a COVID-19 patient to assess lingual tissue viability after several days of mechanical ventilation in the prone ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0885066620965167},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1882,
  title = {Modeling Different Variables in Learning Basic Concepts of Programming in Flipped Classrooms}},
  author = {Hatice Yildiz Durak}},
  year = {2019}},
  journal = {Journal of Educational Computing Research}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Learning the basic concepts of programming and its foundations is considered as a challenging task for students to figure out. It is a challenging process for lecturers to learn these concepts, as well. The current literature on programming training ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/0735633119827956},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1883,
  title = {Computation Error Analysis: Students With Mathematics Difficulty Compared To Typically Achieving Students}},
  author = {Gena Nelson PhD, Sarah R. Powell PhD}},
  year = {2017}},
  journal = {Assessment for Effective Intervention}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Though proficiency with computation is highly emphasized in national mathematics standards, students with mathematics difficulty (MD) continue to struggle with computation. To learn more about the differences in computation error patterns between ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1534508417745627},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1884,
  title = {Time to Teach: Instructional Time and Science Teachers’ Use of Inquiry-Oriented Instructional Practices}},
  author = {Tammy Kolbe, Caitlin Steele, Beth White}},
  year = {2020}},
  journal = {Teachers College Record}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {Background There have been repeated calls for more widespread use of inquiry-oriented science instruction in K–12 education. At the same time, questions have been raised regarding whether the amount of time in school schedules for science instruction is ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/016146812012201211},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1885,
  title = {The Role of Surface Text Processing in Centrality Deficit and Poor Text Comprehension of Adolescents with Attention Deficit Hyperactivity Disorder: A Think–Aloud Study}},
  author = {Menahem Yeari, Anat Lavie}},
  year = {2021}},
  journal = {Learning Disabilities Research & Practice}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The present study employed a think–aloud method to explore the origin of centrality deficit (i.e., poor recall of central ideas) in individuals with Attention Deficit Hyperactivity Disorder (ADHD). Moreover, utilizing the diverse think–aloud responses, we ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1111/ldrp.12237},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1886,
  title = {Ironic Effects of Thought Suppression: A Meta-Analysis}},
  author = {Deming (Adam) Wang, Martin S. Hagger, Nikos L. D. Chatzisarantis}},
  year = {2020}},
  journal = {Perspectives on Psychological Science}},
  tipo = {Research article}},
  publisher = {Unknown}},
  abstract = {The ironic effect of thought suppression refers to the phenomenon in which individuals trying to rid their mind of a target thought ironically experience greater levels of occurrence and accessibility of the thought compared with individuals who ...}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1745691619898795},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

@article{ref_dup1887,
  title = {Computers and Learning: Helping Children Acquire Thinking Skills, Geoffrey Underwood & Jean Underwood. 1990. Basil Blackwell, Cambridge, MA. 209 pages. ISBN: 0-631-15807-3 (hc); 0-631-15808-1 (pb). $42.95 (hc); $14.95 (pb}},
  author = {},
  year = {1992}},
  journal = {Bulletin of Science, Technology & Society}},
  tipo = {Other}},
  publisher = {Unknown}},
  abstract = {Unknown}},
  url = {https://journals.sagepub.com/doi/abs/10.1177/027046769201200262},
  shared_files = {Data/resultados_Sage.bib, Data/resultados_Sage.bib}
}

